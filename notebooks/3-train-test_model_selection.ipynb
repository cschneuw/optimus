{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from itertools import product\n",
    "import warnings\n",
    "\n",
    "# System path modification\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Lasso, LassoCV, MultiTaskLasso, MultiTaskLassoCV,\n",
    "    ElasticNet, ElasticNetCV, MultiTaskElasticNet, MultiTaskElasticNetCV\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Custom modules\n",
    "from src.train import *\n",
    "from src.functions import *\n",
    "from src.plots import *\n",
    "from src.dataset import *\n",
    "from src.multixgboost import *\n",
    "from src.wrapper import *\n",
    "\n",
    "# Visualizatiokn \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning and machine learning specific \n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "\n",
    "from pytorch_tabular.models import (\n",
    "    GatedAdditiveTreeEnsembleConfig,\n",
    "    DANetConfig,\n",
    "    TabTransformerConfig,\n",
    "    FTTransformerConfig,\n",
    "    TabNetModelConfig,\n",
    ")\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Print CUDA availability for PyTorch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_pickle_data_palettes()\n",
    "\n",
    "results_pickle_folder = \"../pickle/\"\n",
    "\n",
    "# Unpack data\n",
    "df_X, df_y, df_all, df_FinalCombination = data[\"df_X\"], data[\"df_y\"], data[\"df_all\"], data[\"df_FinalCombination\"]\n",
    "dict_select = data[\"dict_select\"]\n",
    "\n",
    "# Unpack colormaps\n",
    "full_palette, gender_palette, dx_palette = data[\"colormaps\"].values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = list(df_X.isna().any(axis=1))\n",
    "idx_test = list(~df_X.isna().any(axis=1))\n",
    "\n",
    "set_intersect_rid = set(df_all[idx_train].RID).intersection(set(df_all[idx_test].RID))\n",
    "intersect_rid_idx = df_all.RID.isin(set_intersect_rid)\n",
    "\n",
    "for i, bool_test in enumerate(idx_test): \n",
    "    if intersect_rid_idx.iloc[i] & bool_test:\n",
    "        idx_test[i] = False\n",
    "        idx_train[i] = True\n",
    "        \n",
    "df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]] = df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]].astype(\"category\")\n",
    "\n",
    "df_X_train = df_X.loc[idx_train]\n",
    "df_X_test = df_X.loc[idx_test]\n",
    "\n",
    "df_y_train = df_y.loc[idx_train]\n",
    "df_y_test = df_y.loc[idx_test]\n",
    "\n",
    "c_train = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_train]\n",
    "c_test = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3609    128_S_2002\n",
       "5631    116_S_4167\n",
       "5662    033_S_4176\n",
       "5780    098_S_4215\n",
       "5950    018_S_4349\n",
       "6069    941_S_4292\n",
       "6077    116_S_4453\n",
       "6085    135_S_4489\n",
       "6224    033_S_4505\n",
       "6400    014_S_4576\n",
       "6429    073_S_4300\n",
       "7021    003_S_2374\n",
       "7192    033_S_4179\n",
       "Name: SubjectID, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.SubjectID.iloc[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all the models and combinations to try out with their hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: LinearRegression\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskElasticNet\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskElasticNet_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskLasso\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskLasso_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: RandomForestRegressor\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: XGBoostRegressor\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: XGBoostRegressor_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetRegressor_default\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetRegressor_custom\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: PLSRegression_4_components\n",
      "Combinations of preprocessing and models to test : 11\n"
     ]
    }
   ],
   "source": [
    "random_state=42\n",
    "n_imputation_iter = 10\n",
    "\n",
    "# Define hyperparameters\n",
    "gain_parameters = {\n",
    "    'hint_rate': 0.9,\n",
    "    'alpha': 100,\n",
    "    'iterations': 1000\n",
    "}\n",
    "\n",
    "# Continuous Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "continuous_imputer_list = [\n",
    "    (\"KNNImputer\", KNNImputer(n_neighbors=1)),\n",
    "]\n",
    "\n",
    "# Ordinal Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "ordinal_imputer_list = [\n",
    "    (\"SimpleImputer_most_frequent\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "]\n",
    "\n",
    "# Predictive Models List (list of tuples with unique strings and corresponding instances)\n",
    "predictive_models_list = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"MultiTaskElasticNet\", MultiTaskElasticNet()),\n",
    "    (\"MultiTaskElasticNet_tuned\", MultiTaskElasticNet(**{'alpha': 0.01, 'l1_ratio': 0.01})),\n",
    "    (\"MultiTaskLasso\", MultiTaskLasso()),\n",
    "    (\"MultiTaskLasso_tuned\", MultiTaskLasso(**{'alpha': 0.001})),\n",
    "    (\"RandomForestRegressor\", RandomForestRegressor()),\n",
    "    (\"XGBoostRegressor\", XGBoostRegressor()),\n",
    "    (\"XGBoostRegressor_tuned\", XGBoostRegressor(**{'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105})),\n",
    "    (\"TabNetRegressor_default\", TabNetModelWrapper(n_a=8, n_d=8)),\n",
    "    (\"TabNetRegressor_custom\", TabNetModelWrapper(n_a=32, n_d=32)),\n",
    "    (\"PLSRegression_4_components\", PLSRegression(n_components=4))\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "# Display all combinations\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    print(f\"Continuous Imputer: {continuous_imputer[0]}, Ordinal Imputer: {ordinal_imputer[0]}, Model: {model[0]}\")\n",
    "\n",
    "print(f\"Combinations of preprocessing and models to test : {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HDF5 file\n",
    "results_file = '../pickle/training_2_dict_results.pickle'\n",
    "\n",
    "if os.path.exists(results_file): \n",
    "\n",
    "    with open(results_file, \"rb\") as input_file:\n",
    "        all_dict_results = pickle.load(input_file)\n",
    "\n",
    "else : \n",
    "    all_dict_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'LinearRegression', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.1571178436279297, 'results_adj': {'mse_score': array([1.11192349, 0.67668626, 0.61999815, 0.97750844]), 'mae_score': array([0.72849831, 0.70235973, 0.70986192, 0.71215901]), 'r2': array([-0.12379655,  0.2321367 , -0.29452346, -0.18906157]), 'explained_variance': array([ 0.02475189,  0.28845771, -0.29319881,  0.10973059]), 'corr': array([0.33204585, 0.54765514, 0.01649677, 0.38058137])}, 'results_org': {'mse_score': array([1.11192349, 0.67668626, 0.61999817, 0.97750843]), 'mae_score': array([0.72849832, 0.70235973, 0.70986193, 0.712159  ]), 'r2': array([-0.18280006,  0.24192465, -0.20755938, -0.14289477]), 'explained_variance': array([-0.02645227,  0.29752774, -0.20632372,  0.14429641]), 'corr': array([0.27766053, 0.55012684, 0.07284141, 0.39622517])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskElasticNet', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.015189647674560547, 'results_adj': {'mse_score': array([1.18012247, 0.95041534, 0.50741149, 1.02939713]), 'mae_score': array([0.88414167, 0.78683572, 0.65378091, 0.82689555]), 'r2': array([-0.19272375, -0.07847477, -0.05944845, -0.25218005]), 'explained_variance': array([ 0.11229868,  0.03189858, -0.05332468,  0.12857136]), 'corr': array([ 0.42176386,  0.17864277, -0.16306808,  0.44631445])}, 'results_org': {'mse_score': array([1.18012248, 0.95041534, 0.50741151, 1.02939711]), 'mae_score': array([0.88414168, 0.78683573, 0.65378092, 0.82689554]), 'r2': array([-0.25534621, -0.06472745,  0.01172366, -0.20356258]), 'explained_variance': array([0.06569104, 0.04423898, 0.01743605, 0.16240568]), 'corr': array([0.25863755, 0.21083053, 0.14561964, 0.49623358])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskElasticNet_tuned', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.8243184089660645, 'results_adj': {'mse_score': array([1.09767924, 0.67163602, 0.61727141, 0.96027028]), 'mae_score': array([0.72712752, 0.70316248, 0.71357071, 0.70813001]), 'r2': array([-0.1094002 ,  0.23786741, -0.28883017, -0.16809271]), 'explained_variance': array([ 0.04259982,  0.29559122, -0.28782174,  0.12798901]), 'corr': array([0.33826662, 0.55120885, 0.00953623, 0.39231636])}, 'results_org': {'mse_score': array([1.09767923, 0.67163602, 0.61727143, 0.96027027]), 'mae_score': array([0.72712752, 0.70316248, 0.71357072, 0.70813   ]), 'r2': array([-0.16764784,  0.24758231, -0.20224855, -0.12274005]), 'explained_variance': array([-0.00766726,  0.30457033, -0.20130787,  0.16184592]), 'corr': array([0.28384559, 0.55450481, 0.06742918, 0.41077564])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskLasso', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.007662773132324219, 'results_adj': {'mse_score': array([1.39363767, 1.04056339, 0.48866937, 1.24000841]), 'mae_score': array([0.99127575, 0.78457157, 0.62110666, 0.93446712]), 'r2': array([-0.40851885, -0.18076941, -0.0203159 , -0.50837198]), 'explained_variance': array([-2.22044605e-16,  0.00000000e+00,  0.00000000e+00,  1.11022302e-16]), 'corr': array([nan, nan, nan, nan])}, 'results_org': {'mse_score': array([1.39363768, 1.04056339, 0.48866939, 1.24000838]), 'mae_score': array([0.99127576, 0.78457157, 0.62110667, 0.93446711]), 'r2': array([-0.48247136, -0.16571814,  0.04822734, -0.44980754]), 'explained_variance': array([-0.05250375,  0.01274701,  0.06717846,  0.03882626]), 'corr': array([-0.0848666 ,  0.11455326,  0.2721653 ,  0.20679355])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskLasso_tuned', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.8485329151153564, 'results_adj': {'mse_score': array([1.09986228, 0.67700338, 0.61721354, 0.96322248]), 'mae_score': array([0.72439764, 0.70502407, 0.71262079, 0.70634911]), 'r2': array([-0.11160655,  0.23177686, -0.28870934, -0.17168383]), 'explained_variance': array([ 0.04182356,  0.29179378, -0.28783433,  0.1250568 ]), 'corr': array([0.34030126, 0.54948447, 0.01369735, 0.39123818])}, 'results_org': {'mse_score': array([1.09986228, 0.67700337, 0.61721356, 0.96322247]), 'mae_score': array([0.72439764, 0.70502408, 0.71262081, 0.7063491 ]), 'r2': array([-0.16997004,  0.2415694 , -0.20213584, -0.12619174]), 'explained_variance': array([-0.00848428,  0.30082129, -0.20131962,  0.15902757]), 'corr': array([0.28608858, 0.55225247, 0.07079544, 0.40890542])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'RandomForestRegressor', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 32.46849751472473, 'results_adj': {'mse_score': array([1.00638713, 0.65012684, 0.47869162, 0.82220865]), 'mae_score': array([0.75528169, 0.6841987 , 0.63512616, 0.63151397]), 'r2': array([-1.71332791e-02,  2.62274753e-01,  5.17132936e-04, -1.51675905e-04]), 'explained_variance': array([0.29209145, 0.405549  , 0.02445608, 0.37078241]), 'corr': array([0.54046621, 0.64364297, 0.21686966, 0.60995505])}, 'results_org': {'mse_score': array([1.00638713, 0.65012684, 0.47869164, 0.82220864]), 'mae_score': array([0.7552817 , 0.6841987 , 0.63512617, 0.63151397]), 'r2': array([-0.07053657,  0.27167853,  0.06766084,  0.03868046]), 'explained_variance': array([0.25492361, 0.41312647, 0.08999161, 0.39521257]), 'corr': array([0.50511182, 0.65009756, 0.3086371 , 0.63522421])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'XGBoostRegressor', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 1.580946922302246, 'results_adj': {'mse_score': array([0.99729777, 0.55285888, 0.49806901, 0.7462314 ]), 'mae_score': array([0.75445592, 0.63451078, 0.64294552, 0.64429738]), 'r2': array([-0.00794687,  0.37264865, -0.03994184,  0.09226863]), 'explained_variance': array([ 0.26511088,  0.47604274, -0.03447538,  0.31643386]), 'corr': array([0.52174936, 0.69849083, 0.21172288, 0.56606   ])}, 'results_org': {'mse_score': array([0.99729777, 0.55285888, 0.49806903, 0.74623138]), 'mae_score': array([0.75445593, 0.63451079, 0.64294554, 0.64429738]), 'r2': array([-0.06086784,  0.38064549,  0.02991984,  0.12751245]), 'explained_variance': array([0.22652646, 0.48272162, 0.03501907, 0.34297417]), 'corr': array([0.48966434, 0.70257856, 0.27320828, 0.58583766])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'XGBoostRegressor_tuned', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 2.18463134765625, 'results_adj': {'mse_score': array([1.0153611 , 0.63851973, 0.46345103, 0.70749896]), 'mae_score': array([0.73457703, 0.71458275, 0.63021056, 0.61263546]), 'r2': array([-0.02620307,  0.27544581,  0.03233868,  0.13938357]), 'explained_variance': array([0.22165364, 0.34897739, 0.05887258, 0.43032201]), 'corr': array([0.48104477, 0.59075482, 0.30682341, 0.65598979])}, 'results_org': {'mse_score': array([1.0153611 , 0.63851973, 0.46345105, 0.70749894]), 'mae_score': array([0.73457704, 0.71458276, 0.63021057, 0.61263545]), 'r2': array([-0.08008256,  0.2846817 ,  0.09734466,  0.17279809]), 'explained_variance': array([0.18078756, 0.35727599, 0.12209605, 0.45244048]), 'corr': array([0.44678191, 0.59775758, 0.36862204, 0.67323253])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetRegressor_default', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 13.32374882698059, 'results_adj': {'mse_score': array([1.00024251, 0.85216095, 0.6231349 , 0.78732824]), 'mae_score': array([0.78064659, 0.8013238 , 0.6795552 , 0.67213814]), 'r2': array([-0.01092304,  0.03301847, -0.30107284,  0.04227758]), 'explained_variance': array([ 0.38275983,  0.13551578, -0.29939216,  0.5247155 ]), 'corr': array([0.62946614, 0.47112118, 0.06684408, 0.73037268])}, 'results_org': {'mse_score': array([1.0002425 , 0.85216095, 0.62313492, 0.78732822]), 'mae_score': array([0.78064659, 0.8013238 , 0.67955522, 0.67213815]), 'r2': array([-0.06400027,  0.04534457, -0.21366878,  0.07946235]), 'explained_variance': array([ 0.35035243,  0.14653537, -0.212101  ,  0.54316902]), 'corr': array([0.60852724, 0.4852577 , 0.12261516, 0.74095112])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetRegressor_custom', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 13.630439281463623, 'results_adj': {'mse_score': array([0.92610036, 0.76428738, 0.61165174, 0.87483944]), 'mae_score': array([0.71338154, 0.75841494, 0.67114785, 0.72399585]), 'r2': array([ 0.06401079,  0.13273216, -0.2770966 , -0.06417287]), 'explained_variance': array([ 0.22209961,  0.26101411, -0.27432143,  0.26805491]), 'corr': array([0.55329131, 0.58084529, 0.23700988, 0.59708229])}, 'results_org': {'mse_score': array([0.92610035, 0.76428739, 0.61165176, 0.87483943]), 'mae_score': array([0.71338154, 0.75841494, 0.67114786, 0.72399584]), 'r2': array([ 0.01486787,  0.14378722, -0.19130323, -0.02285503]), 'explained_variance': array([ 0.18125695,  0.27043396, -0.1887145 ,  0.2964736 ]), 'corr': array([0.53795846, 0.59013198, 0.27340583, 0.5986342 ])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'PLSRegression_4_components', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.060524702072143555, 'results_adj': {'mse_score': array([1.08140908, 0.78765117, 0.5694762 , 0.94630352]), 'mae_score': array([0.79248414, 0.79068015, 0.72178585, 0.73452485]), 'r2': array([-0.09295631,  0.10622033, -0.1890363 , -0.15110326]), 'explained_variance': array([ 0.1855152 ,  0.19584409, -0.18846787,  0.20844666]), 'corr': array([ 0.43286088,  0.44542341, -0.05356343,  0.45740887])}, 'results_org': {'mse_score': array([1.08140909, 0.78765116, 0.56947622, 0.9463035 ]), 'mae_score': array([0.79248415, 0.79068015, 0.72178586, 0.73452484]), 'r2': array([-0.1503406 ,  0.11761334, -0.10915869, -0.10641023]), 'explained_variance': array([ 0.14275171,  0.20609467, -0.10862844,  0.23917972]), 'corr': array([0.3812462 , 0.45398549, 0.03688166, 0.49549602])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'GatedAdditiveTreeEnsembleConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'DANetConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabTransformerConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetModelConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n"
     ]
    }
   ],
   "source": [
    "from src.debug import *\n",
    "all_dict_results = clean_dict_list(all_dict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'LinearRegression', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskElasticNet', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskElasticNet_tuned', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskLasso', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskLasso_tuned', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'RandomForestRegressor', (2881, 348), (13, 348)])\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'TabNetRegressor_default', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'TabNetRegressor_custom', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'PLSRegression_4_components', (2881, 348), (13, 348)])\n"
     ]
    }
   ],
   "source": [
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "\n",
    "    if any(result['params'] == params for result in all_dict_results):\n",
    "        # Skip this iteration if the combination exists\n",
    "        print(f\"Skipping existing combination: {params.values()}\")\n",
    "        \n",
    "        continue\n",
    "\n",
    "    try: \n",
    "    \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "        # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open(results_file, 'wb') as handle:\n",
    "    pickle.dump(all_dict_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickle/training_2_dict_results.pickle', \"rb\") as input_file:\n",
    "    dict_results_split = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'LinearRegression',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.2466392517089844,\n",
       "  'fitting_time': 0.22939062118530273,\n",
       "  'results_adj': {'mse_score': array([0.65983176, 0.48974311, 0.40951464, 0.56684282]),\n",
       "   'mae_score': array([0.63965764, 0.55144938, 0.56535002, 0.59999667]),\n",
       "   'r2': array([0.33312259, 0.44426866, 0.144955  , 0.31048062]),\n",
       "   'explained_variance': array([0.36730884, 0.47760179, 0.14545957, 0.43985495]),\n",
       "   'corr': array([0.60633336, 0.69220668, 0.4020632 , 0.66605104])},\n",
       "  'results_org': {'mse_score': array([0.65983176, 0.4897431 , 0.40951466, 0.56684279]),\n",
       "   'mae_score': array([0.63965764, 0.55144938, 0.56535003, 0.59999666]),\n",
       "   'r2': array([0.29810904, 0.45135258, 0.20239561, 0.33725209]),\n",
       "   'explained_variance': array([0.33409019, 0.48426081, 0.20286628, 0.46160331]),\n",
       "   'corr': array([0.57814397, 0.69816204, 0.46213839, 0.68191102])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskElasticNet',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.283444404602051,\n",
       "  'fitting_time': 0.09822368621826172,\n",
       "  'results_adj': {'mse_score': array([1.18012248, 0.95041535, 0.50741149, 1.02939714]),\n",
       "   'mae_score': array([0.88414168, 0.78683573, 0.65378091, 0.82689555]),\n",
       "   'r2': array([-0.19272377, -0.07847478, -0.05944846, -0.25218006]),\n",
       "   'explained_variance': array([ 0.11229868,  0.03189858, -0.05332468,  0.12857136]),\n",
       "   'corr': array([ 0.42176385,  0.17864276, -0.16306812,  0.44631446])},\n",
       "  'results_org': {'mse_score': array([1.18012249, 0.95041535, 0.50741151, 1.02939711]),\n",
       "   'mae_score': array([0.88414168, 0.78683573, 0.65378092, 0.82689554]),\n",
       "   'r2': array([-0.25534622, -0.06472746,  0.01172365, -0.20356259]),\n",
       "   'explained_variance': array([0.06569104, 0.04423898, 0.01743605, 0.16240568]),\n",
       "   'corr': array([0.25863755, 0.21083052, 0.14561963, 0.49623359])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskElasticNet_tuned',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.2664968967437744,\n",
       "  'fitting_time': 1.4926152229309082,\n",
       "  'results_adj': {'mse_score': array([0.66791342, 0.47856617, 0.41412878, 0.59264981]),\n",
       "   'mae_score': array([0.63043587, 0.55392437, 0.58019171, 0.59956285]),\n",
       "   'r2': array([0.32495464, 0.45695159, 0.13532092, 0.27908847]),\n",
       "   'explained_variance': array([0.3700506 , 0.49637797, 0.13598028, 0.44010284]),\n",
       "   'corr': array([0.60831821, 0.70457467, 0.38152442, 0.66946516])},\n",
       "  'results_org': {'mse_score': array([0.66791342, 0.47856617, 0.4141288 , 0.59264978]),\n",
       "   'mae_score': array([0.63043586, 0.55392437, 0.58019172, 0.59956284]),\n",
       "   'r2': array([0.28951224, 0.46387383, 0.19340873, 0.30707878]),\n",
       "   'explained_variance': array([0.33697591, 0.50279765, 0.19402379, 0.46184157]),\n",
       "   'corr': array([0.58049709, 0.70936568, 0.44690586, 0.68551862])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskLasso',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.283153772354126,\n",
       "  'fitting_time': 0.07776927947998047,\n",
       "  'results_adj': {'mse_score': array([1.39363767, 1.04056339, 0.48866937, 1.24000841]),\n",
       "   'mae_score': array([0.99127575, 0.78457157, 0.62110666, 0.93446712]),\n",
       "   'r2': array([-0.40851885, -0.18076941, -0.0203159 , -0.50837198]),\n",
       "   'explained_variance': array([-2.22044605e-16,  0.00000000e+00,  0.00000000e+00,  1.11022302e-16]),\n",
       "   'corr': array([nan, nan, nan, nan])},\n",
       "  'results_org': {'mse_score': array([1.39363768, 1.04056339, 0.48866939, 1.24000838]),\n",
       "   'mae_score': array([0.99127576, 0.78457157, 0.62110667, 0.93446711]),\n",
       "   'r2': array([-0.48247136, -0.16571814,  0.04822734, -0.44980754]),\n",
       "   'explained_variance': array([-0.05250375,  0.01274701,  0.06717846,  0.03882626]),\n",
       "   'corr': array([-0.0848666 ,  0.11455326,  0.2721653 ,  0.20679355])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskLasso_tuned',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.3114430904388428,\n",
       "  'fitting_time': 1.4922456741333008,\n",
       "  'results_adj': {'mse_score': array([0.6555347 , 0.47602153, 0.41898635, 0.58247095]),\n",
       "   'mae_score': array([0.62193632, 0.55058805, 0.57780272, 0.59499437]),\n",
       "   'r2': array([0.33746553, 0.45983909, 0.12517858, 0.29147024]),\n",
       "   'explained_variance': array([0.38303248, 0.49996522, 0.12605199, 0.44788336]),\n",
       "   'corr': array([0.61892446, 0.70715313, 0.37654024, 0.67491708])},\n",
       "  'results_org': {'mse_score': array([0.6555347 , 0.47602153, 0.41898636, 0.58247092]),\n",
       "   'mae_score': array([0.62193631, 0.55058805, 0.57780274, 0.59499436]),\n",
       "   'r2': array([0.30268   , 0.46672453, 0.18394773, 0.31897981]),\n",
       "   'explained_variance': array([0.35063939, 0.50633917, 0.18476246, 0.46932   ]),\n",
       "   'corr': array([0.59223356, 0.71197064, 0.44079738, 0.69050252])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'RandomForestRegressor',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.289330244064331,\n",
       "  'fitting_time': 51.79721212387085,\n",
       "  'results_adj': {'mse_score': array([0.81204962, 0.55738872, 0.42447239, 0.76265381]),\n",
       "   'mae_score': array([0.66916949, 0.63959102, 0.60835909, 0.64774379]),\n",
       "   'r2': array([0.17927936, 0.36750845, 0.11372402, 0.07229206]),\n",
       "   'explained_variance': array([0.38575368, 0.43668934, 0.11735442, 0.37009751]),\n",
       "   'corr': array([0.63471399, 0.67665811, 0.34287971, 0.62716925])},\n",
       "  'results_org': {'mse_score': array([0.81204962, 0.55738872, 0.42447241, 0.76265379]),\n",
       "   'mae_score': array([0.6691695 , 0.63959102, 0.6083591 , 0.64774379]),\n",
       "   'r2': array([0.13618846, 0.37557082, 0.17326266, 0.10831149]),\n",
       "   'explained_variance': array([0.35350346, 0.44386986, 0.17664918, 0.39455427]),\n",
       "   'corr': array([0.60552878, 0.68293217, 0.42214619, 0.66388455])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'PLSRegression_4_components',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.3004002571105957,\n",
       "  'fitting_time': 0.15016818046569824,\n",
       "  'results_adj': {'mse_score': array([0.99283749, 0.68320591, 0.51129344, 0.88042711]),\n",
       "   'mae_score': array([0.8049879 , 0.73842664, 0.68894711, 0.76406833]),\n",
       "   'r2': array([-0.00343896,  0.22473859, -0.06755375, -0.07096982]),\n",
       "   'explained_variance': array([ 0.25778331,  0.29950329, -0.06753554,  0.2736962 ]),\n",
       "   'corr': array([0.50966098, 0.55561882, 0.0742507 , 0.52674345])},\n",
       "  'results_org': {'mse_score': array([0.99283749, 0.68320591, 0.51129345, 0.88042708]),\n",
       "   'mae_score': array([0.80498791, 0.73842664, 0.68894712, 0.76406832]),\n",
       "   'r2': array([-0.05612325,  0.23462085,  0.00416286, -0.02938806]),\n",
       "   'explained_variance': array([0.21881416, 0.30843253, 0.00417985, 0.30189586]),\n",
       "   'corr': array([0.46922857, 0.57093376, 0.17716915, 0.57292837])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'TabNetRegressor_default',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.3759474754333496,\n",
       "  'fitting_time': 13.77210783958435,\n",
       "  'results_adj': {'mse_score': array([0.61824897, 0.61184618, 0.36866036, 0.57763236]),\n",
       "   'mae_score': array([0.69107406, 0.60773177, 0.5361999 , 0.59462053]),\n",
       "   'r2': array([0.3751494 , 0.30571336, 0.23025659, 0.297356  ]),\n",
       "   'explained_variance': array([0.56567234, 0.38915553, 0.23426406, 0.55584824]),\n",
       "   'corr': array([0.77300748, 0.67628222, 0.58077844, 0.81923406])},\n",
       "  'results_org': {'mse_score': array([0.61824897, 0.61184618, 0.36866037, 0.57763234]),\n",
       "   'mae_score': array([0.69107407, 0.60773177, 0.53619991, 0.59462053]),\n",
       "   'r2': array([0.34234241, 0.31456344, 0.28196676, 0.32463705]),\n",
       "   'explained_variance': array([0.54286851, 0.39694198, 0.28570502, 0.57309301]),\n",
       "   'corr': array([0.76293555, 0.67836772, 0.73113403, 0.85040194])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'TabNetRegressor_custom',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.4543991088867188,\n",
       "  'fitting_time': 13.39520812034607,\n",
       "  'results_adj': {'mse_score': array([1.07617402, 1.31252905, 0.77527955, 1.0442195 ]),\n",
       "   'mae_score': array([0.76465789, 0.96862147, 0.76452422, 0.76252167]),\n",
       "   'r2': array([-0.08766535, -0.48937986, -0.61874284, -0.27021028]),\n",
       "   'explained_variance': array([ 0.20827438, -0.34330108, -0.60463007,  0.30404738]),\n",
       "   'corr': array([ 0.54009087,  0.30700851, -0.06850123,  0.58642515])},\n",
       "  'results_org': {'mse_score': array([1.07617403, 1.31252906, 0.77527957, 1.04421947]),\n",
       "   'mae_score': array([0.76465789, 0.96862147, 0.76452423, 0.76252167]),\n",
       "   'r2': array([-0.14477185, -0.47039474, -0.5099982 , -0.22089277]),\n",
       "   'explained_variance': array([ 0.16670582, -0.32617802, -0.4968335 ,  0.33106863]),\n",
       "   'corr': array([0.50999118, 0.31151104, 0.0452715 , 0.59746408])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'XGBoostRegressor',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': None,\n",
       "  'results_adj': None,\n",
       "  'results_org': None},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'XGBoostRegressor_tuned',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': None,\n",
       "  'results_adj': None,\n",
       "  'results_org': None}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_results_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Pytorch Tabular models as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = ['APOE_epsilon2', 'APOE_epsilon3', 'APOE_epsilon4']\n",
    "continuous_features = [col for col in df_X_train.columns if col not in ordinal_features]\n",
    "\n",
    "# Prepare Tabular configurations (shared for all PyTorch models)\n",
    "data_config = DataConfig(\n",
    "    target=df_y_train.columns.tolist(),\n",
    "    continuous_cols=continuous_features,\n",
    "    categorical_cols=ordinal_features\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024, max_epochs=1, auto_lr_find=True,\n",
    "    early_stopping=\"valid_loss\", early_stopping_mode=\"min\", early_stopping_patience=5,\n",
    "    checkpoints=\"valid_loss\", load_best=True, progress_bar=\"nones\",\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "head_config = LinearHeadConfig(dropout=0.1).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'LinearRegression', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskElasticNet', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskElasticNet_tuned', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskLasso', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskLasso_tuned', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'RandomForestRegressor', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'XGBoostRegressor', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'XGBoostRegressor_tuned', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'TabNetRegressor_default', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'TabNetRegressor_custom', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'PLSRegression_4_components', (2881, 348), (13, 348)])\n",
      "GatedAdditiveTreeEnsembleConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:19:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">242</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:19:47\u001b[0m,\u001b[1;36m242\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:19:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">267</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:19:47\u001b[0m,\u001b[1;36m267\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:19:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">273</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:19:47\u001b[0m,\u001b[1;36m273\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:19:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">306</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:19:47\u001b[0m,\u001b[1;36m306\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:19:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">564</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:19:47\u001b[0m,\u001b[1;36m564\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:19:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">733</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:19:47\u001b[0m,\u001b[1;36m733\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:19:47</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">743</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:19:47\u001b[0m,\u001b[1;36m743\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX 6000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547fd6daec9f40938d1966764e57283f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_df83e326-5e12-4be6-98c6-05fcd8a4a59f.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_df83e326-5e12-4be6-98c6-05fcd8a4a59f.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:19:50</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">191</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:19:50\u001b[0m,\u001b[1;36m191\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:19:50</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">205</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:19:50\u001b[0m,\u001b[1;36m205\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:19:50</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">225</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:19:50\u001b[0m,\u001b[1;36m225\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                       | Params | Mode \n",
      "------------------------------------------------------------------------\n",
      "0 | _backbone        | GatedAdditiveTreesBackbone | 5.6 M  | train\n",
      "1 | _embedding_layer | Embedding1dLayer           | 714    | train\n",
      "2 | _head            | CustomHead                 | 156    | train\n",
      "3 | loss             | MSELoss                    | 0      | train\n",
      "------------------------------------------------------------------------\n",
      "5.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.6 M     Total params\n",
      "22.286    Total estimated model params size (MB)\n",
      "692       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5dbb51826b45e79d05709f76913148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd08a0dc81804e9f90081b3d1577f1f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b772885c014a6b96eb10592008f354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:19:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">515</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:19:52\u001b[0m,\u001b[1;36m515\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:19:52</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">516</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:19:52\u001b[0m,\u001b[1;36m516\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "DANetConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:00</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:00\u001b[0m,\u001b[1;36m669\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:00</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">683</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:00\u001b[0m,\u001b[1;36m683\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:00</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:00\u001b[0m,\u001b[1;36m689\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:00</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">721</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: DANetModel             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:00\u001b[0m,\u001b[1;36m721\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: DANetModel             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:00</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">794</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:00\u001b[0m,\u001b[1;36m794\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:00</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">801</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:00\u001b[0m,\u001b[1;36m801\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae83d69ec937493cb55564549d1ed1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_7689edd2-afc9-4094-813c-d03675ff9359.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_7689edd2-afc9-4094-813c-d03675ff9359.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:01</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">318</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:01\u001b[0m,\u001b[1;36m318\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:01</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">326</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:01\u001b[0m,\u001b[1;36m326\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | _backbone        | DANetBackbone    | 2.3 M  | train\n",
      "1 | _embedding_layer | Embedding1dLayer | 714    | train\n",
      "2 | _head            | LinearHead       | 260    | train\n",
      "3 | loss             | MSELoss          | 0      | train\n",
      "--------------------------------------------------------------\n",
      "2.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 M     Total params\n",
      "9.101     Total estimated model params size (MB)\n",
      "159       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c83a54187c445848d788753a945ec1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10833b94097747518fab80a94467f9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34eac71c84ed4edf800d2a01f7d4d234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:01</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">739</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:01\u001b[0m,\u001b[1;36m739\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:01</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">740</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:01\u001b[0m,\u001b[1;36m740\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "TabTransformerConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">279</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:09\u001b[0m,\u001b[1;36m279\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">294</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:09\u001b[0m,\u001b[1;36m294\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">302</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:09\u001b[0m,\u001b[1;36m302\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">336</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabTransformerModel    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:09\u001b[0m,\u001b[1;36m336\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabTransformerModel    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">394</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:09\u001b[0m,\u001b[1;36m394\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">402</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:09\u001b[0m,\u001b[1;36m402\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e763ff23119444c8d93d65e71966558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_169ca75e-9237-4b14-b824-0e762c907f36.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_169ca75e-9237-4b14-b824-0e762c907f36.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">584</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:09\u001b[0m,\u001b[1;36m584\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">586</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:09\u001b[0m,\u001b[1;36m586\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                   | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | _backbone        | TabTransformerBackbone | 271 K  | train\n",
      "1 | _embedding_layer | Embedding2dLayer       | 408    | train\n",
      "2 | _head            | LinearHead             | 1.8 K  | train\n",
      "3 | loss             | MSELoss                | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "274 K     Trainable params\n",
      "0         Non-trainable params\n",
      "274 K     Total params\n",
      "1.097     Total estimated model params size (MB)\n",
      "125       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d908ac9ecc4e4db8e9e3061d56cdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b232b110c8e34fb3beba8ee6594e1241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d3b054dcb147db8aa252272fafa94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">791</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:09\u001b[0m,\u001b[1;36m791\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">792</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:09\u001b[0m,\u001b[1;36m792\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "TabNetModelConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">097</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:17\u001b[0m,\u001b[1;36m097\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">111</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:17\u001b[0m,\u001b[1;36m111\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">118</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:17\u001b[0m,\u001b[1;36m118\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">153</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:17\u001b[0m,\u001b[1;36m153\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">988</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:17\u001b[0m,\u001b[1;36m988\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">996</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:17\u001b[0m,\u001b[1;36m996\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8116d42d5c540658483856b93303a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_39f71446-d1b3-4079-9553-67a326a9ef0b.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_39f71446-d1b3-4079-9553-67a326a9ef0b.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">234</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:18\u001b[0m,\u001b[1;36m234\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">236</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:18\u001b[0m,\u001b[1;36m236\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type           | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | _embedding_layer | Identity       | 0      | train\n",
      "1 | _backbone        | TabNetBackbone | 28.8 K | train\n",
      "2 | _head            | Identity       | 0      | train\n",
      "3 | loss             | MSELoss        | 0      | train\n",
      "------------------------------------------------------------\n",
      "28.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "28.8 K    Total params\n",
      "0.115     Total estimated model params size (MB)\n",
      "111       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5360542a731f4502a61d5ef7e86dcc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7eebab9e7b46d09c21c03e991e9f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1cfe0ed5424e90b1fa234da921f43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">484</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:18\u001b[0m,\u001b[1;36m484\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:20:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">485</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:20:18\u001b[0m,\u001b[1;36m485\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": [
    "predictive_models_list += [\n",
    "    (\"GatedAdditiveTreeEnsembleConfig_tab\", \n",
    "    TabularModelWrapper(\n",
    "        GatedAdditiveTreeEnsembleConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        gflu_stages=6,\n",
    "        gflu_dropout=0.0,\n",
    "        tree_depth=5,\n",
    "        num_trees=20,\n",
    "        chain_trees=False,\n",
    "        share_head_weights=True), data_config, trainer_config, optimizer_config \n",
    "    )),\n",
    "    (\"DANetConfig_tab\",\n",
    "    TabularModelWrapper(\n",
    "        DANetConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_layers=8,\n",
    "        k=5,\n",
    "        dropout_rate=0.1), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabTransformerConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        embedding_initialization=\"kaiming_uniform\",\n",
    "        embedding_bias=False), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    # (\"FTTransformerConfig\",\n",
    "    #     TabularModelWrapper(\n",
    "    #     FTTransformerConfig(\n",
    "    #     task=\"regression\",\n",
    "    #     head=\"LinearHead\",\n",
    "    #     head_config=head_config), data_config, trainer_config, optimizer_config\n",
    "    # )),\n",
    "    (\"TabNetModelConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        n_independent=2,\n",
    "        n_shared=2), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "\n",
    "    if any(result['params'] == params for result in all_dict_results):\n",
    "        # Skip this iteration if the combination exists\n",
    "        print(f\"Skipping existing combination: {params.values()}\")\n",
    "        \n",
    "        continue\n",
    "\n",
    "    try: \n",
    "        print(name_model)\n",
    "        \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "        # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models only on MRI features to compare performances\n",
    "\n",
    "## Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = list(df_X.isna().any(axis=1))\n",
    "idx_test = list(~df_X.isna().any(axis=1))\n",
    "\n",
    "set_intersect_rid = set(df_all[idx_train].RID).intersection(set(df_all[idx_test].RID))\n",
    "intersect_rid_idx = df_all.RID.isin(set_intersect_rid)\n",
    "\n",
    "for i, bool_test in enumerate(idx_test): \n",
    "    if intersect_rid_idx.iloc[i] & bool_test:\n",
    "        idx_test[i] = False\n",
    "        idx_train[i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]] = df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train = df_X[dict_select[\"MRIth\"]].loc[idx_train]\n",
    "df_X_test = df_X[dict_select[\"MRIth\"]].loc[idx_test]\n",
    "\n",
    "df_y_train = df_y.loc[idx_train]\n",
    "df_y_test = df_y.loc[idx_test]\n",
    "\n",
    "c_train = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_train]\n",
    "c_test = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: LinearRegression\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskElasticNet\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskElasticNet_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskLasso\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskLasso_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: RandomForestRegressor\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: XGBoostRegressor\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: XGBoostRegressor_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: TabNetRegressor_default\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: TabNetRegressor_custom\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: PLSRegression_4_components\n",
      "Combinations of preprocessing and models to test : 11\n"
     ]
    }
   ],
   "source": [
    "random_state=42\n",
    "n_imputation_iter = 10\n",
    "\n",
    "# Define hyperparameters\n",
    "gain_parameters = {\n",
    "    'hint_rate': 0.9,\n",
    "    'alpha': 100,\n",
    "    'iterations': 1000\n",
    "}\n",
    "\n",
    "# Continuous Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "continuous_imputer_list = [\n",
    "    (\"NoImputer\", KNNImputer(n_neighbors=1)),\n",
    "\n",
    "]\n",
    "\n",
    "# Ordinal Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "ordinal_imputer_list = [\n",
    "    (\"NoImputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "]\n",
    "\n",
    "# Predictive Models List (list of tuples with unique strings and corresponding instances)\n",
    "predictive_models_list = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"MultiTaskElasticNet\", MultiTaskElasticNet()),\n",
    "    (\"MultiTaskElasticNet_tuned\", MultiTaskElasticNet(**{'alpha': 0.01, 'l1_ratio': 0.01})),\n",
    "    (\"MultiTaskLasso\", MultiTaskLasso()),\n",
    "    (\"MultiTaskLasso_tuned\", MultiTaskLasso(**{'alpha': 0.001})),\n",
    "    (\"RandomForestRegressor\", RandomForestRegressor()),\n",
    "    (\"XGBoostRegressor\", XGBoostRegressor()),\n",
    "    (\"XGBoostRegressor_tuned\", XGBoostRegressor(**{'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105})),\n",
    "    (\"TabNetRegressor_default\", TabNetModelWrapper(n_a=8, n_d=8)),\n",
    "    (\"TabNetRegressor_custom\", TabNetModelWrapper(n_a=32, n_d=32)),\n",
    "    (\"PLSRegression_4_components\", PLSRegression(n_components=4))\n",
    "]\n",
    "\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "# Display all combinations\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    print(f\"Continuous Imputer: {continuous_imputer[0]}, Ordinal Imputer: {ordinal_imputer[0]}, Model: {model[0]}\")\n",
    "\n",
    "print(f\"Combinations of preprocessing and models to test : {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HDF5 file\n",
    "results_file = '../pickle/training_2_dict_results.pickle'\n",
    "\n",
    "with open(results_file, \"rb\") as input_file:\n",
    "    all_dict_results = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.13311 |  0:00:00s\n",
      "epoch 1  | loss: 1.67434 |  0:00:00s\n",
      "epoch 2  | loss: 1.25121 |  0:00:00s\n",
      "epoch 3  | loss: 1.15286 |  0:00:00s\n",
      "epoch 4  | loss: 1.05784 |  0:00:00s\n",
      "epoch 5  | loss: 0.96523 |  0:00:00s\n",
      "epoch 6  | loss: 0.92765 |  0:00:00s\n",
      "epoch 7  | loss: 0.89131 |  0:00:00s\n",
      "epoch 8  | loss: 0.86607 |  0:00:00s\n",
      "epoch 9  | loss: 0.82915 |  0:00:00s\n",
      "epoch 10 | loss: 0.83506 |  0:00:00s\n",
      "epoch 11 | loss: 0.7886  |  0:00:00s\n",
      "epoch 12 | loss: 0.75816 |  0:00:00s\n",
      "epoch 13 | loss: 0.76323 |  0:00:00s\n",
      "epoch 14 | loss: 0.76212 |  0:00:00s\n",
      "epoch 15 | loss: 0.75083 |  0:00:00s\n",
      "epoch 16 | loss: 0.72542 |  0:00:00s\n",
      "epoch 17 | loss: 0.71856 |  0:00:00s\n",
      "epoch 18 | loss: 0.71925 |  0:00:01s\n",
      "epoch 19 | loss: 0.70533 |  0:00:01s\n",
      "epoch 20 | loss: 0.70061 |  0:00:01s\n",
      "epoch 21 | loss: 0.69113 |  0:00:01s\n",
      "epoch 22 | loss: 0.67685 |  0:00:01s\n",
      "epoch 23 | loss: 0.67698 |  0:00:01s\n",
      "epoch 24 | loss: 0.67197 |  0:00:01s\n",
      "epoch 25 | loss: 0.66981 |  0:00:01s\n",
      "epoch 26 | loss: 0.66185 |  0:00:01s\n",
      "epoch 27 | loss: 0.65994 |  0:00:01s\n",
      "epoch 28 | loss: 0.65575 |  0:00:01s\n",
      "epoch 29 | loss: 0.65478 |  0:00:01s\n",
      "epoch 30 | loss: 0.64765 |  0:00:01s\n",
      "epoch 31 | loss: 0.64554 |  0:00:01s\n",
      "epoch 32 | loss: 0.63432 |  0:00:01s\n",
      "epoch 33 | loss: 0.62939 |  0:00:01s\n",
      "epoch 34 | loss: 0.62534 |  0:00:01s\n",
      "epoch 35 | loss: 0.62469 |  0:00:01s\n",
      "epoch 36 | loss: 0.62174 |  0:00:01s\n",
      "epoch 37 | loss: 0.61922 |  0:00:01s\n",
      "epoch 38 | loss: 0.62018 |  0:00:02s\n",
      "epoch 39 | loss: 0.62086 |  0:00:02s\n",
      "epoch 40 | loss: 0.60617 |  0:00:02s\n",
      "epoch 41 | loss: 0.60947 |  0:00:02s\n",
      "epoch 42 | loss: 0.59172 |  0:00:02s\n",
      "epoch 43 | loss: 0.58443 |  0:00:02s\n",
      "epoch 44 | loss: 0.58781 |  0:00:02s\n",
      "epoch 45 | loss: 0.57683 |  0:00:02s\n",
      "epoch 46 | loss: 0.57306 |  0:00:02s\n",
      "epoch 47 | loss: 0.57694 |  0:00:02s\n",
      "epoch 48 | loss: 0.56855 |  0:00:02s\n",
      "epoch 49 | loss: 0.56316 |  0:00:02s\n",
      "epoch 50 | loss: 0.56719 |  0:00:02s\n",
      "epoch 51 | loss: 0.56633 |  0:00:02s\n",
      "epoch 52 | loss: 0.55997 |  0:00:02s\n",
      "epoch 53 | loss: 0.56786 |  0:00:02s\n",
      "epoch 54 | loss: 0.55522 |  0:00:02s\n",
      "epoch 55 | loss: 0.55718 |  0:00:02s\n",
      "epoch 56 | loss: 0.55059 |  0:00:02s\n",
      "epoch 57 | loss: 0.55296 |  0:00:02s\n",
      "epoch 58 | loss: 0.55163 |  0:00:03s\n",
      "epoch 59 | loss: 0.54834 |  0:00:03s\n",
      "epoch 60 | loss: 0.55297 |  0:00:03s\n",
      "epoch 61 | loss: 0.5449  |  0:00:03s\n",
      "epoch 62 | loss: 0.5401  |  0:00:03s\n",
      "epoch 63 | loss: 0.53168 |  0:00:03s\n",
      "epoch 64 | loss: 0.52247 |  0:00:03s\n",
      "epoch 65 | loss: 0.52582 |  0:00:03s\n",
      "epoch 66 | loss: 0.51292 |  0:00:03s\n",
      "epoch 67 | loss: 0.51423 |  0:00:03s\n",
      "epoch 68 | loss: 0.50964 |  0:00:03s\n",
      "epoch 69 | loss: 0.50876 |  0:00:03s\n",
      "epoch 70 | loss: 0.50005 |  0:00:03s\n",
      "epoch 71 | loss: 0.50396 |  0:00:03s\n",
      "epoch 72 | loss: 0.49637 |  0:00:03s\n",
      "epoch 73 | loss: 0.49309 |  0:00:03s\n",
      "epoch 74 | loss: 0.48867 |  0:00:03s\n",
      "epoch 75 | loss: 0.48842 |  0:00:03s\n",
      "epoch 76 | loss: 0.48183 |  0:00:03s\n",
      "epoch 77 | loss: 0.48301 |  0:00:04s\n",
      "epoch 78 | loss: 0.48205 |  0:00:04s\n",
      "epoch 79 | loss: 0.48052 |  0:00:04s\n",
      "epoch 80 | loss: 0.47403 |  0:00:04s\n",
      "epoch 81 | loss: 0.48338 |  0:00:04s\n",
      "epoch 82 | loss: 0.47462 |  0:00:04s\n",
      "epoch 83 | loss: 0.47119 |  0:00:04s\n",
      "epoch 84 | loss: 0.47134 |  0:00:04s\n",
      "epoch 85 | loss: 0.4746  |  0:00:04s\n",
      "epoch 86 | loss: 0.47936 |  0:00:04s\n",
      "epoch 87 | loss: 0.48048 |  0:00:04s\n",
      "epoch 88 | loss: 0.46815 |  0:00:04s\n",
      "epoch 89 | loss: 0.47599 |  0:00:04s\n",
      "epoch 90 | loss: 0.46779 |  0:00:04s\n",
      "epoch 91 | loss: 0.46429 |  0:00:04s\n",
      "epoch 92 | loss: 0.46651 |  0:00:04s\n",
      "epoch 93 | loss: 0.46411 |  0:00:04s\n",
      "epoch 94 | loss: 0.45887 |  0:00:04s\n",
      "epoch 95 | loss: 0.46711 |  0:00:04s\n",
      "epoch 96 | loss: 0.45915 |  0:00:04s\n",
      "epoch 97 | loss: 0.46154 |  0:00:04s\n",
      "epoch 98 | loss: 0.45802 |  0:00:04s\n",
      "epoch 99 | loss: 0.45159 |  0:00:04s\n",
      "epoch 100| loss: 0.45073 |  0:00:05s\n",
      "epoch 101| loss: 0.45322 |  0:00:05s\n",
      "epoch 102| loss: 0.44542 |  0:00:05s\n",
      "epoch 103| loss: 0.44681 |  0:00:05s\n",
      "epoch 104| loss: 0.44443 |  0:00:05s\n",
      "epoch 105| loss: 0.44417 |  0:00:05s\n",
      "epoch 106| loss: 0.44086 |  0:00:05s\n",
      "epoch 107| loss: 0.44396 |  0:00:05s\n",
      "epoch 108| loss: 0.43298 |  0:00:05s\n",
      "epoch 109| loss: 0.44147 |  0:00:05s\n",
      "epoch 110| loss: 0.42825 |  0:00:05s\n",
      "epoch 111| loss: 0.43181 |  0:00:05s\n",
      "epoch 112| loss: 0.43651 |  0:00:05s\n",
      "epoch 113| loss: 0.42791 |  0:00:05s\n",
      "epoch 114| loss: 0.43285 |  0:00:05s\n",
      "epoch 115| loss: 0.43182 |  0:00:05s\n",
      "epoch 116| loss: 0.4306  |  0:00:05s\n",
      "epoch 117| loss: 0.42956 |  0:00:05s\n",
      "epoch 118| loss: 0.42473 |  0:00:05s\n",
      "epoch 119| loss: 0.42655 |  0:00:05s\n",
      "epoch 120| loss: 0.42597 |  0:00:05s\n",
      "epoch 121| loss: 0.41854 |  0:00:05s\n",
      "epoch 122| loss: 0.41561 |  0:00:06s\n",
      "epoch 123| loss: 0.41897 |  0:00:06s\n",
      "epoch 124| loss: 0.41393 |  0:00:06s\n",
      "epoch 125| loss: 0.41506 |  0:00:06s\n",
      "epoch 126| loss: 0.4127  |  0:00:06s\n",
      "epoch 127| loss: 0.41012 |  0:00:06s\n",
      "epoch 128| loss: 0.42201 |  0:00:06s\n",
      "epoch 129| loss: 0.41418 |  0:00:06s\n",
      "epoch 130| loss: 0.4124  |  0:00:06s\n",
      "epoch 131| loss: 0.40676 |  0:00:06s\n",
      "epoch 132| loss: 0.40992 |  0:00:06s\n",
      "epoch 133| loss: 0.41542 |  0:00:06s\n",
      "epoch 134| loss: 0.411   |  0:00:06s\n",
      "epoch 135| loss: 0.40383 |  0:00:06s\n",
      "epoch 136| loss: 0.40702 |  0:00:06s\n",
      "epoch 137| loss: 0.41123 |  0:00:06s\n",
      "epoch 138| loss: 0.40235 |  0:00:06s\n",
      "epoch 139| loss: 0.41131 |  0:00:06s\n",
      "epoch 140| loss: 0.40156 |  0:00:06s\n",
      "epoch 141| loss: 0.40275 |  0:00:06s\n",
      "epoch 142| loss: 0.40435 |  0:00:06s\n",
      "epoch 143| loss: 0.39838 |  0:00:06s\n",
      "epoch 144| loss: 0.39697 |  0:00:07s\n",
      "epoch 145| loss: 0.40627 |  0:00:07s\n",
      "epoch 146| loss: 0.39956 |  0:00:07s\n",
      "epoch 147| loss: 0.39254 |  0:00:07s\n",
      "epoch 148| loss: 0.39673 |  0:00:07s\n",
      "epoch 149| loss: 0.39021 |  0:00:07s\n",
      "epoch 150| loss: 0.38701 |  0:00:07s\n",
      "epoch 151| loss: 0.39155 |  0:00:07s\n",
      "epoch 152| loss: 0.38715 |  0:00:07s\n",
      "epoch 153| loss: 0.38539 |  0:00:07s\n",
      "epoch 154| loss: 0.3805  |  0:00:07s\n",
      "epoch 155| loss: 0.38658 |  0:00:07s\n",
      "epoch 156| loss: 0.38244 |  0:00:07s\n",
      "epoch 157| loss: 0.38939 |  0:00:07s\n",
      "epoch 158| loss: 0.38604 |  0:00:07s\n",
      "epoch 159| loss: 0.38263 |  0:00:07s\n",
      "epoch 160| loss: 0.383   |  0:00:07s\n",
      "epoch 161| loss: 0.38275 |  0:00:07s\n",
      "epoch 162| loss: 0.3747  |  0:00:07s\n",
      "epoch 163| loss: 0.38457 |  0:00:08s\n",
      "epoch 164| loss: 0.38214 |  0:00:08s\n",
      "epoch 165| loss: 0.38842 |  0:00:08s\n",
      "epoch 166| loss: 0.37563 |  0:00:08s\n",
      "epoch 167| loss: 0.38946 |  0:00:08s\n",
      "epoch 168| loss: 0.39004 |  0:00:08s\n",
      "epoch 169| loss: 0.38555 |  0:00:08s\n",
      "epoch 170| loss: 0.38699 |  0:00:08s\n",
      "epoch 171| loss: 0.38251 |  0:00:08s\n",
      "epoch 172| loss: 0.37322 |  0:00:08s\n",
      "epoch 173| loss: 0.37461 |  0:00:08s\n",
      "epoch 174| loss: 0.37934 |  0:00:08s\n",
      "epoch 175| loss: 0.37561 |  0:00:08s\n",
      "epoch 176| loss: 0.37233 |  0:00:08s\n",
      "epoch 177| loss: 0.36703 |  0:00:08s\n",
      "epoch 178| loss: 0.36985 |  0:00:08s\n",
      "epoch 179| loss: 0.37102 |  0:00:08s\n",
      "epoch 180| loss: 0.37611 |  0:00:08s\n",
      "epoch 181| loss: 0.36982 |  0:00:08s\n",
      "epoch 182| loss: 0.37385 |  0:00:08s\n",
      "epoch 183| loss: 0.36204 |  0:00:08s\n",
      "epoch 184| loss: 0.36449 |  0:00:08s\n",
      "epoch 185| loss: 0.36274 |  0:00:09s\n",
      "epoch 186| loss: 0.3675  |  0:00:09s\n",
      "epoch 187| loss: 0.37158 |  0:00:09s\n",
      "epoch 188| loss: 0.36024 |  0:00:09s\n",
      "epoch 189| loss: 0.36222 |  0:00:09s\n",
      "epoch 190| loss: 0.36547 |  0:00:09s\n",
      "epoch 191| loss: 0.35913 |  0:00:09s\n",
      "epoch 192| loss: 0.37019 |  0:00:09s\n",
      "epoch 193| loss: 0.3661  |  0:00:09s\n",
      "epoch 194| loss: 0.36435 |  0:00:09s\n",
      "epoch 195| loss: 0.3665  |  0:00:09s\n",
      "epoch 196| loss: 0.36435 |  0:00:09s\n",
      "epoch 197| loss: 0.36708 |  0:00:09s\n",
      "epoch 198| loss: 0.37635 |  0:00:09s\n",
      "epoch 199| loss: 0.36185 |  0:00:09s\n",
      "epoch 200| loss: 0.372   |  0:00:09s\n",
      "epoch 201| loss: 0.36781 |  0:00:09s\n",
      "epoch 202| loss: 0.37538 |  0:00:09s\n",
      "epoch 203| loss: 0.39584 |  0:00:09s\n",
      "epoch 204| loss: 0.40129 |  0:00:09s\n",
      "epoch 205| loss: 0.39697 |  0:00:09s\n",
      "epoch 206| loss: 0.39629 |  0:00:09s\n",
      "epoch 207| loss: 0.38705 |  0:00:10s\n",
      "epoch 208| loss: 0.38361 |  0:00:10s\n",
      "epoch 209| loss: 0.37977 |  0:00:10s\n",
      "epoch 210| loss: 0.3787  |  0:00:10s\n",
      "epoch 211| loss: 0.37001 |  0:00:10s\n",
      "epoch 212| loss: 0.3723  |  0:00:10s\n",
      "epoch 213| loss: 0.37208 |  0:00:10s\n",
      "epoch 214| loss: 0.36534 |  0:00:10s\n",
      "epoch 215| loss: 0.37599 |  0:00:10s\n",
      "epoch 216| loss: 0.35701 |  0:00:10s\n",
      "epoch 217| loss: 0.36111 |  0:00:10s\n",
      "epoch 218| loss: 0.35919 |  0:00:10s\n",
      "epoch 219| loss: 0.36595 |  0:00:10s\n",
      "epoch 220| loss: 0.35637 |  0:00:10s\n",
      "epoch 221| loss: 0.36044 |  0:00:10s\n",
      "epoch 222| loss: 0.34806 |  0:00:10s\n",
      "epoch 223| loss: 0.35702 |  0:00:10s\n",
      "epoch 224| loss: 0.35781 |  0:00:10s\n",
      "epoch 225| loss: 0.35377 |  0:00:10s\n",
      "epoch 226| loss: 0.35655 |  0:00:10s\n",
      "epoch 227| loss: 0.35704 |  0:00:10s\n",
      "epoch 228| loss: 0.34493 |  0:00:10s\n",
      "epoch 229| loss: 0.35773 |  0:00:10s\n",
      "epoch 230| loss: 0.34637 |  0:00:11s\n",
      "epoch 231| loss: 0.34904 |  0:00:11s\n",
      "epoch 232| loss: 0.34231 |  0:00:11s\n",
      "epoch 233| loss: 0.34362 |  0:00:11s\n",
      "epoch 234| loss: 0.34797 |  0:00:11s\n",
      "epoch 235| loss: 0.34678 |  0:00:11s\n",
      "epoch 236| loss: 0.34557 |  0:00:11s\n",
      "epoch 237| loss: 0.34948 |  0:00:11s\n",
      "epoch 238| loss: 0.34857 |  0:00:11s\n",
      "epoch 239| loss: 0.35335 |  0:00:11s\n",
      "epoch 240| loss: 0.34921 |  0:00:11s\n",
      "epoch 241| loss: 0.34684 |  0:00:11s\n",
      "epoch 242| loss: 0.34874 |  0:00:11s\n",
      "epoch 243| loss: 0.3433  |  0:00:11s\n",
      "epoch 244| loss: 0.34147 |  0:00:11s\n",
      "epoch 245| loss: 0.3383  |  0:00:11s\n",
      "epoch 246| loss: 0.34105 |  0:00:11s\n",
      "epoch 247| loss: 0.34444 |  0:00:11s\n",
      "epoch 248| loss: 0.34859 |  0:00:11s\n",
      "epoch 249| loss: 0.34221 |  0:00:11s\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.24287 |  0:00:00s\n",
      "epoch 1  | loss: 2.15244 |  0:00:00s\n",
      "epoch 2  | loss: 1.58292 |  0:00:00s\n",
      "epoch 3  | loss: 1.26691 |  0:00:00s\n",
      "epoch 4  | loss: 1.06626 |  0:00:00s\n",
      "epoch 5  | loss: 0.9529  |  0:00:00s\n",
      "epoch 6  | loss: 0.88656 |  0:00:00s\n",
      "epoch 7  | loss: 0.84019 |  0:00:00s\n",
      "epoch 8  | loss: 0.79465 |  0:00:00s\n",
      "epoch 9  | loss: 0.76328 |  0:00:00s\n",
      "epoch 10 | loss: 0.72436 |  0:00:00s\n",
      "epoch 11 | loss: 0.69744 |  0:00:00s\n",
      "epoch 12 | loss: 0.69848 |  0:00:00s\n",
      "epoch 13 | loss: 0.68787 |  0:00:00s\n",
      "epoch 14 | loss: 0.67379 |  0:00:00s\n",
      "epoch 15 | loss: 0.67166 |  0:00:00s\n",
      "epoch 16 | loss: 0.65451 |  0:00:00s\n",
      "epoch 17 | loss: 0.66407 |  0:00:00s\n",
      "epoch 18 | loss: 0.65599 |  0:00:00s\n",
      "epoch 19 | loss: 0.65139 |  0:00:00s\n",
      "epoch 20 | loss: 0.64927 |  0:00:01s\n",
      "epoch 21 | loss: 0.64578 |  0:00:01s\n",
      "epoch 22 | loss: 0.63364 |  0:00:01s\n",
      "epoch 23 | loss: 0.62991 |  0:00:01s\n",
      "epoch 24 | loss: 0.62301 |  0:00:01s\n",
      "epoch 25 | loss: 0.62647 |  0:00:01s\n",
      "epoch 26 | loss: 0.61679 |  0:00:01s\n",
      "epoch 27 | loss: 0.60603 |  0:00:01s\n",
      "epoch 28 | loss: 0.59713 |  0:00:01s\n",
      "epoch 29 | loss: 0.59047 |  0:00:01s\n",
      "epoch 30 | loss: 0.58662 |  0:00:01s\n",
      "epoch 31 | loss: 0.59503 |  0:00:01s\n",
      "epoch 32 | loss: 0.58066 |  0:00:01s\n",
      "epoch 33 | loss: 0.5762  |  0:00:01s\n",
      "epoch 34 | loss: 0.57702 |  0:00:01s\n",
      "epoch 35 | loss: 0.5748  |  0:00:01s\n",
      "epoch 36 | loss: 0.58111 |  0:00:01s\n",
      "epoch 37 | loss: 0.56751 |  0:00:01s\n",
      "epoch 38 | loss: 0.56548 |  0:00:01s\n",
      "epoch 39 | loss: 0.56671 |  0:00:01s\n",
      "epoch 40 | loss: 0.54826 |  0:00:02s\n",
      "epoch 41 | loss: 0.55204 |  0:00:02s\n",
      "epoch 42 | loss: 0.54543 |  0:00:02s\n",
      "epoch 43 | loss: 0.54748 |  0:00:02s\n",
      "epoch 44 | loss: 0.53076 |  0:00:02s\n",
      "epoch 45 | loss: 0.534   |  0:00:02s\n",
      "epoch 46 | loss: 0.52653 |  0:00:02s\n",
      "epoch 47 | loss: 0.51791 |  0:00:02s\n",
      "epoch 48 | loss: 0.52555 |  0:00:02s\n",
      "epoch 49 | loss: 0.51958 |  0:00:02s\n",
      "epoch 50 | loss: 0.51854 |  0:00:02s\n",
      "epoch 51 | loss: 0.52509 |  0:00:02s\n",
      "epoch 52 | loss: 0.53795 |  0:00:02s\n",
      "epoch 53 | loss: 0.51252 |  0:00:02s\n",
      "epoch 54 | loss: 0.52148 |  0:00:02s\n",
      "epoch 55 | loss: 0.50706 |  0:00:02s\n",
      "epoch 56 | loss: 0.50415 |  0:00:02s\n",
      "epoch 57 | loss: 0.50179 |  0:00:02s\n",
      "epoch 58 | loss: 0.5014  |  0:00:02s\n",
      "epoch 59 | loss: 0.496   |  0:00:02s\n",
      "epoch 60 | loss: 0.49581 |  0:00:03s\n",
      "epoch 61 | loss: 0.49195 |  0:00:03s\n",
      "epoch 62 | loss: 0.49182 |  0:00:03s\n",
      "epoch 63 | loss: 0.48621 |  0:00:03s\n",
      "epoch 64 | loss: 0.48535 |  0:00:03s\n",
      "epoch 65 | loss: 0.48092 |  0:00:03s\n",
      "epoch 66 | loss: 0.48223 |  0:00:03s\n",
      "epoch 67 | loss: 0.47486 |  0:00:03s\n",
      "epoch 68 | loss: 0.48263 |  0:00:03s\n",
      "epoch 69 | loss: 0.46807 |  0:00:03s\n",
      "epoch 70 | loss: 0.47951 |  0:00:03s\n",
      "epoch 71 | loss: 0.47439 |  0:00:03s\n",
      "epoch 72 | loss: 0.46664 |  0:00:03s\n",
      "epoch 73 | loss: 0.46357 |  0:00:03s\n",
      "epoch 74 | loss: 0.45955 |  0:00:03s\n",
      "epoch 75 | loss: 0.44495 |  0:00:03s\n",
      "epoch 76 | loss: 0.45515 |  0:00:03s\n",
      "epoch 77 | loss: 0.44442 |  0:00:03s\n",
      "epoch 78 | loss: 0.44651 |  0:00:03s\n",
      "epoch 79 | loss: 0.44756 |  0:00:03s\n",
      "epoch 80 | loss: 0.43945 |  0:00:03s\n",
      "epoch 81 | loss: 0.4455  |  0:00:03s\n",
      "epoch 82 | loss: 0.42914 |  0:00:04s\n",
      "epoch 83 | loss: 0.42907 |  0:00:04s\n",
      "epoch 84 | loss: 0.42916 |  0:00:04s\n",
      "epoch 85 | loss: 0.43479 |  0:00:04s\n",
      "epoch 86 | loss: 0.4282  |  0:00:04s\n",
      "epoch 87 | loss: 0.43356 |  0:00:04s\n",
      "epoch 88 | loss: 0.4317  |  0:00:04s\n",
      "epoch 89 | loss: 0.42938 |  0:00:04s\n",
      "epoch 90 | loss: 0.42696 |  0:00:04s\n",
      "epoch 91 | loss: 0.43679 |  0:00:04s\n",
      "epoch 92 | loss: 0.43223 |  0:00:04s\n",
      "epoch 93 | loss: 0.43293 |  0:00:04s\n",
      "epoch 94 | loss: 0.43471 |  0:00:04s\n",
      "epoch 95 | loss: 0.43117 |  0:00:04s\n",
      "epoch 96 | loss: 0.43198 |  0:00:04s\n",
      "epoch 97 | loss: 0.42767 |  0:00:04s\n",
      "epoch 98 | loss: 0.44682 |  0:00:04s\n",
      "epoch 99 | loss: 0.42731 |  0:00:04s\n",
      "epoch 100| loss: 0.42579 |  0:00:04s\n",
      "epoch 101| loss: 0.42668 |  0:00:05s\n",
      "epoch 102| loss: 0.41374 |  0:00:05s\n",
      "epoch 103| loss: 0.40874 |  0:00:05s\n",
      "epoch 104| loss: 0.41486 |  0:00:05s\n",
      "epoch 105| loss: 0.4021  |  0:00:05s\n",
      "epoch 106| loss: 0.40922 |  0:00:05s\n",
      "epoch 107| loss: 0.40808 |  0:00:05s\n",
      "epoch 108| loss: 0.4061  |  0:00:05s\n",
      "epoch 109| loss: 0.40864 |  0:00:05s\n",
      "epoch 110| loss: 0.40504 |  0:00:05s\n",
      "epoch 111| loss: 0.39976 |  0:00:05s\n",
      "epoch 112| loss: 0.40696 |  0:00:05s\n",
      "epoch 113| loss: 0.38797 |  0:00:05s\n",
      "epoch 114| loss: 0.40204 |  0:00:05s\n",
      "epoch 115| loss: 0.3983  |  0:00:05s\n",
      "epoch 116| loss: 0.4002  |  0:00:05s\n",
      "epoch 117| loss: 0.40514 |  0:00:05s\n",
      "epoch 118| loss: 0.40652 |  0:00:05s\n",
      "epoch 119| loss: 0.40779 |  0:00:05s\n",
      "epoch 120| loss: 0.41209 |  0:00:05s\n",
      "epoch 121| loss: 0.40213 |  0:00:05s\n",
      "epoch 122| loss: 0.38867 |  0:00:05s\n",
      "epoch 123| loss: 0.38918 |  0:00:06s\n",
      "epoch 124| loss: 0.39164 |  0:00:06s\n",
      "epoch 125| loss: 0.37698 |  0:00:06s\n",
      "epoch 126| loss: 0.37028 |  0:00:06s\n",
      "epoch 127| loss: 0.37927 |  0:00:06s\n",
      "epoch 128| loss: 0.36095 |  0:00:06s\n",
      "epoch 129| loss: 0.36817 |  0:00:06s\n",
      "epoch 130| loss: 0.35612 |  0:00:06s\n",
      "epoch 131| loss: 0.35326 |  0:00:06s\n",
      "epoch 132| loss: 0.35904 |  0:00:06s\n",
      "epoch 133| loss: 0.35966 |  0:00:06s\n",
      "epoch 134| loss: 0.34819 |  0:00:06s\n",
      "epoch 135| loss: 0.35239 |  0:00:06s\n",
      "epoch 136| loss: 0.35978 |  0:00:06s\n",
      "epoch 137| loss: 0.35151 |  0:00:06s\n",
      "epoch 138| loss: 0.36372 |  0:00:06s\n",
      "epoch 139| loss: 0.36289 |  0:00:06s\n",
      "epoch 140| loss: 0.36773 |  0:00:06s\n",
      "epoch 141| loss: 0.36356 |  0:00:06s\n",
      "epoch 142| loss: 0.35889 |  0:00:06s\n",
      "epoch 143| loss: 0.36573 |  0:00:06s\n",
      "epoch 144| loss: 0.36685 |  0:00:07s\n",
      "epoch 145| loss: 0.36418 |  0:00:07s\n",
      "epoch 146| loss: 0.37643 |  0:00:07s\n",
      "epoch 147| loss: 0.36169 |  0:00:07s\n",
      "epoch 148| loss: 0.3599  |  0:00:07s\n",
      "epoch 149| loss: 0.36358 |  0:00:07s\n",
      "epoch 150| loss: 0.38092 |  0:00:07s\n",
      "epoch 151| loss: 0.37406 |  0:00:07s\n",
      "epoch 152| loss: 0.36953 |  0:00:07s\n",
      "epoch 153| loss: 0.37239 |  0:00:07s\n",
      "epoch 154| loss: 0.36479 |  0:00:07s\n",
      "epoch 155| loss: 0.36082 |  0:00:07s\n",
      "epoch 156| loss: 0.35538 |  0:00:07s\n",
      "epoch 157| loss: 0.34878 |  0:00:07s\n",
      "epoch 158| loss: 0.34607 |  0:00:07s\n",
      "epoch 159| loss: 0.34139 |  0:00:07s\n",
      "epoch 160| loss: 0.33437 |  0:00:07s\n",
      "epoch 161| loss: 0.33563 |  0:00:07s\n",
      "epoch 162| loss: 0.34112 |  0:00:07s\n",
      "epoch 163| loss: 0.35652 |  0:00:07s\n",
      "epoch 164| loss: 0.35128 |  0:00:07s\n",
      "epoch 165| loss: 0.34045 |  0:00:07s\n",
      "epoch 166| loss: 0.34459 |  0:00:08s\n",
      "epoch 167| loss: 0.34769 |  0:00:08s\n",
      "epoch 168| loss: 0.33172 |  0:00:08s\n",
      "epoch 169| loss: 0.33832 |  0:00:08s\n",
      "epoch 170| loss: 0.32454 |  0:00:08s\n",
      "epoch 171| loss: 0.33092 |  0:00:08s\n",
      "epoch 172| loss: 0.33065 |  0:00:08s\n",
      "epoch 173| loss: 0.31971 |  0:00:08s\n",
      "epoch 174| loss: 0.31975 |  0:00:08s\n",
      "epoch 175| loss: 0.30903 |  0:00:08s\n",
      "epoch 176| loss: 0.31415 |  0:00:08s\n",
      "epoch 177| loss: 0.32472 |  0:00:08s\n",
      "epoch 178| loss: 0.30966 |  0:00:08s\n",
      "epoch 179| loss: 0.31315 |  0:00:08s\n",
      "epoch 180| loss: 0.31162 |  0:00:08s\n",
      "epoch 181| loss: 0.32061 |  0:00:08s\n",
      "epoch 182| loss: 0.31373 |  0:00:08s\n",
      "epoch 183| loss: 0.31775 |  0:00:08s\n",
      "epoch 184| loss: 0.31597 |  0:00:08s\n",
      "epoch 185| loss: 0.3111  |  0:00:08s\n",
      "epoch 186| loss: 0.31505 |  0:00:08s\n",
      "epoch 187| loss: 0.30957 |  0:00:08s\n",
      "epoch 188| loss: 0.31093 |  0:00:09s\n",
      "epoch 189| loss: 0.31486 |  0:00:09s\n",
      "epoch 190| loss: 0.31301 |  0:00:09s\n",
      "epoch 191| loss: 0.3108  |  0:00:09s\n",
      "epoch 192| loss: 0.30916 |  0:00:09s\n",
      "epoch 193| loss: 0.30084 |  0:00:09s\n",
      "epoch 194| loss: 0.30258 |  0:00:09s\n",
      "epoch 195| loss: 0.30658 |  0:00:09s\n",
      "epoch 196| loss: 0.30231 |  0:00:09s\n",
      "epoch 197| loss: 0.29887 |  0:00:09s\n",
      "epoch 198| loss: 0.3017  |  0:00:09s\n",
      "epoch 199| loss: 0.2996  |  0:00:09s\n",
      "epoch 200| loss: 0.29747 |  0:00:09s\n",
      "epoch 201| loss: 0.302   |  0:00:09s\n",
      "epoch 202| loss: 0.29356 |  0:00:09s\n",
      "epoch 203| loss: 0.29971 |  0:00:09s\n",
      "epoch 204| loss: 0.28496 |  0:00:09s\n",
      "epoch 205| loss: 0.28985 |  0:00:09s\n",
      "epoch 206| loss: 0.28443 |  0:00:09s\n",
      "epoch 207| loss: 0.28011 |  0:00:09s\n",
      "epoch 208| loss: 0.28006 |  0:00:09s\n",
      "epoch 209| loss: 0.27741 |  0:00:10s\n",
      "epoch 210| loss: 0.27276 |  0:00:10s\n",
      "epoch 211| loss: 0.27628 |  0:00:10s\n",
      "epoch 212| loss: 0.2841  |  0:00:10s\n",
      "epoch 213| loss: 0.28351 |  0:00:10s\n",
      "epoch 214| loss: 0.28181 |  0:00:10s\n",
      "epoch 215| loss: 0.29372 |  0:00:10s\n",
      "epoch 216| loss: 0.27954 |  0:00:10s\n",
      "epoch 217| loss: 0.28504 |  0:00:10s\n",
      "epoch 218| loss: 0.281   |  0:00:10s\n",
      "epoch 219| loss: 0.28408 |  0:00:10s\n",
      "epoch 220| loss: 0.28229 |  0:00:10s\n",
      "epoch 221| loss: 0.27346 |  0:00:10s\n",
      "epoch 222| loss: 0.26955 |  0:00:10s\n",
      "epoch 223| loss: 0.27341 |  0:00:10s\n",
      "epoch 224| loss: 0.27228 |  0:00:10s\n",
      "epoch 225| loss: 0.26034 |  0:00:10s\n",
      "epoch 226| loss: 0.26504 |  0:00:10s\n",
      "epoch 227| loss: 0.26373 |  0:00:10s\n",
      "epoch 228| loss: 0.25678 |  0:00:10s\n",
      "epoch 229| loss: 0.2667  |  0:00:11s\n",
      "epoch 230| loss: 0.25775 |  0:00:11s\n",
      "epoch 231| loss: 0.26326 |  0:00:11s\n",
      "epoch 232| loss: 0.25435 |  0:00:11s\n",
      "epoch 233| loss: 0.25562 |  0:00:11s\n",
      "epoch 234| loss: 0.26014 |  0:00:11s\n",
      "epoch 235| loss: 0.2552  |  0:00:11s\n",
      "epoch 236| loss: 0.25414 |  0:00:11s\n",
      "epoch 237| loss: 0.25566 |  0:00:11s\n",
      "epoch 238| loss: 0.25705 |  0:00:11s\n",
      "epoch 239| loss: 0.25846 |  0:00:11s\n",
      "epoch 240| loss: 0.25685 |  0:00:11s\n",
      "epoch 241| loss: 0.26753 |  0:00:11s\n",
      "epoch 242| loss: 0.27538 |  0:00:11s\n",
      "epoch 243| loss: 0.27838 |  0:00:11s\n",
      "epoch 244| loss: 0.27072 |  0:00:11s\n",
      "epoch 245| loss: 0.26688 |  0:00:11s\n",
      "epoch 246| loss: 0.26624 |  0:00:11s\n",
      "epoch 247| loss: 0.26438 |  0:00:11s\n",
      "epoch 248| loss: 0.26376 |  0:00:12s\n",
      "epoch 249| loss: 0.26156 |  0:00:12s\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    }
   ],
   "source": [
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    try: \n",
    "    \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "    # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open(results_file, 'wb') as handle:\n",
    "    pickle.dump(all_dict_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Pytorch models only on MRI features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = ['APOE_epsilon2', 'APOE_epsilon3', 'APOE_epsilon4']\n",
    "continuous_features = [col for col in df_X_train.columns if col not in ordinal_features]\n",
    "\n",
    "# Prepare Tabular configurations (shared for all PyTorch models)\n",
    "data_config = DataConfig(\n",
    "    target=df_y_train.columns.tolist(),\n",
    "    continuous_cols=continuous_features,\n",
    "    categorical_cols=[]\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024, max_epochs=1, auto_lr_find=True,\n",
    "    early_stopping=\"valid_loss\", early_stopping_mode=\"min\", early_stopping_patience=5,\n",
    "    checkpoints=\"valid_loss\", load_best=True, progress_bar=\"nones\",\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "head_config = LinearHeadConfig(dropout=0.1).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'LinearRegression', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskElasticNet', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskElasticNet_tuned', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskLasso', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskLasso_tuned', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'RandomForestRegressor', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'XGBoostRegressor', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'XGBoostRegressor_tuned', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetRegressor_default', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetRegressor_custom', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'PLSRegression_4_components', (2881, 200), (13, 200)])\n",
      "GatedAdditiveTreeEnsembleConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">234</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:23\u001b[0m,\u001b[1;36m234\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">246</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:23\u001b[0m,\u001b[1;36m246\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">250</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:23\u001b[0m,\u001b[1;36m250\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:23\u001b[0m,\u001b[1;36m265\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">619</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:23\u001b[0m,\u001b[1;36m619\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">632</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:23\u001b[0m,\u001b[1;36m632\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:23</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">641</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:23\u001b[0m,\u001b[1;36m641\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588f6da0e1744f17a1dbcf6e5adbce9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_d16bcaca-1262-4984-bb48-d655e08cd046.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_d16bcaca-1262-4984-bb48-d655e08cd046.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">421</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:25\u001b[0m,\u001b[1;36m421\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">428</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:25\u001b[0m,\u001b[1;36m428\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">441</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:25\u001b[0m,\u001b[1;36m441\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                       | Params | Mode \n",
      "------------------------------------------------------------------------\n",
      "0 | _backbone        | GatedAdditiveTreesBackbone | 2.1 M  | train\n",
      "1 | _embedding_layer | Embedding1dLayer           | 400    | train\n",
      "2 | _head            | CustomHead                 | 156    | train\n",
      "3 | loss             | MSELoss                    | 0      | train\n",
      "------------------------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.417     Total estimated model params size (MB)\n",
      "689       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ca1d9428da439ebf6d8a398e759c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d609a26ef04463b5cf36df43cad875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfba9702cee4141b23abae461c4ba41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">432</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:27\u001b[0m,\u001b[1;36m432\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">433</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:27\u001b[0m,\u001b[1;36m433\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "DANetConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">956</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:27\u001b[0m,\u001b[1;36m956\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">968</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:27\u001b[0m,\u001b[1;36m968\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">971</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:27\u001b[0m,\u001b[1;36m971\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">986</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: DANetModel             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:27\u001b[0m,\u001b[1;36m986\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: DANetModel             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">014</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:28\u001b[0m,\u001b[1;36m014\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">022</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:28\u001b[0m,\u001b[1;36m022\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9a37931c95431181a996299dd8b07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_ead3d129-337d-4460-8ec5-94e4a743580e.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_ead3d129-337d-4460-8ec5-94e4a743580e.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">343</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:28\u001b[0m,\u001b[1;36m343\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">349</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:28\u001b[0m,\u001b[1;36m349\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | _backbone        | DANetBackbone    | 1.4 M  | train\n",
      "1 | _embedding_layer | Embedding1dLayer | 400    | train\n",
      "2 | _head            | LinearHead       | 260    | train\n",
      "3 | loss             | MSELoss          | 0      | train\n",
      "--------------------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.787     Total estimated model params size (MB)\n",
      "156       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c02d814e904d0883ca045ca73db91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d255482d2b40f5a04eff1aadacb927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2db646addb4c488b3b4ad006dccaa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">682</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:28\u001b[0m,\u001b[1;36m682\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">683</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:28\u001b[0m,\u001b[1;36m683\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "TabTransformerConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">041</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m041\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">053</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m053\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">056</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m056\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">071</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabTransformerModel    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m071\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabTransformerModel    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">091</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m091\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">098</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m098\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14815782cb764d2eb24ff29d8b87fa37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_34342d7a-0b99-4b6b-8b04-22620c8a8d9f.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_34342d7a-0b99-4b6b-8b04-22620c8a8d9f.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">221</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m221\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">223</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m223\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                   | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | _backbone        | TabTransformerBackbone | 271 K  | train\n",
      "1 | _embedding_layer | Embedding2dLayer       | 0      | train\n",
      "2 | _head            | LinearHead             | 804    | train\n",
      "3 | loss             | MSELoss                | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "272 K     Trainable params\n",
      "0         Non-trainable params\n",
      "272 K     Total params\n",
      "1.090     Total estimated model params size (MB)\n",
      "119       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54f469465e746ef8e9781d798d188ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6babf250bb6147f08634955a6bd64668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb1a37d47da43249eb84fd84beb74be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">352</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m352\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">353</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m353\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "TabNetModelConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">535</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m535\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">547</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m547\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">550</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m550\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">565</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m565\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">586</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m586\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">593</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m593\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de22adc7be64f789d68fe3ea4e324c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_760bab5e-2abb-4245-97e3-e87fb4095c20.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_760bab5e-2abb-4245-97e3-e87fb4095c20.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">783</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m783\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">785</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m785\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type           | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | _embedding_layer | Identity       | 0      | train\n",
      "1 | _backbone        | TabNetBackbone | 18.9 K | train\n",
      "2 | _head            | Identity       | 0      | train\n",
      "3 | loss             | MSELoss        | 0      | train\n",
      "------------------------------------------------------------\n",
      "18.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "18.9 K    Total params\n",
      "0.075     Total estimated model params size (MB)\n",
      "107       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d37bd66a4c5421d8aa11aad5ec76577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73551b74123462d90ef5717233307c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900c79377d184f769630e906d14c2d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">985</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m985\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:21:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">986</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:21:29\u001b[0m,\u001b[1;36m986\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": [
    "predictive_models_list += [\n",
    "    (\"GatedAdditiveTreeEnsembleConfig_tab\", \n",
    "    TabularModelWrapper(\n",
    "        GatedAdditiveTreeEnsembleConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        gflu_stages=6,\n",
    "        gflu_dropout=0.0,\n",
    "        tree_depth=5,\n",
    "        num_trees=20,\n",
    "        chain_trees=False,\n",
    "        share_head_weights=True), data_config, trainer_config, optimizer_config \n",
    "    )),\n",
    "    (\"DANetConfig_tab\",\n",
    "    TabularModelWrapper(\n",
    "        DANetConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_layers=8,\n",
    "        k=5,\n",
    "        dropout_rate=0.1), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabTransformerConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        embedding_initialization=\"kaiming_uniform\",\n",
    "        embedding_bias=False), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    # (\"FTTransformerConfig\",\n",
    "    #     TabularModelWrapper(\n",
    "    #     FTTransformerConfig(\n",
    "    #     task=\"regression\",\n",
    "    #     head=\"LinearHead\",\n",
    "    #     head_config=head_config), data_config, trainer_config, optimizer_config\n",
    "    # )),\n",
    "    (\"TabNetModelConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        n_independent=2,\n",
    "        n_shared=2), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "\n",
    "    if any(result['params'] == params for result in all_dict_results):\n",
    "        # Skip this iteration if the combination exists\n",
    "        print(f\"Skipping existing combination: {params.values()}\")\n",
    "        \n",
    "        continue\n",
    "\n",
    "    try: \n",
    "        print(name_model)\n",
    "        \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "        # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Table for reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = \"../pickle/training_2_dict_results.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_file, \"rb\") as input_file:\n",
    "    all_dict_results = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'LinearRegression', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskElasticNet', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskElasticNet_tuned', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskLasso', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskLasso_tuned', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'RandomForestRegressor', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'XGBoostRegressor', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'XGBoostRegressor_tuned', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetRegressor_default', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetRegressor_custom', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'PLSRegression_4_components', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'GatedAdditiveTreeEnsembleConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'DANetConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabTransformerConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetModelConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'GatedAdditiveTreeEnsembleConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'DANetConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabTransformerConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetModelConfig_tab', (2881, 200), (13, 200)])\n"
     ]
    }
   ],
   "source": [
    "predictive_models_list += [\n",
    "    (\"GatedAdditiveTreeEnsembleConfig_tab\", \n",
    "    TabularModelWrapper(\n",
    "        GatedAdditiveTreeEnsembleConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        gflu_stages=6,\n",
    "        gflu_dropout=0.0,\n",
    "        tree_depth=5,\n",
    "        num_trees=20,\n",
    "        chain_trees=False,\n",
    "        share_head_weights=True), data_config, trainer_config, optimizer_config \n",
    "    )),\n",
    "    (\"DANetConfig_tab\",\n",
    "    TabularModelWrapper(\n",
    "        DANetConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_layers=8,\n",
    "        k=5,\n",
    "        dropout_rate=0.1), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabTransformerConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        embedding_initialization=\"kaiming_uniform\",\n",
    "        embedding_bias=False), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    # (\"FTTransformerConfig\",\n",
    "    #     TabularModelWrapper(\n",
    "    #     FTTransformerConfig(\n",
    "    #     task=\"regression\",\n",
    "    #     head=\"LinearHead\",\n",
    "    #     head_config=head_config), data_config, trainer_config, optimizer_config\n",
    "    # )),\n",
    "    (\"TabNetModelConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        n_independent=2,\n",
    "        n_shared=2), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "\n",
    "    if any(result['params'] == params for result in all_dict_results):\n",
    "        # Skip this iteration if the combination exists\n",
    "        print(f\"Skipping existing combination: {params.values()}\")\n",
    "        \n",
    "        continue\n",
    "\n",
    "    try: \n",
    "        print(name_model)\n",
    "        \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "        # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'XGBoostRegressor', 'train_shape': (2881, 348), 'test_shape': (13, 348)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'XGBoostRegressor_tuned', 'train_shape': (2881, 348), 'test_shape': (13, 348)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'GatedAdditiveTreeEnsembleConfig_tab', 'train_shape': (2881, 348), 'test_shape': (13, 348)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'DANetConfig_tab', 'train_shape': (2881, 348), 'test_shape': (13, 348)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'TabTransformerConfig_tab', 'train_shape': (2881, 348), 'test_shape': (13, 348)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'TabNetModelConfig_tab', 'train_shape': (2881, 348), 'test_shape': (13, 348)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'LinearRegression', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.31140828132629395, 'results_adj': {'mse_score': array([1.11192349, 0.67668626, 0.61999815, 0.97750844]), 'mae_score': array([0.72849831, 0.70235973, 0.70986192, 0.71215901]), 'r2': array([-0.12379655,  0.2321367 , -0.29452346, -0.18906157]), 'explained_variance': array([ 0.02475189,  0.28845771, -0.29319881,  0.10973059]), 'corr': array([0.33204585, 0.54765514, 0.01649677, 0.38058137])}, 'results_org': {'mse_score': array([1.11192349, 0.67668626, 0.61999817, 0.97750843]), 'mae_score': array([0.72849832, 0.70235973, 0.70986193, 0.712159  ]), 'r2': array([-0.18280006,  0.24192465, -0.20755938, -0.14289477]), 'explained_variance': array([-0.02645227,  0.29752774, -0.20632372,  0.14429641]), 'corr': array([0.27766053, 0.55012684, 0.07284141, 0.39622517])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskElasticNet', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.01563549041748047, 'results_adj': {'mse_score': array([1.18012247, 0.95041534, 0.50741149, 1.02939713]), 'mae_score': array([0.88414167, 0.78683572, 0.65378091, 0.82689555]), 'r2': array([-0.19272375, -0.07847477, -0.05944845, -0.25218005]), 'explained_variance': array([ 0.11229868,  0.03189858, -0.05332468,  0.12857136]), 'corr': array([ 0.42176386,  0.17864277, -0.16306808,  0.44631445])}, 'results_org': {'mse_score': array([1.18012248, 0.95041534, 0.50741151, 1.02939711]), 'mae_score': array([0.88414168, 0.78683573, 0.65378092, 0.82689554]), 'r2': array([-0.25534621, -0.06472745,  0.01172366, -0.20356258]), 'explained_variance': array([0.06569104, 0.04423898, 0.01743605, 0.16240568]), 'corr': array([0.25863755, 0.21083053, 0.14561964, 0.49623358])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskElasticNet_tuned', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.816544771194458, 'results_adj': {'mse_score': array([1.09767924, 0.67163602, 0.61727141, 0.96027028]), 'mae_score': array([0.72712752, 0.70316248, 0.71357071, 0.70813001]), 'r2': array([-0.1094002 ,  0.23786741, -0.28883017, -0.16809271]), 'explained_variance': array([ 0.04259982,  0.29559122, -0.28782174,  0.12798901]), 'corr': array([0.33826662, 0.55120885, 0.00953623, 0.39231636])}, 'results_org': {'mse_score': array([1.09767923, 0.67163602, 0.61727143, 0.96027027]), 'mae_score': array([0.72712752, 0.70316248, 0.71357072, 0.70813   ]), 'r2': array([-0.16764784,  0.24758231, -0.20224855, -0.12274005]), 'explained_variance': array([-0.00766726,  0.30457033, -0.20130787,  0.16184592]), 'corr': array([0.28384559, 0.55450481, 0.06742918, 0.41077564])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskLasso', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.017554283142089844, 'results_adj': {'mse_score': array([1.39363767, 1.04056339, 0.48866937, 1.24000841]), 'mae_score': array([0.99127575, 0.78457157, 0.62110666, 0.93446712]), 'r2': array([-0.40851885, -0.18076941, -0.0203159 , -0.50837198]), 'explained_variance': array([-2.22044605e-16,  0.00000000e+00,  0.00000000e+00,  1.11022302e-16]), 'corr': array([nan, nan, nan, nan])}, 'results_org': {'mse_score': array([1.39363768, 1.04056339, 0.48866939, 1.24000838]), 'mae_score': array([0.99127576, 0.78457157, 0.62110667, 0.93446711]), 'r2': array([-0.48247136, -0.16571814,  0.04822734, -0.44980754]), 'explained_variance': array([-0.05250375,  0.01274701,  0.06717846,  0.03882626]), 'corr': array([-0.0848666 ,  0.11455326,  0.2721653 ,  0.20679355])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskLasso_tuned', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.843498945236206, 'results_adj': {'mse_score': array([1.09986228, 0.67700338, 0.61721354, 0.96322248]), 'mae_score': array([0.72439764, 0.70502407, 0.71262079, 0.70634911]), 'r2': array([-0.11160655,  0.23177686, -0.28870934, -0.17168383]), 'explained_variance': array([ 0.04182356,  0.29179378, -0.28783433,  0.1250568 ]), 'corr': array([0.34030126, 0.54948447, 0.01369735, 0.39123818])}, 'results_org': {'mse_score': array([1.09986228, 0.67700337, 0.61721356, 0.96322247]), 'mae_score': array([0.72439764, 0.70502408, 0.71262081, 0.7063491 ]), 'r2': array([-0.16997004,  0.2415694 , -0.20213584, -0.12619174]), 'explained_variance': array([-0.00848428,  0.30082129, -0.20131962,  0.15902757]), 'corr': array([0.28608858, 0.55225247, 0.07079544, 0.40890542])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'RandomForestRegressor', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 32.67283058166504, 'results_adj': {'mse_score': array([1.00638713, 0.65012684, 0.47869162, 0.82220865]), 'mae_score': array([0.75528169, 0.6841987 , 0.63512616, 0.63151397]), 'r2': array([-1.71332791e-02,  2.62274753e-01,  5.17132936e-04, -1.51675905e-04]), 'explained_variance': array([0.29209145, 0.405549  , 0.02445608, 0.37078241]), 'corr': array([0.54046621, 0.64364297, 0.21686966, 0.60995505])}, 'results_org': {'mse_score': array([1.00638713, 0.65012684, 0.47869164, 0.82220864]), 'mae_score': array([0.7552817 , 0.6841987 , 0.63512617, 0.63151397]), 'r2': array([-0.07053657,  0.27167853,  0.06766084,  0.03868046]), 'explained_variance': array([0.25492361, 0.41312647, 0.08999161, 0.39521257]), 'corr': array([0.50511182, 0.65009756, 0.3086371 , 0.63522421])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'XGBoostRegressor', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.909536600112915, 'results_adj': {'mse_score': array([0.99729777, 0.55285888, 0.49806901, 0.7462314 ]), 'mae_score': array([0.75445592, 0.63451078, 0.64294552, 0.64429738]), 'r2': array([-0.00794687,  0.37264865, -0.03994184,  0.09226863]), 'explained_variance': array([ 0.26511088,  0.47604274, -0.03447538,  0.31643386]), 'corr': array([0.52174936, 0.69849083, 0.21172288, 0.56606   ])}, 'results_org': {'mse_score': array([0.99729777, 0.55285888, 0.49806903, 0.74623138]), 'mae_score': array([0.75445593, 0.63451079, 0.64294554, 0.64429738]), 'r2': array([-0.06086784,  0.38064549,  0.02991984,  0.12751245]), 'explained_variance': array([0.22652646, 0.48272162, 0.03501907, 0.34297417]), 'corr': array([0.48966434, 0.70257856, 0.27320828, 0.58583766])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'XGBoostRegressor_tuned', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 2.325019121170044, 'results_adj': {'mse_score': array([1.0153611 , 0.63851973, 0.46345103, 0.70749896]), 'mae_score': array([0.73457703, 0.71458275, 0.63021056, 0.61263546]), 'r2': array([-0.02620307,  0.27544581,  0.03233868,  0.13938357]), 'explained_variance': array([0.22165364, 0.34897739, 0.05887258, 0.43032201]), 'corr': array([0.48104477, 0.59075482, 0.30682341, 0.65598979])}, 'results_org': {'mse_score': array([1.0153611 , 0.63851973, 0.46345105, 0.70749894]), 'mae_score': array([0.73457704, 0.71458276, 0.63021057, 0.61263545]), 'r2': array([-0.08008256,  0.2846817 ,  0.09734466,  0.17279809]), 'explained_variance': array([0.18078756, 0.35727599, 0.12209605, 0.45244048]), 'corr': array([0.44678191, 0.59775758, 0.36862204, 0.67323253])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetRegressor_default', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 11.987843036651611, 'results_adj': {'mse_score': array([1.00024251, 0.85216095, 0.6231349 , 0.78732824]), 'mae_score': array([0.78064659, 0.8013238 , 0.6795552 , 0.67213814]), 'r2': array([-0.01092304,  0.03301847, -0.30107284,  0.04227758]), 'explained_variance': array([ 0.38275983,  0.13551578, -0.29939216,  0.5247155 ]), 'corr': array([0.62946614, 0.47112118, 0.06684408, 0.73037268])}, 'results_org': {'mse_score': array([1.0002425 , 0.85216095, 0.62313492, 0.78732822]), 'mae_score': array([0.78064659, 0.8013238 , 0.67955522, 0.67213815]), 'r2': array([-0.06400027,  0.04534457, -0.21366878,  0.07946235]), 'explained_variance': array([ 0.35035243,  0.14653537, -0.212101  ,  0.54316902]), 'corr': array([0.60852724, 0.4852577 , 0.12261516, 0.74095112])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetRegressor_custom', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 12.13334846496582, 'results_adj': {'mse_score': array([0.92610036, 0.76428738, 0.61165174, 0.87483944]), 'mae_score': array([0.71338154, 0.75841494, 0.67114785, 0.72399585]), 'r2': array([ 0.06401079,  0.13273216, -0.2770966 , -0.06417287]), 'explained_variance': array([ 0.22209961,  0.26101411, -0.27432143,  0.26805491]), 'corr': array([0.55329131, 0.58084529, 0.23700988, 0.59708229])}, 'results_org': {'mse_score': array([0.92610035, 0.76428739, 0.61165176, 0.87483943]), 'mae_score': array([0.71338154, 0.75841494, 0.67114786, 0.72399584]), 'r2': array([ 0.01486787,  0.14378722, -0.19130323, -0.02285503]), 'explained_variance': array([ 0.18125695,  0.27043396, -0.1887145 ,  0.2964736 ]), 'corr': array([0.53795846, 0.59013198, 0.27340583, 0.5986342 ])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'PLSRegression_4_components', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.05103135108947754, 'results_adj': {'mse_score': array([1.08140908, 0.78765117, 0.5694762 , 0.94630352]), 'mae_score': array([0.79248414, 0.79068015, 0.72178585, 0.73452485]), 'r2': array([-0.09295631,  0.10622033, -0.1890363 , -0.15110326]), 'explained_variance': array([ 0.1855152 ,  0.19584409, -0.18846787,  0.20844666]), 'corr': array([ 0.43286088,  0.44542341, -0.05356343,  0.45740887])}, 'results_org': {'mse_score': array([1.08140909, 0.78765116, 0.56947622, 0.9463035 ]), 'mae_score': array([0.79248415, 0.79068015, 0.72178586, 0.73452484]), 'r2': array([-0.1503406 ,  0.11761334, -0.10915869, -0.10641023]), 'explained_variance': array([ 0.14275171,  0.20609467, -0.10862844,  0.23917972]), 'corr': array([0.3812462 , 0.45398549, 0.03688166, 0.49549602])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'GatedAdditiveTreeEnsembleConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'DANetConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabTransformerConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetModelConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n"
     ]
    }
   ],
   "source": [
    "all_dict_results = clean_dict_list(all_dict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "MultiTaskElasticNet\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "MultiTaskElasticNet_tuned\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "MultiTaskLasso\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "MultiTaskLasso_tuned\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "RandomForestRegressor\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "XGBoostRegressor\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "XGBoostRegressor_tuned\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "TabNetRegressor_default\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.13311 |  0:00:00s\n",
      "epoch 1  | loss: 1.67434 |  0:00:00s\n",
      "epoch 2  | loss: 1.25121 |  0:00:00s\n",
      "epoch 3  | loss: 1.15286 |  0:00:00s\n",
      "epoch 4  | loss: 1.05784 |  0:00:00s\n",
      "epoch 5  | loss: 0.96523 |  0:00:00s\n",
      "epoch 6  | loss: 0.92765 |  0:00:00s\n",
      "epoch 7  | loss: 0.89131 |  0:00:00s\n",
      "epoch 8  | loss: 0.86607 |  0:00:00s\n",
      "epoch 9  | loss: 0.82915 |  0:00:00s\n",
      "epoch 10 | loss: 0.83506 |  0:00:00s\n",
      "epoch 11 | loss: 0.7886  |  0:00:00s\n",
      "epoch 12 | loss: 0.75816 |  0:00:00s\n",
      "epoch 13 | loss: 0.76323 |  0:00:00s\n",
      "epoch 14 | loss: 0.76212 |  0:00:00s\n",
      "epoch 15 | loss: 0.75083 |  0:00:00s\n",
      "epoch 16 | loss: 0.72542 |  0:00:01s\n",
      "epoch 17 | loss: 0.71856 |  0:00:01s\n",
      "epoch 18 | loss: 0.71925 |  0:00:01s\n",
      "epoch 19 | loss: 0.70533 |  0:00:01s\n",
      "epoch 20 | loss: 0.70061 |  0:00:01s\n",
      "epoch 21 | loss: 0.69113 |  0:00:01s\n",
      "epoch 22 | loss: 0.67685 |  0:00:01s\n",
      "epoch 23 | loss: 0.67698 |  0:00:01s\n",
      "epoch 24 | loss: 0.67197 |  0:00:01s\n",
      "epoch 25 | loss: 0.66981 |  0:00:01s\n",
      "epoch 26 | loss: 0.66185 |  0:00:01s\n",
      "epoch 27 | loss: 0.65994 |  0:00:01s\n",
      "epoch 28 | loss: 0.65575 |  0:00:01s\n",
      "epoch 29 | loss: 0.65478 |  0:00:01s\n",
      "epoch 30 | loss: 0.64765 |  0:00:01s\n",
      "epoch 31 | loss: 0.64554 |  0:00:01s\n",
      "epoch 32 | loss: 0.63432 |  0:00:01s\n",
      "epoch 33 | loss: 0.62939 |  0:00:01s\n",
      "epoch 34 | loss: 0.62534 |  0:00:01s\n",
      "epoch 35 | loss: 0.62469 |  0:00:01s\n",
      "epoch 36 | loss: 0.62174 |  0:00:02s\n",
      "epoch 37 | loss: 0.61922 |  0:00:02s\n",
      "epoch 38 | loss: 0.62018 |  0:00:02s\n",
      "epoch 39 | loss: 0.62086 |  0:00:02s\n",
      "epoch 40 | loss: 0.60617 |  0:00:02s\n",
      "epoch 41 | loss: 0.60947 |  0:00:02s\n",
      "epoch 42 | loss: 0.59172 |  0:00:02s\n",
      "epoch 43 | loss: 0.58443 |  0:00:02s\n",
      "epoch 44 | loss: 0.58781 |  0:00:02s\n",
      "epoch 45 | loss: 0.57683 |  0:00:02s\n",
      "epoch 46 | loss: 0.57306 |  0:00:02s\n",
      "epoch 47 | loss: 0.57694 |  0:00:02s\n",
      "epoch 48 | loss: 0.56855 |  0:00:02s\n",
      "epoch 49 | loss: 0.56316 |  0:00:02s\n",
      "epoch 50 | loss: 0.56719 |  0:00:02s\n",
      "epoch 51 | loss: 0.56633 |  0:00:02s\n",
      "epoch 52 | loss: 0.55997 |  0:00:02s\n",
      "epoch 53 | loss: 0.56786 |  0:00:02s\n",
      "epoch 54 | loss: 0.55522 |  0:00:02s\n",
      "epoch 55 | loss: 0.55718 |  0:00:02s\n",
      "epoch 56 | loss: 0.55059 |  0:00:02s\n",
      "epoch 57 | loss: 0.55296 |  0:00:02s\n",
      "epoch 58 | loss: 0.55163 |  0:00:03s\n",
      "epoch 59 | loss: 0.54834 |  0:00:03s\n",
      "epoch 60 | loss: 0.55297 |  0:00:03s\n",
      "epoch 61 | loss: 0.5449  |  0:00:03s\n",
      "epoch 62 | loss: 0.5401  |  0:00:03s\n",
      "epoch 63 | loss: 0.53168 |  0:00:03s\n",
      "epoch 64 | loss: 0.52247 |  0:00:03s\n",
      "epoch 65 | loss: 0.52582 |  0:00:03s\n",
      "epoch 66 | loss: 0.51292 |  0:00:03s\n",
      "epoch 67 | loss: 0.51423 |  0:00:03s\n",
      "epoch 68 | loss: 0.50964 |  0:00:03s\n",
      "epoch 69 | loss: 0.50876 |  0:00:03s\n",
      "epoch 70 | loss: 0.50005 |  0:00:03s\n",
      "epoch 71 | loss: 0.50396 |  0:00:03s\n",
      "epoch 72 | loss: 0.49637 |  0:00:03s\n",
      "epoch 73 | loss: 0.49309 |  0:00:03s\n",
      "epoch 74 | loss: 0.48867 |  0:00:03s\n",
      "epoch 75 | loss: 0.48842 |  0:00:03s\n",
      "epoch 76 | loss: 0.48183 |  0:00:03s\n",
      "epoch 77 | loss: 0.48301 |  0:00:03s\n",
      "epoch 78 | loss: 0.48205 |  0:00:03s\n",
      "epoch 79 | loss: 0.48052 |  0:00:03s\n",
      "epoch 80 | loss: 0.47403 |  0:00:04s\n",
      "epoch 81 | loss: 0.48338 |  0:00:04s\n",
      "epoch 82 | loss: 0.47462 |  0:00:04s\n",
      "epoch 83 | loss: 0.47119 |  0:00:04s\n",
      "epoch 84 | loss: 0.47134 |  0:00:04s\n",
      "epoch 85 | loss: 0.4746  |  0:00:04s\n",
      "epoch 86 | loss: 0.47936 |  0:00:04s\n",
      "epoch 87 | loss: 0.48048 |  0:00:04s\n",
      "epoch 88 | loss: 0.46815 |  0:00:04s\n",
      "epoch 89 | loss: 0.47599 |  0:00:04s\n",
      "epoch 90 | loss: 0.46779 |  0:00:04s\n",
      "epoch 91 | loss: 0.46429 |  0:00:04s\n",
      "epoch 92 | loss: 0.46651 |  0:00:04s\n",
      "epoch 93 | loss: 0.46411 |  0:00:04s\n",
      "epoch 94 | loss: 0.45887 |  0:00:04s\n",
      "epoch 95 | loss: 0.46711 |  0:00:04s\n",
      "epoch 96 | loss: 0.45915 |  0:00:04s\n",
      "epoch 97 | loss: 0.46154 |  0:00:04s\n",
      "epoch 98 | loss: 0.45802 |  0:00:04s\n",
      "epoch 99 | loss: 0.45159 |  0:00:04s\n",
      "epoch 100| loss: 0.45073 |  0:00:04s\n",
      "epoch 101| loss: 0.45322 |  0:00:04s\n",
      "epoch 102| loss: 0.44542 |  0:00:04s\n",
      "epoch 103| loss: 0.44681 |  0:00:05s\n",
      "epoch 104| loss: 0.44443 |  0:00:05s\n",
      "epoch 105| loss: 0.44417 |  0:00:05s\n",
      "epoch 106| loss: 0.44086 |  0:00:05s\n",
      "epoch 107| loss: 0.44396 |  0:00:05s\n",
      "epoch 108| loss: 0.43298 |  0:00:05s\n",
      "epoch 109| loss: 0.44147 |  0:00:05s\n",
      "epoch 110| loss: 0.42825 |  0:00:05s\n",
      "epoch 111| loss: 0.43181 |  0:00:05s\n",
      "epoch 112| loss: 0.43651 |  0:00:05s\n",
      "epoch 113| loss: 0.42791 |  0:00:05s\n",
      "epoch 114| loss: 0.43285 |  0:00:05s\n",
      "epoch 115| loss: 0.43182 |  0:00:05s\n",
      "epoch 116| loss: 0.4306  |  0:00:05s\n",
      "epoch 117| loss: 0.42956 |  0:00:05s\n",
      "epoch 118| loss: 0.42473 |  0:00:05s\n",
      "epoch 119| loss: 0.42655 |  0:00:05s\n",
      "epoch 120| loss: 0.42597 |  0:00:05s\n",
      "epoch 121| loss: 0.41854 |  0:00:05s\n",
      "epoch 122| loss: 0.41561 |  0:00:05s\n",
      "epoch 123| loss: 0.41897 |  0:00:05s\n",
      "epoch 124| loss: 0.41393 |  0:00:05s\n",
      "epoch 125| loss: 0.41506 |  0:00:06s\n",
      "epoch 126| loss: 0.4127  |  0:00:06s\n",
      "epoch 127| loss: 0.41012 |  0:00:06s\n",
      "epoch 128| loss: 0.42201 |  0:00:06s\n",
      "epoch 129| loss: 0.41418 |  0:00:06s\n",
      "epoch 130| loss: 0.4124  |  0:00:06s\n",
      "epoch 131| loss: 0.40676 |  0:00:06s\n",
      "epoch 132| loss: 0.40992 |  0:00:06s\n",
      "epoch 133| loss: 0.41542 |  0:00:06s\n",
      "epoch 134| loss: 0.411   |  0:00:06s\n",
      "epoch 135| loss: 0.40383 |  0:00:06s\n",
      "epoch 136| loss: 0.40702 |  0:00:06s\n",
      "epoch 137| loss: 0.41123 |  0:00:06s\n",
      "epoch 138| loss: 0.40235 |  0:00:06s\n",
      "epoch 139| loss: 0.41131 |  0:00:06s\n",
      "epoch 140| loss: 0.40156 |  0:00:06s\n",
      "epoch 141| loss: 0.40275 |  0:00:06s\n",
      "epoch 142| loss: 0.40435 |  0:00:06s\n",
      "epoch 143| loss: 0.39838 |  0:00:06s\n",
      "epoch 144| loss: 0.39697 |  0:00:06s\n",
      "epoch 145| loss: 0.40627 |  0:00:06s\n",
      "epoch 146| loss: 0.39956 |  0:00:07s\n",
      "epoch 147| loss: 0.39254 |  0:00:07s\n",
      "epoch 148| loss: 0.39673 |  0:00:07s\n",
      "epoch 149| loss: 0.39021 |  0:00:07s\n",
      "epoch 150| loss: 0.38701 |  0:00:07s\n",
      "epoch 151| loss: 0.39155 |  0:00:07s\n",
      "epoch 152| loss: 0.38715 |  0:00:07s\n",
      "epoch 153| loss: 0.38539 |  0:00:07s\n",
      "epoch 154| loss: 0.3805  |  0:00:07s\n",
      "epoch 155| loss: 0.38658 |  0:00:07s\n",
      "epoch 156| loss: 0.38244 |  0:00:07s\n",
      "epoch 157| loss: 0.38939 |  0:00:07s\n",
      "epoch 158| loss: 0.38604 |  0:00:07s\n",
      "epoch 159| loss: 0.38263 |  0:00:07s\n",
      "epoch 160| loss: 0.383   |  0:00:07s\n",
      "epoch 161| loss: 0.38275 |  0:00:07s\n",
      "epoch 162| loss: 0.3747  |  0:00:07s\n",
      "epoch 163| loss: 0.38457 |  0:00:07s\n",
      "epoch 164| loss: 0.38214 |  0:00:07s\n",
      "epoch 165| loss: 0.38842 |  0:00:07s\n",
      "epoch 166| loss: 0.37563 |  0:00:07s\n",
      "epoch 167| loss: 0.38946 |  0:00:07s\n",
      "epoch 168| loss: 0.39004 |  0:00:08s\n",
      "epoch 169| loss: 0.38555 |  0:00:08s\n",
      "epoch 170| loss: 0.38699 |  0:00:08s\n",
      "epoch 171| loss: 0.38251 |  0:00:08s\n",
      "epoch 172| loss: 0.37322 |  0:00:08s\n",
      "epoch 173| loss: 0.37461 |  0:00:08s\n",
      "epoch 174| loss: 0.37934 |  0:00:08s\n",
      "epoch 175| loss: 0.37561 |  0:00:08s\n",
      "epoch 176| loss: 0.37233 |  0:00:08s\n",
      "epoch 177| loss: 0.36703 |  0:00:08s\n",
      "epoch 178| loss: 0.36985 |  0:00:08s\n",
      "epoch 179| loss: 0.37102 |  0:00:08s\n",
      "epoch 180| loss: 0.37611 |  0:00:08s\n",
      "epoch 181| loss: 0.36982 |  0:00:08s\n",
      "epoch 182| loss: 0.37385 |  0:00:08s\n",
      "epoch 183| loss: 0.36204 |  0:00:08s\n",
      "epoch 184| loss: 0.36449 |  0:00:08s\n",
      "epoch 185| loss: 0.36274 |  0:00:08s\n",
      "epoch 186| loss: 0.3675  |  0:00:08s\n",
      "epoch 187| loss: 0.37158 |  0:00:08s\n",
      "epoch 188| loss: 0.36024 |  0:00:08s\n",
      "epoch 189| loss: 0.36222 |  0:00:08s\n",
      "epoch 190| loss: 0.36547 |  0:00:09s\n",
      "epoch 191| loss: 0.35913 |  0:00:09s\n",
      "epoch 192| loss: 0.37019 |  0:00:09s\n",
      "epoch 193| loss: 0.3661  |  0:00:09s\n",
      "epoch 194| loss: 0.36435 |  0:00:09s\n",
      "epoch 195| loss: 0.3665  |  0:00:09s\n",
      "epoch 196| loss: 0.36435 |  0:00:09s\n",
      "epoch 197| loss: 0.36708 |  0:00:09s\n",
      "epoch 198| loss: 0.37635 |  0:00:09s\n",
      "epoch 199| loss: 0.36185 |  0:00:09s\n",
      "epoch 200| loss: 0.372   |  0:00:09s\n",
      "epoch 201| loss: 0.36781 |  0:00:09s\n",
      "epoch 202| loss: 0.37538 |  0:00:09s\n",
      "epoch 203| loss: 0.39584 |  0:00:09s\n",
      "epoch 204| loss: 0.40129 |  0:00:09s\n",
      "epoch 205| loss: 0.39697 |  0:00:09s\n",
      "epoch 206| loss: 0.39629 |  0:00:09s\n",
      "epoch 207| loss: 0.38705 |  0:00:09s\n",
      "epoch 208| loss: 0.38361 |  0:00:09s\n",
      "epoch 209| loss: 0.37977 |  0:00:09s\n",
      "epoch 210| loss: 0.3787  |  0:00:09s\n",
      "epoch 211| loss: 0.37001 |  0:00:09s\n",
      "epoch 212| loss: 0.3723  |  0:00:10s\n",
      "epoch 213| loss: 0.37208 |  0:00:10s\n",
      "epoch 214| loss: 0.36534 |  0:00:10s\n",
      "epoch 215| loss: 0.37599 |  0:00:10s\n",
      "epoch 216| loss: 0.35701 |  0:00:10s\n",
      "epoch 217| loss: 0.36111 |  0:00:10s\n",
      "epoch 218| loss: 0.35919 |  0:00:10s\n",
      "epoch 219| loss: 0.36595 |  0:00:10s\n",
      "epoch 220| loss: 0.35637 |  0:00:10s\n",
      "epoch 221| loss: 0.36044 |  0:00:10s\n",
      "epoch 222| loss: 0.34806 |  0:00:10s\n",
      "epoch 223| loss: 0.35702 |  0:00:10s\n",
      "epoch 224| loss: 0.35781 |  0:00:10s\n",
      "epoch 225| loss: 0.35377 |  0:00:10s\n",
      "epoch 226| loss: 0.35655 |  0:00:10s\n",
      "epoch 227| loss: 0.35704 |  0:00:10s\n",
      "epoch 228| loss: 0.34493 |  0:00:10s\n",
      "epoch 229| loss: 0.35773 |  0:00:10s\n",
      "epoch 230| loss: 0.34637 |  0:00:10s\n",
      "epoch 231| loss: 0.34904 |  0:00:10s\n",
      "epoch 232| loss: 0.34231 |  0:00:10s\n",
      "epoch 233| loss: 0.34362 |  0:00:11s\n",
      "epoch 234| loss: 0.34797 |  0:00:11s\n",
      "epoch 235| loss: 0.34678 |  0:00:11s\n",
      "epoch 236| loss: 0.34557 |  0:00:11s\n",
      "epoch 237| loss: 0.34948 |  0:00:11s\n",
      "epoch 238| loss: 0.34857 |  0:00:11s\n",
      "epoch 239| loss: 0.35335 |  0:00:11s\n",
      "epoch 240| loss: 0.34921 |  0:00:11s\n",
      "epoch 241| loss: 0.34684 |  0:00:11s\n",
      "epoch 242| loss: 0.34874 |  0:00:11s\n",
      "epoch 243| loss: 0.3433  |  0:00:11s\n",
      "epoch 244| loss: 0.34147 |  0:00:11s\n",
      "epoch 245| loss: 0.3383  |  0:00:11s\n",
      "epoch 246| loss: 0.34105 |  0:00:11s\n",
      "epoch 247| loss: 0.34444 |  0:00:11s\n",
      "epoch 248| loss: 0.34859 |  0:00:11s\n",
      "epoch 249| loss: 0.34221 |  0:00:11s\n",
      "TabNetRegressor_custom\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.24287 |  0:00:00s\n",
      "epoch 1  | loss: 2.15244 |  0:00:00s\n",
      "epoch 2  | loss: 1.58292 |  0:00:00s\n",
      "epoch 3  | loss: 1.26691 |  0:00:00s\n",
      "epoch 4  | loss: 1.06626 |  0:00:00s\n",
      "epoch 5  | loss: 0.9529  |  0:00:00s\n",
      "epoch 6  | loss: 0.88656 |  0:00:00s\n",
      "epoch 7  | loss: 0.84019 |  0:00:00s\n",
      "epoch 8  | loss: 0.79465 |  0:00:00s\n",
      "epoch 9  | loss: 0.76328 |  0:00:00s\n",
      "epoch 10 | loss: 0.72436 |  0:00:00s\n",
      "epoch 11 | loss: 0.69744 |  0:00:00s\n",
      "epoch 12 | loss: 0.69848 |  0:00:00s\n",
      "epoch 13 | loss: 0.68787 |  0:00:00s\n",
      "epoch 14 | loss: 0.67379 |  0:00:00s\n",
      "epoch 15 | loss: 0.67166 |  0:00:00s\n",
      "epoch 16 | loss: 0.65451 |  0:00:00s\n",
      "epoch 17 | loss: 0.66407 |  0:00:00s\n",
      "epoch 18 | loss: 0.65599 |  0:00:00s\n",
      "epoch 19 | loss: 0.65139 |  0:00:00s\n",
      "epoch 20 | loss: 0.64927 |  0:00:00s\n",
      "epoch 21 | loss: 0.64578 |  0:00:01s\n",
      "epoch 22 | loss: 0.63364 |  0:00:01s\n",
      "epoch 23 | loss: 0.62991 |  0:00:01s\n",
      "epoch 24 | loss: 0.62301 |  0:00:01s\n",
      "epoch 25 | loss: 0.62647 |  0:00:01s\n",
      "epoch 26 | loss: 0.61679 |  0:00:01s\n",
      "epoch 27 | loss: 0.60603 |  0:00:01s\n",
      "epoch 28 | loss: 0.59713 |  0:00:01s\n",
      "epoch 29 | loss: 0.59047 |  0:00:01s\n",
      "epoch 30 | loss: 0.58662 |  0:00:01s\n",
      "epoch 31 | loss: 0.59503 |  0:00:01s\n",
      "epoch 32 | loss: 0.58066 |  0:00:01s\n",
      "epoch 33 | loss: 0.5762  |  0:00:01s\n",
      "epoch 34 | loss: 0.57702 |  0:00:01s\n",
      "epoch 35 | loss: 0.5748  |  0:00:01s\n",
      "epoch 36 | loss: 0.58111 |  0:00:01s\n",
      "epoch 37 | loss: 0.56751 |  0:00:01s\n",
      "epoch 38 | loss: 0.56548 |  0:00:01s\n",
      "epoch 39 | loss: 0.56671 |  0:00:01s\n",
      "epoch 40 | loss: 0.54826 |  0:00:01s\n",
      "epoch 41 | loss: 0.55204 |  0:00:01s\n",
      "epoch 42 | loss: 0.54543 |  0:00:02s\n",
      "epoch 43 | loss: 0.54748 |  0:00:02s\n",
      "epoch 44 | loss: 0.53076 |  0:00:02s\n",
      "epoch 45 | loss: 0.534   |  0:00:02s\n",
      "epoch 46 | loss: 0.52653 |  0:00:02s\n",
      "epoch 47 | loss: 0.51791 |  0:00:02s\n",
      "epoch 48 | loss: 0.52555 |  0:00:02s\n",
      "epoch 49 | loss: 0.51958 |  0:00:02s\n",
      "epoch 50 | loss: 0.51854 |  0:00:02s\n",
      "epoch 51 | loss: 0.52509 |  0:00:02s\n",
      "epoch 52 | loss: 0.53795 |  0:00:02s\n",
      "epoch 53 | loss: 0.51252 |  0:00:02s\n",
      "epoch 54 | loss: 0.52148 |  0:00:02s\n",
      "epoch 55 | loss: 0.50706 |  0:00:02s\n",
      "epoch 56 | loss: 0.50415 |  0:00:02s\n",
      "epoch 57 | loss: 0.50179 |  0:00:02s\n",
      "epoch 58 | loss: 0.5014  |  0:00:02s\n",
      "epoch 59 | loss: 0.496   |  0:00:02s\n",
      "epoch 60 | loss: 0.49581 |  0:00:02s\n",
      "epoch 61 | loss: 0.49195 |  0:00:02s\n",
      "epoch 62 | loss: 0.49182 |  0:00:02s\n",
      "epoch 63 | loss: 0.48621 |  0:00:03s\n",
      "epoch 64 | loss: 0.48535 |  0:00:03s\n",
      "epoch 65 | loss: 0.48092 |  0:00:03s\n",
      "epoch 66 | loss: 0.48223 |  0:00:03s\n",
      "epoch 67 | loss: 0.47486 |  0:00:03s\n",
      "epoch 68 | loss: 0.48263 |  0:00:03s\n",
      "epoch 69 | loss: 0.46807 |  0:00:03s\n",
      "epoch 70 | loss: 0.47951 |  0:00:03s\n",
      "epoch 71 | loss: 0.47439 |  0:00:03s\n",
      "epoch 72 | loss: 0.46664 |  0:00:03s\n",
      "epoch 73 | loss: 0.46357 |  0:00:03s\n",
      "epoch 74 | loss: 0.45955 |  0:00:03s\n",
      "epoch 75 | loss: 0.44495 |  0:00:03s\n",
      "epoch 76 | loss: 0.45515 |  0:00:03s\n",
      "epoch 77 | loss: 0.44442 |  0:00:03s\n",
      "epoch 78 | loss: 0.44651 |  0:00:03s\n",
      "epoch 79 | loss: 0.44756 |  0:00:03s\n",
      "epoch 80 | loss: 0.43945 |  0:00:03s\n",
      "epoch 81 | loss: 0.4455  |  0:00:03s\n",
      "epoch 82 | loss: 0.42914 |  0:00:03s\n",
      "epoch 83 | loss: 0.42907 |  0:00:03s\n",
      "epoch 84 | loss: 0.42916 |  0:00:03s\n",
      "epoch 85 | loss: 0.43479 |  0:00:04s\n",
      "epoch 86 | loss: 0.4282  |  0:00:04s\n",
      "epoch 87 | loss: 0.43356 |  0:00:04s\n",
      "epoch 88 | loss: 0.4317  |  0:00:04s\n",
      "epoch 89 | loss: 0.42938 |  0:00:04s\n",
      "epoch 90 | loss: 0.42696 |  0:00:04s\n",
      "epoch 91 | loss: 0.43679 |  0:00:04s\n",
      "epoch 92 | loss: 0.43223 |  0:00:04s\n",
      "epoch 93 | loss: 0.43293 |  0:00:04s\n",
      "epoch 94 | loss: 0.43471 |  0:00:04s\n",
      "epoch 95 | loss: 0.43117 |  0:00:04s\n",
      "epoch 96 | loss: 0.43198 |  0:00:04s\n",
      "epoch 97 | loss: 0.42767 |  0:00:04s\n",
      "epoch 98 | loss: 0.44682 |  0:00:04s\n",
      "epoch 99 | loss: 0.42731 |  0:00:04s\n",
      "epoch 100| loss: 0.42579 |  0:00:04s\n",
      "epoch 101| loss: 0.42668 |  0:00:04s\n",
      "epoch 102| loss: 0.41374 |  0:00:04s\n",
      "epoch 103| loss: 0.40874 |  0:00:04s\n",
      "epoch 104| loss: 0.41486 |  0:00:04s\n",
      "epoch 105| loss: 0.4021  |  0:00:04s\n",
      "epoch 106| loss: 0.40922 |  0:00:05s\n",
      "epoch 107| loss: 0.40808 |  0:00:05s\n",
      "epoch 108| loss: 0.4061  |  0:00:05s\n",
      "epoch 109| loss: 0.40864 |  0:00:05s\n",
      "epoch 110| loss: 0.40504 |  0:00:05s\n",
      "epoch 111| loss: 0.39976 |  0:00:05s\n",
      "epoch 112| loss: 0.40696 |  0:00:05s\n",
      "epoch 113| loss: 0.38797 |  0:00:05s\n",
      "epoch 114| loss: 0.40204 |  0:00:05s\n",
      "epoch 115| loss: 0.3983  |  0:00:05s\n",
      "epoch 116| loss: 0.4002  |  0:00:05s\n",
      "epoch 117| loss: 0.40514 |  0:00:05s\n",
      "epoch 118| loss: 0.40652 |  0:00:05s\n",
      "epoch 119| loss: 0.40779 |  0:00:05s\n",
      "epoch 120| loss: 0.41209 |  0:00:05s\n",
      "epoch 121| loss: 0.40213 |  0:00:05s\n",
      "epoch 122| loss: 0.38867 |  0:00:05s\n",
      "epoch 123| loss: 0.38918 |  0:00:05s\n",
      "epoch 124| loss: 0.39164 |  0:00:05s\n",
      "epoch 125| loss: 0.37698 |  0:00:05s\n",
      "epoch 126| loss: 0.37028 |  0:00:05s\n",
      "epoch 127| loss: 0.37927 |  0:00:05s\n",
      "epoch 128| loss: 0.36095 |  0:00:05s\n",
      "epoch 129| loss: 0.36817 |  0:00:06s\n",
      "epoch 130| loss: 0.35612 |  0:00:06s\n",
      "epoch 131| loss: 0.35326 |  0:00:06s\n",
      "epoch 132| loss: 0.35904 |  0:00:06s\n",
      "epoch 133| loss: 0.35966 |  0:00:06s\n",
      "epoch 134| loss: 0.34819 |  0:00:06s\n",
      "epoch 135| loss: 0.35239 |  0:00:06s\n",
      "epoch 136| loss: 0.35978 |  0:00:06s\n",
      "epoch 137| loss: 0.35151 |  0:00:06s\n",
      "epoch 138| loss: 0.36372 |  0:00:06s\n",
      "epoch 139| loss: 0.36289 |  0:00:06s\n",
      "epoch 140| loss: 0.36773 |  0:00:06s\n",
      "epoch 141| loss: 0.36356 |  0:00:06s\n",
      "epoch 142| loss: 0.35889 |  0:00:06s\n",
      "epoch 143| loss: 0.36573 |  0:00:06s\n",
      "epoch 144| loss: 0.36685 |  0:00:06s\n",
      "epoch 145| loss: 0.36418 |  0:00:06s\n",
      "epoch 146| loss: 0.37643 |  0:00:06s\n",
      "epoch 147| loss: 0.36169 |  0:00:06s\n",
      "epoch 148| loss: 0.3599  |  0:00:06s\n",
      "epoch 149| loss: 0.36358 |  0:00:06s\n",
      "epoch 150| loss: 0.38092 |  0:00:07s\n",
      "epoch 151| loss: 0.37406 |  0:00:07s\n",
      "epoch 152| loss: 0.36953 |  0:00:07s\n",
      "epoch 153| loss: 0.37239 |  0:00:07s\n",
      "epoch 154| loss: 0.36479 |  0:00:07s\n",
      "epoch 155| loss: 0.36082 |  0:00:07s\n",
      "epoch 156| loss: 0.35538 |  0:00:07s\n",
      "epoch 157| loss: 0.34878 |  0:00:07s\n",
      "epoch 158| loss: 0.34607 |  0:00:07s\n",
      "epoch 159| loss: 0.34139 |  0:00:07s\n",
      "epoch 160| loss: 0.33437 |  0:00:07s\n",
      "epoch 161| loss: 0.33563 |  0:00:07s\n",
      "epoch 162| loss: 0.34112 |  0:00:07s\n",
      "epoch 163| loss: 0.35652 |  0:00:07s\n",
      "epoch 164| loss: 0.35128 |  0:00:07s\n",
      "epoch 165| loss: 0.34045 |  0:00:07s\n",
      "epoch 166| loss: 0.34459 |  0:00:07s\n",
      "epoch 167| loss: 0.34769 |  0:00:07s\n",
      "epoch 168| loss: 0.33172 |  0:00:07s\n",
      "epoch 169| loss: 0.33832 |  0:00:07s\n",
      "epoch 170| loss: 0.32454 |  0:00:07s\n",
      "epoch 171| loss: 0.33092 |  0:00:07s\n",
      "epoch 172| loss: 0.33065 |  0:00:08s\n",
      "epoch 173| loss: 0.31971 |  0:00:08s\n",
      "epoch 174| loss: 0.31975 |  0:00:08s\n",
      "epoch 175| loss: 0.30903 |  0:00:08s\n",
      "epoch 176| loss: 0.31415 |  0:00:08s\n",
      "epoch 177| loss: 0.32472 |  0:00:08s\n",
      "epoch 178| loss: 0.30966 |  0:00:08s\n",
      "epoch 179| loss: 0.31315 |  0:00:08s\n",
      "epoch 180| loss: 0.31162 |  0:00:08s\n",
      "epoch 181| loss: 0.32061 |  0:00:08s\n",
      "epoch 182| loss: 0.31373 |  0:00:08s\n",
      "epoch 183| loss: 0.31775 |  0:00:08s\n",
      "epoch 184| loss: 0.31597 |  0:00:08s\n",
      "epoch 185| loss: 0.3111  |  0:00:08s\n",
      "epoch 186| loss: 0.31505 |  0:00:08s\n",
      "epoch 187| loss: 0.30957 |  0:00:08s\n",
      "epoch 188| loss: 0.31093 |  0:00:08s\n",
      "epoch 189| loss: 0.31486 |  0:00:08s\n",
      "epoch 190| loss: 0.31301 |  0:00:08s\n",
      "epoch 191| loss: 0.3108  |  0:00:08s\n",
      "epoch 192| loss: 0.30916 |  0:00:08s\n",
      "epoch 193| loss: 0.30084 |  0:00:09s\n",
      "epoch 194| loss: 0.30258 |  0:00:09s\n",
      "epoch 195| loss: 0.30658 |  0:00:09s\n",
      "epoch 196| loss: 0.30231 |  0:00:09s\n",
      "epoch 197| loss: 0.29887 |  0:00:09s\n",
      "epoch 198| loss: 0.3017  |  0:00:09s\n",
      "epoch 199| loss: 0.2996  |  0:00:09s\n",
      "epoch 200| loss: 0.29747 |  0:00:09s\n",
      "epoch 201| loss: 0.302   |  0:00:09s\n",
      "epoch 202| loss: 0.29356 |  0:00:09s\n",
      "epoch 203| loss: 0.29971 |  0:00:09s\n",
      "epoch 204| loss: 0.28496 |  0:00:09s\n",
      "epoch 205| loss: 0.28985 |  0:00:09s\n",
      "epoch 206| loss: 0.28443 |  0:00:09s\n",
      "epoch 207| loss: 0.28011 |  0:00:09s\n",
      "epoch 208| loss: 0.28006 |  0:00:09s\n",
      "epoch 209| loss: 0.27741 |  0:00:09s\n",
      "epoch 210| loss: 0.27276 |  0:00:09s\n",
      "epoch 211| loss: 0.27628 |  0:00:09s\n",
      "epoch 212| loss: 0.2841  |  0:00:09s\n",
      "epoch 213| loss: 0.28351 |  0:00:09s\n",
      "epoch 214| loss: 0.28181 |  0:00:10s\n",
      "epoch 215| loss: 0.29372 |  0:00:10s\n",
      "epoch 216| loss: 0.27954 |  0:00:10s\n",
      "epoch 217| loss: 0.28504 |  0:00:10s\n",
      "epoch 218| loss: 0.281   |  0:00:10s\n",
      "epoch 219| loss: 0.28408 |  0:00:10s\n",
      "epoch 220| loss: 0.28229 |  0:00:10s\n",
      "epoch 221| loss: 0.27346 |  0:00:10s\n",
      "epoch 222| loss: 0.26955 |  0:00:10s\n",
      "epoch 223| loss: 0.27341 |  0:00:10s\n",
      "epoch 224| loss: 0.27228 |  0:00:10s\n",
      "epoch 225| loss: 0.26034 |  0:00:10s\n",
      "epoch 226| loss: 0.26504 |  0:00:10s\n",
      "epoch 227| loss: 0.26373 |  0:00:10s\n",
      "epoch 228| loss: 0.25678 |  0:00:10s\n",
      "epoch 229| loss: 0.2667  |  0:00:10s\n",
      "epoch 230| loss: 0.25775 |  0:00:10s\n",
      "epoch 231| loss: 0.26326 |  0:00:10s\n",
      "epoch 232| loss: 0.25435 |  0:00:10s\n",
      "epoch 233| loss: 0.25562 |  0:00:10s\n",
      "epoch 234| loss: 0.26014 |  0:00:10s\n",
      "epoch 235| loss: 0.2552  |  0:00:11s\n",
      "epoch 236| loss: 0.25414 |  0:00:11s\n",
      "epoch 237| loss: 0.25566 |  0:00:11s\n",
      "epoch 238| loss: 0.25705 |  0:00:11s\n",
      "epoch 239| loss: 0.25846 |  0:00:11s\n",
      "epoch 240| loss: 0.25685 |  0:00:11s\n",
      "epoch 241| loss: 0.26753 |  0:00:11s\n",
      "epoch 242| loss: 0.27538 |  0:00:11s\n",
      "epoch 243| loss: 0.27838 |  0:00:11s\n",
      "epoch 244| loss: 0.27072 |  0:00:11s\n",
      "epoch 245| loss: 0.26688 |  0:00:11s\n",
      "epoch 246| loss: 0.26624 |  0:00:11s\n",
      "epoch 247| loss: 0.26438 |  0:00:11s\n",
      "epoch 248| loss: 0.26376 |  0:00:11s\n",
      "epoch 249| loss: 0.26156 |  0:00:11s\n",
      "PLSRegression_4_components\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "GatedAdditiveTreeEnsembleConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:32</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">737</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:32\u001b[0m,\u001b[1;36m737\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:32</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">748</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:32\u001b[0m,\u001b[1;36m748\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:32</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">752</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:32\u001b[0m,\u001b[1;36m752\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:32</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">769</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:32\u001b[0m,\u001b[1;36m769\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:32</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">967</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:32\u001b[0m,\u001b[1;36m967\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:32</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">980</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:32\u001b[0m,\u001b[1;36m980\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:32</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">989</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:32\u001b[0m,\u001b[1;36m989\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b631ceda52f4b5fb28776c3d0a423ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_876daee4-7a82-43e9-bdf1-09457822dd6f.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_876daee4-7a82-43e9-bdf1-09457822dd6f.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:34</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">791</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:34\u001b[0m,\u001b[1;36m791\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:34</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">798</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:34\u001b[0m,\u001b[1;36m798\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:34</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">811</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:34\u001b[0m,\u001b[1;36m811\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                       | Params | Mode \n",
      "------------------------------------------------------------------------\n",
      "0 | _backbone        | GatedAdditiveTreesBackbone | 2.1 M  | train\n",
      "1 | _embedding_layer | Embedding1dLayer           | 400    | train\n",
      "2 | _head            | CustomHead                 | 156    | train\n",
      "3 | loss             | MSELoss                    | 0      | train\n",
      "------------------------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.417     Total estimated model params size (MB)\n",
      "689       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae2f578894f4003bde8714a18188f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d562253a40cf410ab4fcc64ff7b62496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6953239035bf4c1188dee0c62e1b3ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">888</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:36\u001b[0m,\u001b[1;36m888\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">889</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:36\u001b[0m,\u001b[1;36m889\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "DANetConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">602</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:37\u001b[0m,\u001b[1;36m602\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">613</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:37\u001b[0m,\u001b[1;36m613\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">617</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:37\u001b[0m,\u001b[1;36m617\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">633</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: DANetModel             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:37\u001b[0m,\u001b[1;36m633\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: DANetModel             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">660</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:37\u001b[0m,\u001b[1;36m660\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">668</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:37\u001b[0m,\u001b[1;36m668\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe2954ccd4348c09219348a73035c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_c560011d-ab92-480d-9966-d1c4118aa1cf.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_c560011d-ab92-480d-9966-d1c4118aa1cf.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">978</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:37\u001b[0m,\u001b[1;36m978\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">984</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:37\u001b[0m,\u001b[1;36m984\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | _backbone        | DANetBackbone    | 1.4 M  | train\n",
      "1 | _embedding_layer | Embedding1dLayer | 400    | train\n",
      "2 | _head            | LinearHead       | 260    | train\n",
      "3 | loss             | MSELoss          | 0      | train\n",
      "--------------------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.787     Total estimated model params size (MB)\n",
      "156       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b7b3579bcf45aebca0cbc243030240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ddd1e09d64a4a8a8958c675d277c6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656b80ac3a6941218cfbf40654137c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">321</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:38\u001b[0m,\u001b[1;36m321\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">322</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:38\u001b[0m,\u001b[1;36m322\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "TabTransformerConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">518</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:38\u001b[0m,\u001b[1;36m518\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">529</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:38\u001b[0m,\u001b[1;36m529\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">533</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:38\u001b[0m,\u001b[1;36m533\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabTransformerModel    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:38\u001b[0m,\u001b[1;36m548\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabTransformerModel    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">568</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:38\u001b[0m,\u001b[1;36m568\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">576</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:38\u001b[0m,\u001b[1;36m576\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f1a1509407415d9abc8765a5155eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_ad7a3db4-4aa1-4406-8b59-d38918fcbb27.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_ad7a3db4-4aa1-4406-8b59-d38918fcbb27.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">701</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:38\u001b[0m,\u001b[1;36m701\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">703</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:38\u001b[0m,\u001b[1;36m703\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                   | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | _backbone        | TabTransformerBackbone | 271 K  | train\n",
      "1 | _embedding_layer | Embedding2dLayer       | 0      | train\n",
      "2 | _head            | LinearHead             | 804    | train\n",
      "3 | loss             | MSELoss                | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "272 K     Trainable params\n",
      "0         Non-trainable params\n",
      "272 K     Total params\n",
      "1.090     Total estimated model params size (MB)\n",
      "119       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeaa53e2320c4b48801f288d7ee597c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e051f34025349c9b971f78950adfe3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f58c710fb641a4b7e480c2d9d0bd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">829</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:38\u001b[0m,\u001b[1;36m829\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">830</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:38\u001b[0m,\u001b[1;36m830\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "TabNetModelConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">015</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:39\u001b[0m,\u001b[1;36m015\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">026</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:39\u001b[0m,\u001b[1;36m026\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">030</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:39\u001b[0m,\u001b[1;36m030\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">045</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:39\u001b[0m,\u001b[1;36m045\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">065</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:39\u001b[0m,\u001b[1;36m065\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">072</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:39\u001b[0m,\u001b[1;36m072\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673b1656c8ea4834a2d0c70236ba6134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_6d918c7a-39f4-4830-bad2-2d33363bf743.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_6d918c7a-39f4-4830-bad2-2d33363bf743.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:39\u001b[0m,\u001b[1;36m265\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">267</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:39\u001b[0m,\u001b[1;36m267\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type           | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | _embedding_layer | Identity       | 0      | train\n",
      "1 | _backbone        | TabNetBackbone | 18.9 K | train\n",
      "2 | _head            | Identity       | 0      | train\n",
      "3 | loss             | MSELoss        | 0      | train\n",
      "------------------------------------------------------------\n",
      "18.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "18.9 K    Total params\n",
      "0.075     Total estimated model params size (MB)\n",
      "107       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cd39b81f5f45b686c3ef6f6934ec93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e29e049709340a4a8d78d98a6205d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6dd146feab94759a5f6c4dc31ccc967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">465</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:39\u001b[0m,\u001b[1;36m465\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">12:22:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">466</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m12:22:39\u001b[0m,\u001b[1;36m466\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'GatedAdditiveTreeEnsembleConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'DANetConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabTransformerConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetModelConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'GatedAdditiveTreeEnsembleConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'DANetConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabTransformerConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetModelConfig_tab', (2881, 200), (13, 200)])\n"
     ]
    }
   ],
   "source": [
    "predictive_models_list += [\n",
    "    (\"GatedAdditiveTreeEnsembleConfig_tab\", \n",
    "    TabularModelWrapper(\n",
    "        GatedAdditiveTreeEnsembleConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        gflu_stages=6,\n",
    "        gflu_dropout=0.0,\n",
    "        tree_depth=5,\n",
    "        num_trees=20,\n",
    "        chain_trees=False,\n",
    "        share_head_weights=True), data_config, trainer_config, optimizer_config \n",
    "    )),\n",
    "    (\"DANetConfig_tab\",\n",
    "    TabularModelWrapper(\n",
    "        DANetConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_layers=8,\n",
    "        k=5,\n",
    "        dropout_rate=0.1), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabTransformerConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        embedding_initialization=\"kaiming_uniform\",\n",
    "        embedding_bias=False), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    # (\"FTTransformerConfig\",\n",
    "    #     TabularModelWrapper(\n",
    "    #     FTTransformerConfig(\n",
    "    #     task=\"regression\",\n",
    "    #     head=\"LinearHead\",\n",
    "    #     head_config=head_config), data_config, trainer_config, optimizer_config\n",
    "    # )),\n",
    "    (\"TabNetModelConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        n_independent=2,\n",
    "        n_shared=2), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "\n",
    "    if any(result['params'] == params for result in all_dict_results):\n",
    "        # Skip this iteration if the combination exists\n",
    "        print(f\"Skipping existing combination: {params.values()}\")\n",
    "        \n",
    "        continue\n",
    "\n",
    "    try: \n",
    "        print(name_model)\n",
    "        \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "        # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'LinearRegression',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.2466392517089844,\n",
       "  'fitting_time': 0.22939062118530273,\n",
       "  'results_adj': {'mse_score': array([0.65983176, 0.48974311, 0.40951464, 0.56684282]),\n",
       "   'mae_score': array([0.63965764, 0.55144938, 0.56535002, 0.59999667]),\n",
       "   'r2': array([0.33312259, 0.44426866, 0.144955  , 0.31048062]),\n",
       "   'explained_variance': array([0.36730884, 0.47760179, 0.14545957, 0.43985495]),\n",
       "   'corr': array([0.60633336, 0.69220668, 0.4020632 , 0.66605104])},\n",
       "  'results_org': {'mse_score': array([0.65983176, 0.4897431 , 0.40951466, 0.56684279]),\n",
       "   'mae_score': array([0.63965764, 0.55144938, 0.56535003, 0.59999666]),\n",
       "   'r2': array([0.29810904, 0.45135258, 0.20239561, 0.33725209]),\n",
       "   'explained_variance': array([0.33409019, 0.48426081, 0.20286628, 0.46160331]),\n",
       "   'corr': array([0.57814397, 0.69816204, 0.46213839, 0.68191102])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskElasticNet',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.283444404602051,\n",
       "  'fitting_time': 0.09822368621826172,\n",
       "  'results_adj': {'mse_score': array([1.18012248, 0.95041535, 0.50741149, 1.02939714]),\n",
       "   'mae_score': array([0.88414168, 0.78683573, 0.65378091, 0.82689555]),\n",
       "   'r2': array([-0.19272377, -0.07847478, -0.05944846, -0.25218006]),\n",
       "   'explained_variance': array([ 0.11229868,  0.03189858, -0.05332468,  0.12857136]),\n",
       "   'corr': array([ 0.42176385,  0.17864276, -0.16306812,  0.44631446])},\n",
       "  'results_org': {'mse_score': array([1.18012249, 0.95041535, 0.50741151, 1.02939711]),\n",
       "   'mae_score': array([0.88414168, 0.78683573, 0.65378092, 0.82689554]),\n",
       "   'r2': array([-0.25534622, -0.06472746,  0.01172365, -0.20356259]),\n",
       "   'explained_variance': array([0.06569104, 0.04423898, 0.01743605, 0.16240568]),\n",
       "   'corr': array([0.25863755, 0.21083052, 0.14561963, 0.49623359])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskElasticNet_tuned',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.2664968967437744,\n",
       "  'fitting_time': 1.4926152229309082,\n",
       "  'results_adj': {'mse_score': array([0.66791342, 0.47856617, 0.41412878, 0.59264981]),\n",
       "   'mae_score': array([0.63043587, 0.55392437, 0.58019171, 0.59956285]),\n",
       "   'r2': array([0.32495464, 0.45695159, 0.13532092, 0.27908847]),\n",
       "   'explained_variance': array([0.3700506 , 0.49637797, 0.13598028, 0.44010284]),\n",
       "   'corr': array([0.60831821, 0.70457467, 0.38152442, 0.66946516])},\n",
       "  'results_org': {'mse_score': array([0.66791342, 0.47856617, 0.4141288 , 0.59264978]),\n",
       "   'mae_score': array([0.63043586, 0.55392437, 0.58019172, 0.59956284]),\n",
       "   'r2': array([0.28951224, 0.46387383, 0.19340873, 0.30707878]),\n",
       "   'explained_variance': array([0.33697591, 0.50279765, 0.19402379, 0.46184157]),\n",
       "   'corr': array([0.58049709, 0.70936568, 0.44690586, 0.68551862])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskLasso',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.283153772354126,\n",
       "  'fitting_time': 0.07776927947998047,\n",
       "  'results_adj': {'mse_score': array([1.39363767, 1.04056339, 0.48866937, 1.24000841]),\n",
       "   'mae_score': array([0.99127575, 0.78457157, 0.62110666, 0.93446712]),\n",
       "   'r2': array([-0.40851885, -0.18076941, -0.0203159 , -0.50837198]),\n",
       "   'explained_variance': array([-2.22044605e-16,  0.00000000e+00,  0.00000000e+00,  1.11022302e-16]),\n",
       "   'corr': array([nan, nan, nan, nan])},\n",
       "  'results_org': {'mse_score': array([1.39363768, 1.04056339, 0.48866939, 1.24000838]),\n",
       "   'mae_score': array([0.99127576, 0.78457157, 0.62110667, 0.93446711]),\n",
       "   'r2': array([-0.48247136, -0.16571814,  0.04822734, -0.44980754]),\n",
       "   'explained_variance': array([-0.05250375,  0.01274701,  0.06717846,  0.03882626]),\n",
       "   'corr': array([-0.0848666 ,  0.11455326,  0.2721653 ,  0.20679355])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskLasso_tuned',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.3114430904388428,\n",
       "  'fitting_time': 1.4922456741333008,\n",
       "  'results_adj': {'mse_score': array([0.6555347 , 0.47602153, 0.41898635, 0.58247095]),\n",
       "   'mae_score': array([0.62193632, 0.55058805, 0.57780272, 0.59499437]),\n",
       "   'r2': array([0.33746553, 0.45983909, 0.12517858, 0.29147024]),\n",
       "   'explained_variance': array([0.38303248, 0.49996522, 0.12605199, 0.44788336]),\n",
       "   'corr': array([0.61892446, 0.70715313, 0.37654024, 0.67491708])},\n",
       "  'results_org': {'mse_score': array([0.6555347 , 0.47602153, 0.41898636, 0.58247092]),\n",
       "   'mae_score': array([0.62193631, 0.55058805, 0.57780274, 0.59499436]),\n",
       "   'r2': array([0.30268   , 0.46672453, 0.18394773, 0.31897981]),\n",
       "   'explained_variance': array([0.35063939, 0.50633917, 0.18476246, 0.46932   ]),\n",
       "   'corr': array([0.59223356, 0.71197064, 0.44079738, 0.69050252])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'RandomForestRegressor',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.289330244064331,\n",
       "  'fitting_time': 51.79721212387085,\n",
       "  'results_adj': {'mse_score': array([0.81204962, 0.55738872, 0.42447239, 0.76265381]),\n",
       "   'mae_score': array([0.66916949, 0.63959102, 0.60835909, 0.64774379]),\n",
       "   'r2': array([0.17927936, 0.36750845, 0.11372402, 0.07229206]),\n",
       "   'explained_variance': array([0.38575368, 0.43668934, 0.11735442, 0.37009751]),\n",
       "   'corr': array([0.63471399, 0.67665811, 0.34287971, 0.62716925])},\n",
       "  'results_org': {'mse_score': array([0.81204962, 0.55738872, 0.42447241, 0.76265379]),\n",
       "   'mae_score': array([0.6691695 , 0.63959102, 0.6083591 , 0.64774379]),\n",
       "   'r2': array([0.13618846, 0.37557082, 0.17326266, 0.10831149]),\n",
       "   'explained_variance': array([0.35350346, 0.44386986, 0.17664918, 0.39455427]),\n",
       "   'corr': array([0.60552878, 0.68293217, 0.42214619, 0.66388455])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'PLSRegression_4_components',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.3004002571105957,\n",
       "  'fitting_time': 0.15016818046569824,\n",
       "  'results_adj': {'mse_score': array([0.99283749, 0.68320591, 0.51129344, 0.88042711]),\n",
       "   'mae_score': array([0.8049879 , 0.73842664, 0.68894711, 0.76406833]),\n",
       "   'r2': array([-0.00343896,  0.22473859, -0.06755375, -0.07096982]),\n",
       "   'explained_variance': array([ 0.25778331,  0.29950329, -0.06753554,  0.2736962 ]),\n",
       "   'corr': array([0.50966098, 0.55561882, 0.0742507 , 0.52674345])},\n",
       "  'results_org': {'mse_score': array([0.99283749, 0.68320591, 0.51129345, 0.88042708]),\n",
       "   'mae_score': array([0.80498791, 0.73842664, 0.68894712, 0.76406832]),\n",
       "   'r2': array([-0.05612325,  0.23462085,  0.00416286, -0.02938806]),\n",
       "   'explained_variance': array([0.21881416, 0.30843253, 0.00417985, 0.30189586]),\n",
       "   'corr': array([0.46922857, 0.57093376, 0.17716915, 0.57292837])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'TabNetRegressor_default',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.3759474754333496,\n",
       "  'fitting_time': 13.77210783958435,\n",
       "  'results_adj': {'mse_score': array([0.61824897, 0.61184618, 0.36866036, 0.57763236]),\n",
       "   'mae_score': array([0.69107406, 0.60773177, 0.5361999 , 0.59462053]),\n",
       "   'r2': array([0.3751494 , 0.30571336, 0.23025659, 0.297356  ]),\n",
       "   'explained_variance': array([0.56567234, 0.38915553, 0.23426406, 0.55584824]),\n",
       "   'corr': array([0.77300748, 0.67628222, 0.58077844, 0.81923406])},\n",
       "  'results_org': {'mse_score': array([0.61824897, 0.61184618, 0.36866037, 0.57763234]),\n",
       "   'mae_score': array([0.69107407, 0.60773177, 0.53619991, 0.59462053]),\n",
       "   'r2': array([0.34234241, 0.31456344, 0.28196676, 0.32463705]),\n",
       "   'explained_variance': array([0.54286851, 0.39694198, 0.28570502, 0.57309301]),\n",
       "   'corr': array([0.76293555, 0.67836772, 0.73113403, 0.85040194])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'TabNetRegressor_custom',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.4543991088867188,\n",
       "  'fitting_time': 13.39520812034607,\n",
       "  'results_adj': {'mse_score': array([1.07617402, 1.31252905, 0.77527955, 1.0442195 ]),\n",
       "   'mae_score': array([0.76465789, 0.96862147, 0.76452422, 0.76252167]),\n",
       "   'r2': array([-0.08766535, -0.48937986, -0.61874284, -0.27021028]),\n",
       "   'explained_variance': array([ 0.20827438, -0.34330108, -0.60463007,  0.30404738]),\n",
       "   'corr': array([ 0.54009087,  0.30700851, -0.06850123,  0.58642515])},\n",
       "  'results_org': {'mse_score': array([1.07617403, 1.31252906, 0.77527957, 1.04421947]),\n",
       "   'mae_score': array([0.76465789, 0.96862147, 0.76452423, 0.76252167]),\n",
       "   'r2': array([-0.14477185, -0.47039474, -0.5099982 , -0.22089277]),\n",
       "   'explained_variance': array([ 0.16670582, -0.32617802, -0.4968335 ,  0.33106863]),\n",
       "   'corr': array([0.50999118, 0.31151104, 0.0452715 , 0.59746408])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'LinearRegression',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 0.09009456634521484,\n",
       "  'results_adj': {'mse_score': array([1.11192349, 0.67668626, 0.61999815, 0.97750844]),\n",
       "   'mae_score': array([0.72849831, 0.70235973, 0.70986192, 0.71215901]),\n",
       "   'r2': array([-0.12379655,  0.2321367 , -0.29452346, -0.18906157]),\n",
       "   'explained_variance': array([ 0.02475189,  0.28845771, -0.29319881,  0.10973059]),\n",
       "   'corr': array([0.33204585, 0.54765514, 0.01649677, 0.38058137])},\n",
       "  'results_org': {'mse_score': array([1.11192349, 0.67668626, 0.61999817, 0.97750843]),\n",
       "   'mae_score': array([0.72849832, 0.70235973, 0.70986193, 0.712159  ]),\n",
       "   'r2': array([-0.18280006,  0.24192465, -0.20755938, -0.14289477]),\n",
       "   'explained_variance': array([-0.02645227,  0.29752774, -0.20632372,  0.14429641]),\n",
       "   'corr': array([0.27766053, 0.55012684, 0.07284141, 0.39622517])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'MultiTaskElasticNet',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 0.01528787612915039,\n",
       "  'results_adj': {'mse_score': array([1.18012247, 0.95041534, 0.50741149, 1.02939713]),\n",
       "   'mae_score': array([0.88414167, 0.78683572, 0.65378091, 0.82689555]),\n",
       "   'r2': array([-0.19272375, -0.07847477, -0.05944845, -0.25218005]),\n",
       "   'explained_variance': array([ 0.11229868,  0.03189858, -0.05332468,  0.12857136]),\n",
       "   'corr': array([ 0.42176386,  0.17864277, -0.16306808,  0.44631445])},\n",
       "  'results_org': {'mse_score': array([1.18012248, 0.95041534, 0.50741151, 1.02939711]),\n",
       "   'mae_score': array([0.88414168, 0.78683573, 0.65378092, 0.82689554]),\n",
       "   'r2': array([-0.25534621, -0.06472745,  0.01172366, -0.20356258]),\n",
       "   'explained_variance': array([0.06569104, 0.04423898, 0.01743605, 0.16240568]),\n",
       "   'corr': array([0.25863755, 0.21083053, 0.14561964, 0.49623358])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'MultiTaskElasticNet_tuned',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 0.8313999176025391,\n",
       "  'results_adj': {'mse_score': array([1.09767924, 0.67163602, 0.61727141, 0.96027028]),\n",
       "   'mae_score': array([0.72712752, 0.70316248, 0.71357071, 0.70813001]),\n",
       "   'r2': array([-0.1094002 ,  0.23786741, -0.28883017, -0.16809271]),\n",
       "   'explained_variance': array([ 0.04259982,  0.29559122, -0.28782174,  0.12798901]),\n",
       "   'corr': array([0.33826662, 0.55120885, 0.00953623, 0.39231636])},\n",
       "  'results_org': {'mse_score': array([1.09767923, 0.67163602, 0.61727143, 0.96027027]),\n",
       "   'mae_score': array([0.72712752, 0.70316248, 0.71357072, 0.70813   ]),\n",
       "   'r2': array([-0.16764784,  0.24758231, -0.20224855, -0.12274005]),\n",
       "   'explained_variance': array([-0.00766726,  0.30457033, -0.20130787,  0.16184592]),\n",
       "   'corr': array([0.28384559, 0.55450481, 0.06742918, 0.41077564])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'MultiTaskLasso',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 0.007750749588012695,\n",
       "  'results_adj': {'mse_score': array([1.39363767, 1.04056339, 0.48866937, 1.24000841]),\n",
       "   'mae_score': array([0.99127575, 0.78457157, 0.62110666, 0.93446712]),\n",
       "   'r2': array([-0.40851885, -0.18076941, -0.0203159 , -0.50837198]),\n",
       "   'explained_variance': array([-2.22044605e-16,  0.00000000e+00,  0.00000000e+00,  1.11022302e-16]),\n",
       "   'corr': array([nan, nan, nan, nan])},\n",
       "  'results_org': {'mse_score': array([1.39363768, 1.04056339, 0.48866939, 1.24000838]),\n",
       "   'mae_score': array([0.99127576, 0.78457157, 0.62110667, 0.93446711]),\n",
       "   'r2': array([-0.48247136, -0.16571814,  0.04822734, -0.44980754]),\n",
       "   'explained_variance': array([-0.05250375,  0.01274701,  0.06717846,  0.03882626]),\n",
       "   'corr': array([-0.0848666 ,  0.11455326,  0.2721653 ,  0.20679355])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'MultiTaskLasso_tuned',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 0.8366336822509766,\n",
       "  'results_adj': {'mse_score': array([1.09986228, 0.67700338, 0.61721354, 0.96322248]),\n",
       "   'mae_score': array([0.72439764, 0.70502407, 0.71262079, 0.70634911]),\n",
       "   'r2': array([-0.11160655,  0.23177686, -0.28870934, -0.17168383]),\n",
       "   'explained_variance': array([ 0.04182356,  0.29179378, -0.28783433,  0.1250568 ]),\n",
       "   'corr': array([0.34030126, 0.54948447, 0.01369735, 0.39123818])},\n",
       "  'results_org': {'mse_score': array([1.09986228, 0.67700337, 0.61721356, 0.96322247]),\n",
       "   'mae_score': array([0.72439764, 0.70502408, 0.71262081, 0.7063491 ]),\n",
       "   'r2': array([-0.16997004,  0.2415694 , -0.20213584, -0.12619174]),\n",
       "   'explained_variance': array([-0.00848428,  0.30082129, -0.20131962,  0.15902757]),\n",
       "   'corr': array([0.28608858, 0.55225247, 0.07079544, 0.40890542])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'RandomForestRegressor',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 31.880318641662598,\n",
       "  'results_adj': {'mse_score': array([1.00638713, 0.65012684, 0.47869162, 0.82220865]),\n",
       "   'mae_score': array([0.75528169, 0.6841987 , 0.63512616, 0.63151397]),\n",
       "   'r2': array([-1.71332791e-02,  2.62274753e-01,  5.17132936e-04, -1.51675905e-04]),\n",
       "   'explained_variance': array([0.29209145, 0.405549  , 0.02445608, 0.37078241]),\n",
       "   'corr': array([0.54046621, 0.64364297, 0.21686966, 0.60995505])},\n",
       "  'results_org': {'mse_score': array([1.00638713, 0.65012684, 0.47869164, 0.82220864]),\n",
       "   'mae_score': array([0.7552817 , 0.6841987 , 0.63512617, 0.63151397]),\n",
       "   'r2': array([-0.07053657,  0.27167853,  0.06766084,  0.03868046]),\n",
       "   'explained_variance': array([0.25492361, 0.41312647, 0.08999161, 0.39521257]),\n",
       "   'corr': array([0.50511182, 0.65009756, 0.3086371 , 0.63522421])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'XGBoostRegressor',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 0.8378174304962158,\n",
       "  'results_adj': {'mse_score': array([0.99729777, 0.55285888, 0.49806901, 0.7462314 ]),\n",
       "   'mae_score': array([0.75445592, 0.63451078, 0.64294552, 0.64429738]),\n",
       "   'r2': array([-0.00794687,  0.37264865, -0.03994184,  0.09226863]),\n",
       "   'explained_variance': array([ 0.26511088,  0.47604274, -0.03447538,  0.31643386]),\n",
       "   'corr': array([0.52174936, 0.69849083, 0.21172288, 0.56606   ])},\n",
       "  'results_org': {'mse_score': array([0.99729777, 0.55285888, 0.49806903, 0.74623138]),\n",
       "   'mae_score': array([0.75445593, 0.63451079, 0.64294554, 0.64429738]),\n",
       "   'r2': array([-0.06086784,  0.38064549,  0.02991984,  0.12751245]),\n",
       "   'explained_variance': array([0.22652646, 0.48272162, 0.03501907, 0.34297417]),\n",
       "   'corr': array([0.48966434, 0.70257856, 0.27320828, 0.58583766])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'XGBoostRegressor_tuned',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 1.9914910793304443,\n",
       "  'results_adj': {'mse_score': array([1.0153611 , 0.63851973, 0.46345103, 0.70749896]),\n",
       "   'mae_score': array([0.73457703, 0.71458275, 0.63021056, 0.61263546]),\n",
       "   'r2': array([-0.02620307,  0.27544581,  0.03233868,  0.13938357]),\n",
       "   'explained_variance': array([0.22165364, 0.34897739, 0.05887258, 0.43032201]),\n",
       "   'corr': array([0.48104477, 0.59075482, 0.30682341, 0.65598979])},\n",
       "  'results_org': {'mse_score': array([1.0153611 , 0.63851973, 0.46345105, 0.70749894]),\n",
       "   'mae_score': array([0.73457704, 0.71458276, 0.63021057, 0.61263545]),\n",
       "   'r2': array([-0.08008256,  0.2846817 ,  0.09734466,  0.17279809]),\n",
       "   'explained_variance': array([0.18078756, 0.35727599, 0.12209605, 0.45244048]),\n",
       "   'corr': array([0.44678191, 0.59775758, 0.36862204, 0.67323253])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'TabNetRegressor_default',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 11.830575942993164,\n",
       "  'results_adj': {'mse_score': array([1.00024251, 0.85216095, 0.6231349 , 0.78732824]),\n",
       "   'mae_score': array([0.78064659, 0.8013238 , 0.6795552 , 0.67213814]),\n",
       "   'r2': array([-0.01092304,  0.03301847, -0.30107284,  0.04227758]),\n",
       "   'explained_variance': array([ 0.38275983,  0.13551578, -0.29939216,  0.5247155 ]),\n",
       "   'corr': array([0.62946614, 0.47112118, 0.06684408, 0.73037268])},\n",
       "  'results_org': {'mse_score': array([1.0002425 , 0.85216095, 0.62313492, 0.78732822]),\n",
       "   'mae_score': array([0.78064659, 0.8013238 , 0.67955522, 0.67213815]),\n",
       "   'r2': array([-0.06400027,  0.04534457, -0.21366878,  0.07946235]),\n",
       "   'explained_variance': array([ 0.35035243,  0.14653537, -0.212101  ,  0.54316902]),\n",
       "   'corr': array([0.60852724, 0.4852577 , 0.12261516, 0.74095112])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'TabNetRegressor_custom',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 11.706139326095581,\n",
       "  'results_adj': {'mse_score': array([0.92610036, 0.76428738, 0.61165174, 0.87483944]),\n",
       "   'mae_score': array([0.71338154, 0.75841494, 0.67114785, 0.72399585]),\n",
       "   'r2': array([ 0.06401079,  0.13273216, -0.2770966 , -0.06417287]),\n",
       "   'explained_variance': array([ 0.22209961,  0.26101411, -0.27432143,  0.26805491]),\n",
       "   'corr': array([0.55329131, 0.58084529, 0.23700988, 0.59708229])},\n",
       "  'results_org': {'mse_score': array([0.92610035, 0.76428739, 0.61165176, 0.87483943]),\n",
       "   'mae_score': array([0.71338154, 0.75841494, 0.67114786, 0.72399584]),\n",
       "   'r2': array([ 0.01486787,  0.14378722, -0.19130323, -0.02285503]),\n",
       "   'explained_variance': array([ 0.18125695,  0.27043396, -0.1887145 ,  0.2964736 ]),\n",
       "   'corr': array([0.53795846, 0.59013198, 0.27340583, 0.5986342 ])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'PLSRegression_4_components',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 0.050071001052856445,\n",
       "  'results_adj': {'mse_score': array([1.08140908, 0.78765117, 0.5694762 , 0.94630352]),\n",
       "   'mae_score': array([0.79248414, 0.79068015, 0.72178585, 0.73452485]),\n",
       "   'r2': array([-0.09295631,  0.10622033, -0.1890363 , -0.15110326]),\n",
       "   'explained_variance': array([ 0.1855152 ,  0.19584409, -0.18846787,  0.20844666]),\n",
       "   'corr': array([ 0.43286088,  0.44542341, -0.05356343,  0.45740887])},\n",
       "  'results_org': {'mse_score': array([1.08140909, 0.78765116, 0.56947622, 0.9463035 ]),\n",
       "   'mae_score': array([0.79248415, 0.79068015, 0.72178586, 0.73452484]),\n",
       "   'r2': array([-0.1503406 ,  0.11761334, -0.10915869, -0.10641023]),\n",
       "   'explained_variance': array([ 0.14275171,  0.20609467, -0.10862844,  0.23917972]),\n",
       "   'corr': array([0.3812462 , 0.45398549, 0.03688166, 0.49549602])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'GatedAdditiveTreeEnsembleConfig_tab',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': None,\n",
       "  'results_adj': None,\n",
       "  'results_org': None},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'DANetConfig_tab',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': None,\n",
       "  'results_adj': None,\n",
       "  'results_org': None},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'TabTransformerConfig_tab',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': None,\n",
       "  'results_adj': None,\n",
       "  'results_org': None},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'TabNetModelConfig_tab',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': None,\n",
       "  'results_adj': None,\n",
       "  'results_org': None}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>imputation_time</th>\n",
       "      <th>fitting_time</th>\n",
       "      <th>results_adj</th>\n",
       "      <th>results_org</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'ordinal_imputer': 'SimpleImputer_most_freque...</td>\n",
       "      <td>3.246639</td>\n",
       "      <td>0.229391</td>\n",
       "      <td>{'mse_score': [0.659831758923781, 0.4897431123...</td>\n",
       "      <td>{'mse_score': [0.6598317562332855, 0.489743104...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'ordinal_imputer': 'SimpleImputer_most_freque...</td>\n",
       "      <td>3.283444</td>\n",
       "      <td>0.098224</td>\n",
       "      <td>{'mse_score': [1.1801224802168766, 0.950415349...</td>\n",
       "      <td>{'mse_score': [1.1801224892206006, 0.950415348...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'ordinal_imputer': 'SimpleImputer_most_freque...</td>\n",
       "      <td>3.266497</td>\n",
       "      <td>1.492615</td>\n",
       "      <td>{'mse_score': [0.6679134194781107, 0.478566173...</td>\n",
       "      <td>{'mse_score': [0.6679134155298025, 0.478566167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'ordinal_imputer': 'SimpleImputer_most_freque...</td>\n",
       "      <td>3.283154</td>\n",
       "      <td>0.077769</td>\n",
       "      <td>{'mse_score': [1.3936376654364884, 1.040563386...</td>\n",
       "      <td>{'mse_score': [1.39363767778841, 1.04056338628...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'ordinal_imputer': 'SimpleImputer_most_freque...</td>\n",
       "      <td>3.311443</td>\n",
       "      <td>1.492246</td>\n",
       "      <td>{'mse_score': [0.6555347035833681, 0.476021533...</td>\n",
       "      <td>{'mse_score': [0.655534699980677, 0.4760215269...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'ordinal_imputer': 'SimpleImputer_most_freque...</td>\n",
       "      <td>3.289330</td>\n",
       "      <td>51.797212</td>\n",
       "      <td>{'mse_score': [0.8120496178211589, 0.557388719...</td>\n",
       "      <td>{'mse_score': [0.8120496171306019, 0.557388722...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'ordinal_imputer': 'SimpleImputer_most_freque...</td>\n",
       "      <td>3.300400</td>\n",
       "      <td>0.150168</td>\n",
       "      <td>{'mse_score': [0.9928374904980006, 0.683205912...</td>\n",
       "      <td>{'mse_score': [0.9928374937104268, 0.683205911...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'ordinal_imputer': 'SimpleImputer_most_freque...</td>\n",
       "      <td>3.375947</td>\n",
       "      <td>13.772108</td>\n",
       "      <td>{'mse_score': [0.6182489690113672, 0.611846183...</td>\n",
       "      <td>{'mse_score': [0.6182489748896371, 0.611846177...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'ordinal_imputer': 'SimpleImputer_most_freque...</td>\n",
       "      <td>3.454399</td>\n",
       "      <td>13.395208</td>\n",
       "      <td>{'mse_score': [1.0761740236083321, 1.312529050...</td>\n",
       "      <td>{'mse_score': [1.076174034182812, 1.3125290559...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.090095</td>\n",
       "      <td>{'mse_score': [1.1119234898099377, 0.676686264...</td>\n",
       "      <td>{'mse_score': [1.111923487408025, 0.6766862634...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015288</td>\n",
       "      <td>{'mse_score': [1.1801224687295142, 0.950415342...</td>\n",
       "      <td>{'mse_score': [1.1801224777332378, 0.950415341...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.831400</td>\n",
       "      <td>{'mse_score': [1.0976792354702392, 0.671636024...</td>\n",
       "      <td>{'mse_score': [1.0976792337396566, 0.671636022...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007751</td>\n",
       "      <td>{'mse_score': [1.3936376654364884, 1.040563386...</td>\n",
       "      <td>{'mse_score': [1.39363767778841, 1.04056338628...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.836634</td>\n",
       "      <td>{'mse_score': [1.0998622817940762, 0.677003376...</td>\n",
       "      <td>{'mse_score': [1.0998622798698838, 0.677003374...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.880319</td>\n",
       "      <td>{'mse_score': [1.0063871308109065, 0.650126838...</td>\n",
       "      <td>{'mse_score': [1.0063871300392733, 0.650126843...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.837817</td>\n",
       "      <td>{'mse_score': [0.9972977744677737, 0.552858876...</td>\n",
       "      <td>{'mse_score': [0.9972977727407778, 0.552858879...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.991491</td>\n",
       "      <td>{'mse_score': [1.015361101896433, 0.6385197295...</td>\n",
       "      <td>{'mse_score': [1.0153611005115688, 0.638519728...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.830576</td>\n",
       "      <td>{'mse_score': [1.0002425052752673, 0.852160951...</td>\n",
       "      <td>{'mse_score': [1.0002425013137521, 0.852160952...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.706139</td>\n",
       "      <td>{'mse_score': [0.9261003635990263, 0.764287381...</td>\n",
       "      <td>{'mse_score': [0.9261003504544436, 0.764287386...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050071</td>\n",
       "      <td>{'mse_score': [1.081409082021159, 0.7876511651...</td>\n",
       "      <td>{'mse_score': [1.0814090864925796, 0.787651164...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'ordinal_imputer': 'NoImputer', 'continuous_i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  imputation_time  \\\n",
       "0   {'ordinal_imputer': 'SimpleImputer_most_freque...         3.246639   \n",
       "1   {'ordinal_imputer': 'SimpleImputer_most_freque...         3.283444   \n",
       "2   {'ordinal_imputer': 'SimpleImputer_most_freque...         3.266497   \n",
       "3   {'ordinal_imputer': 'SimpleImputer_most_freque...         3.283154   \n",
       "4   {'ordinal_imputer': 'SimpleImputer_most_freque...         3.311443   \n",
       "5   {'ordinal_imputer': 'SimpleImputer_most_freque...         3.289330   \n",
       "6   {'ordinal_imputer': 'SimpleImputer_most_freque...         3.300400   \n",
       "7   {'ordinal_imputer': 'SimpleImputer_most_freque...         3.375947   \n",
       "8   {'ordinal_imputer': 'SimpleImputer_most_freque...         3.454399   \n",
       "9   {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "10  {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "11  {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "12  {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "13  {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "14  {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "15  {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "16  {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "17  {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "18  {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "19  {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "20  {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "21  {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "22  {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "23  {'ordinal_imputer': 'NoImputer', 'continuous_i...              NaN   \n",
       "\n",
       "    fitting_time                                        results_adj  \\\n",
       "0       0.229391  {'mse_score': [0.659831758923781, 0.4897431123...   \n",
       "1       0.098224  {'mse_score': [1.1801224802168766, 0.950415349...   \n",
       "2       1.492615  {'mse_score': [0.6679134194781107, 0.478566173...   \n",
       "3       0.077769  {'mse_score': [1.3936376654364884, 1.040563386...   \n",
       "4       1.492246  {'mse_score': [0.6555347035833681, 0.476021533...   \n",
       "5      51.797212  {'mse_score': [0.8120496178211589, 0.557388719...   \n",
       "6       0.150168  {'mse_score': [0.9928374904980006, 0.683205912...   \n",
       "7      13.772108  {'mse_score': [0.6182489690113672, 0.611846183...   \n",
       "8      13.395208  {'mse_score': [1.0761740236083321, 1.312529050...   \n",
       "9       0.090095  {'mse_score': [1.1119234898099377, 0.676686264...   \n",
       "10      0.015288  {'mse_score': [1.1801224687295142, 0.950415342...   \n",
       "11      0.831400  {'mse_score': [1.0976792354702392, 0.671636024...   \n",
       "12      0.007751  {'mse_score': [1.3936376654364884, 1.040563386...   \n",
       "13      0.836634  {'mse_score': [1.0998622817940762, 0.677003376...   \n",
       "14     31.880319  {'mse_score': [1.0063871308109065, 0.650126838...   \n",
       "15      0.837817  {'mse_score': [0.9972977744677737, 0.552858876...   \n",
       "16      1.991491  {'mse_score': [1.015361101896433, 0.6385197295...   \n",
       "17     11.830576  {'mse_score': [1.0002425052752673, 0.852160951...   \n",
       "18     11.706139  {'mse_score': [0.9261003635990263, 0.764287381...   \n",
       "19      0.050071  {'mse_score': [1.081409082021159, 0.7876511651...   \n",
       "20           NaN                                               None   \n",
       "21           NaN                                               None   \n",
       "22           NaN                                               None   \n",
       "23           NaN                                               None   \n",
       "\n",
       "                                          results_org  \n",
       "0   {'mse_score': [0.6598317562332855, 0.489743104...  \n",
       "1   {'mse_score': [1.1801224892206006, 0.950415348...  \n",
       "2   {'mse_score': [0.6679134155298025, 0.478566167...  \n",
       "3   {'mse_score': [1.39363767778841, 1.04056338628...  \n",
       "4   {'mse_score': [0.655534699980677, 0.4760215269...  \n",
       "5   {'mse_score': [0.8120496171306019, 0.557388722...  \n",
       "6   {'mse_score': [0.9928374937104268, 0.683205911...  \n",
       "7   {'mse_score': [0.6182489748896371, 0.611846177...  \n",
       "8   {'mse_score': [1.076174034182812, 1.3125290559...  \n",
       "9   {'mse_score': [1.111923487408025, 0.6766862634...  \n",
       "10  {'mse_score': [1.1801224777332378, 0.950415341...  \n",
       "11  {'mse_score': [1.0976792337396566, 0.671636022...  \n",
       "12  {'mse_score': [1.39363767778841, 1.04056338628...  \n",
       "13  {'mse_score': [1.0998622798698838, 0.677003374...  \n",
       "14  {'mse_score': [1.0063871300392733, 0.650126843...  \n",
       "15  {'mse_score': [0.9972977727407778, 0.552858879...  \n",
       "16  {'mse_score': [1.0153611005115688, 0.638519728...  \n",
       "17  {'mse_score': [1.0002425013137521, 0.852160952...  \n",
       "18  {'mse_score': [0.9261003504544436, 0.764287386...  \n",
       "19  {'mse_score': [1.0814090864925796, 0.787651164...  \n",
       "20                                               None  \n",
       "21                                               None  \n",
       "22                                               None  \n",
       "23                                               None  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_dict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metric_table(\n",
    "    results_list,\n",
    "    targets,\n",
    "    metric_name,\n",
    "    source=\"Adjusted\",\n",
    "    float_format=\"%.3f\",\n",
    "    csv_filename=None,\n",
    "    sort_order=\"ascending\"  # or \"descending\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a LaTeX table for a single metric across targets, models, and imputers.\n",
    "    Optionally export the same table as CSV and sort by mean performance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_list : list of dict\n",
    "        List of experiment results.\n",
    "    targets : list of str\n",
    "        Target names (e.g., ['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN']).\n",
    "    metric_name : str\n",
    "        Metric to extract (e.g., 'mae_score').\n",
    "    source : str\n",
    "        'Adjusted' or 'Original'.\n",
    "    float_format : str\n",
    "        Format for floats (e.g., '%.3f').\n",
    "    csv_filename : str or None\n",
    "        If provided, saves the table to CSV.\n",
    "    sort_order : str\n",
    "        'ascending' or 'descending' for sorting by mean.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        LaTeX-formatted table string.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    version_key = \"results_adj\" if source.lower() == \"adjusted\" else \"results_org\"\n",
    "\n",
    "    for res in results_list:\n",
    "        result_block = res.get(version_key)\n",
    "        if result_block is None:\n",
    "            continue\n",
    "\n",
    "        metric_values = result_block.get(metric_name)\n",
    "        if metric_values is None:\n",
    "            continue\n",
    "\n",
    "        if len(metric_values) != len(targets):\n",
    "            continue\n",
    "\n",
    "        ordinal_imputer = res[\"params\"].get(\"ordinal_imputer\")\n",
    "        model = res[\"params\"].get(\"model\")\n",
    "\n",
    "        values = np.array(metric_values, dtype=np.float64)\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "\n",
    "        row = {\n",
    "            \"Ordinal Imputer\": ordinal_imputer,\n",
    "            \"Model\": model,\n",
    "            \"Mean\": mean_val,  # for sorting\n",
    "            \"Mean  SD\": f\"{mean_val:.3f}  {std_val:.3f}\",\n",
    "        }\n",
    "        row.update({target: val for target, val in zip(targets, values)})\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Reorder columns for display\n",
    "    display_cols = [\"Ordinal Imputer\", \"Model\"] + targets + [\"Mean  SD\"]\n",
    "    df = df.sort_values(by=\"Mean\", ascending=(sort_order == \"ascending\"))\n",
    "    df = df[display_cols]\n",
    "\n",
    "    # Save CSV\n",
    "    if csv_filename:\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # LaTeX output\n",
    "    latex_table = df.to_latex(\n",
    "        index=False,\n",
    "        escape=False,\n",
    "        float_format=float_format,\n",
    "        caption=f\"{metric_name.replace('_', ' ').upper()} across targets\",\n",
    "        label=f\"tab:{metric_name}\",\n",
    "        longtable=False\n",
    "    )\n",
    "\n",
    "    return df, latex_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{CORR across targets}\n",
      "\\label{tab:corr}\n",
      "\\begin{tabular}{llrrrrl}\n",
      "\\toprule\n",
      "Ordinal Imputer & Model & ADNI_MEM & ADNI_EF & ADNI_VS & ADNI_LAN & Mean  SD \\\\\n",
      "\\midrule\n",
      "SimpleImputer_most_frequent & TabNetRegressor_default & 0.773 & 0.676 & 0.581 & 0.819 & 0.712  0.092 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskLasso_tuned & 0.619 & 0.707 & 0.377 & 0.675 & 0.594  0.130 \\\\\n",
      "SimpleImputer_most_frequent & LinearRegression & 0.606 & 0.692 & 0.402 & 0.666 & 0.592  0.114 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskElasticNet_tuned & 0.608 & 0.705 & 0.382 & 0.669 & 0.591  0.126 \\\\\n",
      "SimpleImputer_most_frequent & RandomForestRegressor & 0.635 & 0.677 & 0.343 & 0.627 & 0.570  0.133 \\\\\n",
      "NoImputer & XGBoostRegressor_tuned & 0.481 & 0.591 & 0.307 & 0.656 & 0.509  0.132 \\\\\n",
      "NoImputer & RandomForestRegressor & 0.540 & 0.644 & 0.217 & 0.610 & 0.503  0.169 \\\\\n",
      "NoImputer & XGBoostRegressor & 0.522 & 0.698 & 0.212 & 0.566 & 0.500  0.178 \\\\\n",
      "NoImputer & TabNetRegressor_custom & 0.553 & 0.581 & 0.237 & 0.597 & 0.492  0.148 \\\\\n",
      "NoImputer & TabNetRegressor_default & 0.629 & 0.471 & 0.067 & 0.730 & 0.474  0.253 \\\\\n",
      "SimpleImputer_most_frequent & PLSRegression_4_components & 0.510 & 0.556 & 0.074 & 0.527 & 0.417  0.198 \\\\\n",
      "SimpleImputer_most_frequent & TabNetRegressor_custom & 0.540 & 0.307 & -0.069 & 0.586 & 0.341  0.259 \\\\\n",
      "NoImputer & MultiTaskLasso_tuned & 0.340 & 0.549 & 0.014 & 0.391 & 0.324  0.195 \\\\\n",
      "NoImputer & MultiTaskElasticNet_tuned & 0.338 & 0.551 & 0.010 & 0.392 & 0.323  0.197 \\\\\n",
      "NoImputer & PLSRegression_4_components & 0.433 & 0.445 & -0.054 & 0.457 & 0.321  0.216 \\\\\n",
      "NoImputer & LinearRegression & 0.332 & 0.548 & 0.016 & 0.381 & 0.319  0.192 \\\\\n",
      "NoImputer & MultiTaskElasticNet & 0.422 & 0.179 & -0.163 & 0.446 & 0.221  0.245 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskElasticNet & 0.422 & 0.179 & -0.163 & 0.446 & 0.221  0.245 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskLasso & NaN & NaN & NaN & NaN & nan  nan \\\\\n",
      "NoImputer & MultiTaskLasso & NaN & NaN & NaN & NaN & nan  nan \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_df, latex_mae = generate_metric_table(\n",
    "    results_list=all_dict_results,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='corr',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/2_training_train_test_corr_adjusted_sorted.csv\",\n",
    "    sort_order=\"descending\"\n",
    ")\n",
    "print(latex_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{R2 across targets}\n",
      "\\label{tab:r2}\n",
      "\\begin{tabular}{llrrrrl}\n",
      "\\toprule\n",
      "Ordinal Imputer & Model & ADNI_MEM & ADNI_EF & ADNI_VS & ADNI_LAN & Mean  SD \\\\\n",
      "\\midrule\n",
      "SimpleImputer_most_frequent & LinearRegression & 0.333 & 0.444 & 0.145 & 0.310 & 0.308  0.107 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskLasso_tuned & 0.337 & 0.460 & 0.125 & 0.291 & 0.303  0.120 \\\\\n",
      "SimpleImputer_most_frequent & TabNetRegressor_default & 0.375 & 0.306 & 0.230 & 0.297 & 0.302  0.051 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskElasticNet_tuned & 0.325 & 0.457 & 0.135 & 0.279 & 0.299  0.115 \\\\\n",
      "SimpleImputer_most_frequent & RandomForestRegressor & 0.179 & 0.368 & 0.114 & 0.072 & 0.183  0.113 \\\\\n",
      "NoImputer & XGBoostRegressor_tuned & -0.026 & 0.275 & 0.032 & 0.139 & 0.105  0.115 \\\\\n",
      "NoImputer & XGBoostRegressor & -0.008 & 0.373 & -0.040 & 0.092 & 0.104  0.162 \\\\\n",
      "NoImputer & RandomForestRegressor & -0.017 & 0.262 & 0.001 & -0.000 & 0.061  0.116 \\\\\n",
      "SimpleImputer_most_frequent & PLSRegression_4_components & -0.003 & 0.225 & -0.068 & -0.071 & 0.021  0.121 \\\\\n",
      "NoImputer & TabNetRegressor_custom & 0.064 & 0.133 & -0.277 & -0.064 & -0.036  0.156 \\\\\n",
      "NoImputer & TabNetRegressor_default & -0.011 & 0.033 & -0.301 & 0.042 & -0.059  0.141 \\\\\n",
      "NoImputer & PLSRegression_4_components & -0.093 & 0.106 & -0.189 & -0.151 & -0.082  0.114 \\\\\n",
      "NoImputer & MultiTaskElasticNet_tuned & -0.109 & 0.238 & -0.289 & -0.168 & -0.082  0.196 \\\\\n",
      "NoImputer & MultiTaskLasso_tuned & -0.112 & 0.232 & -0.289 & -0.172 & -0.085  0.194 \\\\\n",
      "NoImputer & LinearRegression & -0.124 & 0.232 & -0.295 & -0.189 & -0.094  0.198 \\\\\n",
      "NoImputer & MultiTaskElasticNet & -0.193 & -0.078 & -0.059 & -0.252 & -0.146  0.080 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskElasticNet & -0.193 & -0.078 & -0.059 & -0.252 & -0.146  0.080 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskLasso & -0.409 & -0.181 & -0.020 & -0.508 & -0.279  0.191 \\\\\n",
      "NoImputer & MultiTaskLasso & -0.409 & -0.181 & -0.020 & -0.508 & -0.279  0.191 \\\\\n",
      "SimpleImputer_most_frequent & TabNetRegressor_custom & -0.088 & -0.489 & -0.619 & -0.270 & -0.366  0.204 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_df, latex_mae = generate_metric_table(\n",
    "    results_list=all_dict_results,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='r2',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/2_training_train_test_r2_adjusted_sorted.csv\",\n",
    "    sort_order=\"descending\"\n",
    ")\n",
    "print(latex_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{MSE SCORE across targets}\n",
      "\\label{tab:mse_score}\n",
      "\\begin{tabular}{llrrrrl}\n",
      "\\toprule\n",
      "Ordinal Imputer & Model & ADNI_MEM & ADNI_EF & ADNI_VS & ADNI_LAN & Mean  SD \\\\\n",
      "\\midrule\n",
      "SimpleImputer_most_frequent & LinearRegression & 0.660 & 0.490 & 0.410 & 0.567 & 0.531  0.093 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskLasso_tuned & 0.656 & 0.476 & 0.419 & 0.582 & 0.533  0.092 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskElasticNet_tuned & 0.668 & 0.479 & 0.414 & 0.593 & 0.538  0.098 \\\\\n",
      "SimpleImputer_most_frequent & TabNetRegressor_default & 0.618 & 0.612 & 0.369 & 0.578 & 0.544  0.102 \\\\\n",
      "SimpleImputer_most_frequent & RandomForestRegressor & 0.812 & 0.557 & 0.424 & 0.763 & 0.639  0.156 \\\\\n",
      "NoImputer & XGBoostRegressor & 0.997 & 0.553 & 0.498 & 0.746 & 0.699  0.196 \\\\\n",
      "NoImputer & XGBoostRegressor_tuned & 1.015 & 0.639 & 0.463 & 0.707 & 0.706  0.199 \\\\\n",
      "NoImputer & RandomForestRegressor & 1.006 & 0.650 & 0.479 & 0.822 & 0.739  0.196 \\\\\n",
      "SimpleImputer_most_frequent & PLSRegression_4_components & 0.993 & 0.683 & 0.511 & 0.880 & 0.767  0.185 \\\\\n",
      "NoImputer & TabNetRegressor_custom & 0.926 & 0.764 & 0.612 & 0.875 & 0.794  0.121 \\\\\n",
      "NoImputer & TabNetRegressor_default & 1.000 & 0.852 & 0.623 & 0.787 & 0.816  0.135 \\\\\n",
      "NoImputer & MultiTaskElasticNet_tuned & 1.098 & 0.672 & 0.617 & 0.960 & 0.837  0.199 \\\\\n",
      "NoImputer & MultiTaskLasso_tuned & 1.100 & 0.677 & 0.617 & 0.963 & 0.839  0.199 \\\\\n",
      "NoImputer & PLSRegression_4_components & 1.081 & 0.788 & 0.569 & 0.946 & 0.846  0.191 \\\\\n",
      "NoImputer & LinearRegression & 1.112 & 0.677 & 0.620 & 0.978 & 0.847  0.205 \\\\\n",
      "NoImputer & MultiTaskElasticNet & 1.180 & 0.950 & 0.507 & 1.029 & 0.917  0.250 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskElasticNet & 1.180 & 0.950 & 0.507 & 1.029 & 0.917  0.250 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskLasso & 1.394 & 1.041 & 0.489 & 1.240 & 1.041  0.342 \\\\\n",
      "NoImputer & MultiTaskLasso & 1.394 & 1.041 & 0.489 & 1.240 & 1.041  0.342 \\\\\n",
      "SimpleImputer_most_frequent & TabNetRegressor_custom & 1.076 & 1.313 & 0.775 & 1.044 & 1.052  0.190 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_df, latex_mae = generate_metric_table(\n",
    "    results_list=all_dict_results,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='mse_score',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/2_training_train_test_mse_adjusted_sorted.csv\",\n",
    "    sort_order=\"ascending\"\n",
    ")\n",
    "print(latex_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{MAE SCORE across targets}\n",
      "\\label{tab:mae_score}\n",
      "\\begin{tabular}{llrrrrl}\n",
      "\\toprule\n",
      "Ordinal Imputer & Model & ADNI_MEM & ADNI_EF & ADNI_VS & ADNI_LAN & Mean  SD \\\\\n",
      "\\midrule\n",
      "SimpleImputer_most_frequent & MultiTaskLasso_tuned & 0.622 & 0.551 & 0.578 & 0.595 & 0.586  0.026 \\\\\n",
      "SimpleImputer_most_frequent & LinearRegression & 0.640 & 0.551 & 0.565 & 0.600 & 0.589  0.034 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskElasticNet_tuned & 0.630 & 0.554 & 0.580 & 0.600 & 0.591  0.028 \\\\\n",
      "SimpleImputer_most_frequent & TabNetRegressor_default & 0.691 & 0.608 & 0.536 & 0.595 & 0.607  0.055 \\\\\n",
      "SimpleImputer_most_frequent & RandomForestRegressor & 0.669 & 0.640 & 0.608 & 0.648 & 0.641  0.022 \\\\\n",
      "NoImputer & XGBoostRegressor & 0.754 & 0.635 & 0.643 & 0.644 & 0.669  0.049 \\\\\n",
      "NoImputer & XGBoostRegressor_tuned & 0.735 & 0.715 & 0.630 & 0.613 & 0.673  0.052 \\\\\n",
      "NoImputer & RandomForestRegressor & 0.755 & 0.684 & 0.635 & 0.632 & 0.677  0.050 \\\\\n",
      "NoImputer & MultiTaskLasso_tuned & 0.724 & 0.705 & 0.713 & 0.706 & 0.712  0.008 \\\\\n",
      "NoImputer & MultiTaskElasticNet_tuned & 0.727 & 0.703 & 0.714 & 0.708 & 0.713  0.009 \\\\\n",
      "NoImputer & LinearRegression & 0.728 & 0.702 & 0.710 & 0.712 & 0.713  0.010 \\\\\n",
      "NoImputer & TabNetRegressor_custom & 0.713 & 0.758 & 0.671 & 0.724 & 0.717  0.031 \\\\\n",
      "NoImputer & TabNetRegressor_default & 0.781 & 0.801 & 0.680 & 0.672 & 0.733  0.058 \\\\\n",
      "SimpleImputer_most_frequent & PLSRegression_4_components & 0.805 & 0.738 & 0.689 & 0.764 & 0.749  0.042 \\\\\n",
      "NoImputer & PLSRegression_4_components & 0.792 & 0.791 & 0.722 & 0.735 & 0.760  0.032 \\\\\n",
      "NoImputer & MultiTaskElasticNet & 0.884 & 0.787 & 0.654 & 0.827 & 0.788  0.085 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskElasticNet & 0.884 & 0.787 & 0.654 & 0.827 & 0.788  0.085 \\\\\n",
      "SimpleImputer_most_frequent & TabNetRegressor_custom & 0.765 & 0.969 & 0.765 & 0.763 & 0.815  0.089 \\\\\n",
      "NoImputer & MultiTaskLasso & 0.991 & 0.785 & 0.621 & 0.934 & 0.833  0.144 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskLasso & 0.991 & 0.785 & 0.621 & 0.934 & 0.833  0.144 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_df, latex_mae = generate_metric_table(\n",
    "    results_list=all_dict_results,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='mae_score',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/2_training_train_test_mae_adjusted_sorted.csv\",\n",
    "    sort_order=\"ascending\"\n",
    ")\n",
    "print(latex_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model\n",
       "MultiTaskLasso_tuned          2\n",
       "LinearRegression              2\n",
       "MultiTaskElasticNet_tuned     2\n",
       "TabNetRegressor_default       2\n",
       "RandomForestRegressor         2\n",
       "PLSRegression_4_components    2\n",
       "TabNetRegressor_custom        2\n",
       "MultiTaskElasticNet           2\n",
       "MultiTaskLasso                2\n",
       "XGBoostRegressor_tuned        1\n",
       "XGBoostRegressor              1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_df.Model.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
