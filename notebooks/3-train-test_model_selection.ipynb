{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from itertools import product\n",
    "import warnings\n",
    "\n",
    "# System path modification\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Lasso, LassoCV, MultiTaskLasso, MultiTaskLassoCV,\n",
    "    ElasticNet, ElasticNetCV, MultiTaskElasticNet, MultiTaskElasticNetCV\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Custom modules\n",
    "from src.train import *\n",
    "from src.functions import *\n",
    "from src.plots import *\n",
    "from src.dataset import *\n",
    "from src.multixgboost import *\n",
    "from src.wrapper import *\n",
    "\n",
    "# Visualizatiokn \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning and machine learning specific \n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "\n",
    "from pytorch_tabular.models import (\n",
    "    GatedAdditiveTreeEnsembleConfig,\n",
    "    DANetConfig,\n",
    "    TabTransformerConfig,\n",
    "    FTTransformerConfig,\n",
    "    TabNetModelConfig,\n",
    ")\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Print CUDA availability for PyTorch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_pickle_data_palettes()\n",
    "\n",
    "results_pickle_folder = \"../pickle/\"\n",
    "\n",
    "# Unpack data\n",
    "df_X, df_y, df_all, df_FinalCombination = data[\"df_X\"], data[\"df_y\"], data[\"df_all\"], data[\"df_FinalCombination\"]\n",
    "dict_select = data[\"dict_select\"]\n",
    "\n",
    "# Unpack colormaps\n",
    "full_palette, gender_palette, dx_palette = data[\"colormaps\"].values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = list(df_X.isna().any(axis=1))\n",
    "idx_test = list(~df_X.isna().any(axis=1))\n",
    "\n",
    "set_intersect_rid = set(df_all[idx_train].RID).intersection(set(df_all[idx_test].RID))\n",
    "intersect_rid_idx = df_all.RID.isin(set_intersect_rid)\n",
    "\n",
    "for i, bool_test in enumerate(idx_test): \n",
    "    if intersect_rid_idx.iloc[i] & bool_test:\n",
    "        idx_test[i] = False\n",
    "        idx_train[i] = True\n",
    "        \n",
    "df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]] = df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]].astype(\"category\")\n",
    "\n",
    "df_X_train = df_X.loc[idx_train]\n",
    "df_X_test = df_X.loc[idx_test]\n",
    "\n",
    "df_y_train = df_y.loc[idx_train]\n",
    "df_y_test = df_y.loc[idx_test]\n",
    "\n",
    "c_train = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_train]\n",
    "c_test = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3609    128_S_2002\n",
       "5631    116_S_4167\n",
       "5662    033_S_4176\n",
       "5780    098_S_4215\n",
       "5950    018_S_4349\n",
       "6069    941_S_4292\n",
       "6077    116_S_4453\n",
       "6085    135_S_4489\n",
       "6224    033_S_4505\n",
       "6400    014_S_4576\n",
       "6429    073_S_4300\n",
       "7021    003_S_2374\n",
       "7192    033_S_4179\n",
       "Name: SubjectID, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.SubjectID.iloc[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all the models and combinations to try out with their hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: LinearRegression\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskElasticNet\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskElasticNet_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskLasso\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskLasso_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: RandomForestRegressor\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: XGBoostRegressor\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: XGBoostRegressor_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetRegressor_default\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetRegressor_custom\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: PLSRegression_4_components\n",
      "Combinations of preprocessing and models to test : 11\n"
     ]
    }
   ],
   "source": [
    "random_state=42\n",
    "n_imputation_iter = 10\n",
    "\n",
    "# Define hyperparameters\n",
    "gain_parameters = {\n",
    "    'hint_rate': 0.9,\n",
    "    'alpha': 100,\n",
    "    'iterations': 1000\n",
    "}\n",
    "\n",
    "# Continuous Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "continuous_imputer_list = [\n",
    "    (\"KNNImputer\", KNNImputer(n_neighbors=1)),\n",
    "]\n",
    "\n",
    "# Ordinal Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "ordinal_imputer_list = [\n",
    "    (\"SimpleImputer_most_frequent\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "]\n",
    "\n",
    "# Predictive Models List (list of tuples with unique strings and corresponding instances)\n",
    "predictive_models_list = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"MultiTaskElasticNet\", MultiTaskElasticNet()),\n",
    "    (\"MultiTaskElasticNet_tuned\", MultiTaskElasticNet(**{'alpha': 0.01, 'l1_ratio': 0.01})),\n",
    "    (\"MultiTaskLasso\", MultiTaskLasso()),\n",
    "    (\"MultiTaskLasso_tuned\", MultiTaskLasso(**{'alpha': 0.001})),\n",
    "    (\"RandomForestRegressor\", RandomForestRegressor()),\n",
    "    (\"XGBoostRegressor\", XGBoostRegressor()),\n",
    "    (\"XGBoostRegressor_tuned\", XGBoostRegressor(**{'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105})),\n",
    "    (\"TabNetRegressor_default\", TabNetModelWrapper(n_a=8, n_d=8)),\n",
    "    (\"TabNetRegressor_custom\", TabNetModelWrapper(n_a=32, n_d=32)),\n",
    "    (\"PLSRegression_4_components\", PLSRegression(n_components=4))\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "# Display all combinations\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    print(f\"Continuous Imputer: {continuous_imputer[0]}, Ordinal Imputer: {ordinal_imputer[0]}, Model: {model[0]}\")\n",
    "\n",
    "print(f\"Combinations of preprocessing and models to test : {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HDF5 file\n",
    "results_file = '../pickle/training_2_dict_results.pickle'\n",
    "\n",
    "if os.path.exists(results_file): \n",
    "\n",
    "    with open(results_file, \"rb\") as input_file:\n",
    "        all_dict_results = pickle.load(input_file)\n",
    "\n",
    "else : \n",
    "    all_dict_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'LinearRegression', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.16014862060546875, 'results_adj': {'mse_score': array([1.11192349, 0.67668626, 0.61999815, 0.97750844]), 'mae_score': array([0.72849831, 0.70235973, 0.70986192, 0.71215901]), 'r2': array([-0.12379655,  0.2321367 , -0.29452346, -0.18906157]), 'explained_variance': array([ 0.02475189,  0.28845771, -0.29319881,  0.10973059]), 'corr': array([0.33204585, 0.54765514, 0.01649677, 0.38058137])}, 'results_org': {'mse_score': array([1.11192349, 0.67668626, 0.61999817, 0.97750843]), 'mae_score': array([0.72849832, 0.70235973, 0.70986193, 0.712159  ]), 'r2': array([-0.18280006,  0.24192465, -0.20755938, -0.14289477]), 'explained_variance': array([-0.02645227,  0.29752774, -0.20632372,  0.14429641]), 'corr': array([0.27766053, 0.55012684, 0.07284141, 0.39622517])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskElasticNet', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.015481710433959961, 'results_adj': {'mse_score': array([1.18012247, 0.95041534, 0.50741149, 1.02939713]), 'mae_score': array([0.88414167, 0.78683572, 0.65378091, 0.82689555]), 'r2': array([-0.19272375, -0.07847477, -0.05944845, -0.25218005]), 'explained_variance': array([ 0.11229868,  0.03189858, -0.05332468,  0.12857136]), 'corr': array([ 0.42176386,  0.17864277, -0.16306808,  0.44631445])}, 'results_org': {'mse_score': array([1.18012248, 0.95041534, 0.50741151, 1.02939711]), 'mae_score': array([0.88414168, 0.78683573, 0.65378092, 0.82689554]), 'r2': array([-0.25534621, -0.06472745,  0.01172366, -0.20356258]), 'explained_variance': array([0.06569104, 0.04423898, 0.01743605, 0.16240568]), 'corr': array([0.25863755, 0.21083053, 0.14561964, 0.49623358])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskElasticNet_tuned', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.8295047283172607, 'results_adj': {'mse_score': array([1.09767924, 0.67163602, 0.61727141, 0.96027028]), 'mae_score': array([0.72712752, 0.70316248, 0.71357071, 0.70813001]), 'r2': array([-0.1094002 ,  0.23786741, -0.28883017, -0.16809271]), 'explained_variance': array([ 0.04259982,  0.29559122, -0.28782174,  0.12798901]), 'corr': array([0.33826662, 0.55120885, 0.00953623, 0.39231636])}, 'results_org': {'mse_score': array([1.09767923, 0.67163602, 0.61727143, 0.96027027]), 'mae_score': array([0.72712752, 0.70316248, 0.71357072, 0.70813   ]), 'r2': array([-0.16764784,  0.24758231, -0.20224855, -0.12274005]), 'explained_variance': array([-0.00766726,  0.30457033, -0.20130787,  0.16184592]), 'corr': array([0.28384559, 0.55450481, 0.06742918, 0.41077564])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskLasso', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.011329889297485352, 'results_adj': {'mse_score': array([1.39363767, 1.04056339, 0.48866937, 1.24000841]), 'mae_score': array([0.99127575, 0.78457157, 0.62110666, 0.93446712]), 'r2': array([-0.40851885, -0.18076941, -0.0203159 , -0.50837198]), 'explained_variance': array([-2.22044605e-16,  0.00000000e+00,  0.00000000e+00,  1.11022302e-16]), 'corr': array([nan, nan, nan, nan])}, 'results_org': {'mse_score': array([1.39363768, 1.04056339, 0.48866939, 1.24000838]), 'mae_score': array([0.99127576, 0.78457157, 0.62110667, 0.93446711]), 'r2': array([-0.48247136, -0.16571814,  0.04822734, -0.44980754]), 'explained_variance': array([-0.05250375,  0.01274701,  0.06717846,  0.03882626]), 'corr': array([-0.0848666 ,  0.11455326,  0.2721653 ,  0.20679355])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskLasso_tuned', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.8312158584594727, 'results_adj': {'mse_score': array([1.09986228, 0.67700338, 0.61721354, 0.96322248]), 'mae_score': array([0.72439764, 0.70502407, 0.71262079, 0.70634911]), 'r2': array([-0.11160655,  0.23177686, -0.28870934, -0.17168383]), 'explained_variance': array([ 0.04182356,  0.29179378, -0.28783433,  0.1250568 ]), 'corr': array([0.34030126, 0.54948447, 0.01369735, 0.39123818])}, 'results_org': {'mse_score': array([1.09986228, 0.67700337, 0.61721356, 0.96322247]), 'mae_score': array([0.72439764, 0.70502408, 0.71262081, 0.7063491 ]), 'r2': array([-0.16997004,  0.2415694 , -0.20213584, -0.12619174]), 'explained_variance': array([-0.00848428,  0.30082129, -0.20131962,  0.15902757]), 'corr': array([0.28608858, 0.55225247, 0.07079544, 0.40890542])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'RandomForestRegressor', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 32.779903173446655, 'results_adj': {'mse_score': array([1.00638713, 0.65012684, 0.47869162, 0.82220865]), 'mae_score': array([0.75528169, 0.6841987 , 0.63512616, 0.63151397]), 'r2': array([-1.71332791e-02,  2.62274753e-01,  5.17132936e-04, -1.51675905e-04]), 'explained_variance': array([0.29209145, 0.405549  , 0.02445608, 0.37078241]), 'corr': array([0.54046621, 0.64364297, 0.21686966, 0.60995505])}, 'results_org': {'mse_score': array([1.00638713, 0.65012684, 0.47869164, 0.82220864]), 'mae_score': array([0.7552817 , 0.6841987 , 0.63512617, 0.63151397]), 'r2': array([-0.07053657,  0.27167853,  0.06766084,  0.03868046]), 'explained_variance': array([0.25492361, 0.41312647, 0.08999161, 0.39521257]), 'corr': array([0.50511182, 0.65009756, 0.3086371 , 0.63522421])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'XGBoostRegressor', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'XGBoostRegressor_tuned', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 1.977959394454956, 'results_adj': {'mse_score': array([1.0153611 , 0.63851973, 0.46345103, 0.70749896]), 'mae_score': array([0.73457703, 0.71458275, 0.63021056, 0.61263546]), 'r2': array([-0.02620307,  0.27544581,  0.03233868,  0.13938357]), 'explained_variance': array([0.22165364, 0.34897739, 0.05887258, 0.43032201]), 'corr': array([0.48104477, 0.59075482, 0.30682341, 0.65598979])}, 'results_org': {'mse_score': array([1.0153611 , 0.63851973, 0.46345105, 0.70749894]), 'mae_score': array([0.73457704, 0.71458276, 0.63021057, 0.61263545]), 'r2': array([-0.08008256,  0.2846817 ,  0.09734466,  0.17279809]), 'explained_variance': array([0.18078756, 0.35727599, 0.12209605, 0.45244048]), 'corr': array([0.44678191, 0.59775758, 0.36862204, 0.67323253])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetRegressor_default', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 15.032789468765259, 'results_adj': {'mse_score': array([1.00024251, 0.85216095, 0.6231349 , 0.78732824]), 'mae_score': array([0.78064659, 0.8013238 , 0.6795552 , 0.67213814]), 'r2': array([-0.01092304,  0.03301847, -0.30107284,  0.04227758]), 'explained_variance': array([ 0.38275983,  0.13551578, -0.29939216,  0.5247155 ]), 'corr': array([0.62946614, 0.47112118, 0.06684408, 0.73037268])}, 'results_org': {'mse_score': array([1.0002425 , 0.85216095, 0.62313492, 0.78732822]), 'mae_score': array([0.78064659, 0.8013238 , 0.67955522, 0.67213815]), 'r2': array([-0.06400027,  0.04534457, -0.21366878,  0.07946235]), 'explained_variance': array([ 0.35035243,  0.14653537, -0.212101  ,  0.54316902]), 'corr': array([0.60852724, 0.4852577 , 0.12261516, 0.74095112])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetRegressor_custom', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 14.207815647125244, 'results_adj': {'mse_score': array([0.92610036, 0.76428738, 0.61165174, 0.87483944]), 'mae_score': array([0.71338154, 0.75841494, 0.67114785, 0.72399585]), 'r2': array([ 0.06401079,  0.13273216, -0.2770966 , -0.06417287]), 'explained_variance': array([ 0.22209961,  0.26101411, -0.27432143,  0.26805491]), 'corr': array([0.55329131, 0.58084529, 0.23700988, 0.59708229])}, 'results_org': {'mse_score': array([0.92610035, 0.76428739, 0.61165176, 0.87483943]), 'mae_score': array([0.71338154, 0.75841494, 0.67114786, 0.72399584]), 'r2': array([ 0.01486787,  0.14378722, -0.19130323, -0.02285503]), 'explained_variance': array([ 0.18125695,  0.27043396, -0.1887145 ,  0.2964736 ]), 'corr': array([0.53795846, 0.59013198, 0.27340583, 0.5986342 ])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'PLSRegression_4_components', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.0828557014465332, 'results_adj': {'mse_score': array([1.08140908, 0.78765117, 0.5694762 , 0.94630352]), 'mae_score': array([0.79248414, 0.79068015, 0.72178585, 0.73452485]), 'r2': array([-0.09295631,  0.10622033, -0.1890363 , -0.15110326]), 'explained_variance': array([ 0.1855152 ,  0.19584409, -0.18846787,  0.20844666]), 'corr': array([ 0.43286088,  0.44542341, -0.05356343,  0.45740887])}, 'results_org': {'mse_score': array([1.08140909, 0.78765116, 0.56947622, 0.9463035 ]), 'mae_score': array([0.79248415, 0.79068015, 0.72178586, 0.73452484]), 'r2': array([-0.1503406 ,  0.11761334, -0.10915869, -0.10641023]), 'explained_variance': array([ 0.14275171,  0.20609467, -0.10862844,  0.23917972]), 'corr': array([0.3812462 , 0.45398549, 0.03688166, 0.49549602])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'GatedAdditiveTreeEnsembleConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'DANetConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabTransformerConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetModelConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n"
     ]
    }
   ],
   "source": [
    "from src.debug import *\n",
    "all_dict_results = clean_dict_list(all_dict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'LinearRegression', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskElasticNet', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskElasticNet_tuned', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskLasso', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskLasso_tuned', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'RandomForestRegressor', (2881, 348), (13, 348)])\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.99759 |  0:00:00s\n",
      "epoch 1  | loss: 1.8222  |  0:00:00s\n",
      "epoch 2  | loss: 1.38176 |  0:00:00s\n",
      "epoch 3  | loss: 1.14463 |  0:00:00s\n",
      "epoch 4  | loss: 1.03079 |  0:00:00s\n",
      "epoch 5  | loss: 0.93658 |  0:00:00s\n",
      "epoch 6  | loss: 0.91871 |  0:00:00s\n",
      "epoch 7  | loss: 0.89387 |  0:00:00s\n",
      "epoch 8  | loss: 0.87653 |  0:00:00s\n",
      "epoch 9  | loss: 0.86535 |  0:00:00s\n",
      "epoch 10 | loss: 0.83842 |  0:00:00s\n",
      "epoch 11 | loss: 0.82899 |  0:00:00s\n",
      "epoch 12 | loss: 0.79347 |  0:00:01s\n",
      "epoch 13 | loss: 0.79007 |  0:00:01s\n",
      "epoch 14 | loss: 0.78183 |  0:00:01s\n",
      "epoch 15 | loss: 0.75378 |  0:00:01s\n",
      "epoch 16 | loss: 0.75033 |  0:00:01s\n",
      "epoch 17 | loss: 0.75422 |  0:00:01s\n",
      "epoch 18 | loss: 0.75947 |  0:00:01s\n",
      "epoch 19 | loss: 0.74606 |  0:00:01s\n",
      "epoch 20 | loss: 0.73976 |  0:00:01s\n",
      "epoch 21 | loss: 0.72233 |  0:00:01s\n",
      "epoch 22 | loss: 0.72257 |  0:00:01s\n",
      "epoch 23 | loss: 0.7023  |  0:00:01s\n",
      "epoch 24 | loss: 0.71796 |  0:00:01s\n",
      "epoch 25 | loss: 0.71188 |  0:00:01s\n",
      "epoch 26 | loss: 0.70528 |  0:00:01s\n",
      "epoch 27 | loss: 0.69531 |  0:00:01s\n",
      "epoch 28 | loss: 0.69433 |  0:00:01s\n",
      "epoch 29 | loss: 0.68811 |  0:00:01s\n",
      "epoch 30 | loss: 0.69123 |  0:00:01s\n",
      "epoch 31 | loss: 0.68422 |  0:00:01s\n",
      "epoch 32 | loss: 0.6792  |  0:00:01s\n",
      "epoch 33 | loss: 0.67461 |  0:00:02s\n",
      "epoch 34 | loss: 0.67434 |  0:00:02s\n",
      "epoch 35 | loss: 0.66884 |  0:00:02s\n",
      "epoch 36 | loss: 0.66721 |  0:00:02s\n",
      "epoch 37 | loss: 0.66965 |  0:00:02s\n",
      "epoch 38 | loss: 0.67586 |  0:00:02s\n",
      "epoch 39 | loss: 0.6645  |  0:00:02s\n",
      "epoch 40 | loss: 0.66163 |  0:00:02s\n",
      "epoch 41 | loss: 0.66433 |  0:00:02s\n",
      "epoch 42 | loss: 0.66507 |  0:00:02s\n",
      "epoch 43 | loss: 0.65842 |  0:00:02s\n",
      "epoch 44 | loss: 0.65842 |  0:00:02s\n",
      "epoch 45 | loss: 0.65515 |  0:00:02s\n",
      "epoch 46 | loss: 0.64561 |  0:00:02s\n",
      "epoch 47 | loss: 0.64651 |  0:00:02s\n",
      "epoch 48 | loss: 0.63747 |  0:00:02s\n",
      "epoch 49 | loss: 0.641   |  0:00:02s\n",
      "epoch 50 | loss: 0.63559 |  0:00:02s\n",
      "epoch 51 | loss: 0.62464 |  0:00:02s\n",
      "epoch 52 | loss: 0.63034 |  0:00:02s\n",
      "epoch 53 | loss: 0.6359  |  0:00:03s\n",
      "epoch 54 | loss: 0.64461 |  0:00:03s\n",
      "epoch 55 | loss: 0.62959 |  0:00:03s\n",
      "epoch 56 | loss: 0.62208 |  0:00:03s\n",
      "epoch 57 | loss: 0.62278 |  0:00:03s\n",
      "epoch 58 | loss: 0.61352 |  0:00:03s\n",
      "epoch 59 | loss: 0.61653 |  0:00:03s\n",
      "epoch 60 | loss: 0.61506 |  0:00:03s\n",
      "epoch 61 | loss: 0.61679 |  0:00:03s\n",
      "epoch 62 | loss: 0.61709 |  0:00:03s\n",
      "epoch 63 | loss: 0.60716 |  0:00:03s\n",
      "epoch 64 | loss: 0.59768 |  0:00:03s\n",
      "epoch 65 | loss: 0.58913 |  0:00:03s\n",
      "epoch 66 | loss: 0.59271 |  0:00:03s\n",
      "epoch 67 | loss: 0.59631 |  0:00:03s\n",
      "epoch 68 | loss: 0.5909  |  0:00:03s\n",
      "epoch 69 | loss: 0.58991 |  0:00:03s\n",
      "epoch 70 | loss: 0.58083 |  0:00:03s\n",
      "epoch 71 | loss: 0.57764 |  0:00:03s\n",
      "epoch 72 | loss: 0.57947 |  0:00:03s\n",
      "epoch 73 | loss: 0.57907 |  0:00:03s\n",
      "epoch 74 | loss: 0.57926 |  0:00:04s\n",
      "epoch 75 | loss: 0.5676  |  0:00:04s\n",
      "epoch 76 | loss: 0.56    |  0:00:04s\n",
      "epoch 77 | loss: 0.55882 |  0:00:04s\n",
      "epoch 78 | loss: 0.55351 |  0:00:04s\n",
      "epoch 79 | loss: 0.55582 |  0:00:04s\n",
      "epoch 80 | loss: 0.5484  |  0:00:04s\n",
      "epoch 81 | loss: 0.55077 |  0:00:04s\n",
      "epoch 82 | loss: 0.54179 |  0:00:04s\n",
      "epoch 83 | loss: 0.53671 |  0:00:04s\n",
      "epoch 84 | loss: 0.55362 |  0:00:04s\n",
      "epoch 85 | loss: 0.54354 |  0:00:04s\n",
      "epoch 86 | loss: 0.55255 |  0:00:04s\n",
      "epoch 87 | loss: 0.56125 |  0:00:04s\n",
      "epoch 88 | loss: 0.55025 |  0:00:04s\n",
      "epoch 89 | loss: 0.54381 |  0:00:04s\n",
      "epoch 90 | loss: 0.53392 |  0:00:04s\n",
      "epoch 91 | loss: 0.54269 |  0:00:04s\n",
      "epoch 92 | loss: 0.53795 |  0:00:04s\n",
      "epoch 93 | loss: 0.52287 |  0:00:04s\n",
      "epoch 94 | loss: 0.52621 |  0:00:05s\n",
      "epoch 95 | loss: 0.51827 |  0:00:05s\n",
      "epoch 96 | loss: 0.51992 |  0:00:05s\n",
      "epoch 97 | loss: 0.51715 |  0:00:05s\n",
      "epoch 98 | loss: 0.523   |  0:00:05s\n",
      "epoch 99 | loss: 0.51608 |  0:00:05s\n",
      "epoch 100| loss: 0.51951 |  0:00:05s\n",
      "epoch 101| loss: 0.51915 |  0:00:05s\n",
      "epoch 102| loss: 0.51227 |  0:00:05s\n",
      "epoch 103| loss: 0.50895 |  0:00:05s\n",
      "epoch 104| loss: 0.50889 |  0:00:05s\n",
      "epoch 105| loss: 0.50835 |  0:00:05s\n",
      "epoch 106| loss: 0.51031 |  0:00:05s\n",
      "epoch 107| loss: 0.50875 |  0:00:05s\n",
      "epoch 108| loss: 0.51007 |  0:00:05s\n",
      "epoch 109| loss: 0.5085  |  0:00:05s\n",
      "epoch 110| loss: 0.49617 |  0:00:05s\n",
      "epoch 111| loss: 0.5028  |  0:00:05s\n",
      "epoch 112| loss: 0.50526 |  0:00:05s\n",
      "epoch 113| loss: 0.49245 |  0:00:06s\n",
      "epoch 114| loss: 0.4937  |  0:00:06s\n",
      "epoch 115| loss: 0.49307 |  0:00:06s\n",
      "epoch 116| loss: 0.48696 |  0:00:06s\n",
      "epoch 117| loss: 0.48394 |  0:00:06s\n",
      "epoch 118| loss: 0.49119 |  0:00:06s\n",
      "epoch 119| loss: 0.48603 |  0:00:06s\n",
      "epoch 120| loss: 0.49522 |  0:00:06s\n",
      "epoch 121| loss: 0.48156 |  0:00:06s\n",
      "epoch 122| loss: 0.47976 |  0:00:06s\n",
      "epoch 123| loss: 0.48161 |  0:00:06s\n",
      "epoch 124| loss: 0.48    |  0:00:06s\n",
      "epoch 125| loss: 0.47626 |  0:00:06s\n",
      "epoch 126| loss: 0.48569 |  0:00:06s\n",
      "epoch 127| loss: 0.47191 |  0:00:06s\n",
      "epoch 128| loss: 0.47631 |  0:00:06s\n",
      "epoch 129| loss: 0.47569 |  0:00:06s\n",
      "epoch 130| loss: 0.48745 |  0:00:06s\n",
      "epoch 131| loss: 0.4819  |  0:00:06s\n",
      "epoch 132| loss: 0.49411 |  0:00:07s\n",
      "epoch 133| loss: 0.4847  |  0:00:07s\n",
      "epoch 134| loss: 0.49142 |  0:00:07s\n",
      "epoch 135| loss: 0.48286 |  0:00:07s\n",
      "epoch 136| loss: 0.50763 |  0:00:07s\n",
      "epoch 137| loss: 0.50345 |  0:00:07s\n",
      "epoch 138| loss: 0.5119  |  0:00:07s\n",
      "epoch 139| loss: 0.50969 |  0:00:07s\n",
      "epoch 140| loss: 0.50378 |  0:00:07s\n",
      "epoch 141| loss: 0.49577 |  0:00:07s\n",
      "epoch 142| loss: 0.49291 |  0:00:07s\n",
      "epoch 143| loss: 0.49185 |  0:00:07s\n",
      "epoch 144| loss: 0.4846  |  0:00:07s\n",
      "epoch 145| loss: 0.48176 |  0:00:07s\n",
      "epoch 146| loss: 0.48003 |  0:00:07s\n",
      "epoch 147| loss: 0.48757 |  0:00:07s\n",
      "epoch 148| loss: 0.48524 |  0:00:07s\n",
      "epoch 149| loss: 0.48295 |  0:00:07s\n",
      "epoch 150| loss: 0.48227 |  0:00:08s\n",
      "epoch 151| loss: 0.47919 |  0:00:08s\n",
      "epoch 152| loss: 0.48034 |  0:00:08s\n",
      "epoch 153| loss: 0.47749 |  0:00:08s\n",
      "epoch 154| loss: 0.47843 |  0:00:08s\n",
      "epoch 155| loss: 0.47099 |  0:00:08s\n",
      "epoch 156| loss: 0.4684  |  0:00:08s\n",
      "epoch 157| loss: 0.46482 |  0:00:08s\n",
      "epoch 158| loss: 0.46447 |  0:00:08s\n",
      "epoch 159| loss: 0.46259 |  0:00:08s\n",
      "epoch 160| loss: 0.46365 |  0:00:08s\n",
      "epoch 161| loss: 0.46453 |  0:00:08s\n",
      "epoch 162| loss: 0.45735 |  0:00:08s\n",
      "epoch 163| loss: 0.45891 |  0:00:08s\n",
      "epoch 164| loss: 0.46244 |  0:00:08s\n",
      "epoch 165| loss: 0.45657 |  0:00:08s\n",
      "epoch 166| loss: 0.4686  |  0:00:08s\n",
      "epoch 167| loss: 0.46477 |  0:00:09s\n",
      "epoch 168| loss: 0.46247 |  0:00:09s\n",
      "epoch 169| loss: 0.455   |  0:00:09s\n",
      "epoch 170| loss: 0.45037 |  0:00:09s\n",
      "epoch 171| loss: 0.45    |  0:00:09s\n",
      "epoch 172| loss: 0.43631 |  0:00:09s\n",
      "epoch 173| loss: 0.44147 |  0:00:09s\n",
      "epoch 174| loss: 0.43758 |  0:00:09s\n",
      "epoch 175| loss: 0.44653 |  0:00:09s\n",
      "epoch 176| loss: 0.43889 |  0:00:09s\n",
      "epoch 177| loss: 0.43696 |  0:00:09s\n",
      "epoch 178| loss: 0.431   |  0:00:09s\n",
      "epoch 179| loss: 0.43181 |  0:00:09s\n",
      "epoch 180| loss: 0.42947 |  0:00:09s\n",
      "epoch 181| loss: 0.43465 |  0:00:09s\n",
      "epoch 182| loss: 0.42727 |  0:00:09s\n",
      "epoch 183| loss: 0.42982 |  0:00:10s\n",
      "epoch 184| loss: 0.41802 |  0:00:10s\n",
      "epoch 185| loss: 0.42114 |  0:00:10s\n",
      "epoch 186| loss: 0.41816 |  0:00:10s\n",
      "epoch 187| loss: 0.42274 |  0:00:10s\n",
      "epoch 188| loss: 0.41734 |  0:00:10s\n",
      "epoch 189| loss: 0.42838 |  0:00:10s\n",
      "epoch 190| loss: 0.42982 |  0:00:10s\n",
      "epoch 191| loss: 0.41504 |  0:00:10s\n",
      "epoch 192| loss: 0.41849 |  0:00:10s\n",
      "epoch 193| loss: 0.42254 |  0:00:10s\n",
      "epoch 194| loss: 0.42244 |  0:00:10s\n",
      "epoch 195| loss: 0.41672 |  0:00:10s\n",
      "epoch 196| loss: 0.41477 |  0:00:10s\n",
      "epoch 197| loss: 0.41844 |  0:00:10s\n",
      "epoch 198| loss: 0.40774 |  0:00:10s\n",
      "epoch 199| loss: 0.40729 |  0:00:10s\n",
      "epoch 200| loss: 0.40836 |  0:00:10s\n",
      "epoch 201| loss: 0.40763 |  0:00:10s\n",
      "epoch 202| loss: 0.40344 |  0:00:11s\n",
      "epoch 203| loss: 0.40402 |  0:00:11s\n",
      "epoch 204| loss: 0.40039 |  0:00:11s\n",
      "epoch 205| loss: 0.40296 |  0:00:11s\n",
      "epoch 206| loss: 0.40103 |  0:00:11s\n",
      "epoch 207| loss: 0.40608 |  0:00:11s\n",
      "epoch 208| loss: 0.39961 |  0:00:11s\n",
      "epoch 209| loss: 0.39421 |  0:00:11s\n",
      "epoch 210| loss: 0.39714 |  0:00:11s\n",
      "epoch 211| loss: 0.39417 |  0:00:11s\n",
      "epoch 212| loss: 0.38805 |  0:00:11s\n",
      "epoch 213| loss: 0.38948 |  0:00:11s\n",
      "epoch 214| loss: 0.39456 |  0:00:11s\n",
      "epoch 215| loss: 0.40131 |  0:00:11s\n",
      "epoch 216| loss: 0.41297 |  0:00:11s\n",
      "epoch 217| loss: 0.39998 |  0:00:11s\n",
      "epoch 218| loss: 0.39711 |  0:00:11s\n",
      "epoch 219| loss: 0.39655 |  0:00:12s\n",
      "epoch 220| loss: 0.39952 |  0:00:12s\n",
      "epoch 221| loss: 0.40157 |  0:00:12s\n",
      "epoch 222| loss: 0.39542 |  0:00:12s\n",
      "epoch 223| loss: 0.39806 |  0:00:12s\n",
      "epoch 224| loss: 0.40286 |  0:00:12s\n",
      "epoch 225| loss: 0.39057 |  0:00:12s\n",
      "epoch 226| loss: 0.39275 |  0:00:12s\n",
      "epoch 227| loss: 0.39757 |  0:00:12s\n",
      "epoch 228| loss: 0.38368 |  0:00:12s\n",
      "epoch 229| loss: 0.40278 |  0:00:12s\n",
      "epoch 230| loss: 0.40951 |  0:00:12s\n",
      "epoch 231| loss: 0.40369 |  0:00:12s\n",
      "epoch 232| loss: 0.3952  |  0:00:12s\n",
      "epoch 233| loss: 0.39382 |  0:00:12s\n",
      "epoch 234| loss: 0.39076 |  0:00:12s\n",
      "epoch 235| loss: 0.39362 |  0:00:12s\n",
      "epoch 236| loss: 0.38595 |  0:00:12s\n",
      "epoch 237| loss: 0.38438 |  0:00:12s\n",
      "epoch 238| loss: 0.3905  |  0:00:13s\n",
      "epoch 239| loss: 0.38978 |  0:00:13s\n",
      "epoch 240| loss: 0.39196 |  0:00:13s\n",
      "epoch 241| loss: 0.38214 |  0:00:13s\n",
      "epoch 242| loss: 0.37808 |  0:00:13s\n",
      "epoch 243| loss: 0.37374 |  0:00:13s\n",
      "epoch 244| loss: 0.3729  |  0:00:13s\n",
      "epoch 245| loss: 0.38047 |  0:00:13s\n",
      "epoch 246| loss: 0.38315 |  0:00:13s\n",
      "epoch 247| loss: 0.37653 |  0:00:13s\n",
      "epoch 248| loss: 0.38255 |  0:00:13s\n",
      "epoch 249| loss: 0.37994 |  0:00:13s\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.90231 |  0:00:00s\n",
      "epoch 1  | loss: 1.92949 |  0:00:00s\n",
      "epoch 2  | loss: 1.37742 |  0:00:00s\n",
      "epoch 3  | loss: 1.15669 |  0:00:00s\n",
      "epoch 4  | loss: 1.06807 |  0:00:00s\n",
      "epoch 5  | loss: 0.95705 |  0:00:00s\n",
      "epoch 6  | loss: 0.88122 |  0:00:00s\n",
      "epoch 7  | loss: 0.82636 |  0:00:00s\n",
      "epoch 8  | loss: 0.77003 |  0:00:00s\n",
      "epoch 9  | loss: 0.75357 |  0:00:00s\n",
      "epoch 10 | loss: 0.75704 |  0:00:00s\n",
      "epoch 11 | loss: 0.73747 |  0:00:00s\n",
      "epoch 12 | loss: 0.72338 |  0:00:00s\n",
      "epoch 13 | loss: 0.71209 |  0:00:00s\n",
      "epoch 14 | loss: 0.70145 |  0:00:00s\n",
      "epoch 15 | loss: 0.69366 |  0:00:00s\n",
      "epoch 16 | loss: 0.67982 |  0:00:00s\n",
      "epoch 17 | loss: 0.67365 |  0:00:01s\n",
      "epoch 18 | loss: 0.68209 |  0:00:01s\n",
      "epoch 19 | loss: 0.66388 |  0:00:01s\n",
      "epoch 20 | loss: 0.66684 |  0:00:01s\n",
      "epoch 21 | loss: 0.64925 |  0:00:01s\n",
      "epoch 22 | loss: 0.63807 |  0:00:01s\n",
      "epoch 23 | loss: 0.64677 |  0:00:01s\n",
      "epoch 24 | loss: 0.64434 |  0:00:01s\n",
      "epoch 25 | loss: 0.62204 |  0:00:01s\n",
      "epoch 26 | loss: 0.63246 |  0:00:01s\n",
      "epoch 27 | loss: 0.61476 |  0:00:01s\n",
      "epoch 28 | loss: 0.61632 |  0:00:01s\n",
      "epoch 29 | loss: 0.61703 |  0:00:01s\n",
      "epoch 30 | loss: 0.61985 |  0:00:01s\n",
      "epoch 31 | loss: 0.61738 |  0:00:01s\n",
      "epoch 32 | loss: 0.61439 |  0:00:02s\n",
      "epoch 33 | loss: 0.60382 |  0:00:02s\n",
      "epoch 34 | loss: 0.60846 |  0:00:02s\n",
      "epoch 35 | loss: 0.59406 |  0:00:02s\n",
      "epoch 36 | loss: 0.5898  |  0:00:02s\n",
      "epoch 37 | loss: 0.57685 |  0:00:02s\n",
      "epoch 38 | loss: 0.58001 |  0:00:02s\n",
      "epoch 39 | loss: 0.57484 |  0:00:02s\n",
      "epoch 40 | loss: 0.57902 |  0:00:02s\n",
      "epoch 41 | loss: 0.57488 |  0:00:02s\n",
      "epoch 42 | loss: 0.56293 |  0:00:02s\n",
      "epoch 43 | loss: 0.56299 |  0:00:02s\n",
      "epoch 44 | loss: 0.56321 |  0:00:02s\n",
      "epoch 45 | loss: 0.55667 |  0:00:02s\n",
      "epoch 46 | loss: 0.55822 |  0:00:02s\n",
      "epoch 47 | loss: 0.54653 |  0:00:02s\n",
      "epoch 48 | loss: 0.54754 |  0:00:02s\n",
      "epoch 49 | loss: 0.55463 |  0:00:02s\n",
      "epoch 50 | loss: 0.54925 |  0:00:03s\n",
      "epoch 51 | loss: 0.54462 |  0:00:03s\n",
      "epoch 52 | loss: 0.547   |  0:00:03s\n",
      "epoch 53 | loss: 0.54989 |  0:00:03s\n",
      "epoch 54 | loss: 0.54163 |  0:00:03s\n",
      "epoch 55 | loss: 0.54651 |  0:00:03s\n",
      "epoch 56 | loss: 0.53874 |  0:00:03s\n",
      "epoch 57 | loss: 0.54044 |  0:00:03s\n",
      "epoch 58 | loss: 0.54444 |  0:00:03s\n",
      "epoch 59 | loss: 0.53091 |  0:00:03s\n",
      "epoch 60 | loss: 0.53167 |  0:00:03s\n",
      "epoch 61 | loss: 0.53942 |  0:00:03s\n",
      "epoch 62 | loss: 0.53655 |  0:00:03s\n",
      "epoch 63 | loss: 0.53285 |  0:00:03s\n",
      "epoch 64 | loss: 0.52537 |  0:00:03s\n",
      "epoch 65 | loss: 0.53327 |  0:00:03s\n",
      "epoch 66 | loss: 0.52814 |  0:00:03s\n",
      "epoch 67 | loss: 0.53365 |  0:00:03s\n",
      "epoch 68 | loss: 0.53289 |  0:00:03s\n",
      "epoch 69 | loss: 0.53664 |  0:00:04s\n",
      "epoch 70 | loss: 0.53178 |  0:00:04s\n",
      "epoch 71 | loss: 0.52796 |  0:00:04s\n",
      "epoch 72 | loss: 0.51903 |  0:00:04s\n",
      "epoch 73 | loss: 0.52945 |  0:00:04s\n",
      "epoch 74 | loss: 0.52136 |  0:00:04s\n",
      "epoch 75 | loss: 0.51309 |  0:00:04s\n",
      "epoch 76 | loss: 0.52835 |  0:00:04s\n",
      "epoch 77 | loss: 0.52848 |  0:00:04s\n",
      "epoch 78 | loss: 0.52557 |  0:00:04s\n",
      "epoch 79 | loss: 0.51825 |  0:00:04s\n",
      "epoch 80 | loss: 0.52657 |  0:00:04s\n",
      "epoch 81 | loss: 0.51946 |  0:00:04s\n",
      "epoch 82 | loss: 0.49704 |  0:00:04s\n",
      "epoch 83 | loss: 0.50972 |  0:00:04s\n",
      "epoch 84 | loss: 0.49517 |  0:00:04s\n",
      "epoch 85 | loss: 0.48513 |  0:00:04s\n",
      "epoch 86 | loss: 0.48491 |  0:00:04s\n",
      "epoch 87 | loss: 0.48893 |  0:00:04s\n",
      "epoch 88 | loss: 0.483   |  0:00:05s\n",
      "epoch 89 | loss: 0.48378 |  0:00:05s\n",
      "epoch 90 | loss: 0.48773 |  0:00:05s\n",
      "epoch 91 | loss: 0.47772 |  0:00:05s\n",
      "epoch 92 | loss: 0.46676 |  0:00:05s\n",
      "epoch 93 | loss: 0.45965 |  0:00:05s\n",
      "epoch 94 | loss: 0.45952 |  0:00:05s\n",
      "epoch 95 | loss: 0.45115 |  0:00:05s\n",
      "epoch 96 | loss: 0.45839 |  0:00:05s\n",
      "epoch 97 | loss: 0.44291 |  0:00:05s\n",
      "epoch 98 | loss: 0.45081 |  0:00:05s\n",
      "epoch 99 | loss: 0.44711 |  0:00:05s\n",
      "epoch 100| loss: 0.4434  |  0:00:05s\n",
      "epoch 101| loss: 0.44754 |  0:00:05s\n",
      "epoch 102| loss: 0.43734 |  0:00:05s\n",
      "epoch 103| loss: 0.43996 |  0:00:05s\n",
      "epoch 104| loss: 0.44209 |  0:00:05s\n",
      "epoch 105| loss: 0.43928 |  0:00:05s\n",
      "epoch 106| loss: 0.43713 |  0:00:05s\n",
      "epoch 107| loss: 0.43933 |  0:00:06s\n",
      "epoch 108| loss: 0.43491 |  0:00:06s\n",
      "epoch 109| loss: 0.43381 |  0:00:06s\n",
      "epoch 110| loss: 0.42821 |  0:00:06s\n",
      "epoch 111| loss: 0.43191 |  0:00:06s\n",
      "epoch 112| loss: 0.41788 |  0:00:06s\n",
      "epoch 113| loss: 0.42756 |  0:00:06s\n",
      "epoch 114| loss: 0.4174  |  0:00:06s\n",
      "epoch 115| loss: 0.41497 |  0:00:06s\n",
      "epoch 116| loss: 0.41788 |  0:00:06s\n",
      "epoch 117| loss: 0.41338 |  0:00:06s\n",
      "epoch 118| loss: 0.41789 |  0:00:06s\n",
      "epoch 119| loss: 0.40861 |  0:00:06s\n",
      "epoch 120| loss: 0.40071 |  0:00:06s\n",
      "epoch 121| loss: 0.40509 |  0:00:06s\n",
      "epoch 122| loss: 0.3961  |  0:00:06s\n",
      "epoch 123| loss: 0.39815 |  0:00:06s\n",
      "epoch 124| loss: 0.39326 |  0:00:06s\n",
      "epoch 125| loss: 0.39345 |  0:00:06s\n",
      "epoch 126| loss: 0.38726 |  0:00:07s\n",
      "epoch 127| loss: 0.38424 |  0:00:07s\n",
      "epoch 128| loss: 0.38016 |  0:00:07s\n",
      "epoch 129| loss: 0.39551 |  0:00:07s\n",
      "epoch 130| loss: 0.38695 |  0:00:07s\n",
      "epoch 131| loss: 0.38472 |  0:00:07s\n",
      "epoch 132| loss: 0.38229 |  0:00:07s\n",
      "epoch 133| loss: 0.37813 |  0:00:07s\n",
      "epoch 134| loss: 0.37623 |  0:00:07s\n",
      "epoch 135| loss: 0.37518 |  0:00:07s\n",
      "epoch 136| loss: 0.36069 |  0:00:07s\n",
      "epoch 137| loss: 0.35812 |  0:00:07s\n",
      "epoch 138| loss: 0.36705 |  0:00:07s\n",
      "epoch 139| loss: 0.37056 |  0:00:07s\n",
      "epoch 140| loss: 0.3684  |  0:00:07s\n",
      "epoch 141| loss: 0.35666 |  0:00:07s\n",
      "epoch 142| loss: 0.37975 |  0:00:07s\n",
      "epoch 143| loss: 0.36656 |  0:00:07s\n",
      "epoch 144| loss: 0.35804 |  0:00:08s\n",
      "epoch 145| loss: 0.36312 |  0:00:08s\n",
      "epoch 146| loss: 0.35934 |  0:00:08s\n",
      "epoch 147| loss: 0.36404 |  0:00:08s\n",
      "epoch 148| loss: 0.37216 |  0:00:08s\n",
      "epoch 149| loss: 0.36929 |  0:00:08s\n",
      "epoch 150| loss: 0.37296 |  0:00:08s\n",
      "epoch 151| loss: 0.36823 |  0:00:08s\n",
      "epoch 152| loss: 0.36212 |  0:00:08s\n",
      "epoch 153| loss: 0.36164 |  0:00:08s\n",
      "epoch 154| loss: 0.35263 |  0:00:08s\n",
      "epoch 155| loss: 0.35098 |  0:00:08s\n",
      "epoch 156| loss: 0.34776 |  0:00:08s\n",
      "epoch 157| loss: 0.34541 |  0:00:08s\n",
      "epoch 158| loss: 0.33668 |  0:00:08s\n",
      "epoch 159| loss: 0.33667 |  0:00:08s\n",
      "epoch 160| loss: 0.33258 |  0:00:08s\n",
      "epoch 161| loss: 0.33655 |  0:00:08s\n",
      "epoch 162| loss: 0.33132 |  0:00:08s\n",
      "epoch 163| loss: 0.33108 |  0:00:09s\n",
      "epoch 164| loss: 0.32681 |  0:00:09s\n",
      "epoch 165| loss: 0.33261 |  0:00:09s\n",
      "epoch 166| loss: 0.3261  |  0:00:09s\n",
      "epoch 167| loss: 0.32573 |  0:00:09s\n",
      "epoch 168| loss: 0.32646 |  0:00:09s\n",
      "epoch 169| loss: 0.31939 |  0:00:09s\n",
      "epoch 170| loss: 0.31657 |  0:00:09s\n",
      "epoch 171| loss: 0.32173 |  0:00:09s\n",
      "epoch 172| loss: 0.31425 |  0:00:09s\n",
      "epoch 173| loss: 0.31325 |  0:00:09s\n",
      "epoch 174| loss: 0.32018 |  0:00:09s\n",
      "epoch 175| loss: 0.31977 |  0:00:09s\n",
      "epoch 176| loss: 0.31335 |  0:00:09s\n",
      "epoch 177| loss: 0.31208 |  0:00:09s\n",
      "epoch 178| loss: 0.3053  |  0:00:09s\n",
      "epoch 179| loss: 0.31285 |  0:00:09s\n",
      "epoch 180| loss: 0.31185 |  0:00:09s\n",
      "epoch 181| loss: 0.31028 |  0:00:09s\n",
      "epoch 182| loss: 0.31049 |  0:00:09s\n",
      "epoch 183| loss: 0.31347 |  0:00:09s\n",
      "epoch 184| loss: 0.30319 |  0:00:09s\n",
      "epoch 185| loss: 0.30681 |  0:00:10s\n",
      "epoch 186| loss: 0.305   |  0:00:10s\n",
      "epoch 187| loss: 0.30142 |  0:00:10s\n",
      "epoch 188| loss: 0.29797 |  0:00:10s\n",
      "epoch 189| loss: 0.29225 |  0:00:10s\n",
      "epoch 190| loss: 0.29492 |  0:00:10s\n",
      "epoch 191| loss: 0.30069 |  0:00:10s\n",
      "epoch 192| loss: 0.29927 |  0:00:10s\n",
      "epoch 193| loss: 0.30152 |  0:00:10s\n",
      "epoch 194| loss: 0.30097 |  0:00:10s\n",
      "epoch 195| loss: 0.2913  |  0:00:10s\n",
      "epoch 196| loss: 0.29622 |  0:00:10s\n",
      "epoch 197| loss: 0.29649 |  0:00:10s\n",
      "epoch 198| loss: 0.29487 |  0:00:10s\n",
      "epoch 199| loss: 0.28851 |  0:00:10s\n",
      "epoch 200| loss: 0.2973  |  0:00:10s\n",
      "epoch 201| loss: 0.29846 |  0:00:10s\n",
      "epoch 202| loss: 0.29116 |  0:00:10s\n",
      "epoch 203| loss: 0.29269 |  0:00:10s\n",
      "epoch 204| loss: 0.29241 |  0:00:10s\n",
      "epoch 205| loss: 0.28964 |  0:00:10s\n",
      "epoch 206| loss: 0.29939 |  0:00:11s\n",
      "epoch 207| loss: 0.30707 |  0:00:11s\n",
      "epoch 208| loss: 0.29447 |  0:00:11s\n",
      "epoch 209| loss: 0.29884 |  0:00:11s\n",
      "epoch 210| loss: 0.32105 |  0:00:11s\n",
      "epoch 211| loss: 0.33349 |  0:00:11s\n",
      "epoch 212| loss: 0.32417 |  0:00:11s\n",
      "epoch 213| loss: 0.31875 |  0:00:11s\n",
      "epoch 214| loss: 0.30872 |  0:00:11s\n",
      "epoch 215| loss: 0.31011 |  0:00:11s\n",
      "epoch 216| loss: 0.31316 |  0:00:11s\n",
      "epoch 217| loss: 0.29775 |  0:00:11s\n",
      "epoch 218| loss: 0.29292 |  0:00:11s\n",
      "epoch 219| loss: 0.28855 |  0:00:11s\n",
      "epoch 220| loss: 0.28268 |  0:00:11s\n",
      "epoch 221| loss: 0.28433 |  0:00:11s\n",
      "epoch 222| loss: 0.27778 |  0:00:11s\n",
      "epoch 223| loss: 0.2794  |  0:00:11s\n",
      "epoch 224| loss: 0.27071 |  0:00:11s\n",
      "epoch 225| loss: 0.27839 |  0:00:12s\n",
      "epoch 226| loss: 0.27389 |  0:00:12s\n",
      "epoch 227| loss: 0.27152 |  0:00:12s\n",
      "epoch 228| loss: 0.27819 |  0:00:12s\n",
      "epoch 229| loss: 0.28184 |  0:00:12s\n",
      "epoch 230| loss: 0.27813 |  0:00:12s\n",
      "epoch 231| loss: 0.27032 |  0:00:12s\n",
      "epoch 232| loss: 0.27677 |  0:00:12s\n",
      "epoch 233| loss: 0.27511 |  0:00:12s\n",
      "epoch 234| loss: 0.28196 |  0:00:12s\n",
      "epoch 235| loss: 0.27719 |  0:00:12s\n",
      "epoch 236| loss: 0.2858  |  0:00:12s\n",
      "epoch 237| loss: 0.27507 |  0:00:12s\n",
      "epoch 238| loss: 0.27528 |  0:00:12s\n",
      "epoch 239| loss: 0.27115 |  0:00:12s\n",
      "epoch 240| loss: 0.27142 |  0:00:12s\n",
      "epoch 241| loss: 0.27516 |  0:00:12s\n",
      "epoch 242| loss: 0.27995 |  0:00:12s\n",
      "epoch 243| loss: 0.27896 |  0:00:12s\n",
      "epoch 244| loss: 0.2671  |  0:00:12s\n",
      "epoch 245| loss: 0.26445 |  0:00:13s\n",
      "epoch 246| loss: 0.27289 |  0:00:13s\n",
      "epoch 247| loss: 0.27048 |  0:00:13s\n",
      "epoch 248| loss: 0.27478 |  0:00:13s\n",
      "epoch 249| loss: 0.26778 |  0:00:13s\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'PLSRegression_4_components', (2881, 348), (13, 348)])\n"
     ]
    }
   ],
   "source": [
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "\n",
    "    if any(result['params'] == params for result in all_dict_results):\n",
    "        # Skip this iteration if the combination exists\n",
    "        print(f\"Skipping existing combination: {params.values()}\")\n",
    "        \n",
    "        continue\n",
    "\n",
    "    try: \n",
    "    \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "        # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open(results_file, 'wb') as handle:\n",
    "    pickle.dump(all_dict_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickle/training_2_dict_results.pickle', \"rb\") as input_file:\n",
    "    dict_results_split = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'LinearRegression',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.2466392517089844,\n",
       "  'fitting_time': 0.22939062118530273,\n",
       "  'results_adj': {'mse_score': array([0.65983176, 0.48974311, 0.40951464, 0.56684282]),\n",
       "   'mae_score': array([0.63965764, 0.55144938, 0.56535002, 0.59999667]),\n",
       "   'r2': array([0.33312259, 0.44426866, 0.144955  , 0.31048062]),\n",
       "   'explained_variance': array([0.36730884, 0.47760179, 0.14545957, 0.43985495]),\n",
       "   'corr': array([0.60633336, 0.69220668, 0.4020632 , 0.66605104])},\n",
       "  'results_org': {'mse_score': array([0.65983176, 0.4897431 , 0.40951466, 0.56684279]),\n",
       "   'mae_score': array([0.63965764, 0.55144938, 0.56535003, 0.59999666]),\n",
       "   'r2': array([0.29810904, 0.45135258, 0.20239561, 0.33725209]),\n",
       "   'explained_variance': array([0.33409019, 0.48426081, 0.20286628, 0.46160331]),\n",
       "   'corr': array([0.57814397, 0.69816204, 0.46213839, 0.68191102])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskElasticNet',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.283444404602051,\n",
       "  'fitting_time': 0.09822368621826172,\n",
       "  'results_adj': {'mse_score': array([1.18012248, 0.95041535, 0.50741149, 1.02939714]),\n",
       "   'mae_score': array([0.88414168, 0.78683573, 0.65378091, 0.82689555]),\n",
       "   'r2': array([-0.19272377, -0.07847478, -0.05944846, -0.25218006]),\n",
       "   'explained_variance': array([ 0.11229868,  0.03189858, -0.05332468,  0.12857136]),\n",
       "   'corr': array([ 0.42176385,  0.17864276, -0.16306812,  0.44631446])},\n",
       "  'results_org': {'mse_score': array([1.18012249, 0.95041535, 0.50741151, 1.02939711]),\n",
       "   'mae_score': array([0.88414168, 0.78683573, 0.65378092, 0.82689554]),\n",
       "   'r2': array([-0.25534622, -0.06472746,  0.01172365, -0.20356259]),\n",
       "   'explained_variance': array([0.06569104, 0.04423898, 0.01743605, 0.16240568]),\n",
       "   'corr': array([0.25863755, 0.21083052, 0.14561963, 0.49623359])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskElasticNet_tuned',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.2664968967437744,\n",
       "  'fitting_time': 1.4926152229309082,\n",
       "  'results_adj': {'mse_score': array([0.66791342, 0.47856617, 0.41412878, 0.59264981]),\n",
       "   'mae_score': array([0.63043587, 0.55392437, 0.58019171, 0.59956285]),\n",
       "   'r2': array([0.32495464, 0.45695159, 0.13532092, 0.27908847]),\n",
       "   'explained_variance': array([0.3700506 , 0.49637797, 0.13598028, 0.44010284]),\n",
       "   'corr': array([0.60831821, 0.70457467, 0.38152442, 0.66946516])},\n",
       "  'results_org': {'mse_score': array([0.66791342, 0.47856617, 0.4141288 , 0.59264978]),\n",
       "   'mae_score': array([0.63043586, 0.55392437, 0.58019172, 0.59956284]),\n",
       "   'r2': array([0.28951224, 0.46387383, 0.19340873, 0.30707878]),\n",
       "   'explained_variance': array([0.33697591, 0.50279765, 0.19402379, 0.46184157]),\n",
       "   'corr': array([0.58049709, 0.70936568, 0.44690586, 0.68551862])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskLasso',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.283153772354126,\n",
       "  'fitting_time': 0.07776927947998047,\n",
       "  'results_adj': {'mse_score': array([1.39363767, 1.04056339, 0.48866937, 1.24000841]),\n",
       "   'mae_score': array([0.99127575, 0.78457157, 0.62110666, 0.93446712]),\n",
       "   'r2': array([-0.40851885, -0.18076941, -0.0203159 , -0.50837198]),\n",
       "   'explained_variance': array([-2.22044605e-16,  0.00000000e+00,  0.00000000e+00,  1.11022302e-16]),\n",
       "   'corr': array([nan, nan, nan, nan])},\n",
       "  'results_org': {'mse_score': array([1.39363768, 1.04056339, 0.48866939, 1.24000838]),\n",
       "   'mae_score': array([0.99127576, 0.78457157, 0.62110667, 0.93446711]),\n",
       "   'r2': array([-0.48247136, -0.16571814,  0.04822734, -0.44980754]),\n",
       "   'explained_variance': array([-0.05250375,  0.01274701,  0.06717846,  0.03882626]),\n",
       "   'corr': array([-0.0848666 ,  0.11455326,  0.2721653 ,  0.20679355])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskLasso_tuned',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.3114430904388428,\n",
       "  'fitting_time': 1.4922456741333008,\n",
       "  'results_adj': {'mse_score': array([0.6555347 , 0.47602153, 0.41898635, 0.58247095]),\n",
       "   'mae_score': array([0.62193632, 0.55058805, 0.57780272, 0.59499437]),\n",
       "   'r2': array([0.33746553, 0.45983909, 0.12517858, 0.29147024]),\n",
       "   'explained_variance': array([0.38303248, 0.49996522, 0.12605199, 0.44788336]),\n",
       "   'corr': array([0.61892446, 0.70715313, 0.37654024, 0.67491708])},\n",
       "  'results_org': {'mse_score': array([0.6555347 , 0.47602153, 0.41898636, 0.58247092]),\n",
       "   'mae_score': array([0.62193631, 0.55058805, 0.57780274, 0.59499436]),\n",
       "   'r2': array([0.30268   , 0.46672453, 0.18394773, 0.31897981]),\n",
       "   'explained_variance': array([0.35063939, 0.50633917, 0.18476246, 0.46932   ]),\n",
       "   'corr': array([0.59223356, 0.71197064, 0.44079738, 0.69050252])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'RandomForestRegressor',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.289330244064331,\n",
       "  'fitting_time': 51.79721212387085,\n",
       "  'results_adj': {'mse_score': array([0.81204962, 0.55738872, 0.42447239, 0.76265381]),\n",
       "   'mae_score': array([0.66916949, 0.63959102, 0.60835909, 0.64774379]),\n",
       "   'r2': array([0.17927936, 0.36750845, 0.11372402, 0.07229206]),\n",
       "   'explained_variance': array([0.38575368, 0.43668934, 0.11735442, 0.37009751]),\n",
       "   'corr': array([0.63471399, 0.67665811, 0.34287971, 0.62716925])},\n",
       "  'results_org': {'mse_score': array([0.81204962, 0.55738872, 0.42447241, 0.76265379]),\n",
       "   'mae_score': array([0.6691695 , 0.63959102, 0.6083591 , 0.64774379]),\n",
       "   'r2': array([0.13618846, 0.37557082, 0.17326266, 0.10831149]),\n",
       "   'explained_variance': array([0.35350346, 0.44386986, 0.17664918, 0.39455427]),\n",
       "   'corr': array([0.60552878, 0.68293217, 0.42214619, 0.66388455])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'PLSRegression_4_components',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.3004002571105957,\n",
       "  'fitting_time': 0.15016818046569824,\n",
       "  'results_adj': {'mse_score': array([0.99283749, 0.68320591, 0.51129344, 0.88042711]),\n",
       "   'mae_score': array([0.8049879 , 0.73842664, 0.68894711, 0.76406833]),\n",
       "   'r2': array([-0.00343896,  0.22473859, -0.06755375, -0.07096982]),\n",
       "   'explained_variance': array([ 0.25778331,  0.29950329, -0.06753554,  0.2736962 ]),\n",
       "   'corr': array([0.50966098, 0.55561882, 0.0742507 , 0.52674345])},\n",
       "  'results_org': {'mse_score': array([0.99283749, 0.68320591, 0.51129345, 0.88042708]),\n",
       "   'mae_score': array([0.80498791, 0.73842664, 0.68894712, 0.76406832]),\n",
       "   'r2': array([-0.05612325,  0.23462085,  0.00416286, -0.02938806]),\n",
       "   'explained_variance': array([0.21881416, 0.30843253, 0.00417985, 0.30189586]),\n",
       "   'corr': array([0.46922857, 0.57093376, 0.17716915, 0.57292837])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'XGBoostRegressor',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': None,\n",
       "  'results_adj': None,\n",
       "  'results_org': None},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'XGBoostRegressor_tuned',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': None,\n",
       "  'results_adj': None,\n",
       "  'results_org': None},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'TabNetRegressor_default',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.3759474754333496,\n",
       "  'fitting_time': 13.77210783958435,\n",
       "  'results_adj': {'mse_score': array([0.61824897, 0.61184618, 0.36866036, 0.57763236]),\n",
       "   'mae_score': array([0.69107406, 0.60773177, 0.5361999 , 0.59462053]),\n",
       "   'r2': array([0.3751494 , 0.30571336, 0.23025659, 0.297356  ]),\n",
       "   'explained_variance': array([0.56567234, 0.38915553, 0.23426406, 0.55584824]),\n",
       "   'corr': array([0.77300748, 0.67628222, 0.58077844, 0.81923406])},\n",
       "  'results_org': {'mse_score': array([0.61824897, 0.61184618, 0.36866037, 0.57763234]),\n",
       "   'mae_score': array([0.69107407, 0.60773177, 0.53619991, 0.59462053]),\n",
       "   'r2': array([0.34234241, 0.31456344, 0.28196676, 0.32463705]),\n",
       "   'explained_variance': array([0.54286851, 0.39694198, 0.28570502, 0.57309301]),\n",
       "   'corr': array([0.76293555, 0.67836772, 0.73113403, 0.85040194])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'TabNetRegressor_custom',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.4543991088867188,\n",
       "  'fitting_time': 13.39520812034607,\n",
       "  'results_adj': {'mse_score': array([1.07617402, 1.31252905, 0.77527955, 1.0442195 ]),\n",
       "   'mae_score': array([0.76465789, 0.96862147, 0.76452422, 0.76252167]),\n",
       "   'r2': array([-0.08766535, -0.48937986, -0.61874284, -0.27021028]),\n",
       "   'explained_variance': array([ 0.20827438, -0.34330108, -0.60463007,  0.30404738]),\n",
       "   'corr': array([ 0.54009087,  0.30700851, -0.06850123,  0.58642515])},\n",
       "  'results_org': {'mse_score': array([1.07617403, 1.31252906, 0.77527957, 1.04421947]),\n",
       "   'mae_score': array([0.76465789, 0.96862147, 0.76452423, 0.76252167]),\n",
       "   'r2': array([-0.14477185, -0.47039474, -0.5099982 , -0.22089277]),\n",
       "   'explained_variance': array([ 0.16670582, -0.32617802, -0.4968335 ,  0.33106863]),\n",
       "   'corr': array([0.50999118, 0.31151104, 0.0452715 , 0.59746408])}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_results_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Pytorch Tabular models as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = ['APOE_epsilon2', 'APOE_epsilon3', 'APOE_epsilon4']\n",
    "continuous_features = [col for col in df_X_train.columns if col not in ordinal_features]\n",
    "\n",
    "# Prepare Tabular configurations (shared for all PyTorch models)\n",
    "data_config = DataConfig(\n",
    "    target=df_y_train.columns.tolist(),\n",
    "    continuous_cols=continuous_features,\n",
    "    categorical_cols=ordinal_features\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024, max_epochs=1, auto_lr_find=True,\n",
    "    early_stopping=\"valid_loss\", early_stopping_mode=\"min\", early_stopping_patience=5,\n",
    "    checkpoints=\"valid_loss\", load_best=True, progress_bar=\"nones\",\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "head_config = LinearHeadConfig(dropout=0.1).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'LinearRegression', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskElasticNet', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskElasticNet_tuned', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskLasso', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskLasso_tuned', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'RandomForestRegressor', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'XGBoostRegressor', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'XGBoostRegressor_tuned', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'TabNetRegressor_default', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'TabNetRegressor_custom', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_most_frequent', 'KNNImputer', 'PLSRegression_4_components', (2881, 348), (13, 348)])\n",
      "GatedAdditiveTreeEnsembleConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">625</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:25\u001b[0m,\u001b[1;36m625\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">642</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:25\u001b[0m,\u001b[1;36m642\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">648</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:25\u001b[0m,\u001b[1;36m648\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">681</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:25\u001b[0m,\u001b[1;36m681\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">941</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:25\u001b[0m,\u001b[1;36m941\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">963</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:25\u001b[0m,\u001b[1;36m963\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">973</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:25\u001b[0m,\u001b[1;36m973\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX 6000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9a68e27c3e457ba4e0e042d7465be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_09593d88-896b-4da2-9a80-b0385f2033b3.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_09593d88-896b-4da2-9a80-b0385f2033b3.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">984</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:28\u001b[0m,\u001b[1;36m984\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">999</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:28\u001b[0m,\u001b[1;36m999\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">018</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:29\u001b[0m,\u001b[1;36m018\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                       | Params | Mode \n",
      "------------------------------------------------------------------------\n",
      "0 | _backbone        | GatedAdditiveTreesBackbone | 5.6 M  | train\n",
      "1 | _embedding_layer | Embedding1dLayer           | 714    | train\n",
      "2 | _head            | CustomHead                 | 156    | train\n",
      "3 | loss             | MSELoss                    | 0      | train\n",
      "------------------------------------------------------------------------\n",
      "5.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.6 M     Total params\n",
      "22.286    Total estimated model params size (MB)\n",
      "692       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e92cca7d74449628d506ce3412f913f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28683ec2f4d4d958deb41ccfdbe5bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b289c6d9e344c9a1f501b4f513fa19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">794</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:31\u001b[0m,\u001b[1;36m794\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">795</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:31\u001b[0m,\u001b[1;36m795\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "DANetConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:40</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">070</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:40\u001b[0m,\u001b[1;36m070\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:40</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">085</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:40\u001b[0m,\u001b[1;36m085\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:40</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">093</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:40\u001b[0m,\u001b[1;36m093\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:40</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">126</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: DANetModel             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:40\u001b[0m,\u001b[1;36m126\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: DANetModel             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:40</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">196</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:40\u001b[0m,\u001b[1;36m196\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:40</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">204</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:40\u001b[0m,\u001b[1;36m204\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db00f0bd9ba4e639d66d87b453c64ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_b60ab903-b48c-4002-a8cc-83f9e4029532.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_b60ab903-b48c-4002-a8cc-83f9e4029532.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:40</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:40\u001b[0m,\u001b[1;36m669\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:40</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">677</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:40\u001b[0m,\u001b[1;36m677\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | _backbone        | DANetBackbone    | 2.3 M  | train\n",
      "1 | _embedding_layer | Embedding1dLayer | 714    | train\n",
      "2 | _head            | LinearHead       | 260    | train\n",
      "3 | loss             | MSELoss          | 0      | train\n",
      "--------------------------------------------------------------\n",
      "2.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 M     Total params\n",
      "9.101     Total estimated model params size (MB)\n",
      "159       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e23c42ce8a4328af04723fcb9d6329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9f1901ed2e471d97f10b7daed2c72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84f331556d2465781dd2cea32b49449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:41</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">145</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:41\u001b[0m,\u001b[1;36m145\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:41</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:41\u001b[0m,\u001b[1;36m146\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "TabTransformerConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:48</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">658</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:48\u001b[0m,\u001b[1;36m658\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:48</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">672</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:48\u001b[0m,\u001b[1;36m672\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:48</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">680</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:48\u001b[0m,\u001b[1;36m680\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:48</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">715</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabTransformerModel    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:48\u001b[0m,\u001b[1;36m715\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabTransformerModel    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:48</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">775</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:48\u001b[0m,\u001b[1;36m775\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:48</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">782</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:48\u001b[0m,\u001b[1;36m782\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d91bb6445ee4670bd8c3c642679f2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_26914fa1-e27f-4140-afb8-49bf3e993188.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_26914fa1-e27f-4140-afb8-49bf3e993188.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:49</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">205</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:49\u001b[0m,\u001b[1;36m205\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:49</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">208</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:49\u001b[0m,\u001b[1;36m208\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                   | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | _backbone        | TabTransformerBackbone | 271 K  | train\n",
      "1 | _embedding_layer | Embedding2dLayer       | 408    | train\n",
      "2 | _head            | LinearHead             | 1.8 K  | train\n",
      "3 | loss             | MSELoss                | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "274 K     Trainable params\n",
      "0         Non-trainable params\n",
      "274 K     Total params\n",
      "1.097     Total estimated model params size (MB)\n",
      "125       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943e45ea9534489c84eaffbca679fc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befd41bb109a4bffad8c3bf62fd6df39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5544178850124d14a01576eeef543568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:49</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">437</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:49\u001b[0m,\u001b[1;36m437\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:49</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">438</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:49\u001b[0m,\u001b[1;36m438\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "TabNetModelConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">866</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:56\u001b[0m,\u001b[1;36m866\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">881</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:56\u001b[0m,\u001b[1;36m881\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">889</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:56\u001b[0m,\u001b[1;36m889\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">922</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:56\u001b[0m,\u001b[1;36m922\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:57</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">751</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:57\u001b[0m,\u001b[1;36m751\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:57</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">758</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:57\u001b[0m,\u001b[1;36m758\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6add858973c445da8adb1b69a485fdd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_4f18bcbf-eef6-402a-9ea1-f23087af8d56.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_4f18bcbf-eef6-402a-9ea1-f23087af8d56.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:58</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">053</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:58\u001b[0m,\u001b[1;36m053\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:58</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">055</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:58\u001b[0m,\u001b[1;36m055\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type           | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | _embedding_layer | Identity       | 0      | train\n",
      "1 | _backbone        | TabNetBackbone | 28.8 K | train\n",
      "2 | _head            | Identity       | 0      | train\n",
      "3 | loss             | MSELoss        | 0      | train\n",
      "------------------------------------------------------------\n",
      "28.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "28.8 K    Total params\n",
      "0.115     Total estimated model params size (MB)\n",
      "111       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cae165266f4450abcba57049fece76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52610314a0bf4cc9966fa88b754093b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2a265ef3a249f2be523d20a061fc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:58</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">286</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:58\u001b[0m,\u001b[1;36m286\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:26:58</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">287</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:26:58\u001b[0m,\u001b[1;36m287\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": [
    "predictive_models_list += [\n",
    "    (\"GatedAdditiveTreeEnsembleConfig_tab\", \n",
    "    TabularModelWrapper(\n",
    "        GatedAdditiveTreeEnsembleConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        gflu_stages=6,\n",
    "        gflu_dropout=0.0,\n",
    "        tree_depth=5,\n",
    "        num_trees=20,\n",
    "        chain_trees=False,\n",
    "        share_head_weights=True), data_config, trainer_config, optimizer_config \n",
    "    )),\n",
    "    (\"DANetConfig_tab\",\n",
    "    TabularModelWrapper(\n",
    "        DANetConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_layers=8,\n",
    "        k=5,\n",
    "        dropout_rate=0.1), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabTransformerConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        embedding_initialization=\"kaiming_uniform\",\n",
    "        embedding_bias=False), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    # (\"FTTransformerConfig\",\n",
    "    #     TabularModelWrapper(\n",
    "    #     FTTransformerConfig(\n",
    "    #     task=\"regression\",\n",
    "    #     head=\"LinearHead\",\n",
    "    #     head_config=head_config), data_config, trainer_config, optimizer_config\n",
    "    # )),\n",
    "    (\"TabNetModelConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        n_independent=2,\n",
    "        n_shared=2), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "\n",
    "    if any(result['params'] == params for result in all_dict_results):\n",
    "        # Skip this iteration if the combination exists\n",
    "        print(f\"Skipping existing combination: {params.values()}\")\n",
    "        \n",
    "        continue\n",
    "\n",
    "    try: \n",
    "        print(name_model)\n",
    "        \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "        # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models only on MRI features to compare performances\n",
    "\n",
    "## Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = list(df_X.isna().any(axis=1))\n",
    "idx_test = list(~df_X.isna().any(axis=1))\n",
    "\n",
    "set_intersect_rid = set(df_all[idx_train].RID).intersection(set(df_all[idx_test].RID))\n",
    "intersect_rid_idx = df_all.RID.isin(set_intersect_rid)\n",
    "\n",
    "for i, bool_test in enumerate(idx_test): \n",
    "    if intersect_rid_idx.iloc[i] & bool_test:\n",
    "        idx_test[i] = False\n",
    "        idx_train[i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]] = df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train = df_X[dict_select[\"MRIth\"]].loc[idx_train]\n",
    "df_X_test = df_X[dict_select[\"MRIth\"]].loc[idx_test]\n",
    "\n",
    "df_y_train = df_y.loc[idx_train]\n",
    "df_y_test = df_y.loc[idx_test]\n",
    "\n",
    "c_train = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_train]\n",
    "c_test = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: LinearRegression\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskElasticNet\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskElasticNet_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskLasso\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskLasso_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: RandomForestRegressor\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: XGBoostRegressor\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: XGBoostRegressor_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: TabNetRegressor_default\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: TabNetRegressor_custom\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: PLSRegression_4_components\n",
      "Combinations of preprocessing and models to test : 11\n"
     ]
    }
   ],
   "source": [
    "random_state=42\n",
    "n_imputation_iter = 10\n",
    "\n",
    "# Define hyperparameters\n",
    "gain_parameters = {\n",
    "    'hint_rate': 0.9,\n",
    "    'alpha': 100,\n",
    "    'iterations': 1000\n",
    "}\n",
    "\n",
    "# Continuous Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "continuous_imputer_list = [\n",
    "    (\"NoImputer\", KNNImputer(n_neighbors=1)),\n",
    "\n",
    "]\n",
    "\n",
    "# Ordinal Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "ordinal_imputer_list = [\n",
    "    (\"NoImputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "]\n",
    "\n",
    "# Predictive Models List (list of tuples with unique strings and corresponding instances)\n",
    "predictive_models_list = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"MultiTaskElasticNet\", MultiTaskElasticNet()),\n",
    "    (\"MultiTaskElasticNet_tuned\", MultiTaskElasticNet(**{'alpha': 0.01, 'l1_ratio': 0.01})),\n",
    "    (\"MultiTaskLasso\", MultiTaskLasso()),\n",
    "    (\"MultiTaskLasso_tuned\", MultiTaskLasso(**{'alpha': 0.001})),\n",
    "    (\"RandomForestRegressor\", RandomForestRegressor()),\n",
    "    (\"XGBoostRegressor\", XGBoostRegressor()),\n",
    "    (\"XGBoostRegressor_tuned\", XGBoostRegressor(**{'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105})),\n",
    "    (\"TabNetRegressor_default\", TabNetModelWrapper(n_a=8, n_d=8)),\n",
    "    (\"TabNetRegressor_custom\", TabNetModelWrapper(n_a=32, n_d=32)),\n",
    "    (\"PLSRegression_4_components\", PLSRegression(n_components=4))\n",
    "]\n",
    "\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "# Display all combinations\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    print(f\"Continuous Imputer: {continuous_imputer[0]}, Ordinal Imputer: {ordinal_imputer[0]}, Model: {model[0]}\")\n",
    "\n",
    "print(f\"Combinations of preprocessing and models to test : {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HDF5 file\n",
    "results_file = '../pickle/training_2_dict_results.pickle'\n",
    "\n",
    "with open(results_file, \"rb\") as input_file:\n",
    "    all_dict_results = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.13311 |  0:00:00s\n",
      "epoch 1  | loss: 1.67434 |  0:00:00s\n",
      "epoch 2  | loss: 1.25121 |  0:00:00s\n",
      "epoch 3  | loss: 1.15286 |  0:00:00s\n",
      "epoch 4  | loss: 1.05784 |  0:00:00s\n",
      "epoch 5  | loss: 0.96523 |  0:00:00s\n",
      "epoch 6  | loss: 0.92765 |  0:00:00s\n",
      "epoch 7  | loss: 0.89131 |  0:00:00s\n",
      "epoch 8  | loss: 0.86607 |  0:00:00s\n",
      "epoch 9  | loss: 0.82915 |  0:00:00s\n",
      "epoch 10 | loss: 0.83506 |  0:00:00s\n",
      "epoch 11 | loss: 0.7886  |  0:00:00s\n",
      "epoch 12 | loss: 0.75816 |  0:00:00s\n",
      "epoch 13 | loss: 0.76323 |  0:00:00s\n",
      "epoch 14 | loss: 0.76212 |  0:00:00s\n",
      "epoch 15 | loss: 0.75083 |  0:00:00s\n",
      "epoch 16 | loss: 0.72542 |  0:00:00s\n",
      "epoch 17 | loss: 0.71856 |  0:00:00s\n",
      "epoch 18 | loss: 0.71925 |  0:00:01s\n",
      "epoch 19 | loss: 0.70533 |  0:00:01s\n",
      "epoch 20 | loss: 0.70061 |  0:00:01s\n",
      "epoch 21 | loss: 0.69113 |  0:00:01s\n",
      "epoch 22 | loss: 0.67685 |  0:00:01s\n",
      "epoch 23 | loss: 0.67698 |  0:00:01s\n",
      "epoch 24 | loss: 0.67197 |  0:00:01s\n",
      "epoch 25 | loss: 0.66981 |  0:00:01s\n",
      "epoch 26 | loss: 0.66185 |  0:00:01s\n",
      "epoch 27 | loss: 0.65994 |  0:00:01s\n",
      "epoch 28 | loss: 0.65575 |  0:00:01s\n",
      "epoch 29 | loss: 0.65478 |  0:00:01s\n",
      "epoch 30 | loss: 0.64765 |  0:00:01s\n",
      "epoch 31 | loss: 0.64554 |  0:00:01s\n",
      "epoch 32 | loss: 0.63432 |  0:00:01s\n",
      "epoch 33 | loss: 0.62939 |  0:00:01s\n",
      "epoch 34 | loss: 0.62534 |  0:00:01s\n",
      "epoch 35 | loss: 0.62469 |  0:00:02s\n",
      "epoch 36 | loss: 0.62174 |  0:00:02s\n",
      "epoch 37 | loss: 0.61922 |  0:00:02s\n",
      "epoch 38 | loss: 0.62018 |  0:00:02s\n",
      "epoch 39 | loss: 0.62086 |  0:00:02s\n",
      "epoch 40 | loss: 0.60617 |  0:00:02s\n",
      "epoch 41 | loss: 0.60947 |  0:00:02s\n",
      "epoch 42 | loss: 0.59172 |  0:00:02s\n",
      "epoch 43 | loss: 0.58443 |  0:00:02s\n",
      "epoch 44 | loss: 0.58781 |  0:00:02s\n",
      "epoch 45 | loss: 0.57683 |  0:00:02s\n",
      "epoch 46 | loss: 0.57306 |  0:00:02s\n",
      "epoch 47 | loss: 0.57694 |  0:00:02s\n",
      "epoch 48 | loss: 0.56855 |  0:00:02s\n",
      "epoch 49 | loss: 0.56316 |  0:00:02s\n",
      "epoch 50 | loss: 0.56719 |  0:00:02s\n",
      "epoch 51 | loss: 0.56633 |  0:00:02s\n",
      "epoch 52 | loss: 0.55997 |  0:00:02s\n",
      "epoch 53 | loss: 0.56786 |  0:00:02s\n",
      "epoch 54 | loss: 0.55522 |  0:00:02s\n",
      "epoch 55 | loss: 0.55718 |  0:00:03s\n",
      "epoch 56 | loss: 0.55059 |  0:00:03s\n",
      "epoch 57 | loss: 0.55296 |  0:00:03s\n",
      "epoch 58 | loss: 0.55163 |  0:00:03s\n",
      "epoch 59 | loss: 0.54834 |  0:00:03s\n",
      "epoch 60 | loss: 0.55297 |  0:00:03s\n",
      "epoch 61 | loss: 0.5449  |  0:00:03s\n",
      "epoch 62 | loss: 0.5401  |  0:00:03s\n",
      "epoch 63 | loss: 0.53168 |  0:00:03s\n",
      "epoch 64 | loss: 0.52247 |  0:00:03s\n",
      "epoch 65 | loss: 0.52582 |  0:00:03s\n",
      "epoch 66 | loss: 0.51292 |  0:00:03s\n",
      "epoch 67 | loss: 0.51423 |  0:00:03s\n",
      "epoch 68 | loss: 0.50964 |  0:00:03s\n",
      "epoch 69 | loss: 0.50876 |  0:00:03s\n",
      "epoch 70 | loss: 0.50005 |  0:00:03s\n",
      "epoch 71 | loss: 0.50396 |  0:00:03s\n",
      "epoch 72 | loss: 0.49637 |  0:00:03s\n",
      "epoch 73 | loss: 0.49309 |  0:00:03s\n",
      "epoch 74 | loss: 0.48867 |  0:00:03s\n",
      "epoch 75 | loss: 0.48842 |  0:00:03s\n",
      "epoch 76 | loss: 0.48183 |  0:00:04s\n",
      "epoch 77 | loss: 0.48301 |  0:00:04s\n",
      "epoch 78 | loss: 0.48205 |  0:00:04s\n",
      "epoch 79 | loss: 0.48052 |  0:00:04s\n",
      "epoch 80 | loss: 0.47403 |  0:00:04s\n",
      "epoch 81 | loss: 0.48338 |  0:00:04s\n",
      "epoch 82 | loss: 0.47462 |  0:00:04s\n",
      "epoch 83 | loss: 0.47119 |  0:00:04s\n",
      "epoch 84 | loss: 0.47134 |  0:00:04s\n",
      "epoch 85 | loss: 0.4746  |  0:00:04s\n",
      "epoch 86 | loss: 0.47936 |  0:00:04s\n",
      "epoch 87 | loss: 0.48048 |  0:00:04s\n",
      "epoch 88 | loss: 0.46815 |  0:00:04s\n",
      "epoch 89 | loss: 0.47599 |  0:00:04s\n",
      "epoch 90 | loss: 0.46779 |  0:00:04s\n",
      "epoch 91 | loss: 0.46429 |  0:00:04s\n",
      "epoch 92 | loss: 0.46651 |  0:00:04s\n",
      "epoch 93 | loss: 0.46411 |  0:00:04s\n",
      "epoch 94 | loss: 0.45887 |  0:00:04s\n",
      "epoch 95 | loss: 0.46711 |  0:00:04s\n",
      "epoch 96 | loss: 0.45915 |  0:00:04s\n",
      "epoch 97 | loss: 0.46154 |  0:00:05s\n",
      "epoch 98 | loss: 0.45802 |  0:00:05s\n",
      "epoch 99 | loss: 0.45159 |  0:00:05s\n",
      "epoch 100| loss: 0.45073 |  0:00:05s\n",
      "epoch 101| loss: 0.45322 |  0:00:05s\n",
      "epoch 102| loss: 0.44542 |  0:00:05s\n",
      "epoch 103| loss: 0.44681 |  0:00:05s\n",
      "epoch 104| loss: 0.44443 |  0:00:05s\n",
      "epoch 105| loss: 0.44417 |  0:00:05s\n",
      "epoch 106| loss: 0.44086 |  0:00:05s\n",
      "epoch 107| loss: 0.44396 |  0:00:05s\n",
      "epoch 108| loss: 0.43298 |  0:00:05s\n",
      "epoch 109| loss: 0.44147 |  0:00:05s\n",
      "epoch 110| loss: 0.42825 |  0:00:06s\n",
      "epoch 111| loss: 0.43181 |  0:00:06s\n",
      "epoch 112| loss: 0.43651 |  0:00:06s\n",
      "epoch 113| loss: 0.42791 |  0:00:06s\n",
      "epoch 114| loss: 0.43285 |  0:00:06s\n",
      "epoch 115| loss: 0.43182 |  0:00:06s\n",
      "epoch 116| loss: 0.4306  |  0:00:06s\n",
      "epoch 117| loss: 0.42956 |  0:00:06s\n",
      "epoch 118| loss: 0.42473 |  0:00:06s\n",
      "epoch 119| loss: 0.42655 |  0:00:06s\n",
      "epoch 120| loss: 0.42597 |  0:00:06s\n",
      "epoch 121| loss: 0.41854 |  0:00:06s\n",
      "epoch 122| loss: 0.41561 |  0:00:06s\n",
      "epoch 123| loss: 0.41897 |  0:00:06s\n",
      "epoch 124| loss: 0.41393 |  0:00:06s\n",
      "epoch 125| loss: 0.41506 |  0:00:06s\n",
      "epoch 126| loss: 0.4127  |  0:00:06s\n",
      "epoch 127| loss: 0.41012 |  0:00:06s\n",
      "epoch 128| loss: 0.42201 |  0:00:06s\n",
      "epoch 129| loss: 0.41418 |  0:00:07s\n",
      "epoch 130| loss: 0.4124  |  0:00:07s\n",
      "epoch 131| loss: 0.40676 |  0:00:07s\n",
      "epoch 132| loss: 0.40992 |  0:00:07s\n",
      "epoch 133| loss: 0.41542 |  0:00:07s\n",
      "epoch 134| loss: 0.411   |  0:00:07s\n",
      "epoch 135| loss: 0.40383 |  0:00:07s\n",
      "epoch 136| loss: 0.40702 |  0:00:07s\n",
      "epoch 137| loss: 0.41123 |  0:00:07s\n",
      "epoch 138| loss: 0.40235 |  0:00:07s\n",
      "epoch 139| loss: 0.41131 |  0:00:07s\n",
      "epoch 140| loss: 0.40156 |  0:00:07s\n",
      "epoch 141| loss: 0.40275 |  0:00:07s\n",
      "epoch 142| loss: 0.40435 |  0:00:07s\n",
      "epoch 143| loss: 0.39838 |  0:00:07s\n",
      "epoch 144| loss: 0.39697 |  0:00:07s\n",
      "epoch 145| loss: 0.40627 |  0:00:07s\n",
      "epoch 146| loss: 0.39956 |  0:00:07s\n",
      "epoch 147| loss: 0.39254 |  0:00:08s\n",
      "epoch 148| loss: 0.39673 |  0:00:08s\n",
      "epoch 149| loss: 0.39021 |  0:00:08s\n",
      "epoch 150| loss: 0.38701 |  0:00:08s\n",
      "epoch 151| loss: 0.39155 |  0:00:08s\n",
      "epoch 152| loss: 0.38715 |  0:00:08s\n",
      "epoch 153| loss: 0.38539 |  0:00:08s\n",
      "epoch 154| loss: 0.3805  |  0:00:08s\n",
      "epoch 155| loss: 0.38658 |  0:00:08s\n",
      "epoch 156| loss: 0.38244 |  0:00:08s\n",
      "epoch 157| loss: 0.38939 |  0:00:08s\n",
      "epoch 158| loss: 0.38604 |  0:00:08s\n",
      "epoch 159| loss: 0.38263 |  0:00:08s\n",
      "epoch 160| loss: 0.383   |  0:00:08s\n",
      "epoch 161| loss: 0.38275 |  0:00:08s\n",
      "epoch 162| loss: 0.3747  |  0:00:08s\n",
      "epoch 163| loss: 0.38457 |  0:00:09s\n",
      "epoch 164| loss: 0.38214 |  0:00:09s\n",
      "epoch 165| loss: 0.38842 |  0:00:09s\n",
      "epoch 166| loss: 0.37563 |  0:00:09s\n",
      "epoch 167| loss: 0.38946 |  0:00:09s\n",
      "epoch 168| loss: 0.39004 |  0:00:09s\n",
      "epoch 169| loss: 0.38555 |  0:00:09s\n",
      "epoch 170| loss: 0.38699 |  0:00:09s\n",
      "epoch 171| loss: 0.38251 |  0:00:09s\n",
      "epoch 172| loss: 0.37322 |  0:00:09s\n",
      "epoch 173| loss: 0.37461 |  0:00:09s\n",
      "epoch 174| loss: 0.37934 |  0:00:09s\n",
      "epoch 175| loss: 0.37561 |  0:00:09s\n",
      "epoch 176| loss: 0.37233 |  0:00:09s\n",
      "epoch 177| loss: 0.36703 |  0:00:09s\n",
      "epoch 178| loss: 0.36985 |  0:00:09s\n",
      "epoch 179| loss: 0.37102 |  0:00:09s\n",
      "epoch 180| loss: 0.37611 |  0:00:10s\n",
      "epoch 181| loss: 0.36982 |  0:00:10s\n",
      "epoch 182| loss: 0.37385 |  0:00:10s\n",
      "epoch 183| loss: 0.36204 |  0:00:10s\n",
      "epoch 184| loss: 0.36449 |  0:00:10s\n",
      "epoch 185| loss: 0.36274 |  0:00:10s\n",
      "epoch 186| loss: 0.3675  |  0:00:10s\n",
      "epoch 187| loss: 0.37158 |  0:00:10s\n",
      "epoch 188| loss: 0.36024 |  0:00:10s\n",
      "epoch 189| loss: 0.36222 |  0:00:10s\n",
      "epoch 190| loss: 0.36547 |  0:00:10s\n",
      "epoch 191| loss: 0.35913 |  0:00:10s\n",
      "epoch 192| loss: 0.37019 |  0:00:10s\n",
      "epoch 193| loss: 0.3661  |  0:00:10s\n",
      "epoch 194| loss: 0.36435 |  0:00:10s\n",
      "epoch 195| loss: 0.3665  |  0:00:10s\n",
      "epoch 196| loss: 0.36435 |  0:00:10s\n",
      "epoch 197| loss: 0.36708 |  0:00:10s\n",
      "epoch 198| loss: 0.37635 |  0:00:10s\n",
      "epoch 199| loss: 0.36185 |  0:00:11s\n",
      "epoch 200| loss: 0.372   |  0:00:11s\n",
      "epoch 201| loss: 0.36781 |  0:00:11s\n",
      "epoch 202| loss: 0.37538 |  0:00:11s\n",
      "epoch 203| loss: 0.39584 |  0:00:11s\n",
      "epoch 204| loss: 0.40129 |  0:00:11s\n",
      "epoch 205| loss: 0.39697 |  0:00:11s\n",
      "epoch 206| loss: 0.39629 |  0:00:11s\n",
      "epoch 207| loss: 0.38705 |  0:00:11s\n",
      "epoch 208| loss: 0.38361 |  0:00:11s\n",
      "epoch 209| loss: 0.37977 |  0:00:11s\n",
      "epoch 210| loss: 0.3787  |  0:00:11s\n",
      "epoch 211| loss: 0.37001 |  0:00:11s\n",
      "epoch 212| loss: 0.3723  |  0:00:11s\n",
      "epoch 213| loss: 0.37208 |  0:00:11s\n",
      "epoch 214| loss: 0.36534 |  0:00:11s\n",
      "epoch 215| loss: 0.37599 |  0:00:11s\n",
      "epoch 216| loss: 0.35701 |  0:00:12s\n",
      "epoch 217| loss: 0.36111 |  0:00:12s\n",
      "epoch 218| loss: 0.35919 |  0:00:12s\n",
      "epoch 219| loss: 0.36595 |  0:00:12s\n",
      "epoch 220| loss: 0.35637 |  0:00:12s\n",
      "epoch 221| loss: 0.36044 |  0:00:12s\n",
      "epoch 222| loss: 0.34806 |  0:00:12s\n",
      "epoch 223| loss: 0.35702 |  0:00:12s\n",
      "epoch 224| loss: 0.35781 |  0:00:12s\n",
      "epoch 225| loss: 0.35377 |  0:00:12s\n",
      "epoch 226| loss: 0.35655 |  0:00:12s\n",
      "epoch 227| loss: 0.35704 |  0:00:12s\n",
      "epoch 228| loss: 0.34493 |  0:00:12s\n",
      "epoch 229| loss: 0.35773 |  0:00:12s\n",
      "epoch 230| loss: 0.34637 |  0:00:12s\n",
      "epoch 231| loss: 0.34904 |  0:00:12s\n",
      "epoch 232| loss: 0.34231 |  0:00:12s\n",
      "epoch 233| loss: 0.34362 |  0:00:13s\n",
      "epoch 234| loss: 0.34797 |  0:00:13s\n",
      "epoch 235| loss: 0.34678 |  0:00:13s\n",
      "epoch 236| loss: 0.34557 |  0:00:13s\n",
      "epoch 237| loss: 0.34948 |  0:00:13s\n",
      "epoch 238| loss: 0.34857 |  0:00:13s\n",
      "epoch 239| loss: 0.35335 |  0:00:13s\n",
      "epoch 240| loss: 0.34921 |  0:00:13s\n",
      "epoch 241| loss: 0.34684 |  0:00:13s\n",
      "epoch 242| loss: 0.34874 |  0:00:13s\n",
      "epoch 243| loss: 0.3433  |  0:00:13s\n",
      "epoch 244| loss: 0.34147 |  0:00:13s\n",
      "epoch 245| loss: 0.3383  |  0:00:13s\n",
      "epoch 246| loss: 0.34105 |  0:00:13s\n",
      "epoch 247| loss: 0.34444 |  0:00:13s\n",
      "epoch 248| loss: 0.34859 |  0:00:14s\n",
      "epoch 249| loss: 0.34221 |  0:00:14s\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.24287 |  0:00:00s\n",
      "epoch 1  | loss: 2.15244 |  0:00:00s\n",
      "epoch 2  | loss: 1.58292 |  0:00:00s\n",
      "epoch 3  | loss: 1.26691 |  0:00:00s\n",
      "epoch 4  | loss: 1.06626 |  0:00:00s\n",
      "epoch 5  | loss: 0.9529  |  0:00:00s\n",
      "epoch 6  | loss: 0.88656 |  0:00:00s\n",
      "epoch 7  | loss: 0.84019 |  0:00:00s\n",
      "epoch 8  | loss: 0.79465 |  0:00:00s\n",
      "epoch 9  | loss: 0.76328 |  0:00:00s\n",
      "epoch 10 | loss: 0.72436 |  0:00:00s\n",
      "epoch 11 | loss: 0.69744 |  0:00:00s\n",
      "epoch 12 | loss: 0.69848 |  0:00:00s\n",
      "epoch 13 | loss: 0.68787 |  0:00:00s\n",
      "epoch 14 | loss: 0.67379 |  0:00:00s\n",
      "epoch 15 | loss: 0.67166 |  0:00:00s\n",
      "epoch 16 | loss: 0.65451 |  0:00:00s\n",
      "epoch 17 | loss: 0.66407 |  0:00:00s\n",
      "epoch 18 | loss: 0.65599 |  0:00:01s\n",
      "epoch 19 | loss: 0.65139 |  0:00:01s\n",
      "epoch 20 | loss: 0.64927 |  0:00:01s\n",
      "epoch 21 | loss: 0.64578 |  0:00:01s\n",
      "epoch 22 | loss: 0.63364 |  0:00:01s\n",
      "epoch 23 | loss: 0.62991 |  0:00:01s\n",
      "epoch 24 | loss: 0.62301 |  0:00:01s\n",
      "epoch 25 | loss: 0.62647 |  0:00:01s\n",
      "epoch 26 | loss: 0.61679 |  0:00:01s\n",
      "epoch 27 | loss: 0.60603 |  0:00:01s\n",
      "epoch 28 | loss: 0.59713 |  0:00:01s\n",
      "epoch 29 | loss: 0.59047 |  0:00:01s\n",
      "epoch 30 | loss: 0.58662 |  0:00:01s\n",
      "epoch 31 | loss: 0.59503 |  0:00:01s\n",
      "epoch 32 | loss: 0.58066 |  0:00:01s\n",
      "epoch 33 | loss: 0.5762  |  0:00:01s\n",
      "epoch 34 | loss: 0.57702 |  0:00:01s\n",
      "epoch 35 | loss: 0.5748  |  0:00:01s\n",
      "epoch 36 | loss: 0.58111 |  0:00:01s\n",
      "epoch 37 | loss: 0.56751 |  0:00:02s\n",
      "epoch 38 | loss: 0.56548 |  0:00:02s\n",
      "epoch 39 | loss: 0.56671 |  0:00:02s\n",
      "epoch 40 | loss: 0.54826 |  0:00:02s\n",
      "epoch 41 | loss: 0.55204 |  0:00:02s\n",
      "epoch 42 | loss: 0.54543 |  0:00:02s\n",
      "epoch 43 | loss: 0.54748 |  0:00:02s\n",
      "epoch 44 | loss: 0.53076 |  0:00:02s\n",
      "epoch 45 | loss: 0.534   |  0:00:02s\n",
      "epoch 46 | loss: 0.52653 |  0:00:02s\n",
      "epoch 47 | loss: 0.51791 |  0:00:02s\n",
      "epoch 48 | loss: 0.52555 |  0:00:02s\n",
      "epoch 49 | loss: 0.51958 |  0:00:02s\n",
      "epoch 50 | loss: 0.51854 |  0:00:02s\n",
      "epoch 51 | loss: 0.52509 |  0:00:02s\n",
      "epoch 52 | loss: 0.53795 |  0:00:02s\n",
      "epoch 53 | loss: 0.51252 |  0:00:02s\n",
      "epoch 54 | loss: 0.52148 |  0:00:03s\n",
      "epoch 55 | loss: 0.50706 |  0:00:03s\n",
      "epoch 56 | loss: 0.50415 |  0:00:03s\n",
      "epoch 57 | loss: 0.50179 |  0:00:03s\n",
      "epoch 58 | loss: 0.5014  |  0:00:03s\n",
      "epoch 59 | loss: 0.496   |  0:00:03s\n",
      "epoch 60 | loss: 0.49581 |  0:00:03s\n",
      "epoch 61 | loss: 0.49195 |  0:00:03s\n",
      "epoch 62 | loss: 0.49182 |  0:00:03s\n",
      "epoch 63 | loss: 0.48621 |  0:00:03s\n",
      "epoch 64 | loss: 0.48535 |  0:00:03s\n",
      "epoch 65 | loss: 0.48092 |  0:00:03s\n",
      "epoch 66 | loss: 0.48223 |  0:00:03s\n",
      "epoch 67 | loss: 0.47486 |  0:00:03s\n",
      "epoch 68 | loss: 0.48263 |  0:00:03s\n",
      "epoch 69 | loss: 0.46807 |  0:00:03s\n",
      "epoch 70 | loss: 0.47951 |  0:00:03s\n",
      "epoch 71 | loss: 0.47439 |  0:00:03s\n",
      "epoch 72 | loss: 0.46664 |  0:00:04s\n",
      "epoch 73 | loss: 0.46357 |  0:00:04s\n",
      "epoch 74 | loss: 0.45955 |  0:00:04s\n",
      "epoch 75 | loss: 0.44495 |  0:00:04s\n",
      "epoch 76 | loss: 0.45515 |  0:00:04s\n",
      "epoch 77 | loss: 0.44442 |  0:00:04s\n",
      "epoch 78 | loss: 0.44651 |  0:00:04s\n",
      "epoch 79 | loss: 0.44756 |  0:00:04s\n",
      "epoch 80 | loss: 0.43945 |  0:00:04s\n",
      "epoch 81 | loss: 0.4455  |  0:00:04s\n",
      "epoch 82 | loss: 0.42914 |  0:00:04s\n",
      "epoch 83 | loss: 0.42907 |  0:00:04s\n",
      "epoch 84 | loss: 0.42916 |  0:00:04s\n",
      "epoch 85 | loss: 0.43479 |  0:00:04s\n",
      "epoch 86 | loss: 0.4282  |  0:00:04s\n",
      "epoch 87 | loss: 0.43356 |  0:00:04s\n",
      "epoch 88 | loss: 0.4317  |  0:00:05s\n",
      "epoch 89 | loss: 0.42938 |  0:00:05s\n",
      "epoch 90 | loss: 0.42696 |  0:00:05s\n",
      "epoch 91 | loss: 0.43679 |  0:00:05s\n",
      "epoch 92 | loss: 0.43223 |  0:00:05s\n",
      "epoch 93 | loss: 0.43293 |  0:00:05s\n",
      "epoch 94 | loss: 0.43471 |  0:00:05s\n",
      "epoch 95 | loss: 0.43117 |  0:00:05s\n",
      "epoch 96 | loss: 0.43198 |  0:00:05s\n",
      "epoch 97 | loss: 0.42767 |  0:00:05s\n",
      "epoch 98 | loss: 0.44682 |  0:00:05s\n",
      "epoch 99 | loss: 0.42731 |  0:00:05s\n",
      "epoch 100| loss: 0.42579 |  0:00:05s\n",
      "epoch 101| loss: 0.42668 |  0:00:05s\n",
      "epoch 102| loss: 0.41374 |  0:00:05s\n",
      "epoch 103| loss: 0.40874 |  0:00:05s\n",
      "epoch 104| loss: 0.41486 |  0:00:05s\n",
      "epoch 105| loss: 0.4021  |  0:00:05s\n",
      "epoch 106| loss: 0.40922 |  0:00:06s\n",
      "epoch 107| loss: 0.40808 |  0:00:06s\n",
      "epoch 108| loss: 0.4061  |  0:00:06s\n",
      "epoch 109| loss: 0.40864 |  0:00:06s\n",
      "epoch 110| loss: 0.40504 |  0:00:06s\n",
      "epoch 111| loss: 0.39976 |  0:00:06s\n",
      "epoch 112| loss: 0.40696 |  0:00:06s\n",
      "epoch 113| loss: 0.38797 |  0:00:06s\n",
      "epoch 114| loss: 0.40204 |  0:00:06s\n",
      "epoch 115| loss: 0.3983  |  0:00:06s\n",
      "epoch 116| loss: 0.4002  |  0:00:06s\n",
      "epoch 117| loss: 0.40514 |  0:00:06s\n",
      "epoch 118| loss: 0.40652 |  0:00:06s\n",
      "epoch 119| loss: 0.40779 |  0:00:06s\n",
      "epoch 120| loss: 0.41209 |  0:00:06s\n",
      "epoch 121| loss: 0.40213 |  0:00:06s\n",
      "epoch 122| loss: 0.38867 |  0:00:06s\n",
      "epoch 123| loss: 0.38918 |  0:00:07s\n",
      "epoch 124| loss: 0.39164 |  0:00:07s\n",
      "epoch 125| loss: 0.37698 |  0:00:07s\n",
      "epoch 126| loss: 0.37028 |  0:00:07s\n",
      "epoch 127| loss: 0.37927 |  0:00:07s\n",
      "epoch 128| loss: 0.36095 |  0:00:07s\n",
      "epoch 129| loss: 0.36817 |  0:00:07s\n",
      "epoch 130| loss: 0.35612 |  0:00:07s\n",
      "epoch 131| loss: 0.35326 |  0:00:07s\n",
      "epoch 132| loss: 0.35904 |  0:00:07s\n",
      "epoch 133| loss: 0.35966 |  0:00:07s\n",
      "epoch 134| loss: 0.34819 |  0:00:07s\n",
      "epoch 135| loss: 0.35239 |  0:00:07s\n",
      "epoch 136| loss: 0.35978 |  0:00:07s\n",
      "epoch 137| loss: 0.35151 |  0:00:07s\n",
      "epoch 138| loss: 0.36372 |  0:00:07s\n",
      "epoch 139| loss: 0.36289 |  0:00:07s\n",
      "epoch 140| loss: 0.36773 |  0:00:07s\n",
      "epoch 141| loss: 0.36356 |  0:00:07s\n",
      "epoch 142| loss: 0.35889 |  0:00:07s\n",
      "epoch 143| loss: 0.36573 |  0:00:08s\n",
      "epoch 144| loss: 0.36685 |  0:00:08s\n",
      "epoch 145| loss: 0.36418 |  0:00:08s\n",
      "epoch 146| loss: 0.37643 |  0:00:08s\n",
      "epoch 147| loss: 0.36169 |  0:00:08s\n",
      "epoch 148| loss: 0.3599  |  0:00:08s\n",
      "epoch 149| loss: 0.36358 |  0:00:08s\n",
      "epoch 150| loss: 0.38092 |  0:00:08s\n",
      "epoch 151| loss: 0.37406 |  0:00:08s\n",
      "epoch 152| loss: 0.36953 |  0:00:08s\n",
      "epoch 153| loss: 0.37239 |  0:00:08s\n",
      "epoch 154| loss: 0.36479 |  0:00:08s\n",
      "epoch 155| loss: 0.36082 |  0:00:08s\n",
      "epoch 156| loss: 0.35538 |  0:00:08s\n",
      "epoch 157| loss: 0.34878 |  0:00:08s\n",
      "epoch 158| loss: 0.34607 |  0:00:08s\n",
      "epoch 159| loss: 0.34139 |  0:00:08s\n",
      "epoch 160| loss: 0.33437 |  0:00:08s\n",
      "epoch 161| loss: 0.33563 |  0:00:08s\n",
      "epoch 162| loss: 0.34112 |  0:00:09s\n",
      "epoch 163| loss: 0.35652 |  0:00:09s\n",
      "epoch 164| loss: 0.35128 |  0:00:09s\n",
      "epoch 165| loss: 0.34045 |  0:00:09s\n",
      "epoch 166| loss: 0.34459 |  0:00:09s\n",
      "epoch 167| loss: 0.34769 |  0:00:09s\n",
      "epoch 168| loss: 0.33172 |  0:00:09s\n",
      "epoch 169| loss: 0.33832 |  0:00:09s\n",
      "epoch 170| loss: 0.32454 |  0:00:09s\n",
      "epoch 171| loss: 0.33092 |  0:00:09s\n",
      "epoch 172| loss: 0.33065 |  0:00:09s\n",
      "epoch 173| loss: 0.31971 |  0:00:09s\n",
      "epoch 174| loss: 0.31975 |  0:00:09s\n",
      "epoch 175| loss: 0.30903 |  0:00:09s\n",
      "epoch 176| loss: 0.31415 |  0:00:09s\n",
      "epoch 177| loss: 0.32472 |  0:00:09s\n",
      "epoch 178| loss: 0.30966 |  0:00:10s\n",
      "epoch 179| loss: 0.31315 |  0:00:10s\n",
      "epoch 180| loss: 0.31162 |  0:00:10s\n",
      "epoch 181| loss: 0.32061 |  0:00:10s\n",
      "epoch 182| loss: 0.31373 |  0:00:10s\n",
      "epoch 183| loss: 0.31775 |  0:00:10s\n",
      "epoch 184| loss: 0.31597 |  0:00:10s\n",
      "epoch 185| loss: 0.3111  |  0:00:10s\n",
      "epoch 186| loss: 0.31505 |  0:00:10s\n",
      "epoch 187| loss: 0.30957 |  0:00:10s\n",
      "epoch 188| loss: 0.31093 |  0:00:10s\n",
      "epoch 189| loss: 0.31486 |  0:00:10s\n",
      "epoch 190| loss: 0.31301 |  0:00:10s\n",
      "epoch 191| loss: 0.3108  |  0:00:10s\n",
      "epoch 192| loss: 0.30916 |  0:00:10s\n",
      "epoch 193| loss: 0.30084 |  0:00:10s\n",
      "epoch 194| loss: 0.30258 |  0:00:10s\n",
      "epoch 195| loss: 0.30658 |  0:00:10s\n",
      "epoch 196| loss: 0.30231 |  0:00:10s\n",
      "epoch 197| loss: 0.29887 |  0:00:11s\n",
      "epoch 198| loss: 0.3017  |  0:00:11s\n",
      "epoch 199| loss: 0.2996  |  0:00:11s\n",
      "epoch 200| loss: 0.29747 |  0:00:11s\n",
      "epoch 201| loss: 0.302   |  0:00:11s\n",
      "epoch 202| loss: 0.29356 |  0:00:11s\n",
      "epoch 203| loss: 0.29971 |  0:00:11s\n",
      "epoch 204| loss: 0.28496 |  0:00:11s\n",
      "epoch 205| loss: 0.28985 |  0:00:11s\n",
      "epoch 206| loss: 0.28443 |  0:00:11s\n",
      "epoch 207| loss: 0.28011 |  0:00:11s\n",
      "epoch 208| loss: 0.28006 |  0:00:11s\n",
      "epoch 209| loss: 0.27741 |  0:00:11s\n",
      "epoch 210| loss: 0.27276 |  0:00:11s\n",
      "epoch 211| loss: 0.27628 |  0:00:11s\n",
      "epoch 212| loss: 0.2841  |  0:00:11s\n",
      "epoch 213| loss: 0.28351 |  0:00:11s\n",
      "epoch 214| loss: 0.28181 |  0:00:11s\n",
      "epoch 215| loss: 0.29372 |  0:00:11s\n",
      "epoch 216| loss: 0.27954 |  0:00:12s\n",
      "epoch 217| loss: 0.28504 |  0:00:12s\n",
      "epoch 218| loss: 0.281   |  0:00:12s\n",
      "epoch 219| loss: 0.28408 |  0:00:12s\n",
      "epoch 220| loss: 0.28229 |  0:00:12s\n",
      "epoch 221| loss: 0.27346 |  0:00:12s\n",
      "epoch 222| loss: 0.26955 |  0:00:12s\n",
      "epoch 223| loss: 0.27341 |  0:00:12s\n",
      "epoch 224| loss: 0.27228 |  0:00:12s\n",
      "epoch 225| loss: 0.26034 |  0:00:12s\n",
      "epoch 226| loss: 0.26504 |  0:00:12s\n",
      "epoch 227| loss: 0.26373 |  0:00:12s\n",
      "epoch 228| loss: 0.25678 |  0:00:12s\n",
      "epoch 229| loss: 0.2667  |  0:00:12s\n",
      "epoch 230| loss: 0.25775 |  0:00:12s\n",
      "epoch 231| loss: 0.26326 |  0:00:12s\n",
      "epoch 232| loss: 0.25435 |  0:00:12s\n",
      "epoch 233| loss: 0.25562 |  0:00:12s\n",
      "epoch 234| loss: 0.26014 |  0:00:12s\n",
      "epoch 235| loss: 0.2552  |  0:00:13s\n",
      "epoch 236| loss: 0.25414 |  0:00:13s\n",
      "epoch 237| loss: 0.25566 |  0:00:13s\n",
      "epoch 238| loss: 0.25705 |  0:00:13s\n",
      "epoch 239| loss: 0.25846 |  0:00:13s\n",
      "epoch 240| loss: 0.25685 |  0:00:13s\n",
      "epoch 241| loss: 0.26753 |  0:00:13s\n",
      "epoch 242| loss: 0.27538 |  0:00:13s\n",
      "epoch 243| loss: 0.27838 |  0:00:13s\n",
      "epoch 244| loss: 0.27072 |  0:00:13s\n",
      "epoch 245| loss: 0.26688 |  0:00:13s\n",
      "epoch 246| loss: 0.26624 |  0:00:13s\n",
      "epoch 247| loss: 0.26438 |  0:00:13s\n",
      "epoch 248| loss: 0.26376 |  0:00:13s\n",
      "epoch 249| loss: 0.26156 |  0:00:13s\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    }
   ],
   "source": [
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    try: \n",
    "    \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "    # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open(results_file, 'wb') as handle:\n",
    "    pickle.dump(all_dict_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Pytorch models only on MRI features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = ['APOE_epsilon2', 'APOE_epsilon3', 'APOE_epsilon4']\n",
    "continuous_features = [col for col in df_X_train.columns if col not in ordinal_features]\n",
    "\n",
    "# Prepare Tabular configurations (shared for all PyTorch models)\n",
    "data_config = DataConfig(\n",
    "    target=df_y_train.columns.tolist(),\n",
    "    continuous_cols=continuous_features,\n",
    "    categorical_cols=[]\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024, max_epochs=1, auto_lr_find=True,\n",
    "    early_stopping=\"valid_loss\", early_stopping_mode=\"min\", early_stopping_patience=5,\n",
    "    checkpoints=\"valid_loss\", load_best=True, progress_bar=\"nones\",\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "head_config = LinearHeadConfig(dropout=0.1).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'LinearRegression', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskElasticNet', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskElasticNet_tuned', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskLasso', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskLasso_tuned', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'RandomForestRegressor', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'XGBoostRegressor', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'XGBoostRegressor_tuned', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetRegressor_default', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetRegressor_custom', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'PLSRegression_4_components', (2881, 200), (13, 200)])\n",
      "GatedAdditiveTreeEnsembleConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:07</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">216</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:07\u001b[0m,\u001b[1;36m216\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:07</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">228</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:07\u001b[0m,\u001b[1;36m228\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:07</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">232</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:07\u001b[0m,\u001b[1;36m232\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:07</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">248</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:07\u001b[0m,\u001b[1;36m248\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:07</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">449</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:07\u001b[0m,\u001b[1;36m449\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:07</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">462</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:07\u001b[0m,\u001b[1;36m462\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:07</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">471</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:07\u001b[0m,\u001b[1;36m471\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55430d19a4b41ba879adc4bc7a4f0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_89177ad6-da73-4552-8da9-e00d1962e3ec.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_89177ad6-da73-4552-8da9-e00d1962e3ec.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">389</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:09\u001b[0m,\u001b[1;36m389\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">396</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:09\u001b[0m,\u001b[1;36m396\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">411</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:09\u001b[0m,\u001b[1;36m411\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                       | Params | Mode \n",
      "------------------------------------------------------------------------\n",
      "0 | _backbone        | GatedAdditiveTreesBackbone | 2.1 M  | train\n",
      "1 | _embedding_layer | Embedding1dLayer           | 400    | train\n",
      "2 | _head            | CustomHead                 | 156    | train\n",
      "3 | loss             | MSELoss                    | 0      | train\n",
      "------------------------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.417     Total estimated model params size (MB)\n",
      "689       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5682fd70b89641bfa336885f927b3eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d8ab04fbba4243b421248f2bccd6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044c0d1754fd4f21baabb8c4051fba26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">647</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:11\u001b[0m,\u001b[1;36m647\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">648</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:11\u001b[0m,\u001b[1;36m648\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "DANetConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">186</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:12\u001b[0m,\u001b[1;36m186\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">197</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:12\u001b[0m,\u001b[1;36m197\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">201</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:12\u001b[0m,\u001b[1;36m201\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">216</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: DANetModel             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:12\u001b[0m,\u001b[1;36m216\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: DANetModel             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">245</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:12\u001b[0m,\u001b[1;36m245\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">252</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:12\u001b[0m,\u001b[1;36m252\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28432e1fd6b4d0bb7bae16e9f5b3d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_9afac7d6-da9b-4b1c-81aa-7a1383d20fb0.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_9afac7d6-da9b-4b1c-81aa-7a1383d20fb0.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">586</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:12\u001b[0m,\u001b[1;36m586\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">592</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:12\u001b[0m,\u001b[1;36m592\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | _backbone        | DANetBackbone    | 1.4 M  | train\n",
      "1 | _embedding_layer | Embedding1dLayer | 400    | train\n",
      "2 | _head            | LinearHead       | 260    | train\n",
      "3 | loss             | MSELoss          | 0      | train\n",
      "--------------------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.787     Total estimated model params size (MB)\n",
      "156       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34684d5a3e60428389a2c116ef60beeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d8233659e8475a995d55b86fc95f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586a7b218d044139aa069a472d6c53fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">941</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:12\u001b[0m,\u001b[1;36m941\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:12</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">942</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:12\u001b[0m,\u001b[1;36m942\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "TabTransformerConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">355</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:13\u001b[0m,\u001b[1;36m355\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">366</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:13\u001b[0m,\u001b[1;36m366\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">370</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:13\u001b[0m,\u001b[1;36m370\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">386</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabTransformerModel    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:13\u001b[0m,\u001b[1;36m386\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabTransformerModel    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">406</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:13\u001b[0m,\u001b[1;36m406\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">420</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:13\u001b[0m,\u001b[1;36m420\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a7d2bb9cf94a12be17346c617aed09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_ae297db2-aa40-4f2b-9d8d-e5cc2cfb5818.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_ae297db2-aa40-4f2b-9d8d-e5cc2cfb5818.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">618</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:13\u001b[0m,\u001b[1;36m618\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">620</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:13\u001b[0m,\u001b[1;36m620\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                   | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | _backbone        | TabTransformerBackbone | 271 K  | train\n",
      "1 | _embedding_layer | Embedding2dLayer       | 0      | train\n",
      "2 | _head            | LinearHead             | 804    | train\n",
      "3 | loss             | MSELoss                | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "272 K     Trainable params\n",
      "0         Non-trainable params\n",
      "272 K     Total params\n",
      "1.090     Total estimated model params size (MB)\n",
      "119       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4608ed835a4efeb5c85228cdaba5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd361f25e964c758b94fe0476f771d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbab05115f0403a8d53da19829dba93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">759</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:13\u001b[0m,\u001b[1;36m759\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">760</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:13\u001b[0m,\u001b[1;36m760\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "TabNetModelConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">963</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:13\u001b[0m,\u001b[1;36m963\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">979</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:13\u001b[0m,\u001b[1;36m979\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">983</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:13\u001b[0m,\u001b[1;36m983\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">004</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:14\u001b[0m,\u001b[1;36m004\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">030</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:14\u001b[0m,\u001b[1;36m030\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">044</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:14\u001b[0m,\u001b[1;36m044\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b06d42edec4ec4bd7cb750f7c475d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_7fd1c78e-339f-4632-89e2-836af26fd0dd.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_7fd1c78e-339f-4632-89e2-836af26fd0dd.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">343</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:14\u001b[0m,\u001b[1;36m343\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">345</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:14\u001b[0m,\u001b[1;36m345\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type           | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | _embedding_layer | Identity       | 0      | train\n",
      "1 | _backbone        | TabNetBackbone | 18.9 K | train\n",
      "2 | _head            | Identity       | 0      | train\n",
      "3 | loss             | MSELoss        | 0      | train\n",
      "------------------------------------------------------------\n",
      "18.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "18.9 K    Total params\n",
      "0.075     Total estimated model params size (MB)\n",
      "107       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878bb501de40415fa6cb50a5542dd3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d04d837a5bc45838883bf49d791b89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301ebe331e0a430783434140648a1279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">556</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:14\u001b[0m,\u001b[1;36m556\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:28:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">557</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:28:14\u001b[0m,\u001b[1;36m557\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": [
    "predictive_models_list += [\n",
    "    (\"GatedAdditiveTreeEnsembleConfig_tab\", \n",
    "    TabularModelWrapper(\n",
    "        GatedAdditiveTreeEnsembleConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        gflu_stages=6,\n",
    "        gflu_dropout=0.0,\n",
    "        tree_depth=5,\n",
    "        num_trees=20,\n",
    "        chain_trees=False,\n",
    "        share_head_weights=True), data_config, trainer_config, optimizer_config \n",
    "    )),\n",
    "    (\"DANetConfig_tab\",\n",
    "    TabularModelWrapper(\n",
    "        DANetConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_layers=8,\n",
    "        k=5,\n",
    "        dropout_rate=0.1), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabTransformerConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        embedding_initialization=\"kaiming_uniform\",\n",
    "        embedding_bias=False), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    # (\"FTTransformerConfig\",\n",
    "    #     TabularModelWrapper(\n",
    "    #     FTTransformerConfig(\n",
    "    #     task=\"regression\",\n",
    "    #     head=\"LinearHead\",\n",
    "    #     head_config=head_config), data_config, trainer_config, optimizer_config\n",
    "    # )),\n",
    "    (\"TabNetModelConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        n_independent=2,\n",
    "        n_shared=2), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "\n",
    "    if any(result['params'] == params for result in all_dict_results):\n",
    "        # Skip this iteration if the combination exists\n",
    "        print(f\"Skipping existing combination: {params.values()}\")\n",
    "        \n",
    "        continue\n",
    "\n",
    "    try: \n",
    "        print(name_model)\n",
    "        \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "        # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Table for reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = \"../pickle/training_2_dict_results.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_file, \"rb\") as input_file:\n",
    "    all_dict_results = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'LinearRegression', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskElasticNet', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskElasticNet_tuned', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskLasso', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'MultiTaskLasso_tuned', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'RandomForestRegressor', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'XGBoostRegressor', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'XGBoostRegressor_tuned', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetRegressor_default', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetRegressor_custom', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'PLSRegression_4_components', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'GatedAdditiveTreeEnsembleConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'DANetConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabTransformerConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetModelConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'GatedAdditiveTreeEnsembleConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'DANetConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabTransformerConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetModelConfig_tab', (2881, 200), (13, 200)])\n"
     ]
    }
   ],
   "source": [
    "predictive_models_list += [\n",
    "    (\"GatedAdditiveTreeEnsembleConfig_tab\", \n",
    "    TabularModelWrapper(\n",
    "        GatedAdditiveTreeEnsembleConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        gflu_stages=6,\n",
    "        gflu_dropout=0.0,\n",
    "        tree_depth=5,\n",
    "        num_trees=20,\n",
    "        chain_trees=False,\n",
    "        share_head_weights=True), data_config, trainer_config, optimizer_config \n",
    "    )),\n",
    "    (\"DANetConfig_tab\",\n",
    "    TabularModelWrapper(\n",
    "        DANetConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_layers=8,\n",
    "        k=5,\n",
    "        dropout_rate=0.1), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabTransformerConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        embedding_initialization=\"kaiming_uniform\",\n",
    "        embedding_bias=False), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    # (\"FTTransformerConfig\",\n",
    "    #     TabularModelWrapper(\n",
    "    #     FTTransformerConfig(\n",
    "    #     task=\"regression\",\n",
    "    #     head=\"LinearHead\",\n",
    "    #     head_config=head_config), data_config, trainer_config, optimizer_config\n",
    "    # )),\n",
    "    (\"TabNetModelConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        n_independent=2,\n",
    "        n_shared=2), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "\n",
    "    if any(result['params'] == params for result in all_dict_results):\n",
    "        # Skip this iteration if the combination exists\n",
    "        print(f\"Skipping existing combination: {params.values()}\")\n",
    "        \n",
    "        continue\n",
    "\n",
    "    try: \n",
    "        print(name_model)\n",
    "        \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "        # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'XGBoostRegressor', 'train_shape': (2881, 348), 'test_shape': (13, 348)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'XGBoostRegressor_tuned', 'train_shape': (2881, 348), 'test_shape': (13, 348)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'GatedAdditiveTreeEnsembleConfig_tab', 'train_shape': (2881, 348), 'test_shape': (13, 348)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'DANetConfig_tab', 'train_shape': (2881, 348), 'test_shape': (13, 348)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'TabTransformerConfig_tab', 'train_shape': (2881, 348), 'test_shape': (13, 348)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'TabNetModelConfig_tab', 'train_shape': (2881, 348), 'test_shape': (13, 348)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'LinearRegression', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.07905030250549316, 'results_adj': {'mse_score': array([1.11192349, 0.67668626, 0.61999815, 0.97750844]), 'mae_score': array([0.72849831, 0.70235973, 0.70986192, 0.71215901]), 'r2': array([-0.12379655,  0.2321367 , -0.29452346, -0.18906157]), 'explained_variance': array([ 0.02475189,  0.28845771, -0.29319881,  0.10973059]), 'corr': array([0.33204585, 0.54765514, 0.01649677, 0.38058137])}, 'results_org': {'mse_score': array([1.11192349, 0.67668626, 0.61999817, 0.97750843]), 'mae_score': array([0.72849832, 0.70235973, 0.70986193, 0.712159  ]), 'r2': array([-0.18280006,  0.24192465, -0.20755938, -0.14289477]), 'explained_variance': array([-0.02645227,  0.29752774, -0.20632372,  0.14429641]), 'corr': array([0.27766053, 0.55012684, 0.07284141, 0.39622517])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskElasticNet', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.01588749885559082, 'results_adj': {'mse_score': array([1.18012247, 0.95041534, 0.50741149, 1.02939713]), 'mae_score': array([0.88414167, 0.78683572, 0.65378091, 0.82689555]), 'r2': array([-0.19272375, -0.07847477, -0.05944845, -0.25218005]), 'explained_variance': array([ 0.11229868,  0.03189858, -0.05332468,  0.12857136]), 'corr': array([ 0.42176386,  0.17864277, -0.16306808,  0.44631445])}, 'results_org': {'mse_score': array([1.18012248, 0.95041534, 0.50741151, 1.02939711]), 'mae_score': array([0.88414168, 0.78683573, 0.65378092, 0.82689554]), 'r2': array([-0.25534621, -0.06472745,  0.01172366, -0.20356258]), 'explained_variance': array([0.06569104, 0.04423898, 0.01743605, 0.16240568]), 'corr': array([0.25863755, 0.21083053, 0.14561964, 0.49623358])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskElasticNet_tuned', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.8308308124542236, 'results_adj': {'mse_score': array([1.09767924, 0.67163602, 0.61727141, 0.96027028]), 'mae_score': array([0.72712752, 0.70316248, 0.71357071, 0.70813001]), 'r2': array([-0.1094002 ,  0.23786741, -0.28883017, -0.16809271]), 'explained_variance': array([ 0.04259982,  0.29559122, -0.28782174,  0.12798901]), 'corr': array([0.33826662, 0.55120885, 0.00953623, 0.39231636])}, 'results_org': {'mse_score': array([1.09767923, 0.67163602, 0.61727143, 0.96027027]), 'mae_score': array([0.72712752, 0.70316248, 0.71357072, 0.70813   ]), 'r2': array([-0.16764784,  0.24758231, -0.20224855, -0.12274005]), 'explained_variance': array([-0.00766726,  0.30457033, -0.20130787,  0.16184592]), 'corr': array([0.28384559, 0.55450481, 0.06742918, 0.41077564])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskLasso', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.004647254943847656, 'results_adj': {'mse_score': array([1.39363767, 1.04056339, 0.48866937, 1.24000841]), 'mae_score': array([0.99127575, 0.78457157, 0.62110666, 0.93446712]), 'r2': array([-0.40851885, -0.18076941, -0.0203159 , -0.50837198]), 'explained_variance': array([-2.22044605e-16,  0.00000000e+00,  0.00000000e+00,  1.11022302e-16]), 'corr': array([nan, nan, nan, nan])}, 'results_org': {'mse_score': array([1.39363768, 1.04056339, 0.48866939, 1.24000838]), 'mae_score': array([0.99127576, 0.78457157, 0.62110667, 0.93446711]), 'r2': array([-0.48247136, -0.16571814,  0.04822734, -0.44980754]), 'explained_variance': array([-0.05250375,  0.01274701,  0.06717846,  0.03882626]), 'corr': array([-0.0848666 ,  0.11455326,  0.2721653 ,  0.20679355])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskLasso_tuned', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.8418209552764893, 'results_adj': {'mse_score': array([1.09986228, 0.67700338, 0.61721354, 0.96322248]), 'mae_score': array([0.72439764, 0.70502407, 0.71262079, 0.70634911]), 'r2': array([-0.11160655,  0.23177686, -0.28870934, -0.17168383]), 'explained_variance': array([ 0.04182356,  0.29179378, -0.28783433,  0.1250568 ]), 'corr': array([0.34030126, 0.54948447, 0.01369735, 0.39123818])}, 'results_org': {'mse_score': array([1.09986228, 0.67700337, 0.61721356, 0.96322247]), 'mae_score': array([0.72439764, 0.70502408, 0.71262081, 0.7063491 ]), 'r2': array([-0.16997004,  0.2415694 , -0.20213584, -0.12619174]), 'explained_variance': array([-0.00848428,  0.30082129, -0.20131962,  0.15902757]), 'corr': array([0.28608858, 0.55225247, 0.07079544, 0.40890542])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'RandomForestRegressor', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 32.762316942214966, 'results_adj': {'mse_score': array([1.00638713, 0.65012684, 0.47869162, 0.82220865]), 'mae_score': array([0.75528169, 0.6841987 , 0.63512616, 0.63151397]), 'r2': array([-1.71332791e-02,  2.62274753e-01,  5.17132936e-04, -1.51675905e-04]), 'explained_variance': array([0.29209145, 0.405549  , 0.02445608, 0.37078241]), 'corr': array([0.54046621, 0.64364297, 0.21686966, 0.60995505])}, 'results_org': {'mse_score': array([1.00638713, 0.65012684, 0.47869164, 0.82220864]), 'mae_score': array([0.7552817 , 0.6841987 , 0.63512617, 0.63151397]), 'r2': array([-0.07053657,  0.27167853,  0.06766084,  0.03868046]), 'explained_variance': array([0.25492361, 0.41312647, 0.08999161, 0.39521257]), 'corr': array([0.50511182, 0.65009756, 0.3086371 , 0.63522421])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'XGBoostRegressor', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.903601884841919, 'results_adj': {'mse_score': array([0.99729777, 0.55285888, 0.49806901, 0.7462314 ]), 'mae_score': array([0.75445592, 0.63451078, 0.64294552, 0.64429738]), 'r2': array([-0.00794687,  0.37264865, -0.03994184,  0.09226863]), 'explained_variance': array([ 0.26511088,  0.47604274, -0.03447538,  0.31643386]), 'corr': array([0.52174936, 0.69849083, 0.21172288, 0.56606   ])}, 'results_org': {'mse_score': array([0.99729777, 0.55285888, 0.49806903, 0.74623138]), 'mae_score': array([0.75445593, 0.63451079, 0.64294554, 0.64429738]), 'r2': array([-0.06086784,  0.38064549,  0.02991984,  0.12751245]), 'explained_variance': array([0.22652646, 0.48272162, 0.03501907, 0.34297417]), 'corr': array([0.48966434, 0.70257856, 0.27320828, 0.58583766])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'XGBoostRegressor_tuned', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 2.453415632247925, 'results_adj': {'mse_score': array([1.0153611 , 0.63851973, 0.46345103, 0.70749896]), 'mae_score': array([0.73457703, 0.71458275, 0.63021056, 0.61263546]), 'r2': array([-0.02620307,  0.27544581,  0.03233868,  0.13938357]), 'explained_variance': array([0.22165364, 0.34897739, 0.05887258, 0.43032201]), 'corr': array([0.48104477, 0.59075482, 0.30682341, 0.65598979])}, 'results_org': {'mse_score': array([1.0153611 , 0.63851973, 0.46345105, 0.70749894]), 'mae_score': array([0.73457704, 0.71458276, 0.63021057, 0.61263545]), 'r2': array([-0.08008256,  0.2846817 ,  0.09734466,  0.17279809]), 'explained_variance': array([0.18078756, 0.35727599, 0.12209605, 0.45244048]), 'corr': array([0.44678191, 0.59775758, 0.36862204, 0.67323253])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetRegressor_default', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 14.12218189239502, 'results_adj': {'mse_score': array([1.00024251, 0.85216095, 0.6231349 , 0.78732824]), 'mae_score': array([0.78064659, 0.8013238 , 0.6795552 , 0.67213814]), 'r2': array([-0.01092304,  0.03301847, -0.30107284,  0.04227758]), 'explained_variance': array([ 0.38275983,  0.13551578, -0.29939216,  0.5247155 ]), 'corr': array([0.62946614, 0.47112118, 0.06684408, 0.73037268])}, 'results_org': {'mse_score': array([1.0002425 , 0.85216095, 0.62313492, 0.78732822]), 'mae_score': array([0.78064659, 0.8013238 , 0.67955522, 0.67213815]), 'r2': array([-0.06400027,  0.04534457, -0.21366878,  0.07946235]), 'explained_variance': array([ 0.35035243,  0.14653537, -0.212101  ,  0.54316902]), 'corr': array([0.60852724, 0.4852577 , 0.12261516, 0.74095112])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetRegressor_custom', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 13.83526086807251, 'results_adj': {'mse_score': array([0.92610036, 0.76428738, 0.61165174, 0.87483944]), 'mae_score': array([0.71338154, 0.75841494, 0.67114785, 0.72399585]), 'r2': array([ 0.06401079,  0.13273216, -0.2770966 , -0.06417287]), 'explained_variance': array([ 0.22209961,  0.26101411, -0.27432143,  0.26805491]), 'corr': array([0.55329131, 0.58084529, 0.23700988, 0.59708229])}, 'results_org': {'mse_score': array([0.92610035, 0.76428739, 0.61165176, 0.87483943]), 'mae_score': array([0.71338154, 0.75841494, 0.67114786, 0.72399584]), 'r2': array([ 0.01486787,  0.14378722, -0.19130323, -0.02285503]), 'explained_variance': array([ 0.18125695,  0.27043396, -0.1887145 ,  0.2964736 ]), 'corr': array([0.53795846, 0.59013198, 0.27340583, 0.5986342 ])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'PLSRegression_4_components', 'train_shape': (2881, 200), 'test_shape': (13, 200)}, 'fitting_time': 0.16752839088439941, 'results_adj': {'mse_score': array([1.08140908, 0.78765117, 0.5694762 , 0.94630352]), 'mae_score': array([0.79248414, 0.79068015, 0.72178585, 0.73452485]), 'r2': array([-0.09295631,  0.10622033, -0.1890363 , -0.15110326]), 'explained_variance': array([ 0.1855152 ,  0.19584409, -0.18846787,  0.20844666]), 'corr': array([ 0.43286088,  0.44542341, -0.05356343,  0.45740887])}, 'results_org': {'mse_score': array([1.08140909, 0.78765116, 0.56947622, 0.9463035 ]), 'mae_score': array([0.79248415, 0.79068015, 0.72178586, 0.73452484]), 'r2': array([-0.1503406 ,  0.11761334, -0.10915869, -0.10641023]), 'explained_variance': array([ 0.14275171,  0.20609467, -0.10862844,  0.23917972]), 'corr': array([0.3812462 , 0.45398549, 0.03688166, 0.49549602])}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'GatedAdditiveTreeEnsembleConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'DANetConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabTransformerConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n",
      "Removed due to None value. Other values: {'params': {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetModelConfig_tab', 'train_shape': (2881, 200), 'test_shape': (13, 200)}}\n"
     ]
    }
   ],
   "source": [
    "all_dict_results = clean_dict_list(all_dict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "MultiTaskElasticNet\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "MultiTaskElasticNet_tuned\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "MultiTaskLasso\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "MultiTaskLasso_tuned\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "RandomForestRegressor\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "XGBoostRegressor\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "XGBoostRegressor_tuned\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "TabNetRegressor_default\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.13311 |  0:00:00s\n",
      "epoch 1  | loss: 1.67434 |  0:00:00s\n",
      "epoch 2  | loss: 1.25121 |  0:00:00s\n",
      "epoch 3  | loss: 1.15286 |  0:00:00s\n",
      "epoch 4  | loss: 1.05784 |  0:00:00s\n",
      "epoch 5  | loss: 0.96523 |  0:00:00s\n",
      "epoch 6  | loss: 0.92765 |  0:00:00s\n",
      "epoch 7  | loss: 0.89131 |  0:00:00s\n",
      "epoch 8  | loss: 0.86607 |  0:00:00s\n",
      "epoch 9  | loss: 0.82915 |  0:00:00s\n",
      "epoch 10 | loss: 0.83506 |  0:00:00s\n",
      "epoch 11 | loss: 0.7886  |  0:00:00s\n",
      "epoch 12 | loss: 0.75816 |  0:00:00s\n",
      "epoch 13 | loss: 0.76323 |  0:00:00s\n",
      "epoch 14 | loss: 0.76212 |  0:00:00s\n",
      "epoch 15 | loss: 0.75083 |  0:00:00s\n",
      "epoch 16 | loss: 0.72542 |  0:00:00s\n",
      "epoch 17 | loss: 0.71856 |  0:00:00s\n",
      "epoch 18 | loss: 0.71925 |  0:00:01s\n",
      "epoch 19 | loss: 0.70533 |  0:00:01s\n",
      "epoch 20 | loss: 0.70061 |  0:00:01s\n",
      "epoch 21 | loss: 0.69113 |  0:00:01s\n",
      "epoch 22 | loss: 0.67685 |  0:00:01s\n",
      "epoch 23 | loss: 0.67698 |  0:00:01s\n",
      "epoch 24 | loss: 0.67197 |  0:00:01s\n",
      "epoch 25 | loss: 0.66981 |  0:00:01s\n",
      "epoch 26 | loss: 0.66185 |  0:00:01s\n",
      "epoch 27 | loss: 0.65994 |  0:00:01s\n",
      "epoch 28 | loss: 0.65575 |  0:00:01s\n",
      "epoch 29 | loss: 0.65478 |  0:00:01s\n",
      "epoch 30 | loss: 0.64765 |  0:00:01s\n",
      "epoch 31 | loss: 0.64554 |  0:00:01s\n",
      "epoch 32 | loss: 0.63432 |  0:00:01s\n",
      "epoch 33 | loss: 0.62939 |  0:00:01s\n",
      "epoch 34 | loss: 0.62534 |  0:00:01s\n",
      "epoch 35 | loss: 0.62469 |  0:00:01s\n",
      "epoch 36 | loss: 0.62174 |  0:00:01s\n",
      "epoch 37 | loss: 0.61922 |  0:00:01s\n",
      "epoch 38 | loss: 0.62018 |  0:00:01s\n",
      "epoch 39 | loss: 0.62086 |  0:00:01s\n",
      "epoch 40 | loss: 0.60617 |  0:00:02s\n",
      "epoch 41 | loss: 0.60947 |  0:00:02s\n",
      "epoch 42 | loss: 0.59172 |  0:00:02s\n",
      "epoch 43 | loss: 0.58443 |  0:00:02s\n",
      "epoch 44 | loss: 0.58781 |  0:00:02s\n",
      "epoch 45 | loss: 0.57683 |  0:00:02s\n",
      "epoch 46 | loss: 0.57306 |  0:00:02s\n",
      "epoch 47 | loss: 0.57694 |  0:00:02s\n",
      "epoch 48 | loss: 0.56855 |  0:00:02s\n",
      "epoch 49 | loss: 0.56316 |  0:00:02s\n",
      "epoch 50 | loss: 0.56719 |  0:00:02s\n",
      "epoch 51 | loss: 0.56633 |  0:00:02s\n",
      "epoch 52 | loss: 0.55997 |  0:00:02s\n",
      "epoch 53 | loss: 0.56786 |  0:00:02s\n",
      "epoch 54 | loss: 0.55522 |  0:00:02s\n",
      "epoch 55 | loss: 0.55718 |  0:00:02s\n",
      "epoch 56 | loss: 0.55059 |  0:00:02s\n",
      "epoch 57 | loss: 0.55296 |  0:00:02s\n",
      "epoch 58 | loss: 0.55163 |  0:00:02s\n",
      "epoch 59 | loss: 0.54834 |  0:00:02s\n",
      "epoch 60 | loss: 0.55297 |  0:00:03s\n",
      "epoch 61 | loss: 0.5449  |  0:00:03s\n",
      "epoch 62 | loss: 0.5401  |  0:00:03s\n",
      "epoch 63 | loss: 0.53168 |  0:00:03s\n",
      "epoch 64 | loss: 0.52247 |  0:00:03s\n",
      "epoch 65 | loss: 0.52582 |  0:00:03s\n",
      "epoch 66 | loss: 0.51292 |  0:00:03s\n",
      "epoch 67 | loss: 0.51423 |  0:00:03s\n",
      "epoch 68 | loss: 0.50964 |  0:00:03s\n",
      "epoch 69 | loss: 0.50876 |  0:00:03s\n",
      "epoch 70 | loss: 0.50005 |  0:00:03s\n",
      "epoch 71 | loss: 0.50396 |  0:00:03s\n",
      "epoch 72 | loss: 0.49637 |  0:00:03s\n",
      "epoch 73 | loss: 0.49309 |  0:00:03s\n",
      "epoch 74 | loss: 0.48867 |  0:00:03s\n",
      "epoch 75 | loss: 0.48842 |  0:00:03s\n",
      "epoch 76 | loss: 0.48183 |  0:00:03s\n",
      "epoch 77 | loss: 0.48301 |  0:00:03s\n",
      "epoch 78 | loss: 0.48205 |  0:00:04s\n",
      "epoch 79 | loss: 0.48052 |  0:00:04s\n",
      "epoch 80 | loss: 0.47403 |  0:00:04s\n",
      "epoch 81 | loss: 0.48338 |  0:00:04s\n",
      "epoch 82 | loss: 0.47462 |  0:00:04s\n",
      "epoch 83 | loss: 0.47119 |  0:00:04s\n",
      "epoch 84 | loss: 0.47134 |  0:00:04s\n",
      "epoch 85 | loss: 0.4746  |  0:00:04s\n",
      "epoch 86 | loss: 0.47936 |  0:00:04s\n",
      "epoch 87 | loss: 0.48048 |  0:00:04s\n",
      "epoch 88 | loss: 0.46815 |  0:00:04s\n",
      "epoch 89 | loss: 0.47599 |  0:00:04s\n",
      "epoch 90 | loss: 0.46779 |  0:00:04s\n",
      "epoch 91 | loss: 0.46429 |  0:00:04s\n",
      "epoch 92 | loss: 0.46651 |  0:00:04s\n",
      "epoch 93 | loss: 0.46411 |  0:00:04s\n",
      "epoch 94 | loss: 0.45887 |  0:00:04s\n",
      "epoch 95 | loss: 0.46711 |  0:00:04s\n",
      "epoch 96 | loss: 0.45915 |  0:00:05s\n",
      "epoch 97 | loss: 0.46154 |  0:00:05s\n",
      "epoch 98 | loss: 0.45802 |  0:00:05s\n",
      "epoch 99 | loss: 0.45159 |  0:00:05s\n",
      "epoch 100| loss: 0.45073 |  0:00:05s\n",
      "epoch 101| loss: 0.45322 |  0:00:05s\n",
      "epoch 102| loss: 0.44542 |  0:00:05s\n",
      "epoch 103| loss: 0.44681 |  0:00:05s\n",
      "epoch 104| loss: 0.44443 |  0:00:05s\n",
      "epoch 105| loss: 0.44417 |  0:00:05s\n",
      "epoch 106| loss: 0.44086 |  0:00:05s\n",
      "epoch 107| loss: 0.44396 |  0:00:05s\n",
      "epoch 108| loss: 0.43298 |  0:00:05s\n",
      "epoch 109| loss: 0.44147 |  0:00:05s\n",
      "epoch 110| loss: 0.42825 |  0:00:05s\n",
      "epoch 111| loss: 0.43181 |  0:00:05s\n",
      "epoch 112| loss: 0.43651 |  0:00:05s\n",
      "epoch 113| loss: 0.42791 |  0:00:05s\n",
      "epoch 114| loss: 0.43285 |  0:00:05s\n",
      "epoch 115| loss: 0.43182 |  0:00:05s\n",
      "epoch 116| loss: 0.4306  |  0:00:06s\n",
      "epoch 117| loss: 0.42956 |  0:00:06s\n",
      "epoch 118| loss: 0.42473 |  0:00:06s\n",
      "epoch 119| loss: 0.42655 |  0:00:06s\n",
      "epoch 120| loss: 0.42597 |  0:00:06s\n",
      "epoch 121| loss: 0.41854 |  0:00:06s\n",
      "epoch 122| loss: 0.41561 |  0:00:06s\n",
      "epoch 123| loss: 0.41897 |  0:00:06s\n",
      "epoch 124| loss: 0.41393 |  0:00:06s\n",
      "epoch 125| loss: 0.41506 |  0:00:06s\n",
      "epoch 126| loss: 0.4127  |  0:00:06s\n",
      "epoch 127| loss: 0.41012 |  0:00:06s\n",
      "epoch 128| loss: 0.42201 |  0:00:06s\n",
      "epoch 129| loss: 0.41418 |  0:00:06s\n",
      "epoch 130| loss: 0.4124  |  0:00:06s\n",
      "epoch 131| loss: 0.40676 |  0:00:06s\n",
      "epoch 132| loss: 0.40992 |  0:00:06s\n",
      "epoch 133| loss: 0.41542 |  0:00:06s\n",
      "epoch 134| loss: 0.411   |  0:00:06s\n",
      "epoch 135| loss: 0.40383 |  0:00:06s\n",
      "epoch 136| loss: 0.40702 |  0:00:06s\n",
      "epoch 137| loss: 0.41123 |  0:00:07s\n",
      "epoch 138| loss: 0.40235 |  0:00:07s\n",
      "epoch 139| loss: 0.41131 |  0:00:07s\n",
      "epoch 140| loss: 0.40156 |  0:00:07s\n",
      "epoch 141| loss: 0.40275 |  0:00:07s\n",
      "epoch 142| loss: 0.40435 |  0:00:07s\n",
      "epoch 143| loss: 0.39838 |  0:00:07s\n",
      "epoch 144| loss: 0.39697 |  0:00:07s\n",
      "epoch 145| loss: 0.40627 |  0:00:07s\n",
      "epoch 146| loss: 0.39956 |  0:00:07s\n",
      "epoch 147| loss: 0.39254 |  0:00:07s\n",
      "epoch 148| loss: 0.39673 |  0:00:07s\n",
      "epoch 149| loss: 0.39021 |  0:00:07s\n",
      "epoch 150| loss: 0.38701 |  0:00:07s\n",
      "epoch 151| loss: 0.39155 |  0:00:07s\n",
      "epoch 152| loss: 0.38715 |  0:00:07s\n",
      "epoch 153| loss: 0.38539 |  0:00:07s\n",
      "epoch 154| loss: 0.3805  |  0:00:07s\n",
      "epoch 155| loss: 0.38658 |  0:00:07s\n",
      "epoch 156| loss: 0.38244 |  0:00:07s\n",
      "epoch 157| loss: 0.38939 |  0:00:07s\n",
      "epoch 158| loss: 0.38604 |  0:00:08s\n",
      "epoch 159| loss: 0.38263 |  0:00:08s\n",
      "epoch 160| loss: 0.383   |  0:00:08s\n",
      "epoch 161| loss: 0.38275 |  0:00:08s\n",
      "epoch 162| loss: 0.3747  |  0:00:08s\n",
      "epoch 163| loss: 0.38457 |  0:00:08s\n",
      "epoch 164| loss: 0.38214 |  0:00:08s\n",
      "epoch 165| loss: 0.38842 |  0:00:08s\n",
      "epoch 166| loss: 0.37563 |  0:00:08s\n",
      "epoch 167| loss: 0.38946 |  0:00:08s\n",
      "epoch 168| loss: 0.39004 |  0:00:08s\n",
      "epoch 169| loss: 0.38555 |  0:00:08s\n",
      "epoch 170| loss: 0.38699 |  0:00:08s\n",
      "epoch 171| loss: 0.38251 |  0:00:08s\n",
      "epoch 172| loss: 0.37322 |  0:00:08s\n",
      "epoch 173| loss: 0.37461 |  0:00:08s\n",
      "epoch 174| loss: 0.37934 |  0:00:08s\n",
      "epoch 175| loss: 0.37561 |  0:00:08s\n",
      "epoch 176| loss: 0.37233 |  0:00:08s\n",
      "epoch 177| loss: 0.36703 |  0:00:09s\n",
      "epoch 178| loss: 0.36985 |  0:00:09s\n",
      "epoch 179| loss: 0.37102 |  0:00:09s\n",
      "epoch 180| loss: 0.37611 |  0:00:09s\n",
      "epoch 181| loss: 0.36982 |  0:00:09s\n",
      "epoch 182| loss: 0.37385 |  0:00:09s\n",
      "epoch 183| loss: 0.36204 |  0:00:09s\n",
      "epoch 184| loss: 0.36449 |  0:00:09s\n",
      "epoch 185| loss: 0.36274 |  0:00:09s\n",
      "epoch 186| loss: 0.3675  |  0:00:09s\n",
      "epoch 187| loss: 0.37158 |  0:00:09s\n",
      "epoch 188| loss: 0.36024 |  0:00:09s\n",
      "epoch 189| loss: 0.36222 |  0:00:09s\n",
      "epoch 190| loss: 0.36547 |  0:00:09s\n",
      "epoch 191| loss: 0.35913 |  0:00:09s\n",
      "epoch 192| loss: 0.37019 |  0:00:09s\n",
      "epoch 193| loss: 0.3661  |  0:00:09s\n",
      "epoch 194| loss: 0.36435 |  0:00:09s\n",
      "epoch 195| loss: 0.3665  |  0:00:09s\n",
      "epoch 196| loss: 0.36435 |  0:00:10s\n",
      "epoch 197| loss: 0.36708 |  0:00:10s\n",
      "epoch 198| loss: 0.37635 |  0:00:10s\n",
      "epoch 199| loss: 0.36185 |  0:00:10s\n",
      "epoch 200| loss: 0.372   |  0:00:10s\n",
      "epoch 201| loss: 0.36781 |  0:00:10s\n",
      "epoch 202| loss: 0.37538 |  0:00:10s\n",
      "epoch 203| loss: 0.39584 |  0:00:10s\n",
      "epoch 204| loss: 0.40129 |  0:00:10s\n",
      "epoch 205| loss: 0.39697 |  0:00:10s\n",
      "epoch 206| loss: 0.39629 |  0:00:10s\n",
      "epoch 207| loss: 0.38705 |  0:00:10s\n",
      "epoch 208| loss: 0.38361 |  0:00:10s\n",
      "epoch 209| loss: 0.37977 |  0:00:10s\n",
      "epoch 210| loss: 0.3787  |  0:00:10s\n",
      "epoch 211| loss: 0.37001 |  0:00:11s\n",
      "epoch 212| loss: 0.3723  |  0:00:11s\n",
      "epoch 213| loss: 0.37208 |  0:00:11s\n",
      "epoch 214| loss: 0.36534 |  0:00:11s\n",
      "epoch 215| loss: 0.37599 |  0:00:11s\n",
      "epoch 216| loss: 0.35701 |  0:00:11s\n",
      "epoch 217| loss: 0.36111 |  0:00:11s\n",
      "epoch 218| loss: 0.35919 |  0:00:11s\n",
      "epoch 219| loss: 0.36595 |  0:00:11s\n",
      "epoch 220| loss: 0.35637 |  0:00:11s\n",
      "epoch 221| loss: 0.36044 |  0:00:11s\n",
      "epoch 222| loss: 0.34806 |  0:00:11s\n",
      "epoch 223| loss: 0.35702 |  0:00:11s\n",
      "epoch 224| loss: 0.35781 |  0:00:11s\n",
      "epoch 225| loss: 0.35377 |  0:00:11s\n",
      "epoch 226| loss: 0.35655 |  0:00:12s\n",
      "epoch 227| loss: 0.35704 |  0:00:12s\n",
      "epoch 228| loss: 0.34493 |  0:00:12s\n",
      "epoch 229| loss: 0.35773 |  0:00:12s\n",
      "epoch 230| loss: 0.34637 |  0:00:12s\n",
      "epoch 231| loss: 0.34904 |  0:00:12s\n",
      "epoch 232| loss: 0.34231 |  0:00:12s\n",
      "epoch 233| loss: 0.34362 |  0:00:12s\n",
      "epoch 234| loss: 0.34797 |  0:00:12s\n",
      "epoch 235| loss: 0.34678 |  0:00:12s\n",
      "epoch 236| loss: 0.34557 |  0:00:12s\n",
      "epoch 237| loss: 0.34948 |  0:00:12s\n",
      "epoch 238| loss: 0.34857 |  0:00:12s\n",
      "epoch 239| loss: 0.35335 |  0:00:12s\n",
      "epoch 240| loss: 0.34921 |  0:00:12s\n",
      "epoch 241| loss: 0.34684 |  0:00:12s\n",
      "epoch 242| loss: 0.34874 |  0:00:12s\n",
      "epoch 243| loss: 0.3433  |  0:00:12s\n",
      "epoch 244| loss: 0.34147 |  0:00:13s\n",
      "epoch 245| loss: 0.3383  |  0:00:13s\n",
      "epoch 246| loss: 0.34105 |  0:00:13s\n",
      "epoch 247| loss: 0.34444 |  0:00:13s\n",
      "epoch 248| loss: 0.34859 |  0:00:13s\n",
      "epoch 249| loss: 0.34221 |  0:00:13s\n",
      "TabNetRegressor_custom\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.24287 |  0:00:00s\n",
      "epoch 1  | loss: 2.15244 |  0:00:00s\n",
      "epoch 2  | loss: 1.58292 |  0:00:00s\n",
      "epoch 3  | loss: 1.26691 |  0:00:00s\n",
      "epoch 4  | loss: 1.06626 |  0:00:00s\n",
      "epoch 5  | loss: 0.9529  |  0:00:00s\n",
      "epoch 6  | loss: 0.88656 |  0:00:00s\n",
      "epoch 7  | loss: 0.84019 |  0:00:00s\n",
      "epoch 8  | loss: 0.79465 |  0:00:00s\n",
      "epoch 9  | loss: 0.76328 |  0:00:00s\n",
      "epoch 10 | loss: 0.72436 |  0:00:00s\n",
      "epoch 11 | loss: 0.69744 |  0:00:00s\n",
      "epoch 12 | loss: 0.69848 |  0:00:00s\n",
      "epoch 13 | loss: 0.68787 |  0:00:00s\n",
      "epoch 14 | loss: 0.67379 |  0:00:00s\n",
      "epoch 15 | loss: 0.67166 |  0:00:00s\n",
      "epoch 16 | loss: 0.65451 |  0:00:00s\n",
      "epoch 17 | loss: 0.66407 |  0:00:00s\n",
      "epoch 18 | loss: 0.65599 |  0:00:00s\n",
      "epoch 19 | loss: 0.65139 |  0:00:00s\n",
      "epoch 20 | loss: 0.64927 |  0:00:01s\n",
      "epoch 21 | loss: 0.64578 |  0:00:01s\n",
      "epoch 22 | loss: 0.63364 |  0:00:01s\n",
      "epoch 23 | loss: 0.62991 |  0:00:01s\n",
      "epoch 24 | loss: 0.62301 |  0:00:01s\n",
      "epoch 25 | loss: 0.62647 |  0:00:01s\n",
      "epoch 26 | loss: 0.61679 |  0:00:01s\n",
      "epoch 27 | loss: 0.60603 |  0:00:01s\n",
      "epoch 28 | loss: 0.59713 |  0:00:01s\n",
      "epoch 29 | loss: 0.59047 |  0:00:01s\n",
      "epoch 30 | loss: 0.58662 |  0:00:01s\n",
      "epoch 31 | loss: 0.59503 |  0:00:01s\n",
      "epoch 32 | loss: 0.58066 |  0:00:01s\n",
      "epoch 33 | loss: 0.5762  |  0:00:01s\n",
      "epoch 34 | loss: 0.57702 |  0:00:01s\n",
      "epoch 35 | loss: 0.5748  |  0:00:01s\n",
      "epoch 36 | loss: 0.58111 |  0:00:01s\n",
      "epoch 37 | loss: 0.56751 |  0:00:01s\n",
      "epoch 38 | loss: 0.56548 |  0:00:01s\n",
      "epoch 39 | loss: 0.56671 |  0:00:01s\n",
      "epoch 40 | loss: 0.54826 |  0:00:02s\n",
      "epoch 41 | loss: 0.55204 |  0:00:02s\n",
      "epoch 42 | loss: 0.54543 |  0:00:02s\n",
      "epoch 43 | loss: 0.54748 |  0:00:02s\n",
      "epoch 44 | loss: 0.53076 |  0:00:02s\n",
      "epoch 45 | loss: 0.534   |  0:00:02s\n",
      "epoch 46 | loss: 0.52653 |  0:00:02s\n",
      "epoch 47 | loss: 0.51791 |  0:00:02s\n",
      "epoch 48 | loss: 0.52555 |  0:00:02s\n",
      "epoch 49 | loss: 0.51958 |  0:00:02s\n",
      "epoch 50 | loss: 0.51854 |  0:00:02s\n",
      "epoch 51 | loss: 0.52509 |  0:00:02s\n",
      "epoch 52 | loss: 0.53795 |  0:00:02s\n",
      "epoch 53 | loss: 0.51252 |  0:00:02s\n",
      "epoch 54 | loss: 0.52148 |  0:00:02s\n",
      "epoch 55 | loss: 0.50706 |  0:00:02s\n",
      "epoch 56 | loss: 0.50415 |  0:00:02s\n",
      "epoch 57 | loss: 0.50179 |  0:00:02s\n",
      "epoch 58 | loss: 0.5014  |  0:00:02s\n",
      "epoch 59 | loss: 0.496   |  0:00:02s\n",
      "epoch 60 | loss: 0.49581 |  0:00:03s\n",
      "epoch 61 | loss: 0.49195 |  0:00:03s\n",
      "epoch 62 | loss: 0.49182 |  0:00:03s\n",
      "epoch 63 | loss: 0.48621 |  0:00:03s\n",
      "epoch 64 | loss: 0.48535 |  0:00:03s\n",
      "epoch 65 | loss: 0.48092 |  0:00:03s\n",
      "epoch 66 | loss: 0.48223 |  0:00:03s\n",
      "epoch 67 | loss: 0.47486 |  0:00:03s\n",
      "epoch 68 | loss: 0.48263 |  0:00:03s\n",
      "epoch 69 | loss: 0.46807 |  0:00:03s\n",
      "epoch 70 | loss: 0.47951 |  0:00:03s\n",
      "epoch 71 | loss: 0.47439 |  0:00:03s\n",
      "epoch 72 | loss: 0.46664 |  0:00:03s\n",
      "epoch 73 | loss: 0.46357 |  0:00:03s\n",
      "epoch 74 | loss: 0.45955 |  0:00:03s\n",
      "epoch 75 | loss: 0.44495 |  0:00:03s\n",
      "epoch 76 | loss: 0.45515 |  0:00:03s\n",
      "epoch 77 | loss: 0.44442 |  0:00:03s\n",
      "epoch 78 | loss: 0.44651 |  0:00:03s\n",
      "epoch 79 | loss: 0.44756 |  0:00:03s\n",
      "epoch 80 | loss: 0.43945 |  0:00:04s\n",
      "epoch 81 | loss: 0.4455  |  0:00:04s\n",
      "epoch 82 | loss: 0.42914 |  0:00:04s\n",
      "epoch 83 | loss: 0.42907 |  0:00:04s\n",
      "epoch 84 | loss: 0.42916 |  0:00:04s\n",
      "epoch 85 | loss: 0.43479 |  0:00:04s\n",
      "epoch 86 | loss: 0.4282  |  0:00:04s\n",
      "epoch 87 | loss: 0.43356 |  0:00:04s\n",
      "epoch 88 | loss: 0.4317  |  0:00:04s\n",
      "epoch 89 | loss: 0.42938 |  0:00:04s\n",
      "epoch 90 | loss: 0.42696 |  0:00:04s\n",
      "epoch 91 | loss: 0.43679 |  0:00:04s\n",
      "epoch 92 | loss: 0.43223 |  0:00:04s\n",
      "epoch 93 | loss: 0.43293 |  0:00:04s\n",
      "epoch 94 | loss: 0.43471 |  0:00:04s\n",
      "epoch 95 | loss: 0.43117 |  0:00:04s\n",
      "epoch 96 | loss: 0.43198 |  0:00:04s\n",
      "epoch 97 | loss: 0.42767 |  0:00:04s\n",
      "epoch 98 | loss: 0.44682 |  0:00:04s\n",
      "epoch 99 | loss: 0.42731 |  0:00:05s\n",
      "epoch 100| loss: 0.42579 |  0:00:05s\n",
      "epoch 101| loss: 0.42668 |  0:00:05s\n",
      "epoch 102| loss: 0.41374 |  0:00:05s\n",
      "epoch 103| loss: 0.40874 |  0:00:05s\n",
      "epoch 104| loss: 0.41486 |  0:00:05s\n",
      "epoch 105| loss: 0.4021  |  0:00:05s\n",
      "epoch 106| loss: 0.40922 |  0:00:05s\n",
      "epoch 107| loss: 0.40808 |  0:00:05s\n",
      "epoch 108| loss: 0.4061  |  0:00:05s\n",
      "epoch 109| loss: 0.40864 |  0:00:05s\n",
      "epoch 110| loss: 0.40504 |  0:00:05s\n",
      "epoch 111| loss: 0.39976 |  0:00:05s\n",
      "epoch 112| loss: 0.40696 |  0:00:05s\n",
      "epoch 113| loss: 0.38797 |  0:00:05s\n",
      "epoch 114| loss: 0.40204 |  0:00:06s\n",
      "epoch 115| loss: 0.3983  |  0:00:06s\n",
      "epoch 116| loss: 0.4002  |  0:00:06s\n",
      "epoch 117| loss: 0.40514 |  0:00:06s\n",
      "epoch 118| loss: 0.40652 |  0:00:06s\n",
      "epoch 119| loss: 0.40779 |  0:00:06s\n",
      "epoch 120| loss: 0.41209 |  0:00:06s\n",
      "epoch 121| loss: 0.40213 |  0:00:06s\n",
      "epoch 122| loss: 0.38867 |  0:00:06s\n",
      "epoch 123| loss: 0.38918 |  0:00:06s\n",
      "epoch 124| loss: 0.39164 |  0:00:06s\n",
      "epoch 125| loss: 0.37698 |  0:00:06s\n",
      "epoch 126| loss: 0.37028 |  0:00:06s\n",
      "epoch 127| loss: 0.37927 |  0:00:06s\n",
      "epoch 128| loss: 0.36095 |  0:00:06s\n",
      "epoch 129| loss: 0.36817 |  0:00:06s\n",
      "epoch 130| loss: 0.35612 |  0:00:06s\n",
      "epoch 131| loss: 0.35326 |  0:00:07s\n",
      "epoch 132| loss: 0.35904 |  0:00:07s\n",
      "epoch 133| loss: 0.35966 |  0:00:07s\n",
      "epoch 134| loss: 0.34819 |  0:00:07s\n",
      "epoch 135| loss: 0.35239 |  0:00:07s\n",
      "epoch 136| loss: 0.35978 |  0:00:07s\n",
      "epoch 137| loss: 0.35151 |  0:00:07s\n",
      "epoch 138| loss: 0.36372 |  0:00:07s\n",
      "epoch 139| loss: 0.36289 |  0:00:07s\n",
      "epoch 140| loss: 0.36773 |  0:00:07s\n",
      "epoch 141| loss: 0.36356 |  0:00:07s\n",
      "epoch 142| loss: 0.35889 |  0:00:07s\n",
      "epoch 143| loss: 0.36573 |  0:00:07s\n",
      "epoch 144| loss: 0.36685 |  0:00:07s\n",
      "epoch 145| loss: 0.36418 |  0:00:07s\n",
      "epoch 146| loss: 0.37643 |  0:00:07s\n",
      "epoch 147| loss: 0.36169 |  0:00:08s\n",
      "epoch 148| loss: 0.3599  |  0:00:08s\n",
      "epoch 149| loss: 0.36358 |  0:00:08s\n",
      "epoch 150| loss: 0.38092 |  0:00:08s\n",
      "epoch 151| loss: 0.37406 |  0:00:08s\n",
      "epoch 152| loss: 0.36953 |  0:00:08s\n",
      "epoch 153| loss: 0.37239 |  0:00:08s\n",
      "epoch 154| loss: 0.36479 |  0:00:08s\n",
      "epoch 155| loss: 0.36082 |  0:00:08s\n",
      "epoch 156| loss: 0.35538 |  0:00:08s\n",
      "epoch 157| loss: 0.34878 |  0:00:08s\n",
      "epoch 158| loss: 0.34607 |  0:00:08s\n",
      "epoch 159| loss: 0.34139 |  0:00:08s\n",
      "epoch 160| loss: 0.33437 |  0:00:08s\n",
      "epoch 161| loss: 0.33563 |  0:00:08s\n",
      "epoch 162| loss: 0.34112 |  0:00:08s\n",
      "epoch 163| loss: 0.35652 |  0:00:08s\n",
      "epoch 164| loss: 0.35128 |  0:00:08s\n",
      "epoch 165| loss: 0.34045 |  0:00:08s\n",
      "epoch 166| loss: 0.34459 |  0:00:08s\n",
      "epoch 167| loss: 0.34769 |  0:00:09s\n",
      "epoch 168| loss: 0.33172 |  0:00:09s\n",
      "epoch 169| loss: 0.33832 |  0:00:09s\n",
      "epoch 170| loss: 0.32454 |  0:00:09s\n",
      "epoch 171| loss: 0.33092 |  0:00:09s\n",
      "epoch 172| loss: 0.33065 |  0:00:09s\n",
      "epoch 173| loss: 0.31971 |  0:00:09s\n",
      "epoch 174| loss: 0.31975 |  0:00:09s\n",
      "epoch 175| loss: 0.30903 |  0:00:09s\n",
      "epoch 176| loss: 0.31415 |  0:00:09s\n",
      "epoch 177| loss: 0.32472 |  0:00:09s\n",
      "epoch 178| loss: 0.30966 |  0:00:09s\n",
      "epoch 179| loss: 0.31315 |  0:00:09s\n",
      "epoch 180| loss: 0.31162 |  0:00:09s\n",
      "epoch 181| loss: 0.32061 |  0:00:09s\n",
      "epoch 182| loss: 0.31373 |  0:00:09s\n",
      "epoch 183| loss: 0.31775 |  0:00:09s\n",
      "epoch 184| loss: 0.31597 |  0:00:09s\n",
      "epoch 185| loss: 0.3111  |  0:00:09s\n",
      "epoch 186| loss: 0.31505 |  0:00:10s\n",
      "epoch 187| loss: 0.30957 |  0:00:10s\n",
      "epoch 188| loss: 0.31093 |  0:00:10s\n",
      "epoch 189| loss: 0.31486 |  0:00:10s\n",
      "epoch 190| loss: 0.31301 |  0:00:10s\n",
      "epoch 191| loss: 0.3108  |  0:00:10s\n",
      "epoch 192| loss: 0.30916 |  0:00:10s\n",
      "epoch 193| loss: 0.30084 |  0:00:10s\n",
      "epoch 194| loss: 0.30258 |  0:00:10s\n",
      "epoch 195| loss: 0.30658 |  0:00:10s\n",
      "epoch 196| loss: 0.30231 |  0:00:10s\n",
      "epoch 197| loss: 0.29887 |  0:00:10s\n",
      "epoch 198| loss: 0.3017  |  0:00:10s\n",
      "epoch 199| loss: 0.2996  |  0:00:10s\n",
      "epoch 200| loss: 0.29747 |  0:00:10s\n",
      "epoch 201| loss: 0.302   |  0:00:10s\n",
      "epoch 202| loss: 0.29356 |  0:00:10s\n",
      "epoch 203| loss: 0.29971 |  0:00:10s\n",
      "epoch 204| loss: 0.28496 |  0:00:11s\n",
      "epoch 205| loss: 0.28985 |  0:00:11s\n",
      "epoch 206| loss: 0.28443 |  0:00:11s\n",
      "epoch 207| loss: 0.28011 |  0:00:11s\n",
      "epoch 208| loss: 0.28006 |  0:00:11s\n",
      "epoch 209| loss: 0.27741 |  0:00:11s\n",
      "epoch 210| loss: 0.27276 |  0:00:11s\n",
      "epoch 211| loss: 0.27628 |  0:00:11s\n",
      "epoch 212| loss: 0.2841  |  0:00:11s\n",
      "epoch 213| loss: 0.28351 |  0:00:11s\n",
      "epoch 214| loss: 0.28181 |  0:00:11s\n",
      "epoch 215| loss: 0.29372 |  0:00:11s\n",
      "epoch 216| loss: 0.27954 |  0:00:11s\n",
      "epoch 217| loss: 0.28504 |  0:00:11s\n",
      "epoch 218| loss: 0.281   |  0:00:11s\n",
      "epoch 219| loss: 0.28408 |  0:00:11s\n",
      "epoch 220| loss: 0.28229 |  0:00:11s\n",
      "epoch 221| loss: 0.27346 |  0:00:11s\n",
      "epoch 222| loss: 0.26955 |  0:00:12s\n",
      "epoch 223| loss: 0.27341 |  0:00:12s\n",
      "epoch 224| loss: 0.27228 |  0:00:12s\n",
      "epoch 225| loss: 0.26034 |  0:00:12s\n",
      "epoch 226| loss: 0.26504 |  0:00:12s\n",
      "epoch 227| loss: 0.26373 |  0:00:12s\n",
      "epoch 228| loss: 0.25678 |  0:00:12s\n",
      "epoch 229| loss: 0.2667  |  0:00:12s\n",
      "epoch 230| loss: 0.25775 |  0:00:12s\n",
      "epoch 231| loss: 0.26326 |  0:00:12s\n",
      "epoch 232| loss: 0.25435 |  0:00:12s\n",
      "epoch 233| loss: 0.25562 |  0:00:12s\n",
      "epoch 234| loss: 0.26014 |  0:00:12s\n",
      "epoch 235| loss: 0.2552  |  0:00:12s\n",
      "epoch 236| loss: 0.25414 |  0:00:12s\n",
      "epoch 237| loss: 0.25566 |  0:00:12s\n",
      "epoch 238| loss: 0.25705 |  0:00:12s\n",
      "epoch 239| loss: 0.25846 |  0:00:12s\n",
      "epoch 240| loss: 0.25685 |  0:00:13s\n",
      "epoch 241| loss: 0.26753 |  0:00:13s\n",
      "epoch 242| loss: 0.27538 |  0:00:13s\n",
      "epoch 243| loss: 0.27838 |  0:00:13s\n",
      "epoch 244| loss: 0.27072 |  0:00:13s\n",
      "epoch 245| loss: 0.26688 |  0:00:13s\n",
      "epoch 246| loss: 0.26624 |  0:00:13s\n",
      "epoch 247| loss: 0.26438 |  0:00:13s\n",
      "epoch 248| loss: 0.26376 |  0:00:13s\n",
      "epoch 249| loss: 0.26156 |  0:00:13s\n",
      "PLSRegression_4_components\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "GatedAdditiveTreeEnsembleConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">358</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:22\u001b[0m,\u001b[1;36m358\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">370</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:22\u001b[0m,\u001b[1;36m370\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">374</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:22\u001b[0m,\u001b[1;36m374\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">390</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:22\u001b[0m,\u001b[1;36m390\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">587</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:22\u001b[0m,\u001b[1;36m587\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">600</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:22\u001b[0m,\u001b[1;36m600\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">609</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:22\u001b[0m,\u001b[1;36m609\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419cb980752f4f58bb49f8730d1f778c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_fdc2f73a-0972-4f52-862d-9959bd6ffb46.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_fdc2f73a-0972-4f52-862d-9959bd6ffb46.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:24</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">724</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:24\u001b[0m,\u001b[1;36m724\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:24</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">731</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:24\u001b[0m,\u001b[1;36m731\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:24</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">746</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:24\u001b[0m,\u001b[1;36m746\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                       | Params | Mode \n",
      "------------------------------------------------------------------------\n",
      "0 | _backbone        | GatedAdditiveTreesBackbone | 2.1 M  | train\n",
      "1 | _embedding_layer | Embedding1dLayer           | 400    | train\n",
      "2 | _head            | CustomHead                 | 156    | train\n",
      "3 | loss             | MSELoss                    | 0      | train\n",
      "------------------------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.417     Total estimated model params size (MB)\n",
      "689       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc922f0f95ed4d7c9dc13878cdfcf17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574bd0ae222f451abac83870ce2d724f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30994e895323405285bb634717ee309e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">693</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:26\u001b[0m,\u001b[1;36m693\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">694</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:26\u001b[0m,\u001b[1;36m694\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "DANetConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">471</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:27\u001b[0m,\u001b[1;36m471\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">483</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:27\u001b[0m,\u001b[1;36m483\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">486</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:27\u001b[0m,\u001b[1;36m486\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">503</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: DANetModel             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:27\u001b[0m,\u001b[1;36m503\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: DANetModel             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">531</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:27\u001b[0m,\u001b[1;36m531\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">539</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:27\u001b[0m,\u001b[1;36m539\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601f62ed1dad4c23aa8398460c04276f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_dcc91ffe-d58c-4f17-b31b-67883609d955.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_dcc91ffe-d58c-4f17-b31b-67883609d955.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">869</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:27\u001b[0m,\u001b[1;36m869\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">875</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:27\u001b[0m,\u001b[1;36m875\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | _backbone        | DANetBackbone    | 1.4 M  | train\n",
      "1 | _embedding_layer | Embedding1dLayer | 400    | train\n",
      "2 | _head            | LinearHead       | 260    | train\n",
      "3 | loss             | MSELoss          | 0      | train\n",
      "--------------------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.787     Total estimated model params size (MB)\n",
      "156       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac350a2199d64b7ea019cb54b57c3091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5bbbbbe35db46c19a7cfd189ed5ddb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7914cf62ee54c259670e0e637edcbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">214</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m214\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">215</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m215\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "TabTransformerConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m400\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">412</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m412\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">416</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m416\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">431</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabTransformerModel    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m431\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabTransformerModel    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">451</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m451\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">458</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m458\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09477a965a0640a2b5f10f05d2a8cadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_fb473d1c-7bec-422a-9a30-d6be50802bf1.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_fb473d1c-7bec-422a-9a30-d6be50802bf1.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">584</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m584\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">586</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m586\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                   | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | _backbone        | TabTransformerBackbone | 271 K  | train\n",
      "1 | _embedding_layer | Embedding2dLayer       | 0      | train\n",
      "2 | _head            | LinearHead             | 804    | train\n",
      "3 | loss             | MSELoss                | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "272 K     Trainable params\n",
      "0         Non-trainable params\n",
      "272 K     Total params\n",
      "1.090     Total estimated model params size (MB)\n",
      "119       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291eee68ccb741f8876453e3cc914945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8dcf24c23a34c08b78dc3b11292d50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ed2851c78b4caf865760b5626dd9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">715</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m715\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">716</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m716\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "TabNetModelConfig_tab\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">903</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m903\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">915</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m915\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">918</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m918\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">934</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m934\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">955</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m955\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">963</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:28\u001b[0m,\u001b[1;36m963\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52fa36a28f54decadd74e02dbe47fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_f77870ee-873b-43ca-8736-3a727b812d0f.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_f77870ee-873b-43ca-8736-3a727b812d0f.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">164</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:29\u001b[0m,\u001b[1;36m164\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">166</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:29\u001b[0m,\u001b[1;36m166\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type           | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | _embedding_layer | Identity       | 0      | train\n",
      "1 | _backbone        | TabNetBackbone | 18.9 K | train\n",
      "2 | _head            | Identity       | 0      | train\n",
      "3 | loss             | MSELoss        | 0      | train\n",
      "------------------------------------------------------------\n",
      "18.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "18.9 K    Total params\n",
      "0.075     Total estimated model params size (MB)\n",
      "107       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0a223058464d0882bd57eb186e53df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b9663275474c37aeaf7ebf3618155f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0f9bac32b14481b860c29bacecf4c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">588</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:29\u001b[0m,\u001b[1;36m588\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:29</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">589</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:29\u001b[0m,\u001b[1;36m589\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'GatedAdditiveTreeEnsembleConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'DANetConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabTransformerConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetModelConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'GatedAdditiveTreeEnsembleConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'DANetConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabTransformerConfig_tab', (2881, 200), (13, 200)])\n",
      "Skipping existing combination: dict_values(['NoImputer', 'NoImputer', 'TabNetModelConfig_tab', (2881, 200), (13, 200)])\n"
     ]
    }
   ],
   "source": [
    "predictive_models_list += [\n",
    "    (\"GatedAdditiveTreeEnsembleConfig_tab\", \n",
    "    TabularModelWrapper(\n",
    "        GatedAdditiveTreeEnsembleConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        gflu_stages=6,\n",
    "        gflu_dropout=0.0,\n",
    "        tree_depth=5,\n",
    "        num_trees=20,\n",
    "        chain_trees=False,\n",
    "        share_head_weights=True), data_config, trainer_config, optimizer_config \n",
    "    )),\n",
    "    (\"DANetConfig_tab\",\n",
    "    TabularModelWrapper(\n",
    "        DANetConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_layers=8,\n",
    "        k=5,\n",
    "        dropout_rate=0.1), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabTransformerConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        embedding_initialization=\"kaiming_uniform\",\n",
    "        embedding_bias=False), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    # (\"FTTransformerConfig\",\n",
    "    #     TabularModelWrapper(\n",
    "    #     FTTransformerConfig(\n",
    "    #     task=\"regression\",\n",
    "    #     head=\"LinearHead\",\n",
    "    #     head_config=head_config), data_config, trainer_config, optimizer_config\n",
    "    # )),\n",
    "    (\"TabNetModelConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        n_independent=2,\n",
    "        n_shared=2), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "\n",
    "    if any(result['params'] == params for result in all_dict_results):\n",
    "        # Skip this iteration if the combination exists\n",
    "        print(f\"Skipping existing combination: {params.values()}\")\n",
    "        \n",
    "        continue\n",
    "\n",
    "    try: \n",
    "        print(name_model)\n",
    "        \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "        # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'LinearRegression',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.2466392517089844,\n",
       "  'fitting_time': 0.22939062118530273,\n",
       "  'results_adj': {'mse_score': array([0.65983176, 0.48974311, 0.40951464, 0.56684282]),\n",
       "   'mae_score': array([0.63965764, 0.55144938, 0.56535002, 0.59999667]),\n",
       "   'r2': array([0.33312259, 0.44426866, 0.144955  , 0.31048062]),\n",
       "   'explained_variance': array([0.36730884, 0.47760179, 0.14545957, 0.43985495]),\n",
       "   'corr': array([0.60633336, 0.69220668, 0.4020632 , 0.66605104])},\n",
       "  'results_org': {'mse_score': array([0.65983176, 0.4897431 , 0.40951466, 0.56684279]),\n",
       "   'mae_score': array([0.63965764, 0.55144938, 0.56535003, 0.59999666]),\n",
       "   'r2': array([0.29810904, 0.45135258, 0.20239561, 0.33725209]),\n",
       "   'explained_variance': array([0.33409019, 0.48426081, 0.20286628, 0.46160331]),\n",
       "   'corr': array([0.57814397, 0.69816204, 0.46213839, 0.68191102])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskElasticNet',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.283444404602051,\n",
       "  'fitting_time': 0.09822368621826172,\n",
       "  'results_adj': {'mse_score': array([1.18012248, 0.95041535, 0.50741149, 1.02939714]),\n",
       "   'mae_score': array([0.88414168, 0.78683573, 0.65378091, 0.82689555]),\n",
       "   'r2': array([-0.19272377, -0.07847478, -0.05944846, -0.25218006]),\n",
       "   'explained_variance': array([ 0.11229868,  0.03189858, -0.05332468,  0.12857136]),\n",
       "   'corr': array([ 0.42176385,  0.17864276, -0.16306812,  0.44631446])},\n",
       "  'results_org': {'mse_score': array([1.18012249, 0.95041535, 0.50741151, 1.02939711]),\n",
       "   'mae_score': array([0.88414168, 0.78683573, 0.65378092, 0.82689554]),\n",
       "   'r2': array([-0.25534622, -0.06472746,  0.01172365, -0.20356259]),\n",
       "   'explained_variance': array([0.06569104, 0.04423898, 0.01743605, 0.16240568]),\n",
       "   'corr': array([0.25863755, 0.21083052, 0.14561963, 0.49623359])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskElasticNet_tuned',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.2664968967437744,\n",
       "  'fitting_time': 1.4926152229309082,\n",
       "  'results_adj': {'mse_score': array([0.66791342, 0.47856617, 0.41412878, 0.59264981]),\n",
       "   'mae_score': array([0.63043587, 0.55392437, 0.58019171, 0.59956285]),\n",
       "   'r2': array([0.32495464, 0.45695159, 0.13532092, 0.27908847]),\n",
       "   'explained_variance': array([0.3700506 , 0.49637797, 0.13598028, 0.44010284]),\n",
       "   'corr': array([0.60831821, 0.70457467, 0.38152442, 0.66946516])},\n",
       "  'results_org': {'mse_score': array([0.66791342, 0.47856617, 0.4141288 , 0.59264978]),\n",
       "   'mae_score': array([0.63043586, 0.55392437, 0.58019172, 0.59956284]),\n",
       "   'r2': array([0.28951224, 0.46387383, 0.19340873, 0.30707878]),\n",
       "   'explained_variance': array([0.33697591, 0.50279765, 0.19402379, 0.46184157]),\n",
       "   'corr': array([0.58049709, 0.70936568, 0.44690586, 0.68551862])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskLasso',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.283153772354126,\n",
       "  'fitting_time': 0.07776927947998047,\n",
       "  'results_adj': {'mse_score': array([1.39363767, 1.04056339, 0.48866937, 1.24000841]),\n",
       "   'mae_score': array([0.99127575, 0.78457157, 0.62110666, 0.93446712]),\n",
       "   'r2': array([-0.40851885, -0.18076941, -0.0203159 , -0.50837198]),\n",
       "   'explained_variance': array([-2.22044605e-16,  0.00000000e+00,  0.00000000e+00,  1.11022302e-16]),\n",
       "   'corr': array([nan, nan, nan, nan])},\n",
       "  'results_org': {'mse_score': array([1.39363768, 1.04056339, 0.48866939, 1.24000838]),\n",
       "   'mae_score': array([0.99127576, 0.78457157, 0.62110667, 0.93446711]),\n",
       "   'r2': array([-0.48247136, -0.16571814,  0.04822734, -0.44980754]),\n",
       "   'explained_variance': array([-0.05250375,  0.01274701,  0.06717846,  0.03882626]),\n",
       "   'corr': array([-0.0848666 ,  0.11455326,  0.2721653 ,  0.20679355])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'MultiTaskLasso_tuned',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.3114430904388428,\n",
       "  'fitting_time': 1.4922456741333008,\n",
       "  'results_adj': {'mse_score': array([0.6555347 , 0.47602153, 0.41898635, 0.58247095]),\n",
       "   'mae_score': array([0.62193632, 0.55058805, 0.57780272, 0.59499437]),\n",
       "   'r2': array([0.33746553, 0.45983909, 0.12517858, 0.29147024]),\n",
       "   'explained_variance': array([0.38303248, 0.49996522, 0.12605199, 0.44788336]),\n",
       "   'corr': array([0.61892446, 0.70715313, 0.37654024, 0.67491708])},\n",
       "  'results_org': {'mse_score': array([0.6555347 , 0.47602153, 0.41898636, 0.58247092]),\n",
       "   'mae_score': array([0.62193631, 0.55058805, 0.57780274, 0.59499436]),\n",
       "   'r2': array([0.30268   , 0.46672453, 0.18394773, 0.31897981]),\n",
       "   'explained_variance': array([0.35063939, 0.50633917, 0.18476246, 0.46932   ]),\n",
       "   'corr': array([0.59223356, 0.71197064, 0.44079738, 0.69050252])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'RandomForestRegressor',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.289330244064331,\n",
       "  'fitting_time': 51.79721212387085,\n",
       "  'results_adj': {'mse_score': array([0.81204962, 0.55738872, 0.42447239, 0.76265381]),\n",
       "   'mae_score': array([0.66916949, 0.63959102, 0.60835909, 0.64774379]),\n",
       "   'r2': array([0.17927936, 0.36750845, 0.11372402, 0.07229206]),\n",
       "   'explained_variance': array([0.38575368, 0.43668934, 0.11735442, 0.37009751]),\n",
       "   'corr': array([0.63471399, 0.67665811, 0.34287971, 0.62716925])},\n",
       "  'results_org': {'mse_score': array([0.81204962, 0.55738872, 0.42447241, 0.76265379]),\n",
       "   'mae_score': array([0.6691695 , 0.63959102, 0.6083591 , 0.64774379]),\n",
       "   'r2': array([0.13618846, 0.37557082, 0.17326266, 0.10831149]),\n",
       "   'explained_variance': array([0.35350346, 0.44386986, 0.17664918, 0.39455427]),\n",
       "   'corr': array([0.60552878, 0.68293217, 0.42214619, 0.66388455])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'PLSRegression_4_components',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.3004002571105957,\n",
       "  'fitting_time': 0.15016818046569824,\n",
       "  'results_adj': {'mse_score': array([0.99283749, 0.68320591, 0.51129344, 0.88042711]),\n",
       "   'mae_score': array([0.8049879 , 0.73842664, 0.68894711, 0.76406833]),\n",
       "   'r2': array([-0.00343896,  0.22473859, -0.06755375, -0.07096982]),\n",
       "   'explained_variance': array([ 0.25778331,  0.29950329, -0.06753554,  0.2736962 ]),\n",
       "   'corr': array([0.50966098, 0.55561882, 0.0742507 , 0.52674345])},\n",
       "  'results_org': {'mse_score': array([0.99283749, 0.68320591, 0.51129345, 0.88042708]),\n",
       "   'mae_score': array([0.80498791, 0.73842664, 0.68894712, 0.76406832]),\n",
       "   'r2': array([-0.05612325,  0.23462085,  0.00416286, -0.02938806]),\n",
       "   'explained_variance': array([0.21881416, 0.30843253, 0.00417985, 0.30189586]),\n",
       "   'corr': array([0.46922857, 0.57093376, 0.17716915, 0.57292837])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'TabNetRegressor_default',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.3759474754333496,\n",
       "  'fitting_time': 13.77210783958435,\n",
       "  'results_adj': {'mse_score': array([0.61824897, 0.61184618, 0.36866036, 0.57763236]),\n",
       "   'mae_score': array([0.69107406, 0.60773177, 0.5361999 , 0.59462053]),\n",
       "   'r2': array([0.3751494 , 0.30571336, 0.23025659, 0.297356  ]),\n",
       "   'explained_variance': array([0.56567234, 0.38915553, 0.23426406, 0.55584824]),\n",
       "   'corr': array([0.77300748, 0.67628222, 0.58077844, 0.81923406])},\n",
       "  'results_org': {'mse_score': array([0.61824897, 0.61184618, 0.36866037, 0.57763234]),\n",
       "   'mae_score': array([0.69107407, 0.60773177, 0.53619991, 0.59462053]),\n",
       "   'r2': array([0.34234241, 0.31456344, 0.28196676, 0.32463705]),\n",
       "   'explained_variance': array([0.54286851, 0.39694198, 0.28570502, 0.57309301]),\n",
       "   'corr': array([0.76293555, 0.67836772, 0.73113403, 0.85040194])}},\n",
       " {'params': {'ordinal_imputer': 'SimpleImputer_most_frequent',\n",
       "   'continuous_imputer': 'KNNImputer',\n",
       "   'model': 'TabNetRegressor_custom',\n",
       "   'train_shape': (2881, 348),\n",
       "   'test_shape': (13, 348)},\n",
       "  'imputation_time': 3.4543991088867188,\n",
       "  'fitting_time': 13.39520812034607,\n",
       "  'results_adj': {'mse_score': array([1.07617402, 1.31252905, 0.77527955, 1.0442195 ]),\n",
       "   'mae_score': array([0.76465789, 0.96862147, 0.76452422, 0.76252167]),\n",
       "   'r2': array([-0.08766535, -0.48937986, -0.61874284, -0.27021028]),\n",
       "   'explained_variance': array([ 0.20827438, -0.34330108, -0.60463007,  0.30404738]),\n",
       "   'corr': array([ 0.54009087,  0.30700851, -0.06850123,  0.58642515])},\n",
       "  'results_org': {'mse_score': array([1.07617403, 1.31252906, 0.77527957, 1.04421947]),\n",
       "   'mae_score': array([0.76465789, 0.96862147, 0.76452423, 0.76252167]),\n",
       "   'r2': array([-0.14477185, -0.47039474, -0.5099982 , -0.22089277]),\n",
       "   'explained_variance': array([ 0.16670582, -0.32617802, -0.4968335 ,  0.33106863]),\n",
       "   'corr': array([0.50999118, 0.31151104, 0.0452715 , 0.59746408])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'LinearRegression',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 0.1571178436279297,\n",
       "  'results_adj': {'mse_score': array([1.11192349, 0.67668626, 0.61999815, 0.97750844]),\n",
       "   'mae_score': array([0.72849831, 0.70235973, 0.70986192, 0.71215901]),\n",
       "   'r2': array([-0.12379655,  0.2321367 , -0.29452346, -0.18906157]),\n",
       "   'explained_variance': array([ 0.02475189,  0.28845771, -0.29319881,  0.10973059]),\n",
       "   'corr': array([0.33204585, 0.54765514, 0.01649677, 0.38058137])},\n",
       "  'results_org': {'mse_score': array([1.11192349, 0.67668626, 0.61999817, 0.97750843]),\n",
       "   'mae_score': array([0.72849832, 0.70235973, 0.70986193, 0.712159  ]),\n",
       "   'r2': array([-0.18280006,  0.24192465, -0.20755938, -0.14289477]),\n",
       "   'explained_variance': array([-0.02645227,  0.29752774, -0.20632372,  0.14429641]),\n",
       "   'corr': array([0.27766053, 0.55012684, 0.07284141, 0.39622517])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'MultiTaskElasticNet',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 0.015189647674560547,\n",
       "  'results_adj': {'mse_score': array([1.18012247, 0.95041534, 0.50741149, 1.02939713]),\n",
       "   'mae_score': array([0.88414167, 0.78683572, 0.65378091, 0.82689555]),\n",
       "   'r2': array([-0.19272375, -0.07847477, -0.05944845, -0.25218005]),\n",
       "   'explained_variance': array([ 0.11229868,  0.03189858, -0.05332468,  0.12857136]),\n",
       "   'corr': array([ 0.42176386,  0.17864277, -0.16306808,  0.44631445])},\n",
       "  'results_org': {'mse_score': array([1.18012248, 0.95041534, 0.50741151, 1.02939711]),\n",
       "   'mae_score': array([0.88414168, 0.78683573, 0.65378092, 0.82689554]),\n",
       "   'r2': array([-0.25534621, -0.06472745,  0.01172366, -0.20356258]),\n",
       "   'explained_variance': array([0.06569104, 0.04423898, 0.01743605, 0.16240568]),\n",
       "   'corr': array([0.25863755, 0.21083053, 0.14561964, 0.49623358])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'MultiTaskElasticNet_tuned',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 0.8243184089660645,\n",
       "  'results_adj': {'mse_score': array([1.09767924, 0.67163602, 0.61727141, 0.96027028]),\n",
       "   'mae_score': array([0.72712752, 0.70316248, 0.71357071, 0.70813001]),\n",
       "   'r2': array([-0.1094002 ,  0.23786741, -0.28883017, -0.16809271]),\n",
       "   'explained_variance': array([ 0.04259982,  0.29559122, -0.28782174,  0.12798901]),\n",
       "   'corr': array([0.33826662, 0.55120885, 0.00953623, 0.39231636])},\n",
       "  'results_org': {'mse_score': array([1.09767923, 0.67163602, 0.61727143, 0.96027027]),\n",
       "   'mae_score': array([0.72712752, 0.70316248, 0.71357072, 0.70813   ]),\n",
       "   'r2': array([-0.16764784,  0.24758231, -0.20224855, -0.12274005]),\n",
       "   'explained_variance': array([-0.00766726,  0.30457033, -0.20130787,  0.16184592]),\n",
       "   'corr': array([0.28384559, 0.55450481, 0.06742918, 0.41077564])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'MultiTaskLasso',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 0.007662773132324219,\n",
       "  'results_adj': {'mse_score': array([1.39363767, 1.04056339, 0.48866937, 1.24000841]),\n",
       "   'mae_score': array([0.99127575, 0.78457157, 0.62110666, 0.93446712]),\n",
       "   'r2': array([-0.40851885, -0.18076941, -0.0203159 , -0.50837198]),\n",
       "   'explained_variance': array([-2.22044605e-16,  0.00000000e+00,  0.00000000e+00,  1.11022302e-16]),\n",
       "   'corr': array([nan, nan, nan, nan])},\n",
       "  'results_org': {'mse_score': array([1.39363768, 1.04056339, 0.48866939, 1.24000838]),\n",
       "   'mae_score': array([0.99127576, 0.78457157, 0.62110667, 0.93446711]),\n",
       "   'r2': array([-0.48247136, -0.16571814,  0.04822734, -0.44980754]),\n",
       "   'explained_variance': array([-0.05250375,  0.01274701,  0.06717846,  0.03882626]),\n",
       "   'corr': array([-0.0848666 ,  0.11455326,  0.2721653 ,  0.20679355])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'MultiTaskLasso_tuned',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 0.8485329151153564,\n",
       "  'results_adj': {'mse_score': array([1.09986228, 0.67700338, 0.61721354, 0.96322248]),\n",
       "   'mae_score': array([0.72439764, 0.70502407, 0.71262079, 0.70634911]),\n",
       "   'r2': array([-0.11160655,  0.23177686, -0.28870934, -0.17168383]),\n",
       "   'explained_variance': array([ 0.04182356,  0.29179378, -0.28783433,  0.1250568 ]),\n",
       "   'corr': array([0.34030126, 0.54948447, 0.01369735, 0.39123818])},\n",
       "  'results_org': {'mse_score': array([1.09986228, 0.67700337, 0.61721356, 0.96322247]),\n",
       "   'mae_score': array([0.72439764, 0.70502408, 0.71262081, 0.7063491 ]),\n",
       "   'r2': array([-0.16997004,  0.2415694 , -0.20213584, -0.12619174]),\n",
       "   'explained_variance': array([-0.00848428,  0.30082129, -0.20131962,  0.15902757]),\n",
       "   'corr': array([0.28608858, 0.55225247, 0.07079544, 0.40890542])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'RandomForestRegressor',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 32.46849751472473,\n",
       "  'results_adj': {'mse_score': array([1.00638713, 0.65012684, 0.47869162, 0.82220865]),\n",
       "   'mae_score': array([0.75528169, 0.6841987 , 0.63512616, 0.63151397]),\n",
       "   'r2': array([-1.71332791e-02,  2.62274753e-01,  5.17132936e-04, -1.51675905e-04]),\n",
       "   'explained_variance': array([0.29209145, 0.405549  , 0.02445608, 0.37078241]),\n",
       "   'corr': array([0.54046621, 0.64364297, 0.21686966, 0.60995505])},\n",
       "  'results_org': {'mse_score': array([1.00638713, 0.65012684, 0.47869164, 0.82220864]),\n",
       "   'mae_score': array([0.7552817 , 0.6841987 , 0.63512617, 0.63151397]),\n",
       "   'r2': array([-0.07053657,  0.27167853,  0.06766084,  0.03868046]),\n",
       "   'explained_variance': array([0.25492361, 0.41312647, 0.08999161, 0.39521257]),\n",
       "   'corr': array([0.50511182, 0.65009756, 0.3086371 , 0.63522421])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'XGBoostRegressor',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 1.580946922302246,\n",
       "  'results_adj': {'mse_score': array([0.99729777, 0.55285888, 0.49806901, 0.7462314 ]),\n",
       "   'mae_score': array([0.75445592, 0.63451078, 0.64294552, 0.64429738]),\n",
       "   'r2': array([-0.00794687,  0.37264865, -0.03994184,  0.09226863]),\n",
       "   'explained_variance': array([ 0.26511088,  0.47604274, -0.03447538,  0.31643386]),\n",
       "   'corr': array([0.52174936, 0.69849083, 0.21172288, 0.56606   ])},\n",
       "  'results_org': {'mse_score': array([0.99729777, 0.55285888, 0.49806903, 0.74623138]),\n",
       "   'mae_score': array([0.75445593, 0.63451079, 0.64294554, 0.64429738]),\n",
       "   'r2': array([-0.06086784,  0.38064549,  0.02991984,  0.12751245]),\n",
       "   'explained_variance': array([0.22652646, 0.48272162, 0.03501907, 0.34297417]),\n",
       "   'corr': array([0.48966434, 0.70257856, 0.27320828, 0.58583766])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'XGBoostRegressor_tuned',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 2.18463134765625,\n",
       "  'results_adj': {'mse_score': array([1.0153611 , 0.63851973, 0.46345103, 0.70749896]),\n",
       "   'mae_score': array([0.73457703, 0.71458275, 0.63021056, 0.61263546]),\n",
       "   'r2': array([-0.02620307,  0.27544581,  0.03233868,  0.13938357]),\n",
       "   'explained_variance': array([0.22165364, 0.34897739, 0.05887258, 0.43032201]),\n",
       "   'corr': array([0.48104477, 0.59075482, 0.30682341, 0.65598979])},\n",
       "  'results_org': {'mse_score': array([1.0153611 , 0.63851973, 0.46345105, 0.70749894]),\n",
       "   'mae_score': array([0.73457704, 0.71458276, 0.63021057, 0.61263545]),\n",
       "   'r2': array([-0.08008256,  0.2846817 ,  0.09734466,  0.17279809]),\n",
       "   'explained_variance': array([0.18078756, 0.35727599, 0.12209605, 0.45244048]),\n",
       "   'corr': array([0.44678191, 0.59775758, 0.36862204, 0.67323253])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'TabNetRegressor_default',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 13.32374882698059,\n",
       "  'results_adj': {'mse_score': array([1.00024251, 0.85216095, 0.6231349 , 0.78732824]),\n",
       "   'mae_score': array([0.78064659, 0.8013238 , 0.6795552 , 0.67213814]),\n",
       "   'r2': array([-0.01092304,  0.03301847, -0.30107284,  0.04227758]),\n",
       "   'explained_variance': array([ 0.38275983,  0.13551578, -0.29939216,  0.5247155 ]),\n",
       "   'corr': array([0.62946614, 0.47112118, 0.06684408, 0.73037268])},\n",
       "  'results_org': {'mse_score': array([1.0002425 , 0.85216095, 0.62313492, 0.78732822]),\n",
       "   'mae_score': array([0.78064659, 0.8013238 , 0.67955522, 0.67213815]),\n",
       "   'r2': array([-0.06400027,  0.04534457, -0.21366878,  0.07946235]),\n",
       "   'explained_variance': array([ 0.35035243,  0.14653537, -0.212101  ,  0.54316902]),\n",
       "   'corr': array([0.60852724, 0.4852577 , 0.12261516, 0.74095112])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'TabNetRegressor_custom',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 13.630439281463623,\n",
       "  'results_adj': {'mse_score': array([0.92610036, 0.76428738, 0.61165174, 0.87483944]),\n",
       "   'mae_score': array([0.71338154, 0.75841494, 0.67114785, 0.72399585]),\n",
       "   'r2': array([ 0.06401079,  0.13273216, -0.2770966 , -0.06417287]),\n",
       "   'explained_variance': array([ 0.22209961,  0.26101411, -0.27432143,  0.26805491]),\n",
       "   'corr': array([0.55329131, 0.58084529, 0.23700988, 0.59708229])},\n",
       "  'results_org': {'mse_score': array([0.92610035, 0.76428739, 0.61165176, 0.87483943]),\n",
       "   'mae_score': array([0.71338154, 0.75841494, 0.67114786, 0.72399584]),\n",
       "   'r2': array([ 0.01486787,  0.14378722, -0.19130323, -0.02285503]),\n",
       "   'explained_variance': array([ 0.18125695,  0.27043396, -0.1887145 ,  0.2964736 ]),\n",
       "   'corr': array([0.53795846, 0.59013198, 0.27340583, 0.5986342 ])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'PLSRegression_4_components',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': 0.060524702072143555,\n",
       "  'results_adj': {'mse_score': array([1.08140908, 0.78765117, 0.5694762 , 0.94630352]),\n",
       "   'mae_score': array([0.79248414, 0.79068015, 0.72178585, 0.73452485]),\n",
       "   'r2': array([-0.09295631,  0.10622033, -0.1890363 , -0.15110326]),\n",
       "   'explained_variance': array([ 0.1855152 ,  0.19584409, -0.18846787,  0.20844666]),\n",
       "   'corr': array([ 0.43286088,  0.44542341, -0.05356343,  0.45740887])},\n",
       "  'results_org': {'mse_score': array([1.08140909, 0.78765116, 0.56947622, 0.9463035 ]),\n",
       "   'mae_score': array([0.79248415, 0.79068015, 0.72178586, 0.73452484]),\n",
       "   'r2': array([-0.1503406 ,  0.11761334, -0.10915869, -0.10641023]),\n",
       "   'explained_variance': array([ 0.14275171,  0.20609467, -0.10862844,  0.23917972]),\n",
       "   'corr': array([0.3812462 , 0.45398549, 0.03688166, 0.49549602])}},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'GatedAdditiveTreeEnsembleConfig_tab',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': None,\n",
       "  'results_adj': None,\n",
       "  'results_org': None},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'DANetConfig_tab',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': None,\n",
       "  'results_adj': None,\n",
       "  'results_org': None},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'TabTransformerConfig_tab',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': None,\n",
       "  'results_adj': None,\n",
       "  'results_org': None},\n",
       " {'params': {'ordinal_imputer': 'NoImputer',\n",
       "   'continuous_imputer': 'NoImputer',\n",
       "   'model': 'TabNetModelConfig_tab',\n",
       "   'train_shape': (2881, 200),\n",
       "   'test_shape': (13, 200)},\n",
       "  'imputation_time': None,\n",
       "  'fitting_time': None,\n",
       "  'results_adj': None,\n",
       "  'results_org': None}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metric_table(\n",
    "    results_list,\n",
    "    targets,\n",
    "    metric_name,\n",
    "    source=\"Adjusted\",\n",
    "    float_format=\"%.3f\",\n",
    "    csv_filename=None,\n",
    "    sort_order=\"ascending\"  # or \"descending\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a LaTeX table for a single metric across targets, models, and imputers.\n",
    "    Optionally export the same table as CSV and sort by mean performance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_list : list of dict\n",
    "        List of experiment results.\n",
    "    targets : list of str\n",
    "        Target names (e.g., ['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN']).\n",
    "    metric_name : str\n",
    "        Metric to extract (e.g., 'mae_score').\n",
    "    source : str\n",
    "        'Adjusted' or 'Original'.\n",
    "    float_format : str\n",
    "        Format for floats (e.g., '%.3f').\n",
    "    csv_filename : str or None\n",
    "        If provided, saves the table to CSV.\n",
    "    sort_order : str\n",
    "        'ascending' or 'descending' for sorting by mean.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        LaTeX-formatted table string.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    version_key = \"results_adj\" if source.lower() == \"adjusted\" else \"results_org\"\n",
    "\n",
    "    for res in results_list:\n",
    "        result_block = res.get(version_key)\n",
    "        if result_block is None:\n",
    "            continue\n",
    "\n",
    "        metric_values = result_block.get(metric_name)\n",
    "        if metric_values is None:\n",
    "            continue\n",
    "\n",
    "        if len(metric_values) != len(targets):\n",
    "            continue\n",
    "\n",
    "        ordinal_imputer = res[\"params\"].get(\"ordinal_imputer\")\n",
    "        model = res[\"params\"].get(\"model\")\n",
    "\n",
    "        values = np.array(metric_values, dtype=np.float64)\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "\n",
    "        row = {\n",
    "            \"Ordinal Imputer\": ordinal_imputer,\n",
    "            \"Model\": model,\n",
    "            \"Mean\": mean_val,  # for sorting\n",
    "            \"Mean ± SD\": f\"{mean_val:.3f} ± {std_val:.3f}\",\n",
    "        }\n",
    "        row.update({target: val for target, val in zip(targets, values)})\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Reorder columns for display\n",
    "    display_cols = [\"Ordinal Imputer\", \"Model\"] + targets + [\"Mean ± SD\"]\n",
    "    df = df.sort_values(by=\"Mean\", ascending=(sort_order == \"ascending\"))\n",
    "    df = df[display_cols]\n",
    "\n",
    "    # Save CSV\n",
    "    if csv_filename:\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # LaTeX output\n",
    "    latex_table = df.to_latex(\n",
    "        index=False,\n",
    "        escape=False,\n",
    "        float_format=float_format,\n",
    "        caption=f\"{metric_name.replace('_', ' ').upper()} across targets\",\n",
    "        label=f\"tab:{metric_name}\",\n",
    "        longtable=False\n",
    "    )\n",
    "\n",
    "    return df, latex_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_df, latex_mae = generate_metric_table(\n",
    "    results_list=all_dict_results,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='corr',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/2_training_train_test_corr_adjusted_sorted.csv\",\n",
    "    sort_order=\"descending\"\n",
    ")\n",
    "print(latex_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_df, latex_mae = generate_metric_table(\n",
    "    results_list=all_dict_results,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='r2',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/2_training_train_test_r2_adjusted_sorted.csv\",\n",
    "    sort_order=\"descending\"\n",
    ")\n",
    "print(latex_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_df, latex_mae = generate_metric_table(\n",
    "    results_list=all_dict_results,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='mse_score',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/2_training_train_test_mse_adjusted_sorted.csv\",\n",
    "    sort_order=\"ascending\"\n",
    ")\n",
    "print(latex_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{MAE SCORE across targets}\n",
      "\\label{tab:mae_score}\n",
      "\\begin{tabular}{llrrrrl}\n",
      "\\toprule\n",
      "Ordinal Imputer & Model & ADNI_MEM & ADNI_EF & ADNI_VS & ADNI_LAN & Mean ± SD \\\\\n",
      "\\midrule\n",
      "SimpleImputer_most_frequent & MultiTaskLasso_tuned & 0.622 & 0.551 & 0.578 & 0.595 & 0.586 ± 0.026 \\\\\n",
      "SimpleImputer_most_frequent & LinearRegression & 0.640 & 0.551 & 0.565 & 0.600 & 0.589 ± 0.034 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskElasticNet_tuned & 0.630 & 0.554 & 0.580 & 0.600 & 0.591 ± 0.028 \\\\\n",
      "SimpleImputer_most_frequent & TabNetRegressor_default & 0.691 & 0.608 & 0.536 & 0.595 & 0.607 ± 0.055 \\\\\n",
      "SimpleImputer_most_frequent & RandomForestRegressor & 0.669 & 0.640 & 0.608 & 0.648 & 0.641 ± 0.022 \\\\\n",
      "NoImputer & XGBoostRegressor & 0.754 & 0.635 & 0.643 & 0.644 & 0.669 ± 0.049 \\\\\n",
      "NoImputer & XGBoostRegressor_tuned & 0.735 & 0.715 & 0.630 & 0.613 & 0.673 ± 0.052 \\\\\n",
      "NoImputer & RandomForestRegressor & 0.755 & 0.684 & 0.635 & 0.632 & 0.677 ± 0.050 \\\\\n",
      "NoImputer & MultiTaskLasso_tuned & 0.724 & 0.705 & 0.713 & 0.706 & 0.712 ± 0.008 \\\\\n",
      "NoImputer & MultiTaskElasticNet_tuned & 0.727 & 0.703 & 0.714 & 0.708 & 0.713 ± 0.009 \\\\\n",
      "NoImputer & LinearRegression & 0.728 & 0.702 & 0.710 & 0.712 & 0.713 ± 0.010 \\\\\n",
      "NoImputer & TabNetRegressor_custom & 0.713 & 0.758 & 0.671 & 0.724 & 0.717 ± 0.031 \\\\\n",
      "NoImputer & TabNetRegressor_default & 0.781 & 0.801 & 0.680 & 0.672 & 0.733 ± 0.058 \\\\\n",
      "SimpleImputer_most_frequent & PLSRegression_4_components & 0.805 & 0.738 & 0.689 & 0.764 & 0.749 ± 0.042 \\\\\n",
      "NoImputer & PLSRegression_4_components & 0.792 & 0.791 & 0.722 & 0.735 & 0.760 ± 0.032 \\\\\n",
      "NoImputer & MultiTaskElasticNet & 0.884 & 0.787 & 0.654 & 0.827 & 0.788 ± 0.085 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskElasticNet & 0.884 & 0.787 & 0.654 & 0.827 & 0.788 ± 0.085 \\\\\n",
      "SimpleImputer_most_frequent & TabNetRegressor_custom & 0.765 & 0.969 & 0.765 & 0.763 & 0.815 ± 0.089 \\\\\n",
      "NoImputer & MultiTaskLasso & 0.991 & 0.785 & 0.621 & 0.934 & 0.833 ± 0.144 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskLasso & 0.991 & 0.785 & 0.621 & 0.934 & 0.833 ± 0.144 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_df, latex_mae = generate_metric_table(\n",
    "    results_list=all_dict_results,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='mae_score',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/2_training_train_test_mae_adjusted_sorted.csv\",\n",
    "    sort_order=\"ascending\"\n",
    ")\n",
    "print(latex_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model\n",
       "MultiTaskLasso_tuned          2\n",
       "LinearRegression              2\n",
       "MultiTaskElasticNet_tuned     2\n",
       "TabNetRegressor_default       2\n",
       "RandomForestRegressor         2\n",
       "PLSRegression_4_components    2\n",
       "TabNetRegressor_custom        2\n",
       "MultiTaskElasticNet           2\n",
       "MultiTaskLasso                2\n",
       "XGBoostRegressor_tuned        1\n",
       "XGBoostRegressor              1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_df.Model.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
