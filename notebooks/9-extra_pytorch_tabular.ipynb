{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2e668c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabular import model_sweep\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9364b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from itertools import product\n",
    "import warnings\n",
    "\n",
    "# System path modification\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Lasso, LassoCV, MultiTaskLasso, MultiTaskLassoCV,\n",
    "    ElasticNet, ElasticNetCV, MultiTaskElasticNet, MultiTaskElasticNetCV\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Custom modules\n",
    "from src.train import *\n",
    "from src.functions import *\n",
    "from src.plots import *\n",
    "from src.dataset import *\n",
    "from src.multixgboost import *\n",
    "from src.wrapper import *\n",
    "\n",
    "# Visualizatiokn \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning and machine learning specific \n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Print CUDA availability for PyTorch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a7d81",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd912e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_pickle_data_palettes()\n",
    "\n",
    "results_pickle_folder = \"../pickle/\"\n",
    "\n",
    "# Unpack data\n",
    "df_X, df_y, df_all, df_FinalCombination = data[\"df_X\"], data[\"df_y\"], data[\"df_all\"], data[\"df_FinalCombination\"]\n",
    "df_select_features = data[\"df_select_features\"]\n",
    "\n",
    "# Unpack feature selections\n",
    "select_RNA, select_CSF, select_gene, select_MRIthickness = df_select_features.T.values\n",
    "\n",
    "# Unpack colormaps\n",
    "full_palette, gender_palette, dx_palette = data[\"colormaps\"].values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e013fd",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46a1beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = list(df_X.isna().any(axis=1))\n",
    "idx_test = list(~df_X.isna().any(axis=1))\n",
    "\n",
    "set_intersect_rid = set(df_all[idx_train].RID).intersection(set(df_all[idx_test].RID))\n",
    "intersect_rid_idx = df_all.RID.isin(set_intersect_rid)\n",
    "\n",
    "for i, bool_test in enumerate(idx_test): \n",
    "    if intersect_rid_idx.iloc[i] & bool_test:\n",
    "        idx_test[i] = False\n",
    "        idx_train[i] = True\n",
    "        \n",
    "df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]] = df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]].astype(\"category\")\n",
    "\n",
    "df_X_train = df_X.loc[idx_train]\n",
    "df_X_test = df_X.loc[idx_test]\n",
    "\n",
    "df_y_train = df_y.loc[idx_train]\n",
    "df_y_test = df_y.loc[idx_test]\n",
    "\n",
    "c_train = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_train]\n",
    "c_test = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7af85d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = ['APOE_epsilon2', 'APOE_epsilon3', 'APOE_epsilon4']\n",
    "continuous_features = [col for col in df_X_train.columns if col not in ordinal_features]\n",
    "\n",
    "# Imputation pipelines\n",
    "ordinal_imputer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "continuous_imputer = Pipeline([\n",
    "    ('imputer', KNNImputer())\n",
    "])\n",
    "\n",
    "# Combine with ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', ordinal_imputer, ordinal_features),\n",
    "        ('cont', continuous_imputer, continuous_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full X pipeline\n",
    "X_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    # ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "df_X_train_imputed = df_X_train.copy()\n",
    "df_X_test_imputed = df_X_test.copy()\n",
    "\n",
    "# Fit-transform X once\n",
    "df_X_train_imputed[ordinal_features+continuous_features] = X_pipeline.fit_transform(df_X_train)\n",
    "df_X_test_imputed[ordinal_features+continuous_features]  = X_pipeline.transform(df_X_test)\n",
    "\n",
    "# Demographics adjustment for y\n",
    "demographic_adjustment_y = DemographicAdjustmentTransformer()\n",
    "y_train_adjusted = demographic_adjustment_y.fit_transform(df_y_train, c_train)\n",
    "y_test_adjusted = demographic_adjustment_y.transform(df_y_test, c_test)\n",
    "\n",
    "# Demographics adjustment for X\n",
    "demographic_adjustment_X = DemographicAdjustmentTransformer(categorical_columns=ordinal_features)\n",
    "X_train_adjusted = demographic_adjustment_X.fit_transform(df_X_train_imputed, c_train)\n",
    "X_test_adjusted = demographic_adjustment_X.transform(df_X_test_imputed, c_test)\n",
    "\n",
    "# Standardize only continuous features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_adjusted[continuous_features] = scaler.fit_transform(X_train_adjusted[continuous_features])\n",
    "X_test_adjusted[continuous_features] = scaler.transform(X_test_adjusted[continuous_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd43a55",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7febe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_imputer_model(\n",
    "    df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "    c_train, c_test,\n",
    "    ordinal_model, name_ordinal_imputer, \n",
    "    continuous_model, name_continuous_imputer, \n",
    "    model, name_model, \n",
    "    imputer_model=None, name_imputer=None, \n",
    "    separate_imputers=True,\n",
    "    ordinal_features = ['APOE_epsilon2', 'APOE_epsilon3', 'APOE_epsilon4']\n",
    "): \n",
    "    # Define which columns are ordinal and which are continuous\n",
    "    continuous_features = [col for col in df_X_train.columns if col not in ordinal_features]\n",
    "\n",
    "    # Check if a general imputer model (like MissForest or MICEForest) is provided\n",
    "    if imputer_model is not None and name_imputer is not None and not separate_imputers:\n",
    "        # If `imputer_model` can handle both categorical and continuous data types\n",
    "        print(f\"Using general imputer model: {name_imputer}\")\n",
    "\n",
    "        # Ensure that ordinal columns are marked as categorical\n",
    "        df_X_train = df_X_train.copy()\n",
    "        df_X_test = df_X_test.copy()\n",
    "        \n",
    "        for col in ordinal_features:\n",
    "            df_X_train[col] = df_X_train[col].astype(\"category\")\n",
    "            df_X_test[col] = df_X_test[col].astype(\"category\")\n",
    "\n",
    "        # Create a pipeline with the general imputer\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (name_imputer, imputer_model)\n",
    "        ])\n",
    "\n",
    "        # Fit and transform the entire dataset with the general imputer\n",
    "        pipeline.fit(df_X_train)\n",
    "        X_train_imputed = pipeline.transform(df_X_train)\n",
    "        X_test_imputed = pipeline.transform(df_X_test)\n",
    "\n",
    "        # Convert transformed output back to DataFrame with original column names\n",
    "        df_X_train_imputed = pd.DataFrame(X_train_imputed, columns=df_X_train.columns)\n",
    "        df_X_test_imputed = pd.DataFrame(X_test_imputed, columns=df_X_test.columns)\n",
    "\n",
    "    else:\n",
    "        # Separate imputers for ordinal and continuous data\n",
    "        print(\"Using separate imputers for ordinal and continuous data.\")\n",
    "\n",
    "        df_X_train = df_X_train.copy()\n",
    "        df_X_test = df_X_test.copy()\n",
    "\n",
    "        # Continuous Imputation Transformer (Example: SimpleImputer)\n",
    "        continuous_imputer = Pipeline([\n",
    "            (name_continuous_imputer, continuous_model),\n",
    "        ])\n",
    "\n",
    "        # Ordinal Imputation Transformer (Example: KNN Imputer)\n",
    "        ordinal_imputer = Pipeline([\n",
    "            (name_ordinal_imputer, ordinal_model)\n",
    "        ])\n",
    "\n",
    "        # Create a ColumnTransformer to apply the appropriate imputer to each type of variable\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('ordinal', ordinal_imputer, ordinal_features),\n",
    "                ('continuous', continuous_imputer, continuous_features)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "\n",
    "        # Create the pipeline\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor)\n",
    "        ])\n",
    "\n",
    "         # Separate imputers for ordinal and continuous data\n",
    "\n",
    "        # Fit and transform, then convert back to DataFrame with original column names\n",
    "\n",
    "        if df_X_train.isna().any().any():\n",
    "                \n",
    "            start = time.time()\n",
    "            pipeline.fit(df_X_train)\n",
    "            end = time.time()\n",
    "\n",
    "            impute_model_time = end - start\n",
    "\n",
    "            X_train_imputed = pipeline.transform(df_X_train)\n",
    "            df_X_train_imputed = df_X_train.copy()\n",
    "            df_X_train_imputed[ordinal_features+continuous_features] = X_train_imputed\n",
    "        else :\n",
    "            print(\"No NaN in train data -> Keep as it is. \")\n",
    "            df_X_train_imputed = df_X_train\n",
    "            \n",
    "            impute_model_time = None\n",
    "\n",
    "        # Transform the test set\n",
    "        if df_X_test.isna().any().any(): \n",
    "            X_test_imputed = pipeline.transform(df_X_test)\n",
    "            df_X_test_imputed = df_X_test.copy()\n",
    "            df_X_test_imputed[ordinal_features+continuous_features] = X_test_imputed\n",
    "        else : \n",
    "            print(\"No NaN in test data -> Keep as it is. \")\n",
    "            df_X_test_imputed = df_X_test\n",
    "\n",
    "    # Demographics adjustment for y\n",
    "    demographic_adjustment_y = DemographicAdjustmentTransformer()\n",
    "    y_train_adjusted = demographic_adjustment_y.fit_transform(df_y_train, c_train)\n",
    "    y_test_adjusted = demographic_adjustment_y.transform(df_y_test, c_test)\n",
    "\n",
    "    # Demographics adjustment for X\n",
    "    demographic_adjustment_X = DemographicAdjustmentTransformer(categorical_columns=ordinal_features)\n",
    "    X_train_adjusted = demographic_adjustment_X.fit_transform(df_X_train_imputed, c_train)\n",
    "    X_test_adjusted = demographic_adjustment_X.transform(df_X_test_imputed, c_test)\n",
    "\n",
    "    print(X_train_adjusted)\n",
    "\n",
    "    # Standardize only continuous features\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train_adjusted[continuous_features] = scaler.fit_transform(X_train_adjusted[continuous_features])\n",
    "    X_test_adjusted[continuous_features] = scaler.transform(X_test_adjusted[continuous_features])\n",
    "\n",
    "    # Perform prediction and save variables\n",
    "    start = time.time()\n",
    "\n",
    "    if isinstance(model, TabNetRegressor): \n",
    "        X_train_adjusted = X_train_adjusted.values\n",
    "        y_train_adjusted = y_train_adjusted.values\n",
    "\n",
    "        X_test_adjusted = X_test_adjusted.values\n",
    "        #y_test_adjusted = y_test_adjusted.values\n",
    "    \n",
    "    model.fit(X_train_adjusted, y_train_adjusted) \n",
    "    end = time.time()\n",
    "\n",
    "    predict_model_time = end - start\n",
    "\n",
    "    y_pred_adjusted = model.predict(X_test_adjusted)\n",
    "\n",
    "    y_pred_adjusted = pd.DataFrame(y_pred_adjusted, columns=y_test_adjusted.columns)\n",
    "\n",
    "    # Metrics computed in original space\n",
    "    y_pred = demographic_adjustment_y.inverse_transform(y_pred_adjusted, c_test)\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : X_train_adjusted.shape, \n",
    "        \"test_shape\": X_test_adjusted.shape\n",
    "    }\n",
    "    \n",
    "    if df_X_test.shape[0] != 1: \n",
    "\n",
    "        # Metrics computed in adjusted space\n",
    "        mse_score_adj, mae_score_ajd, r2_adj, explained_variance_adj, corr_adj = compute_all_metrics(y_test_adjusted.values, y_pred_adjusted)\n",
    "\n",
    "        results_adj = {\n",
    "            \"mse_score\": mse_score_adj, \n",
    "            \"mae_score\":mae_score_ajd, \n",
    "            \"r2\":r2_adj, \n",
    "            \"explained_variance\":explained_variance_adj, \n",
    "            \"corr\":corr_adj, \n",
    "        }\n",
    "\n",
    "        mse_score, mae_score, r2, explained_variance, corr = compute_all_metrics(df_y_test.values, y_pred)\n",
    "\n",
    "        results_org = {\n",
    "            \"mse_score\": mse_score, \n",
    "            \"mae_score\": mae_score, \n",
    "            \"r2\": r2, \n",
    "            \"explained_variance\": explained_variance, \n",
    "            \"corr\": corr, \n",
    "        }\n",
    "\n",
    "    else : \n",
    "        print(\"Saving predictions in dict!\")\n",
    "        results_adj = {\n",
    "            \"y_pred\": y_pred_adjusted.values, \n",
    "            \"y_test\": y_test_adjusted.values,\n",
    "        }\n",
    "\n",
    "        results_org = {\n",
    "            \"y_pred\": y_pred.values, \n",
    "            \"y_test\": df_y_test.values,\n",
    "        }\n",
    "\n",
    "\n",
    "    dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": impute_model_time,\n",
    "        \"fitting_time\": predict_model_time, \n",
    "        \"results_adj\": results_adj, \n",
    "        \"results_org\": results_org\n",
    "        }\n",
    "\n",
    "    return dict_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec542872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_imputer_model(\n",
    "    df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "    c_train, c_test,\n",
    "    ordinal_model, name_ordinal_imputer, \n",
    "    continuous_model, name_continuous_imputer, \n",
    "    model, name_model, \n",
    "    imputer_model=None, name_imputer=None, \n",
    "    separate_imputers=True,\n",
    "    ordinal_features = ['APOE_epsilon2', 'APOE_epsilon3', 'APOE_epsilon4']\n",
    "): \n",
    "    # Define which columns are ordinal and which are continuous\n",
    "    continuous_features = [col for col in df_X_train.columns if col not in ordinal_features]\n",
    "\n",
    "    # Check if a general imputer model (like MissForest or MICEForest) is provided\n",
    "    if imputer_model is not None and name_imputer is not None and not separate_imputers:\n",
    "        # If `imputer_model` can handle both categorical and continuous data types\n",
    "        print(f\"Using general imputer model: {name_imputer}\")\n",
    "\n",
    "        # Ensure that ordinal columns are marked as categorical\n",
    "        df_X_train = df_X_train.copy()\n",
    "        df_X_test = df_X_test.copy()\n",
    "        \n",
    "        for col in ordinal_features:\n",
    "            df_X_train[col] = df_X_train[col].astype(\"category\")\n",
    "            df_X_test[col] = df_X_test[col].astype(\"category\")\n",
    "\n",
    "        # Create a pipeline with the general imputer\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (name_imputer, imputer_model)\n",
    "        ])\n",
    "\n",
    "        # Fit and transform the entire dataset with the general imputer\n",
    "        pipeline.fit(df_X_train)\n",
    "        X_train_imputed = pipeline.transform(df_X_train)\n",
    "        X_test_imputed = pipeline.transform(df_X_test)\n",
    "\n",
    "        # Convert transformed output back to DataFrame with original column names\n",
    "        df_X_train_imputed = pd.DataFrame(X_train_imputed, columns=df_X_train.columns)\n",
    "        df_X_test_imputed = pd.DataFrame(X_test_imputed, columns=df_X_test.columns)\n",
    "\n",
    "    else:\n",
    "        # Separate imputers for ordinal and continuous data\n",
    "        print(\"Using separate imputers for ordinal and continuous data.\")\n",
    "\n",
    "        df_X_train = df_X_train.copy()\n",
    "        df_X_test = df_X_test.copy()\n",
    "\n",
    "        # Continuous Imputation Transformer (Example: SimpleImputer)\n",
    "        continuous_imputer = Pipeline([\n",
    "            (name_continuous_imputer, continuous_model),\n",
    "        ])\n",
    "\n",
    "        # Ordinal Imputation Transformer (Example: KNN Imputer)\n",
    "        ordinal_imputer = Pipeline([\n",
    "            (name_ordinal_imputer, ordinal_model)\n",
    "        ])\n",
    "\n",
    "        # Create a ColumnTransformer to apply the appropriate imputer to each type of variable\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('ordinal', ordinal_imputer, ordinal_features),\n",
    "                ('continuous', continuous_imputer, continuous_features)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "\n",
    "        # Create the pipeline\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor)\n",
    "        ])\n",
    "\n",
    "         # Separate imputers for ordinal and continuous data\n",
    "        # Fit and transform, then convert back to DataFrame with original column names\n",
    "\n",
    "        if df_X_train.isna().any().any():\n",
    "                \n",
    "            start = time.time()\n",
    "            pipeline.fit(df_X_train)\n",
    "            end = time.time()\n",
    "\n",
    "            impute_model_time = end - start\n",
    "\n",
    "            X_train_imputed = pipeline.transform(df_X_train)\n",
    "            df_X_train_imputed = df_X_train.copy()\n",
    "            df_X_train_imputed[ordinal_features+continuous_features] = X_train_imputed\n",
    "        else :\n",
    "            print(\"No NaN in train data -> Keep as it is. \")\n",
    "            df_X_train_imputed = df_X_train\n",
    "            \n",
    "            impute_model_time = None\n",
    "\n",
    "        # Transform the test set\n",
    "        if df_X_test.isna().any().any(): \n",
    "            X_test_imputed = pipeline.transform(df_X_test)\n",
    "            df_X_test_imputed = df_X_test.copy()\n",
    "            df_X_test_imputed[ordinal_features+continuous_features] = X_test_imputed\n",
    "        else : \n",
    "            print(\"No NaN in test data -> Keep as it is. \")\n",
    "            df_X_test_imputed = df_X_test\n",
    "\n",
    "    # Demographics adjustment for y\n",
    "    demographic_adjustment_y = DemographicAdjustmentTransformer()\n",
    "    y_train_adjusted = demographic_adjustment_y.fit_transform(df_y_train, c_train)\n",
    "    y_test_adjusted = demographic_adjustment_y.transform(df_y_test, c_test)\n",
    "\n",
    "    # Demographics adjustment for X\n",
    "    demographic_adjustment_X = DemographicAdjustmentTransformer(categorical_columns=ordinal_features)\n",
    "    X_train_adjusted = demographic_adjustment_X.fit_transform(df_X_train_imputed, c_train)\n",
    "    X_test_adjusted = demographic_adjustment_X.transform(df_X_test_imputed, c_test)\n",
    "\n",
    "    print(X_train_adjusted)\n",
    "\n",
    "    # Standardize only continuous features\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train_adjusted[continuous_features] = scaler.fit_transform(X_train_adjusted[continuous_features])\n",
    "    X_test_adjusted[continuous_features] = scaler.transform(X_test_adjusted[continuous_features])\n",
    "\n",
    "    # Perform prediction and save variables\n",
    "    start = time.time()\n",
    "\n",
    "    if isinstance(model, TabNetRegressor): \n",
    "        X_train_adjusted = X_train_adjusted.values\n",
    "        y_train_adjusted = y_train_adjusted.values\n",
    "\n",
    "        X_test_adjusted = X_test_adjusted.values\n",
    "        #y_test_adjusted = y_test_adjusted.values\n",
    "    \n",
    "    model.fit(X_train_adjusted, y_train_adjusted) \n",
    "    end = time.time()\n",
    "\n",
    "    predict_model_time = end - start\n",
    "\n",
    "    y_pred_adjusted = model.predict(X_test_adjusted)\n",
    "\n",
    "    y_pred_adjusted = pd.DataFrame(y_pred_adjusted, columns=y_test_adjusted.columns)\n",
    "\n",
    "    # Metrics computed in original space\n",
    "    y_pred = demographic_adjustment_y.inverse_transform(y_pred_adjusted, c_test)\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : X_train_adjusted.shape, \n",
    "        \"test_shape\": X_test_adjusted.shape\n",
    "    }\n",
    "    \n",
    "    if df_X_test.shape[0] != 1: \n",
    "\n",
    "        # Metrics computed in adjusted space\n",
    "        mse_score_adj, mae_score_ajd, r2_adj, explained_variance_adj, corr_adj = compute_all_metrics(y_test_adjusted.values, y_pred_adjusted)\n",
    "\n",
    "        results_adj = {\n",
    "            \"mse_score\": mse_score_adj, \n",
    "            \"mae_score\":mae_score_ajd, \n",
    "            \"r2\":r2_adj, \n",
    "            \"explained_variance\":explained_variance_adj, \n",
    "            \"corr\":corr_adj, \n",
    "        }\n",
    "\n",
    "        mse_score, mae_score, r2, explained_variance, corr = compute_all_metrics(df_y_test.values, y_pred)\n",
    "\n",
    "        results_org = {\n",
    "            \"mse_score\": mse_score, \n",
    "            \"mae_score\": mae_score, \n",
    "            \"r2\": r2, \n",
    "            \"explained_variance\": explained_variance, \n",
    "            \"corr\": corr, \n",
    "        }\n",
    "\n",
    "    else : \n",
    "        print(\"Saving predictions in dict!\")\n",
    "        results_adj = {\n",
    "            \"y_pred\": y_pred_adjusted.values, \n",
    "            \"y_test\": y_test_adjusted.values,\n",
    "        }\n",
    "\n",
    "        results_org = {\n",
    "            \"y_pred\": y_pred.values, \n",
    "            \"y_test\": df_y_test.values,\n",
    "        }\n",
    "\n",
    "\n",
    "    dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": impute_model_time,\n",
    "        \"fitting_time\": predict_model_time, \n",
    "        \"results_adj\": results_adj, \n",
    "        \"results_org\": results_org\n",
    "        }\n",
    "\n",
    "    return dict_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3cec90",
   "metadata": {},
   "source": [
    "# Hyperparameter search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce4872",
   "metadata": {},
   "source": [
    "## Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2da3509c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ElasticNet Parameters: {'alpha': 0.01, 'l1_ratio': 0.01}\n"
     ]
    }
   ],
   "source": [
    "enet = MultiTaskElasticNet(max_iter=10000)\n",
    "\n",
    "param_grid_enet = {\n",
    "    'alpha': [1e-4, 1e-3, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    'l1_ratio': [0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "grid_search_enet = GridSearchCV(\n",
    "    estimator=enet,\n",
    "    param_grid=param_grid_enet,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_enet.fit(X_train_preprocessed, y_train_adjusted)\n",
    "best_enet = grid_search_enet.best_estimator_\n",
    "\n",
    "print(\"Best ElasticNet Parameters:\", grid_search_enet.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b53a2997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lasso Parameters: {'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "lasso = MultiTaskLasso(max_iter=10000)\n",
    "\n",
    "param_grid_lasso = {\n",
    "    'alpha': [1e-4, 1e-3, 1e-2, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0]\n",
    "}\n",
    "\n",
    "grid_search_lasso = GridSearchCV(\n",
    "    estimator=lasso,\n",
    "    param_grid=param_grid_lasso,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_lasso.fit(X_train_preprocessed, y_train_adjusted)\n",
    "best_lasso = grid_search_lasso.best_estimator_\n",
    "\n",
    "print(\"Best Lasso Parameters:\", grid_search_lasso.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f821f4f",
   "metadata": {},
   "source": [
    "## Partial Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd7255fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best PLS Parameters: {'n_components': 4}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the estimator\n",
    "pls = PLSRegression()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_pls = {\n",
    "    'n_components': list(range(1, min(X_train_preprocessed.shape[1], y_train_adjusted.shape[1]) + 1))\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search_pls = GridSearchCV(\n",
    "    estimator=pls,\n",
    "    param_grid=param_grid_pls,\n",
    "    cv=5,  # Use cross-validation\n",
    "    scoring='r2',  # Can be changed to 'neg_mean_squared_error' or others\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search_pls.fit(X_train_preprocessed, y_train_adjusted)\n",
    "\n",
    "# Extract the best model\n",
    "best_pls = grid_search_pls.best_estimator_\n",
    "print(\"Best PLS Parameters:\", grid_search_pls.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c66b9",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb5a41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import xgboost as xgb\n",
    "\n",
    "class XGBoostRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,\n",
    "                 num_boost_round=100,\n",
    "                 max_depth=3,\n",
    "                 lambda_=1.0,\n",
    "                 learning_rate=0.1,\n",
    "                 subsample=1.0,\n",
    "                 colsample_bytree=1.0,\n",
    "                 alpha=0.0,\n",
    "                 tree_method=\"hist\",\n",
    "                 custom_obj=True,\n",
    "                 custom_metric=True):\n",
    "        \n",
    "        # Expose all as attributes\n",
    "        self.num_boost_round = num_boost_round\n",
    "        self.max_depth = max_depth\n",
    "        self.lambda_ = lambda_\n",
    "        self.learning_rate = learning_rate\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.alpha = alpha\n",
    "        self.tree_method = tree_method\n",
    "        self.custom_obj = custom_obj\n",
    "        self.custom_metric = custom_metric\n",
    "        \n",
    "        self.model = None\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        # Return all parameters\n",
    "        return {\n",
    "            'num_boost_round': self.num_boost_round,\n",
    "            'max_depth': self.max_depth,\n",
    "            'lambda_': self.lambda_,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree,\n",
    "            'alpha': self.alpha,\n",
    "            'tree_method': self.tree_method,\n",
    "            'custom_obj': self.custom_obj,\n",
    "            'custom_metric': self.custom_metric\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, val in params.items():\n",
    "            setattr(self, key, val)\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        dtrain = xgb.DMatrix(data=X, label=y)\n",
    "        params = {\n",
    "            'max_depth': self.max_depth,\n",
    "            'lambda': self.lambda_,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree,\n",
    "            'alpha': self.alpha,\n",
    "            'tree_method': self.tree_method,\n",
    "            'num_target': y.shape[1]\n",
    "        }\n",
    "        obj_fn = squared_log if self.custom_obj else 'reg:squarederror'\n",
    "        metric_fn = rmse if self.custom_metric else None\n",
    "\n",
    "        self.model = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=self.num_boost_round,\n",
    "            obj=obj_fn,\n",
    "            custom_metric=metric_fn\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        dtest = xgb.DMatrix(data=X)\n",
    "        return self.model.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a4eefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Params: {'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105}\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'learning_rate': uniform(0.01, 0.29),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5)\n",
    "}\n",
    "\n",
    "rand_search_xgb = RandomizedSearchCV(\n",
    "    estimator=XGBoostRegressor(tree_method=\"hist\", custom_obj=True, custom_metric=True),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rand_search_xgb.fit(X_train_preprocessed, y_train_adjusted)\n",
    "print(\"Best XGBoost Params:\", rand_search_xgb.best_params_)\n",
    "best_xgb = rand_search_xgb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "419856a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Params: {'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best XGBoost Params:\", rand_search_xgb.best_params_)\n",
    "best_xgb = rand_search_xgb.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261df8d1",
   "metadata": {},
   "source": [
    "# Pytorch Tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e6d8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataConfig.__init__() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m targets\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADNI_MEM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADNI_EF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADNI_VS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADNI_LAN\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m data_config \u001b[38;5;241m=\u001b[39m \u001b[43mDataConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuous_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinuous_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mordinal_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m trainer_config \u001b[38;5;241m=\u001b[39m TrainerConfig(\n\u001b[1;32m     11\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     12\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m                    \u001b[38;5;66;03m# Change to \"gpu\" if available\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m optimizer_config \u001b[38;5;241m=\u001b[39m OptimizerConfig()\n",
      "\u001b[0;31mTypeError\u001b[0m: DataConfig.__init__() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "targets=[\"ADNI_MEM\", \"ADNI_EF\", \"ADNI_VS\", \"ADNI_LAN\"]\n",
    "\n",
    "data_config = DataConfig(\n",
    "    target=targets,\n",
    "    continuous_cols=continuous_features,\n",
    "    categorical_cols=ordinal_features\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    auto_lr_find=True,\n",
    "    early_stopping=\"valid_loss\",          # Monitor validation loss\n",
    "    early_stopping_mode=\"min\",           # Stop when it stops decreasing\n",
    "    early_stopping_patience=5,           # Wait up to 5 epochs with no improvement\n",
    "    early_stopping_min_delta=0.001,      # Minimum change to qualify as an improvement\n",
    "    checkpoints=\"valid_loss\",            # Save best model on val_loss\n",
    "    checkpoints_mode=\"min\",\n",
    "    checkpoints_save_top_k=1,\n",
    "    load_best=True,                      # Load best model post-training\n",
    "    progress_bar=\"simple\",               # Basic progress bar (or \"none\")\n",
    "    trainer_kwargs=dict(enable_model_summary=False),\n",
    "    accelerator=\"cpu\"                    # Change to \"gpu\" if available\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9114b072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lite', 'standard', 'full', 'high_memory']\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "from pytorch_tabular import MODEL_SWEEP_PRESETS\n",
    "\n",
    "print(list(MODEL_SWEEP_PRESETS.keys()))\n",
    "\n",
    "head_config = LinearHeadConfig(\n",
    "    layers=\"\",\n",
    "    dropout=0.1,\n",
    "    initialization=(  # No additional layer in head, just a mapping layer to output_dim\n",
    "        \"kaiming\"\n",
    "    ),\n",
    ").__dict__  # Convert to dict to pass to the model config (OmegaConf doesn't accept objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46c6bf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CategoryEmbeddingModelConfig', {'layers': '256-128-64'}), ('CategoryEmbeddingModelConfig', {'layers': '512-128-64'}), ('GANDALFConfig', {'gflu_stages': 6}), ('GANDALFConfig', {'gflu_stages': 15}), ('TabNetModelConfig', {'n_d': 32, 'n_a': 32, 'n_steps': 3, 'gamma': 1.5, 'n_independent': 1, 'n_shared': 2}), ('TabNetModelConfig', {'n_d': 32, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5, 'n_independent': 2, 'n_shared': 3}), ('FTTransformerConfig', {'num_heads': 4, 'num_attn_blocks': 4})]\n"
     ]
    }
   ],
   "source": [
    "print(list(MODEL_SWEEP_PRESETS[\"standard\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35c2fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xy_preprocessed_adjusted = pd.DataFrame(np.concatenate([X_train_adjusted, y_train_adjusted], axis=1), columns = ordinal_features+continuous_features+targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c5a4afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (2304, 280) | Test Shape: (577, 280)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df_Xy_preprocessed_adjusted, random_state=42, test_size=0.2)\n",
    "print(f\"Train Shape: {train.shape} | Test Shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36666e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2877250e76464e148e80c8946b7e903f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80366a68ca0446c19f2dd5387b5d5661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0c2fb0e5b84c13837a6c3b47dab860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8813805245f0492891fb210319f82b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93bd4c5e706437980a34b88a2082ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">03:06:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">697</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gandalf.gandal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">f:109</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m10\u001b[0m \u001b[1;92m03:06:36\u001b[0m,\u001b[1;36m697\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gandalf.gandal\u001b[1;92mf:109\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651ef0e62343443599943786ace76288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">03:06:45</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">701</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gandalf.gandal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">f:109</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m10\u001b[0m \u001b[1;92m03:06:45\u001b[0m,\u001b[1;36m701\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gandalf.gandal\u001b[1;92mf:109\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">03:16:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">228</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m10\u001b[0m \u001b[1;92m03:16:10\u001b[0m,\u001b[1;36m228\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f1b23e50ca403893a12357dcc319bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">03:16:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">171</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m10\u001b[0m \u001b[1;92m03:16:56\u001b[0m,\u001b[1;36m171\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d43772e0f443d19f6afe8649b740a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b288bab46e194068ad8a31cc2f3eaaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = [\n",
    "    #\"r2_score\",\n",
    "    \"mean_absolute_error\",\n",
    "    \"explained_variance\",\n",
    "]\n",
    "\n",
    "# Provide an empty dict for each metric (no extra params needed):\n",
    "metrics_params = [{}, {}]\n",
    "\n",
    "# For regression, all metrics are computed on predictions themselves:\n",
    "metrics_prob_input = [False] * len(metrics)\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "torch.serialization.safe_globals([DictConfig])\n",
    "\n",
    "# Filtering out the warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    sweep_df, best_model = model_sweep(\n",
    "        task=\"regression\",  # One of \"classification\", \"regression\"\n",
    "        train=train,\n",
    "        test=test,\n",
    "        data_config=data_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "        model_list=\"full\",\n",
    "        common_model_args=dict(head=\"LinearHead\", head_config=head_config),\n",
    "        metrics = metrics,\n",
    "        metrics_params=metrics_params,\n",
    "        metrics_prob_input=metrics_prob_input,\n",
    "        rank_metric=(\"mean_absolute_error\", \"higher_is_better\"),\n",
    "        progress_bar=True,\n",
    "        verbose=False,\n",
    "        suppress_lightning_logger=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d00b7703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th># Params</th>\n",
       "      <th>epochs</th>\n",
       "      <th>test_loss_0</th>\n",
       "      <th>test_loss_1</th>\n",
       "      <th>test_loss_2</th>\n",
       "      <th>test_loss_3</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_mean_absolute_error_0</th>\n",
       "      <th>test_mean_absolute_error_1</th>\n",
       "      <th>...</th>\n",
       "      <th>test_mean_absolute_error_3</th>\n",
       "      <th>test_mean_absolute_error</th>\n",
       "      <th>test_explained_variance_0</th>\n",
       "      <th>test_explained_variance_1</th>\n",
       "      <th>test_explained_variance_2</th>\n",
       "      <th>test_explained_variance_3</th>\n",
       "      <th>test_explained_variance</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>time_taken_per_epoch</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TabNetModel</td>\n",
       "      <td>227 T</td>\n",
       "      <td>5</td>\n",
       "      <td>0.897485</td>\n",
       "      <td>1.073166</td>\n",
       "      <td>0.678909</td>\n",
       "      <td>0.881682</td>\n",
       "      <td>3.531243</td>\n",
       "      <td>0.782892</td>\n",
       "      <td>0.807683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743692</td>\n",
       "      <td>2.992384</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.006342</td>\n",
       "      <td>33.088309</td>\n",
       "      <td>6.617662</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AutoIntModel</td>\n",
       "      <td>121 T</td>\n",
       "      <td>85</td>\n",
       "      <td>0.567992</td>\n",
       "      <td>0.729492</td>\n",
       "      <td>0.633248</td>\n",
       "      <td>0.626819</td>\n",
       "      <td>2.557550</td>\n",
       "      <td>0.607697</td>\n",
       "      <td>0.660484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629624</td>\n",
       "      <td>2.534326</td>\n",
       "      <td>0.367111</td>\n",
       "      <td>0.324409</td>\n",
       "      <td>0.028654</td>\n",
       "      <td>0.289390</td>\n",
       "      <td>1.009563</td>\n",
       "      <td>700.243122</td>\n",
       "      <td>8.238154</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GANDALFModel</td>\n",
       "      <td>6 M</td>\n",
       "      <td>100</td>\n",
       "      <td>0.522728</td>\n",
       "      <td>0.706339</td>\n",
       "      <td>0.603901</td>\n",
       "      <td>0.597320</td>\n",
       "      <td>2.430287</td>\n",
       "      <td>0.577457</td>\n",
       "      <td>0.658918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610478</td>\n",
       "      <td>2.468199</td>\n",
       "      <td>0.425688</td>\n",
       "      <td>0.343637</td>\n",
       "      <td>0.103546</td>\n",
       "      <td>0.327892</td>\n",
       "      <td>1.200763</td>\n",
       "      <td>573.442258</td>\n",
       "      <td>5.734423</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TabTransformerModel</td>\n",
       "      <td>400 T</td>\n",
       "      <td>13</td>\n",
       "      <td>0.380481</td>\n",
       "      <td>0.766874</td>\n",
       "      <td>0.606936</td>\n",
       "      <td>0.507441</td>\n",
       "      <td>2.261732</td>\n",
       "      <td>0.477530</td>\n",
       "      <td>0.670824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556025</td>\n",
       "      <td>2.317068</td>\n",
       "      <td>0.584447</td>\n",
       "      <td>0.366383</td>\n",
       "      <td>0.095381</td>\n",
       "      <td>0.399458</td>\n",
       "      <td>1.445669</td>\n",
       "      <td>62.998656</td>\n",
       "      <td>4.846050</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FTTransformerModel</td>\n",
       "      <td>416 T</td>\n",
       "      <td>20</td>\n",
       "      <td>0.427512</td>\n",
       "      <td>0.608372</td>\n",
       "      <td>0.574697</td>\n",
       "      <td>0.530267</td>\n",
       "      <td>2.140848</td>\n",
       "      <td>0.507553</td>\n",
       "      <td>0.613245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573561</td>\n",
       "      <td>2.309888</td>\n",
       "      <td>0.525614</td>\n",
       "      <td>0.432071</td>\n",
       "      <td>0.133033</td>\n",
       "      <td>0.392044</td>\n",
       "      <td>1.482762</td>\n",
       "      <td>4357.219138</td>\n",
       "      <td>217.860957</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CategoryEmbeddingModel</td>\n",
       "      <td>263 T</td>\n",
       "      <td>9</td>\n",
       "      <td>0.355338</td>\n",
       "      <td>0.612130</td>\n",
       "      <td>0.602721</td>\n",
       "      <td>0.433786</td>\n",
       "      <td>2.003975</td>\n",
       "      <td>0.463190</td>\n",
       "      <td>0.613986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515381</td>\n",
       "      <td>2.206153</td>\n",
       "      <td>0.601280</td>\n",
       "      <td>0.441098</td>\n",
       "      <td>0.099351</td>\n",
       "      <td>0.485393</td>\n",
       "      <td>1.627123</td>\n",
       "      <td>21.759632</td>\n",
       "      <td>2.417737</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DANetModel</td>\n",
       "      <td>2 M</td>\n",
       "      <td>16</td>\n",
       "      <td>0.354974</td>\n",
       "      <td>0.540967</td>\n",
       "      <td>0.669353</td>\n",
       "      <td>0.441169</td>\n",
       "      <td>2.006463</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517689</td>\n",
       "      <td>2.191942</td>\n",
       "      <td>0.595836</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>-0.013647</td>\n",
       "      <td>0.489327</td>\n",
       "      <td>1.559807</td>\n",
       "      <td>151.719178</td>\n",
       "      <td>9.482449</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GatedAdditiveTreeEnsembleModel</td>\n",
       "      <td>7 M</td>\n",
       "      <td>11</td>\n",
       "      <td>0.323990</td>\n",
       "      <td>0.542354</td>\n",
       "      <td>0.560329</td>\n",
       "      <td>0.408959</td>\n",
       "      <td>1.835631</td>\n",
       "      <td>0.437651</td>\n",
       "      <td>0.571564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492840</td>\n",
       "      <td>2.101103</td>\n",
       "      <td>0.625733</td>\n",
       "      <td>0.485292</td>\n",
       "      <td>0.153444</td>\n",
       "      <td>0.506565</td>\n",
       "      <td>1.771033</td>\n",
       "      <td>346.253147</td>\n",
       "      <td>31.477559</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model # Params  epochs  test_loss_0  test_loss_1  \\\n",
       "6                     TabNetModel    227 T       5     0.897485     1.073166   \n",
       "0                    AutoIntModel    121 T      85     0.567992     0.729492   \n",
       "4                    GANDALFModel      6 M     100     0.522728     0.706339   \n",
       "7             TabTransformerModel    400 T      13     0.380481     0.766874   \n",
       "3              FTTransformerModel    416 T      20     0.427512     0.608372   \n",
       "1          CategoryEmbeddingModel    263 T       9     0.355338     0.612130   \n",
       "2                      DANetModel      2 M      16     0.354974     0.540967   \n",
       "5  GatedAdditiveTreeEnsembleModel      7 M      11     0.323990     0.542354   \n",
       "\n",
       "   test_loss_2  test_loss_3  test_loss  test_mean_absolute_error_0  \\\n",
       "6     0.678909     0.881682   3.531243                    0.782892   \n",
       "0     0.633248     0.626819   2.557550                    0.607697   \n",
       "4     0.603901     0.597320   2.430287                    0.577457   \n",
       "7     0.606936     0.507441   2.261732                    0.477530   \n",
       "3     0.574697     0.530267   2.140848                    0.507553   \n",
       "1     0.602721     0.433786   2.003975                    0.463190   \n",
       "2     0.669353     0.441169   2.006463                    0.457900   \n",
       "5     0.560329     0.408959   1.835631                    0.437651   \n",
       "\n",
       "   test_mean_absolute_error_1  ...  test_mean_absolute_error_3  \\\n",
       "6                    0.807683  ...                    0.743692   \n",
       "0                    0.660484  ...                    0.629624   \n",
       "4                    0.658918  ...                    0.610478   \n",
       "7                    0.670824  ...                    0.556025   \n",
       "3                    0.613245  ...                    0.573561   \n",
       "1                    0.613986  ...                    0.515381   \n",
       "2                    0.564277  ...                    0.517689   \n",
       "5                    0.571564  ...                    0.492840   \n",
       "\n",
       "   test_mean_absolute_error  test_explained_variance_0  \\\n",
       "6                  2.992384                   0.002445   \n",
       "0                  2.534326                   0.367111   \n",
       "4                  2.468199                   0.425688   \n",
       "7                  2.317068                   0.584447   \n",
       "3                  2.309888                   0.525614   \n",
       "1                  2.206153                   0.601280   \n",
       "2                  2.191942                   0.595836   \n",
       "5                  2.101103                   0.625733   \n",
       "\n",
       "   test_explained_variance_1  test_explained_variance_2  \\\n",
       "6                   0.001495                   0.000981   \n",
       "0                   0.324409                   0.028654   \n",
       "4                   0.343637                   0.103546   \n",
       "7                   0.366383                   0.095381   \n",
       "3                   0.432071                   0.133033   \n",
       "1                   0.441098                   0.099351   \n",
       "2                   0.488291                  -0.013647   \n",
       "5                   0.485292                   0.153444   \n",
       "\n",
       "   test_explained_variance_3  test_explained_variance   time_taken  \\\n",
       "6                   0.001422                 0.006342    33.088309   \n",
       "0                   0.289390                 1.009563   700.243122   \n",
       "4                   0.327892                 1.200763   573.442258   \n",
       "7                   0.399458                 1.445669    62.998656   \n",
       "3                   0.392044                 1.482762  4357.219138   \n",
       "1                   0.485393                 1.627123    21.759632   \n",
       "2                   0.489327                 1.559807   151.719178   \n",
       "5                   0.506565                 1.771033   346.253147   \n",
       "\n",
       "   time_taken_per_epoch                                             params  \n",
       "6              6.617662  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "0              8.238154  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "4              5.734423  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "7              4.846050  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "3            217.860957  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "1              2.417737  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "2              9.482449  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "5             31.477559  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sweep_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
