{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a5c114",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e668c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabular import model_sweep\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9364b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from itertools import product\n",
    "import warnings\n",
    "\n",
    "# System path modification\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Lasso, LassoCV, MultiTaskLasso, MultiTaskLassoCV,\n",
    "    ElasticNet, ElasticNetCV, MultiTaskElasticNet, MultiTaskElasticNetCV\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom modules\n",
    "from src.train import *\n",
    "from src.functions import *\n",
    "from src.plots import *\n",
    "from src.dataset import *\n",
    "from src.multixgboost import *\n",
    "from src.wrapper import *\n",
    "\n",
    "# Visualizatiokn \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning and machine learning specific \n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Print CUDA availability for PyTorch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a7d81",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd912e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_pickle_data_palettes()\n",
    "\n",
    "results_pickle_folder = \"../pickle/\"\n",
    "\n",
    "# Unpack data\n",
    "df_X, df_y, df_all, df_FinalCombination = data[\"df_X\"], data[\"df_y\"], data[\"df_all\"], data[\"df_FinalCombination\"]\n",
    "dict_select = data[\"dict_select\"]\n",
    "\n",
    "# Unpack colormaps\n",
    "full_palette, gender_palette, dx_palette = data[\"colormaps\"].values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e013fd",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46a1beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = list(df_X.isna().any(axis=1))\n",
    "idx_test = list(~df_X.isna().any(axis=1))\n",
    "\n",
    "set_intersect_rid = set(df_all[idx_train].RID).intersection(set(df_all[idx_test].RID))\n",
    "intersect_rid_idx = df_all.RID.isin(set_intersect_rid)\n",
    "\n",
    "for i, bool_test in enumerate(idx_test): \n",
    "    if intersect_rid_idx.iloc[i] & bool_test:\n",
    "        idx_test[i] = False\n",
    "        idx_train[i] = True\n",
    "        \n",
    "df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]] = df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]].astype(\"category\")\n",
    "\n",
    "df_X_train = df_X.loc[idx_train]\n",
    "df_X_test = df_X.loc[idx_test]\n",
    "\n",
    "df_y_train = df_y.loc[idx_train]\n",
    "df_y_test = df_y.loc[idx_test]\n",
    "\n",
    "c_train = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_train]\n",
    "c_test = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7af85d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = ['APOE_epsilon2', 'APOE_epsilon3', 'APOE_epsilon4']\n",
    "continuous_features = [col for col in df_X_train.columns if col not in ordinal_features]\n",
    "\n",
    "# Imputation pipelines\n",
    "ordinal_imputer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "continuous_imputer = Pipeline([\n",
    "    ('imputer', KNNImputer())\n",
    "])\n",
    "\n",
    "# Combine with ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', ordinal_imputer, ordinal_features),\n",
    "        ('cont', continuous_imputer, continuous_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full X pipeline\n",
    "X_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    # ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "df_X_train_imputed = df_X_train.copy()\n",
    "df_X_test_imputed = df_X_test.copy()\n",
    "\n",
    "# Fit-transform X once\n",
    "df_X_train_imputed[ordinal_features+continuous_features] = X_pipeline.fit_transform(df_X_train)\n",
    "df_X_test_imputed[ordinal_features+continuous_features]  = X_pipeline.transform(df_X_test)\n",
    "\n",
    "# Demographics adjustment for y\n",
    "demographic_adjustment_y = DemographicAdjustmentTransformer()\n",
    "y_train_adjusted = demographic_adjustment_y.fit_transform(df_y_train, c_train)\n",
    "y_test_adjusted = demographic_adjustment_y.transform(df_y_test, c_test)\n",
    "\n",
    "# Demographics adjustment for X\n",
    "demographic_adjustment_X = DemographicAdjustmentTransformer(categorical_columns=ordinal_features)\n",
    "X_train_adjusted = demographic_adjustment_X.fit_transform(df_X_train_imputed, c_train)\n",
    "X_test_adjusted = demographic_adjustment_X.transform(df_X_test_imputed, c_test)\n",
    "\n",
    "# Standardize only continuous features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_adjusted[continuous_features] = scaler.fit_transform(X_train_adjusted[continuous_features])\n",
    "X_test_adjusted[continuous_features] = scaler.transform(X_test_adjusted[continuous_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd43a55",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec542872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_imputer_model(\n",
    "    df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "    c_train, c_test,\n",
    "    ordinal_model, name_ordinal_imputer, \n",
    "    continuous_model, name_continuous_imputer, \n",
    "    model, name_model, \n",
    "    imputer_model=None, name_imputer=None, \n",
    "    separate_imputers=True,\n",
    "    ordinal_features = ['APOE_epsilon2', 'APOE_epsilon3', 'APOE_epsilon4']\n",
    "): \n",
    "    # Define which columns are ordinal and which are continuous\n",
    "    continuous_features = [col for col in df_X_train.columns if col not in ordinal_features]\n",
    "\n",
    "    # Check if a general imputer model (like MissForest or MICEForest) is provided\n",
    "    if imputer_model is not None and name_imputer is not None and not separate_imputers:\n",
    "        # If `imputer_model` can handle both categorical and continuous data types\n",
    "        print(f\"Using general imputer model: {name_imputer}\")\n",
    "\n",
    "        # Ensure that ordinal columns are marked as categorical\n",
    "        df_X_train = df_X_train.copy()\n",
    "        df_X_test = df_X_test.copy()\n",
    "        \n",
    "        for col in ordinal_features:\n",
    "            df_X_train[col] = df_X_train[col].astype(\"category\")\n",
    "            df_X_test[col] = df_X_test[col].astype(\"category\")\n",
    "\n",
    "        # Create a pipeline with the general imputer\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (name_imputer, imputer_model)\n",
    "        ])\n",
    "\n",
    "        # Fit and transform the entire dataset with the general imputer\n",
    "        pipeline.fit(df_X_train)\n",
    "        X_train_imputed = pipeline.transform(df_X_train)\n",
    "        X_test_imputed = pipeline.transform(df_X_test)\n",
    "\n",
    "        # Convert transformed output back to DataFrame with original column names\n",
    "        df_X_train_imputed = pd.DataFrame(X_train_imputed, columns=df_X_train.columns)\n",
    "        df_X_test_imputed = pd.DataFrame(X_test_imputed, columns=df_X_test.columns)\n",
    "\n",
    "    else:\n",
    "        # Separate imputers for ordinal and continuous data\n",
    "        print(\"Using separate imputers for ordinal and continuous data.\")\n",
    "\n",
    "        df_X_train = df_X_train.copy()\n",
    "        df_X_test = df_X_test.copy()\n",
    "\n",
    "        # Continuous Imputation Transformer (Example: SimpleImputer)\n",
    "        continuous_imputer = Pipeline([\n",
    "            (name_continuous_imputer, continuous_model),\n",
    "        ])\n",
    "\n",
    "        # Ordinal Imputation Transformer (Example: KNN Imputer)\n",
    "        ordinal_imputer = Pipeline([\n",
    "            (name_ordinal_imputer, ordinal_model)\n",
    "        ])\n",
    "\n",
    "        # Create a ColumnTransformer to apply the appropriate imputer to each type of variable\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('ordinal', ordinal_imputer, ordinal_features),\n",
    "                ('continuous', continuous_imputer, continuous_features)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "\n",
    "        # Create the pipeline\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor)\n",
    "        ])\n",
    "\n",
    "         # Separate imputers for ordinal and continuous data\n",
    "        # Fit and transform, then convert back to DataFrame with original column names\n",
    "\n",
    "        if df_X_train.isna().any().any():\n",
    "                \n",
    "            start = time.time()\n",
    "            pipeline.fit(df_X_train)\n",
    "            end = time.time()\n",
    "\n",
    "            impute_model_time = end - start\n",
    "\n",
    "            X_train_imputed = pipeline.transform(df_X_train)\n",
    "            df_X_train_imputed = df_X_train.copy()\n",
    "            df_X_train_imputed[ordinal_features+continuous_features] = X_train_imputed\n",
    "        else :\n",
    "            print(\"No NaN in train data -> Keep as it is. \")\n",
    "            df_X_train_imputed = df_X_train\n",
    "            \n",
    "            impute_model_time = None\n",
    "\n",
    "        # Transform the test set\n",
    "        if df_X_test.isna().any().any(): \n",
    "            X_test_imputed = pipeline.transform(df_X_test)\n",
    "            df_X_test_imputed = df_X_test.copy()\n",
    "            df_X_test_imputed[ordinal_features+continuous_features] = X_test_imputed\n",
    "        else : \n",
    "            print(\"No NaN in test data -> Keep as it is. \")\n",
    "            df_X_test_imputed = df_X_test\n",
    "\n",
    "    # Demographics adjustment for y\n",
    "    demographic_adjustment_y = DemographicAdjustmentTransformer()\n",
    "    y_train_adjusted = demographic_adjustment_y.fit_transform(df_y_train, c_train)\n",
    "    y_test_adjusted = demographic_adjustment_y.transform(df_y_test, c_test)\n",
    "\n",
    "    # Demographics adjustment for X\n",
    "    demographic_adjustment_X = DemographicAdjustmentTransformer(categorical_columns=ordinal_features)\n",
    "    X_train_adjusted = demographic_adjustment_X.fit_transform(df_X_train_imputed, c_train)\n",
    "    X_test_adjusted = demographic_adjustment_X.transform(df_X_test_imputed, c_test)\n",
    "\n",
    "    # Standardize only continuous features\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train_adjusted[continuous_features] = scaler.fit_transform(X_train_adjusted[continuous_features])\n",
    "    X_test_adjusted[continuous_features] = scaler.transform(X_test_adjusted[continuous_features])\n",
    "\n",
    "    # Perform prediction and save variables\n",
    "    start = time.time()\n",
    "\n",
    "    if isinstance(model, TabNetRegressor): \n",
    "        X_train_adjusted = X_train_adjusted.values\n",
    "        y_train_adjusted = y_train_adjusted.values\n",
    "\n",
    "        X_test_adjusted = X_test_adjusted.values\n",
    "        #y_test_adjusted = y_test_adjusted.values\n",
    "    \n",
    "    model.fit(X_train_adjusted, y_train_adjusted) \n",
    "    end = time.time()\n",
    "\n",
    "    predict_model_time = end - start\n",
    "\n",
    "    y_pred_adjusted = model.predict(X_test_adjusted)\n",
    "\n",
    "    if isinstance(y_pred_adjusted, pd.DataFrame) and all(\n",
    "    isinstance(col, str) and col.endswith(\"_prediction\") for col in y_pred_adjusted.columns\n",
    "    ):\n",
    "        y_pred_adjusted.columns = [col.replace(\"_prediction\", \"\") for col in y_pred_adjusted.columns]\n",
    "    else:\n",
    "        y_pred_adjusted = pd.DataFrame(y_pred_adjusted, columns=y_test_adjusted.columns)\n",
    "\n",
    "    # Metrics computed in original space\n",
    "    print(y_pred_adjusted) \n",
    "    y_pred = demographic_adjustment_y.inverse_transform(y_pred_adjusted, c_test)\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : X_train_adjusted.shape, \n",
    "        \"test_shape\": X_test_adjusted.shape\n",
    "    }\n",
    "    \n",
    "    if df_X_test.shape[0] != 1: \n",
    "\n",
    "        # Metrics computed in adjusted space\n",
    "        mse_score_adj, mae_score_ajd, r2_adj, explained_variance_adj, corr_adj = compute_all_metrics(y_test_adjusted.values, y_pred_adjusted)\n",
    "\n",
    "        results_adj = {\n",
    "            \"mse_score\": mse_score_adj, \n",
    "            \"mae_score\":mae_score_ajd, \n",
    "            \"r2\":r2_adj, \n",
    "            \"explained_variance\":explained_variance_adj, \n",
    "            \"corr\":corr_adj, \n",
    "        }\n",
    "\n",
    "        mse_score, mae_score, r2, explained_variance, corr = compute_all_metrics(df_y_test.values, y_pred)\n",
    "\n",
    "        results_org = {\n",
    "            \"mse_score\": mse_score, \n",
    "            \"mae_score\": mae_score, \n",
    "            \"r2\": r2, \n",
    "            \"explained_variance\": explained_variance, \n",
    "            \"corr\": corr, \n",
    "        }\n",
    "\n",
    "    else : \n",
    "        print(\"Saving predictions in dict!\")\n",
    "        results_adj = {\n",
    "            \"y_pred\": y_pred_adjusted.values, \n",
    "            \"y_test\": y_test_adjusted.values,\n",
    "        }\n",
    "\n",
    "        results_org = {\n",
    "            \"y_pred\": y_pred.values, \n",
    "            \"y_test\": df_y_test.values,\n",
    "        }\n",
    "\n",
    "\n",
    "    dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": impute_model_time,\n",
    "        \"fitting_time\": predict_model_time, \n",
    "        \"results_adj\": results_adj, \n",
    "        \"results_org\": results_org\n",
    "        }\n",
    "\n",
    "    return dict_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3cec90",
   "metadata": {},
   "source": [
    "# Hyperparameter search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce4872",
   "metadata": {},
   "source": [
    "## Linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3509c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ElasticNet Parameters: {'alpha': 0.01, 'l1_ratio': 0.01}\n"
     ]
    }
   ],
   "source": [
    "enet = MultiTaskElasticNet(max_iter=10000)\n",
    "\n",
    "param_grid_enet = {\n",
    "    'alpha': [1e-4, 1e-3, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    'l1_ratio': [0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "grid_search_enet = GridSearchCV(\n",
    "    estimator=enet,\n",
    "    param_grid=param_grid_enet,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_enet.fit(X_train_adjusted, y_train_adjusted)\n",
    "best_enet = grid_search_enet.best_estimator_\n",
    "\n",
    "print(\"Best ElasticNet Parameters:\", grid_search_enet.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a2997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Lasso Parameters: {'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "lasso = MultiTaskLasso(max_iter=10000)\n",
    "\n",
    "param_grid_lasso = {\n",
    "    'alpha': [1e-4, 1e-3, 1e-2, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0]\n",
    "}\n",
    "\n",
    "grid_search_lasso = GridSearchCV(\n",
    "    estimator=lasso,\n",
    "    param_grid=param_grid_lasso,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_lasso.fit(X_train_adjusted, y_train_adjusted)\n",
    "best_lasso = grid_search_lasso.best_estimator_\n",
    "\n",
    "print(\"Best Lasso Parameters:\", grid_search_lasso.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f821f4f",
   "metadata": {},
   "source": [
    "## Partial Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7255fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best PLS Parameters: {'n_components': 4}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the estimator\n",
    "pls = PLSRegression()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_pls = {\n",
    "    'n_components': list(range(1, min(X_train_adjusted.shape[1], y_train_adjusted.shape[1]) + 1))\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search_pls = GridSearchCV(\n",
    "    estimator=pls,\n",
    "    param_grid=param_grid_pls,\n",
    "    cv=5,  # Use cross-validation\n",
    "    scoring='r2',  # Can be changed to 'neg_mean_squared_error' or others\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search_pls.fit(X_train_adjusted, y_train_adjusted)\n",
    "\n",
    "# Extract the best model\n",
    "best_pls = grid_search_pls.best_estimator_\n",
    "print(\"Best PLS Parameters:\", grid_search_pls.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c66b9",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb5a41c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import xgboost as xgb\n",
    "\n",
    "class XGBoostRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,\n",
    "                 num_boost_round=100,\n",
    "                 max_depth=3,\n",
    "                 lambda_=1.0,\n",
    "                 learning_rate=0.1,\n",
    "                 subsample=1.0,\n",
    "                 colsample_bytree=1.0,\n",
    "                 alpha=0.0,\n",
    "                 tree_method=\"hist\",\n",
    "                 custom_obj=True,\n",
    "                 custom_metric=True):\n",
    "        \n",
    "        # Expose all as attributes\n",
    "        self.num_boost_round = num_boost_round\n",
    "        self.max_depth = max_depth\n",
    "        self.lambda_ = lambda_\n",
    "        self.learning_rate = learning_rate\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.alpha = alpha\n",
    "        self.tree_method = tree_method\n",
    "        self.custom_obj = custom_obj\n",
    "        self.custom_metric = custom_metric\n",
    "        \n",
    "        self.model = None\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        # Return all parameters\n",
    "        return {\n",
    "            'num_boost_round': self.num_boost_round,\n",
    "            'max_depth': self.max_depth,\n",
    "            'lambda_': self.lambda_,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree,\n",
    "            'alpha': self.alpha,\n",
    "            'tree_method': self.tree_method,\n",
    "            'custom_obj': self.custom_obj,\n",
    "            'custom_metric': self.custom_metric\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, val in params.items():\n",
    "            setattr(self, key, val)\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        dtrain = xgb.DMatrix(data=X, label=y)\n",
    "        params = {\n",
    "            'max_depth': self.max_depth,\n",
    "            'lambda': self.lambda_,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree,\n",
    "            'alpha': self.alpha,\n",
    "            'tree_method': self.tree_method,\n",
    "            'num_target': y.shape[1]\n",
    "        }\n",
    "        obj_fn = squared_log if self.custom_obj else 'reg:squarederror'\n",
    "        metric_fn = rmse if self.custom_metric else None\n",
    "\n",
    "        self.model = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=self.num_boost_round,\n",
    "            obj=obj_fn,\n",
    "            custom_metric=metric_fn\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        dtest = xgb.DMatrix(data=X)\n",
    "        return self.model.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a4eefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n",
      "/home/chris/miniforge3/envs/optimus/lib/python3.10/site-packages/xgboost/core.py:2150: FutureWarning: Since 2.1.0, the shape of the gradient and hessian is required to be (n_samples, n_targets) or (n_samples, n_classes).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Params: {'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105}\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'learning_rate': uniform(0.01, 0.29),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5)\n",
    "}\n",
    "\n",
    "rand_search_xgb = RandomizedSearchCV(\n",
    "    estimator=XGBoostRegressor(tree_method=\"hist\", custom_obj=True, custom_metric=True),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rand_search_xgb.fit(X_train_adjusted, y_train_adjusted)\n",
    "print(\"Best XGBoost Params:\", rand_search_xgb.best_params_)\n",
    "best_xgb = rand_search_xgb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "419856a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Params: {'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best XGBoost Params:\", rand_search_xgb.best_params_)\n",
    "best_xgb = rand_search_xgb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37903d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,\n",
    "                 num_boost_round=100,\n",
    "                 max_depth=3,\n",
    "                 lambda_=1.0,\n",
    "                 learning_rate=0.1,\n",
    "                 subsample=1.0,\n",
    "                 colsample_bytree=1.0,\n",
    "                 alpha=0.0,\n",
    "                 tree_method=\"hist\",\n",
    "                 custom_obj=True,\n",
    "                 custom_metric=True):\n",
    "        \n",
    "        # Expose all as attributes\n",
    "        self.num_boost_round = num_boost_round\n",
    "        self.max_depth = max_depth\n",
    "        self.lambda_ = lambda_\n",
    "        self.learning_rate = learning_rate\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.alpha = alpha\n",
    "        self.tree_method = tree_method\n",
    "        self.custom_obj = custom_obj\n",
    "        self.custom_metric = custom_metric\n",
    "        \n",
    "        self.model = None\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        # Return all parameters\n",
    "        return {\n",
    "            'num_boost_round': self.num_boost_round,\n",
    "            'max_depth': self.max_depth,\n",
    "            'lambda_': self.lambda_,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree,\n",
    "            'alpha': self.alpha,\n",
    "            'tree_method': self.tree_method,\n",
    "            'custom_obj': self.custom_obj,\n",
    "            'custom_metric': self.custom_metric\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, val in params.items():\n",
    "            setattr(self, key, val)\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        dtrain = xgb.DMatrix(data=X, label=y)\n",
    "        params = {\n",
    "            'max_depth': self.max_depth,\n",
    "            'lambda': self.lambda_,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree,\n",
    "            'alpha': self.alpha,\n",
    "            'tree_method': self.tree_method,\n",
    "            'num_target': y.shape[1]\n",
    "        }\n",
    "        obj_fn = squared_log if self.custom_obj else 'reg:squarederror'\n",
    "        metric_fn = rmse if self.custom_metric else None\n",
    "\n",
    "        self.model = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=self.num_boost_round,\n",
    "            obj=obj_fn,\n",
    "            custom_metric=metric_fn\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        dtest = xgb.DMatrix(data=X)\n",
    "        return self.model.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8db8c711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBoostRegressor(num_boost_round={&#x27;colsample_bytree&#x27;: 0.5079831261101071,\n",
       "                                  &#x27;learning_rate&#x27;: 0.0769592094304232,\n",
       "                                  &#x27;max_depth&#x27;: 6, &#x27;min_child_weight&#x27;: 7,\n",
       "                                  &#x27;subsample&#x27;: 0.8049983288913105})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBoostRegressor</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBoostRegressor(num_boost_round={&#x27;colsample_bytree&#x27;: 0.5079831261101071,\n",
       "                                  &#x27;learning_rate&#x27;: 0.0769592094304232,\n",
       "                                  &#x27;max_depth&#x27;: 6, &#x27;min_child_weight&#x27;: 7,\n",
       "                                  &#x27;subsample&#x27;: 0.8049983288913105})</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBoostRegressor(num_boost_round={'colsample_bytree': 0.5079831261101071,\n",
       "                                  'learning_rate': 0.0769592094304232,\n",
       "                                  'max_depth': 6, 'min_child_weight': 7,\n",
       "                                  'subsample': 0.8049983288913105})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBoostRegressor({'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261df8d1",
   "metadata": {},
   "source": [
    "##  Pytorch Tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c59e6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=[\"ADNI_MEM\", \"ADNI_EF\", \"ADNI_VS\", \"ADNI_LAN\"]\n",
    "\n",
    "data_config = DataConfig(\n",
    "    target=targets,\n",
    "    continuous_cols=continuous_features,\n",
    "    categorical_cols=ordinal_features\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    auto_lr_find=True,\n",
    "    early_stopping=\"valid_loss\",          # Monitor validation loss\n",
    "    early_stopping_mode=\"min\",           # Stop when it stops decreasing\n",
    "    early_stopping_patience=5,           # Wait up to 5 epochs with no improvement\n",
    "    early_stopping_min_delta=0.001,      # Minimum change to qualify as an improvement\n",
    "    checkpoints=\"valid_loss\",            # Save best model on val_loss\n",
    "    checkpoints_mode=\"min\",\n",
    "    checkpoints_save_top_k=1,\n",
    "    load_best=True,                      # Load best model post-training\n",
    "    progress_bar=\"simple\",               # Basic progress bar (or \"none\")\n",
    "    trainer_kwargs=dict(enable_model_summary=False),\n",
    "    accelerator=\"cpu\"                    # Change to \"gpu\" if available\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9114b072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lite', 'standard', 'full', 'high_memory']\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "from pytorch_tabular import MODEL_SWEEP_PRESETS\n",
    "\n",
    "print(list(MODEL_SWEEP_PRESETS.keys()))\n",
    "\n",
    "head_config = LinearHeadConfig(\n",
    "    layers=\"\",\n",
    "    dropout=0.1,\n",
    "    initialization=(  # No additional layer in head, just a mapping layer to output_dim\n",
    "        \"kaiming\"\n",
    "    ),\n",
    ").__dict__  # Convert to dict to pass to the model config (OmegaConf doesn't accept objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46c6bf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CategoryEmbeddingModelConfig', {'layers': '256-128-64'}), ('CategoryEmbeddingModelConfig', {'layers': '512-128-64'}), ('GANDALFConfig', {'gflu_stages': 6}), ('GANDALFConfig', {'gflu_stages': 15}), ('TabNetModelConfig', {'n_d': 32, 'n_a': 32, 'n_steps': 3, 'gamma': 1.5, 'n_independent': 1, 'n_shared': 2}), ('TabNetModelConfig', {'n_d': 32, 'n_a': 32, 'n_steps': 5, 'gamma': 1.5, 'n_independent': 2, 'n_shared': 3}), ('FTTransformerConfig', {'num_heads': 4, 'num_attn_blocks': 4})]\n"
     ]
    }
   ],
   "source": [
    "print(list(MODEL_SWEEP_PRESETS[\"standard\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c5a4afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (2304, 280) | Test Shape: (577, 280)\n"
     ]
    }
   ],
   "source": [
    "df_Xy_preprocessed_adjusted = pd.DataFrame(np.concatenate([X_train_adjusted, y_train_adjusted], axis=1), columns = ordinal_features+continuous_features+targets)\n",
    "train, test = train_test_split(df_Xy_preprocessed_adjusted, random_state=42, test_size=0.2)\n",
    "print(f\"Train Shape: {train.shape} | Test Shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36666e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2877250e76464e148e80c8946b7e903f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80366a68ca0446c19f2dd5387b5d5661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0c2fb0e5b84c13837a6c3b47dab860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8813805245f0492891fb210319f82b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93bd4c5e706437980a34b88a2082ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">03:06:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">697</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gandalf.gandal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">f:109</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m10\u001b[0m \u001b[1;92m03:06:36\u001b[0m,\u001b[1;36m697\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gandalf.gandal\u001b[1;92mf:109\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651ef0e62343443599943786ace76288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">03:06:45</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">701</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gandalf.gandal<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">f:109</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m10\u001b[0m \u001b[1;92m03:06:45\u001b[0m,\u001b[1;36m701\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gandalf.gandal\u001b[1;92mf:109\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">03:16:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">228</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m10\u001b[0m \u001b[1;92m03:16:10\u001b[0m,\u001b[1;36m228\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f1b23e50ca403893a12357dcc319bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">03:16:56</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">171</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m10\u001b[0m \u001b[1;92m03:16:56\u001b[0m,\u001b[1;36m171\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d43772e0f443d19f6afe8649b740a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b288bab46e194068ad8a31cc2f3eaaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = [\n",
    "    #\"r2_score\",\n",
    "    \"mean_absolute_error\",\n",
    "    \"explained_variance\",\n",
    "]\n",
    "\n",
    "# Provide an empty dict for each metric (no extra params needed):\n",
    "metrics_params = [{}, {}]\n",
    "\n",
    "# For regression, all metrics are computed on predictions themselves:\n",
    "metrics_prob_input = [False] * len(metrics)\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "torch.serialization.safe_globals([DictConfig])\n",
    "\n",
    "# Filtering out the warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    sweep_df, best_model = model_sweep(\n",
    "        task=\"regression\",  # One of \"classification\", \"regression\"\n",
    "        train=train,\n",
    "        test=test,\n",
    "        data_config=data_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "        model_list=\"full\",\n",
    "        common_model_args=dict(head=\"LinearHead\", head_config=head_config),\n",
    "        metrics = metrics,\n",
    "        metrics_params=metrics_params,\n",
    "        metrics_prob_input=metrics_prob_input,\n",
    "        rank_metric=(\"mean_absolute_error\", \"higher_is_better\"),\n",
    "        progress_bar=True,\n",
    "        verbose=False,\n",
    "        suppress_lightning_logger=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d00b7703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th># Params</th>\n",
       "      <th>epochs</th>\n",
       "      <th>test_loss_0</th>\n",
       "      <th>test_loss_1</th>\n",
       "      <th>test_loss_2</th>\n",
       "      <th>test_loss_3</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_mean_absolute_error_0</th>\n",
       "      <th>test_mean_absolute_error_1</th>\n",
       "      <th>...</th>\n",
       "      <th>test_mean_absolute_error_3</th>\n",
       "      <th>test_mean_absolute_error</th>\n",
       "      <th>test_explained_variance_0</th>\n",
       "      <th>test_explained_variance_1</th>\n",
       "      <th>test_explained_variance_2</th>\n",
       "      <th>test_explained_variance_3</th>\n",
       "      <th>test_explained_variance</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>time_taken_per_epoch</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GatedAdditiveTreeEnsembleModel</td>\n",
       "      <td>7 M</td>\n",
       "      <td>11</td>\n",
       "      <td>0.323990</td>\n",
       "      <td>0.542354</td>\n",
       "      <td>0.560329</td>\n",
       "      <td>0.408959</td>\n",
       "      <td>1.835631</td>\n",
       "      <td>0.437651</td>\n",
       "      <td>0.571564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492840</td>\n",
       "      <td>2.101103</td>\n",
       "      <td>0.625733</td>\n",
       "      <td>0.485292</td>\n",
       "      <td>0.153444</td>\n",
       "      <td>0.506565</td>\n",
       "      <td>1.771033</td>\n",
       "      <td>346.253147</td>\n",
       "      <td>31.477559</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DANetModel</td>\n",
       "      <td>2 M</td>\n",
       "      <td>16</td>\n",
       "      <td>0.354974</td>\n",
       "      <td>0.540967</td>\n",
       "      <td>0.669353</td>\n",
       "      <td>0.441169</td>\n",
       "      <td>2.006463</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>0.564277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517689</td>\n",
       "      <td>2.191942</td>\n",
       "      <td>0.595836</td>\n",
       "      <td>0.488291</td>\n",
       "      <td>-0.013647</td>\n",
       "      <td>0.489327</td>\n",
       "      <td>1.559807</td>\n",
       "      <td>151.719178</td>\n",
       "      <td>9.482449</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CategoryEmbeddingModel</td>\n",
       "      <td>263 T</td>\n",
       "      <td>9</td>\n",
       "      <td>0.355338</td>\n",
       "      <td>0.612130</td>\n",
       "      <td>0.602721</td>\n",
       "      <td>0.433786</td>\n",
       "      <td>2.003975</td>\n",
       "      <td>0.463190</td>\n",
       "      <td>0.613986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515381</td>\n",
       "      <td>2.206153</td>\n",
       "      <td>0.601280</td>\n",
       "      <td>0.441098</td>\n",
       "      <td>0.099351</td>\n",
       "      <td>0.485393</td>\n",
       "      <td>1.627123</td>\n",
       "      <td>21.759632</td>\n",
       "      <td>2.417737</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TabTransformerModel</td>\n",
       "      <td>400 T</td>\n",
       "      <td>13</td>\n",
       "      <td>0.380481</td>\n",
       "      <td>0.766874</td>\n",
       "      <td>0.606936</td>\n",
       "      <td>0.507441</td>\n",
       "      <td>2.261732</td>\n",
       "      <td>0.477530</td>\n",
       "      <td>0.670824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556025</td>\n",
       "      <td>2.317068</td>\n",
       "      <td>0.584447</td>\n",
       "      <td>0.366383</td>\n",
       "      <td>0.095381</td>\n",
       "      <td>0.399458</td>\n",
       "      <td>1.445669</td>\n",
       "      <td>62.998656</td>\n",
       "      <td>4.846050</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FTTransformerModel</td>\n",
       "      <td>416 T</td>\n",
       "      <td>20</td>\n",
       "      <td>0.427512</td>\n",
       "      <td>0.608372</td>\n",
       "      <td>0.574697</td>\n",
       "      <td>0.530267</td>\n",
       "      <td>2.140848</td>\n",
       "      <td>0.507553</td>\n",
       "      <td>0.613245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573561</td>\n",
       "      <td>2.309888</td>\n",
       "      <td>0.525614</td>\n",
       "      <td>0.432071</td>\n",
       "      <td>0.133033</td>\n",
       "      <td>0.392044</td>\n",
       "      <td>1.482762</td>\n",
       "      <td>4357.219138</td>\n",
       "      <td>217.860957</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GANDALFModel</td>\n",
       "      <td>6 M</td>\n",
       "      <td>100</td>\n",
       "      <td>0.522728</td>\n",
       "      <td>0.706339</td>\n",
       "      <td>0.603901</td>\n",
       "      <td>0.597320</td>\n",
       "      <td>2.430287</td>\n",
       "      <td>0.577457</td>\n",
       "      <td>0.658918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610478</td>\n",
       "      <td>2.468199</td>\n",
       "      <td>0.425688</td>\n",
       "      <td>0.343637</td>\n",
       "      <td>0.103546</td>\n",
       "      <td>0.327892</td>\n",
       "      <td>1.200763</td>\n",
       "      <td>573.442258</td>\n",
       "      <td>5.734423</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AutoIntModel</td>\n",
       "      <td>121 T</td>\n",
       "      <td>85</td>\n",
       "      <td>0.567992</td>\n",
       "      <td>0.729492</td>\n",
       "      <td>0.633248</td>\n",
       "      <td>0.626819</td>\n",
       "      <td>2.557550</td>\n",
       "      <td>0.607697</td>\n",
       "      <td>0.660484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629624</td>\n",
       "      <td>2.534326</td>\n",
       "      <td>0.367111</td>\n",
       "      <td>0.324409</td>\n",
       "      <td>0.028654</td>\n",
       "      <td>0.289390</td>\n",
       "      <td>1.009563</td>\n",
       "      <td>700.243122</td>\n",
       "      <td>8.238154</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TabNetModel</td>\n",
       "      <td>227 T</td>\n",
       "      <td>5</td>\n",
       "      <td>0.897485</td>\n",
       "      <td>1.073166</td>\n",
       "      <td>0.678909</td>\n",
       "      <td>0.881682</td>\n",
       "      <td>3.531243</td>\n",
       "      <td>0.782892</td>\n",
       "      <td>0.807683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743692</td>\n",
       "      <td>2.992384</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.006342</td>\n",
       "      <td>33.088309</td>\n",
       "      <td>6.617662</td>\n",
       "      <td>{'task': 'regression', 'head': 'LinearHead', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model # Params  epochs  test_loss_0  test_loss_1  \\\n",
       "5  GatedAdditiveTreeEnsembleModel      7 M      11     0.323990     0.542354   \n",
       "2                      DANetModel      2 M      16     0.354974     0.540967   \n",
       "1          CategoryEmbeddingModel    263 T       9     0.355338     0.612130   \n",
       "7             TabTransformerModel    400 T      13     0.380481     0.766874   \n",
       "3              FTTransformerModel    416 T      20     0.427512     0.608372   \n",
       "4                    GANDALFModel      6 M     100     0.522728     0.706339   \n",
       "0                    AutoIntModel    121 T      85     0.567992     0.729492   \n",
       "6                     TabNetModel    227 T       5     0.897485     1.073166   \n",
       "\n",
       "   test_loss_2  test_loss_3  test_loss  test_mean_absolute_error_0  \\\n",
       "5     0.560329     0.408959   1.835631                    0.437651   \n",
       "2     0.669353     0.441169   2.006463                    0.457900   \n",
       "1     0.602721     0.433786   2.003975                    0.463190   \n",
       "7     0.606936     0.507441   2.261732                    0.477530   \n",
       "3     0.574697     0.530267   2.140848                    0.507553   \n",
       "4     0.603901     0.597320   2.430287                    0.577457   \n",
       "0     0.633248     0.626819   2.557550                    0.607697   \n",
       "6     0.678909     0.881682   3.531243                    0.782892   \n",
       "\n",
       "   test_mean_absolute_error_1  ...  test_mean_absolute_error_3  \\\n",
       "5                    0.571564  ...                    0.492840   \n",
       "2                    0.564277  ...                    0.517689   \n",
       "1                    0.613986  ...                    0.515381   \n",
       "7                    0.670824  ...                    0.556025   \n",
       "3                    0.613245  ...                    0.573561   \n",
       "4                    0.658918  ...                    0.610478   \n",
       "0                    0.660484  ...                    0.629624   \n",
       "6                    0.807683  ...                    0.743692   \n",
       "\n",
       "   test_mean_absolute_error  test_explained_variance_0  \\\n",
       "5                  2.101103                   0.625733   \n",
       "2                  2.191942                   0.595836   \n",
       "1                  2.206153                   0.601280   \n",
       "7                  2.317068                   0.584447   \n",
       "3                  2.309888                   0.525614   \n",
       "4                  2.468199                   0.425688   \n",
       "0                  2.534326                   0.367111   \n",
       "6                  2.992384                   0.002445   \n",
       "\n",
       "   test_explained_variance_1  test_explained_variance_2  \\\n",
       "5                   0.485292                   0.153444   \n",
       "2                   0.488291                  -0.013647   \n",
       "1                   0.441098                   0.099351   \n",
       "7                   0.366383                   0.095381   \n",
       "3                   0.432071                   0.133033   \n",
       "4                   0.343637                   0.103546   \n",
       "0                   0.324409                   0.028654   \n",
       "6                   0.001495                   0.000981   \n",
       "\n",
       "   test_explained_variance_3  test_explained_variance   time_taken  \\\n",
       "5                   0.506565                 1.771033   346.253147   \n",
       "2                   0.489327                 1.559807   151.719178   \n",
       "1                   0.485393                 1.627123    21.759632   \n",
       "7                   0.399458                 1.445669    62.998656   \n",
       "3                   0.392044                 1.482762  4357.219138   \n",
       "4                   0.327892                 1.200763   573.442258   \n",
       "0                   0.289390                 1.009563   700.243122   \n",
       "6                   0.001422                 0.006342    33.088309   \n",
       "\n",
       "   time_taken_per_epoch                                             params  \n",
       "5             31.477559  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "2              9.482449  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "1              2.417737  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "7              4.846050  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "3            217.860957  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "4              5.734423  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "0              8.238154  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "6              6.617662  {'task': 'regression', 'head': 'LinearHead', '...  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sweep_df.sort_values(\"test_mean_absolute_error_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c2a9d",
   "metadata": {},
   "source": [
    "# PyTorch Tabular : match pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "292c5a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_constant, Model: LinearRegression\n",
      "Combinations of preprocessing and models to test : 1\n"
     ]
    }
   ],
   "source": [
    "random_state=42\n",
    "n_imputation_iter = 10\n",
    "\n",
    "# Continuous Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "continuous_imputer_list = [\n",
    "    (\"KNNImputer\", KNNImputer(n_neighbors=1)),\n",
    "]\n",
    "\n",
    "# Ordinal Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "ordinal_imputer_list = [\n",
    "    (\"SimpleImputer_constant\", SimpleImputer(strategy=\"constant\", fill_value=-1))\n",
    "]\n",
    "\n",
    "# Predictive Models List (list of tuples with unique strings and corresponding instances)\n",
    "predictive_models_list = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "# Display all combinations\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    print(f\"Continuous Imputer: {continuous_imputer[0]}, Ordinal Imputer: {ordinal_imputer[0]}, Model: {model[0]}\")\n",
    "\n",
    "print(f\"Combinations of preprocessing and models to test : {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50bce9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing combination: dict_values(['SimpleImputer_constant', 'KNNImputer', 'LinearRegression', (2881, 348), (13, 348)])\n"
     ]
    }
   ],
   "source": [
    "# Initialize HDF5 file\n",
    "results_file = '../pickle/test.pickle'\n",
    "\n",
    "if os.path.exists(results_file): \n",
    "\n",
    "    with open(results_file, \"rb\") as input_file:\n",
    "        all_dict_results = pickle.load(input_file)\n",
    "\n",
    "else : \n",
    "    all_dict_results = []\n",
    "            \n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "\n",
    "    if any(result['params'] == params for result in all_dict_results):\n",
    "        # Skip this iteration if the combination exists\n",
    "        print(f\"Skipping existing combination: {params.values()}\")\n",
    "        \n",
    "        continue\n",
    "\n",
    "    try: \n",
    "    \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "        # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b7cbe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing combination: dict_values(['SimpleImputer_constant', 'KNNImputer', 'LinearRegression', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_constant', 'KNNImputer', 'GatedAdditiveTreeEnsembleConfig_tab', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_constant', 'KNNImputer', 'DANetConfig_tab', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_constant', 'KNNImputer', 'TabTransformerConfig_tab', (2881, 348), (13, 348)])\n",
      "Skipping existing combination: dict_values(['SimpleImputer_constant', 'KNNImputer', 'TabNetModelConfig_tab', (2881, 348), (13, 348)])\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import (\n",
    "    GatedAdditiveTreeEnsembleConfig,\n",
    "    DANetConfig,\n",
    "    TabTransformerConfig,\n",
    "    FTTransformerConfig,\n",
    "    TabNetModelConfig,\n",
    ")\n",
    "from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "\n",
    "# Prepare Tabular configurations (shared for all PyTorch models)\n",
    "data_config = DataConfig(\n",
    "    target=df_y_train.columns.tolist(),\n",
    "    continuous_cols=continuous_features,\n",
    "    categorical_cols=ordinal_features\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024, max_epochs=1, auto_lr_find=True,\n",
    "    early_stopping=\"valid_loss\", early_stopping_mode=\"min\", early_stopping_patience=5,\n",
    "    checkpoints=\"valid_loss\", load_best=True, progress_bar=\"nones\", accelerator=\"cpu\"\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "head_config = LinearHeadConfig(dropout=0.1).__dict__\n",
    "\n",
    "# Utility to wrap TabularModel into sklearn-like fit/predict\n",
    "class TabularModelWrapper:\n",
    "    def __init__(self, model_config, data_config, trainer_config, optimizer_config):\n",
    "        self.model_config = model_config\n",
    "        self.data_config = data_config\n",
    "        self.trainer_config = trainer_config\n",
    "        self.optimizer_config = optimizer_config\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        df = X.copy().reset_index(drop=True)\n",
    "        for c in y.columns:\n",
    "            df[c] = y[c].reset_index(drop=True)\n",
    "\n",
    "        self.model = TabularModel(\n",
    "            data_config=self.data_config,\n",
    "            model_config=self.model_config,\n",
    "            optimizer_config=self.optimizer_config,\n",
    "            trainer_config=self.trainer_config\n",
    "        )\n",
    "        self.model.fit(train=df, validation=df)  # ideally separate validation\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        assert self.model is not None, \"You must call .fit(...) before .predict(...)\"\n",
    "        preds = self.model.predict(X.reset_index(drop=True))\n",
    "        return preds[[f\"{c}_prediction\" for c in df_y_train.columns]].values\n",
    "\n",
    "# Add PyTorch models to list\n",
    "head_config = LinearHeadConfig(dropout=0.1).__dict__\n",
    "\n",
    "predictive_models_list += [\n",
    "    (\"GatedAdditiveTreeEnsembleConfig_tab\", \n",
    "    TabularModelWrapper(\n",
    "        GatedAdditiveTreeEnsembleConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        gflu_stages=6,\n",
    "        gflu_dropout=0.0,\n",
    "        tree_depth=5,\n",
    "        num_trees=20,\n",
    "        chain_trees=False,\n",
    "        share_head_weights=True), data_config, trainer_config, optimizer_config \n",
    "    )),\n",
    "    (\"DANetConfig_tab\",\n",
    "    TabularModelWrapper(\n",
    "        DANetConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_layers=8,\n",
    "        k=5,\n",
    "        dropout_rate=0.1), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabTransformerConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        embedding_initialization=\"kaiming_uniform\",\n",
    "        embedding_bias=False), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    # (\"FTTransformerConfig\",\n",
    "    #     TabularModelWrapper(\n",
    "    #     FTTransformerConfig(\n",
    "    #     task=\"regression\",\n",
    "    #     head=\"LinearHead\",\n",
    "    #     head_config=head_config), data_config, trainer_config, optimizer_config\n",
    "    # )),\n",
    "    (\"TabNetModelConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        n_independent=2,\n",
    "        n_shared=2), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "\n",
    "    if any(result['params'] == params for result in all_dict_results):\n",
    "        # Skip this iteration if the combination exists\n",
    "        print(f\"Skipping existing combination: {params.values()}\")\n",
    "        \n",
    "        continue\n",
    "\n",
    "    try: \n",
    "        print(name_model)\n",
    "        print(model_instance)\n",
    "    \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "        # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ad3cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metric_table(\n",
    "    results_list,\n",
    "    targets,\n",
    "    metric_name,\n",
    "    source=\"Adjusted\",\n",
    "    float_format=\"%.3f\",\n",
    "    csv_filename=None,\n",
    "    sort_order=\"ascending\"  # or \"descending\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a LaTeX table for a single metric across targets, models, and imputers.\n",
    "    Optionally export the same table as CSV and sort by mean performance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_list : list of dict\n",
    "        List of experiment results.\n",
    "    targets : list of str\n",
    "        Target names (e.g., ['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN']).\n",
    "    metric_name : str\n",
    "        Metric to extract (e.g., 'mae_score').\n",
    "    source : str\n",
    "        'Adjusted' or 'Original'.\n",
    "    float_format : str\n",
    "        Format for floats (e.g., '%.3f').\n",
    "    csv_filename : str or None\n",
    "        If provided, saves the table to CSV.\n",
    "    sort_order : str\n",
    "        'ascending' or 'descending' for sorting by mean.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        LaTeX-formatted table string.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    version_key = \"results_adj\" if source.lower() == \"adjusted\" else \"results_org\"\n",
    "\n",
    "    for res in results_list:\n",
    "        result_block = res.get(version_key)\n",
    "        if result_block is None:\n",
    "            continue\n",
    "\n",
    "        metric_values = result_block.get(metric_name)\n",
    "        if metric_values is None:\n",
    "            continue\n",
    "\n",
    "        if len(metric_values) != len(targets):\n",
    "            continue\n",
    "\n",
    "        ordinal_imputer = res[\"params\"].get(\"ordinal_imputer\")\n",
    "        model = res[\"params\"].get(\"model\")\n",
    "\n",
    "        values = np.array(metric_values, dtype=np.float64)\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "\n",
    "        row = {\n",
    "            \"Ordinal Imputer\": ordinal_imputer,\n",
    "            \"Model\": model,\n",
    "            \"Mean\": mean_val,  # for sorting\n",
    "            \"Mean  SD\": f\"{mean_val:.3f}  {std_val:.3f}\",\n",
    "        }\n",
    "        row.update({target: val for target, val in zip(targets, values)})\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Reorder columns for display\n",
    "    display_cols = [\"Ordinal Imputer\", \"Model\"] + targets + [\"Mean  SD\"]\n",
    "    df = df.sort_values(by=\"Mean\", ascending=(sort_order == \"ascending\"))\n",
    "    df = df[display_cols]\n",
    "\n",
    "    # Save CSV\n",
    "    if csv_filename:\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # LaTeX output\n",
    "    latex_table = df.to_latex(\n",
    "        index=False,\n",
    "        escape=False,\n",
    "        float_format=float_format,\n",
    "        caption=f\"{metric_name.replace('_', ' ').upper()} across targets\",\n",
    "        label=f\"tab:{metric_name}\",\n",
    "        longtable=False\n",
    "    )\n",
    "\n",
    "    return df, latex_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bc9d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, latex_table = generate_metric_table(all_dict_results, targets=dict_select[\"ADNI_cog\"], metric_name=\"mae_score\", source=\"Adjusted\", csv_filename=None, sort_order=\"ascending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df605aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ordinal Imputer</th>\n",
       "      <th>Model</th>\n",
       "      <th>ADNI_MEM</th>\n",
       "      <th>ADNI_EF</th>\n",
       "      <th>ADNI_VS</th>\n",
       "      <th>ADNI_LAN</th>\n",
       "      <th>Mean  SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SimpleImputer_constant</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.659125</td>\n",
       "      <td>0.558940</td>\n",
       "      <td>0.565599</td>\n",
       "      <td>0.602758</td>\n",
       "      <td>0.597  0.040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SimpleImputer_constant</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.684685</td>\n",
       "      <td>0.522703</td>\n",
       "      <td>0.607183</td>\n",
       "      <td>0.640623</td>\n",
       "      <td>0.614  0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SimpleImputer_constant</td>\n",
       "      <td>TabTransformerConfig_tab</td>\n",
       "      <td>0.835669</td>\n",
       "      <td>0.716633</td>\n",
       "      <td>0.761407</td>\n",
       "      <td>0.629682</td>\n",
       "      <td>0.736  0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SimpleImputer_constant</td>\n",
       "      <td>TabTransformerConfig_tab</td>\n",
       "      <td>0.744698</td>\n",
       "      <td>0.940382</td>\n",
       "      <td>0.689909</td>\n",
       "      <td>0.811252</td>\n",
       "      <td>0.797  0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SimpleImputer_constant</td>\n",
       "      <td>DANetConfig_tab</td>\n",
       "      <td>0.996245</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.615216</td>\n",
       "      <td>0.928531</td>\n",
       "      <td>0.822  0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SimpleImputer_constant</td>\n",
       "      <td>TabNetModelConfig_tab</td>\n",
       "      <td>0.992423</td>\n",
       "      <td>0.783072</td>\n",
       "      <td>0.621848</td>\n",
       "      <td>0.932718</td>\n",
       "      <td>0.833  0.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SimpleImputer_constant</td>\n",
       "      <td>DANetConfig_tab</td>\n",
       "      <td>1.025889</td>\n",
       "      <td>0.778495</td>\n",
       "      <td>0.616289</td>\n",
       "      <td>0.917141</td>\n",
       "      <td>0.834  0.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SimpleImputer_constant</td>\n",
       "      <td>GatedAdditiveTreeEnsembleConfig_tab</td>\n",
       "      <td>1.165538</td>\n",
       "      <td>0.740761</td>\n",
       "      <td>0.645170</td>\n",
       "      <td>0.807058</td>\n",
       "      <td>0.840  0.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SimpleImputer_constant</td>\n",
       "      <td>GatedAdditiveTreeEnsembleConfig_tab</td>\n",
       "      <td>0.957282</td>\n",
       "      <td>2.199720</td>\n",
       "      <td>1.690917</td>\n",
       "      <td>0.922638</td>\n",
       "      <td>1.443  0.534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Ordinal Imputer                                Model  ADNI_MEM  \\\n",
       "4  SimpleImputer_constant                     LinearRegression  0.659125   \n",
       "0  SimpleImputer_constant                     LinearRegression  0.684685   \n",
       "3  SimpleImputer_constant             TabTransformerConfig_tab  0.835669   \n",
       "7  SimpleImputer_constant             TabTransformerConfig_tab  0.744698   \n",
       "2  SimpleImputer_constant                      DANetConfig_tab  0.996245   \n",
       "8  SimpleImputer_constant                TabNetModelConfig_tab  0.992423   \n",
       "6  SimpleImputer_constant                      DANetConfig_tab  1.025889   \n",
       "5  SimpleImputer_constant  GatedAdditiveTreeEnsembleConfig_tab  1.165538   \n",
       "1  SimpleImputer_constant  GatedAdditiveTreeEnsembleConfig_tab  0.957282   \n",
       "\n",
       "    ADNI_EF   ADNI_VS  ADNI_LAN      Mean  SD  \n",
       "4  0.558940  0.565599  0.602758  0.597  0.040  \n",
       "0  0.522703  0.607183  0.640623  0.614  0.059  \n",
       "3  0.716633  0.761407  0.629682  0.736  0.075  \n",
       "7  0.940382  0.689909  0.811252  0.797  0.093  \n",
       "2  0.749981  0.615216  0.928531  0.822  0.150  \n",
       "8  0.783072  0.621848  0.932718  0.833  0.144  \n",
       "6  0.778495  0.616289  0.917141  0.834  0.153  \n",
       "5  0.740761  0.645170  0.807058  0.840  0.197  \n",
       "1  2.199720  1.690917  0.922638  1.443  0.534  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
