{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "from itertools import product\n",
    "import warnings\n",
    "\n",
    "# System path modification\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer, MissingIndicator\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, KFold, StratifiedKFold, GroupKFold, StratifiedGroupKFold, LeaveOneOut, cross_validate, cross_val_score\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Lasso, LassoCV, MultiTaskLasso, MultiTaskLassoCV,\n",
    "    ElasticNet, ElasticNetCV, MultiTaskElasticNet, MultiTaskElasticNetCV\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "# Statistic imports \n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.special import kl_div\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Specialized imputation and visualization packages\n",
    "import miceforest as mf\n",
    "import missingno as msno\n",
    "#from missforest import MissForest\n",
    "#import magic\n",
    "from src.gain import *\n",
    "\n",
    "# Custom modules\n",
    "from src.train import *\n",
    "from src.functions import *\n",
    "from src.plots import *\n",
    "from src.dataset import *\n",
    "from src.multixgboost import *\n",
    "from src.wrapper import *\n",
    "\n",
    "# Visualizatiokn \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning and machine learning specific \n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "from pytorch_tabular.models import (\n",
    "    GatedAdditiveTreeEnsembleConfig,\n",
    "    DANetConfig,\n",
    "    TabTransformerConfig,\n",
    "    TabNetModelConfig,\n",
    ")\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Print CUDA availability for PyTorch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_pickle_data_palettes()\n",
    "\n",
    "results_pickle_folder = \"../pickle/\"\n",
    "\n",
    "# Unpack data\n",
    "df_X, df_y, df_all, df_FinalCombination = data[\"df_X\"], data[\"df_y\"], data[\"df_all\"], data[\"df_FinalCombination\"]\n",
    "dict_select = data[\"dict_select\"]\n",
    "\n",
    "# Unpack colormaps\n",
    "full_palette, gender_palette, dx_palette = data[\"colormaps\"].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "idx_na = list(df_X.isna().any(axis=1))\n",
    "idx_nona = list(~df_X.isna().any(axis=1))\n",
    "\n",
    "# Identify unique RIDs for rows with and without missing data\n",
    "rid_na = df_all.loc[idx_na, \"RID\"].unique()\n",
    "rid_nona = df_all.loc[idx_nona, \"RID\"].unique()\n",
    "\n",
    "# Identify RIDs eligible for test set (not in train set)\n",
    "eligible_rid_na = np.setdiff1d(rid_na, rid_nona)\n",
    "\n",
    "df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]] = df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]].astype(\"category\")\n",
    "\n",
    "\n",
    "single_measurement_rids = (\n",
    "    df_all[df_all[\"RID\"].isin(eligible_rid_na)]\n",
    "    .groupby(\"RID\")\n",
    "    .filter(lambda x: len(x) == 1)\n",
    "    .RID.unique()\n",
    ")\n",
    "\n",
    "test_rids = np.random.choice(single_measurement_rids, size=20, replace=False)\n",
    "\n",
    "# Create boolean mask for the test set\n",
    "idx_test = df_all[\"RID\"].isin(test_rids)\n",
    "\n",
    "# Create boolean mask for the train set (all not in test)\n",
    "idx_train = ~idx_test\n",
    "\n",
    "print(idx_test.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave-One-Missing-Out (LOMO-CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: LinearRegression\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskElasticNet\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskElasticNet_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskLasso\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskLasso_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: RandomForestRegressor\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: XGBoostRegressor\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: XGBoostRegressor_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetRegressor_default\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetRegressor_custom\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: PLSRegression_4_components\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: GatedAdditiveTreeEnsembleConfig_tab\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: DANetConfig_tab\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabTransformerConfig_tab\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetModelConfig_tab\n",
      "Combinations of preprocessing and models to test : 15\n"
     ]
    }
   ],
   "source": [
    "random_state=42\n",
    "n_imputation_iter = 10\n",
    "\n",
    "# Continuous Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "continuous_imputer_list = [\n",
    "    #(\"SimpleImputer_mean\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"KNNImputer\", KNNImputer(n_neighbors=1)),\n",
    "]\n",
    "\n",
    "# Ordinal Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "ordinal_imputer_list = [\n",
    "    (\"SimpleImputer_most_frequent\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "]\n",
    "\n",
    "# Predictive Models List (list of tuples with unique strings and corresponding instances)\n",
    "predictive_models_list = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"MultiTaskElasticNet\", MultiTaskElasticNet()),\n",
    "    (\"MultiTaskElasticNet_tuned\", MultiTaskElasticNet(**{'alpha': 0.01, 'l1_ratio': 0.01})),\n",
    "    (\"MultiTaskLasso\", MultiTaskLasso()),\n",
    "    (\"MultiTaskLasso_tuned\", MultiTaskLasso(**{'alpha': 0.001})),\n",
    "    (\"RandomForestRegressor\", RandomForestRegressor()),\n",
    "    (\"XGBoostRegressor\", XGBoostRegressor()),\n",
    "    (\"XGBoostRegressor_tuned\", XGBoostRegressor(**{'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105})),\n",
    "    (\"TabNetRegressor_default\", TabNetModelWrapper(n_a=8, n_d=8)),\n",
    "    (\"TabNetRegressor_custom\", TabNetModelWrapper(n_a=32, n_d=32)),\n",
    "    (\"PLSRegression_4_components\", PLSRegression(n_components=4))\n",
    "]\n",
    "\n",
    "ordinal_features = ['APOE_epsilon2', 'APOE_epsilon3', 'APOE_epsilon4']\n",
    "continuous_features = [col for col in df_X.columns if col not in ordinal_features]\n",
    "\n",
    "# Prepare Tabular configurations (shared for all PyTorch models)\n",
    "data_config = DataConfig(\n",
    "    target=df_y.columns.tolist(),\n",
    "    continuous_cols=continuous_features,\n",
    "    categorical_cols=continuous_features\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024, max_epochs=1, auto_lr_find=True,\n",
    "    early_stopping=\"valid_loss\", early_stopping_mode=\"min\", early_stopping_patience=5,\n",
    "    checkpoints=\"valid_loss\", load_best=True, progress_bar=\"nones\",\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "head_config = LinearHeadConfig(dropout=0.1).__dict__\n",
    "predictive_models_list += [\n",
    "    (\"GatedAdditiveTreeEnsembleConfig_tab\", \n",
    "    TabularModelWrapper(\n",
    "        GatedAdditiveTreeEnsembleConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        gflu_stages=6,\n",
    "        gflu_dropout=0.0,\n",
    "        tree_depth=5,\n",
    "        num_trees=20,\n",
    "        chain_trees=False,\n",
    "        share_head_weights=True), data_config, trainer_config, optimizer_config \n",
    "    )),\n",
    "    (\"DANetConfig_tab\",\n",
    "    TabularModelWrapper(\n",
    "        DANetConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_layers=8,\n",
    "        k=5,\n",
    "        dropout_rate=0.1), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabTransformerConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        embedding_initialization=\"kaiming_uniform\",\n",
    "        embedding_bias=False), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabNetModelConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        n_independent=2,\n",
    "        n_shared=2), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "# Display all combinations\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    print(f\"Continuous Imputer: {continuous_imputer[0]}, Ordinal Imputer: {ordinal_imputer[0]}, Model: {model[0]}\")\n",
    "\n",
    "print(f\"Combinations of preprocessing and models to test : {len(combinations)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: LinearRegression\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskElasticNet\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskElasticNet_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskLasso\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskLasso_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: RandomForestRegressor\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: XGBoostRegressor\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: XGBoostRegressor_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetRegressor_default\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetRegressor_custom\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: PLSRegression_4_components\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: GatedAdditiveTreeEnsembleConfig_tab\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: DANetConfig_tab\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabTransformerConfig_tab\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetModelConfig_tab\n",
      "Combinations of preprocessing and models to test : 15\n"
     ]
    }
   ],
   "source": [
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "# Display all combinations\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    print(f\"Continuous Imputer: {continuous_imputer[0]}, Ordinal Imputer: {ordinal_imputer[0]}, Model: {model[0]}\")\n",
    "\n",
    "print(f\"Combinations of preprocessing and models to test : {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_indices = [i for i, val in enumerate(idx_test) if val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'LinearRegression']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskElasticNet']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskElasticNet_tuned']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskLasso']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskLasso_tuned']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'RandomForestRegressor']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'XGBoostRegressor']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'XGBoostRegressor_tuned']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'TabNetRegressor_default']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'TabNetRegressor_custom']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'PLSRegression_4_components']\n",
      "Using separate imputers for ordinal and continuous data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:32:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">882</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m18:32:14\u001b[0m,\u001b[1;36m882\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:32:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">960</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m18:32:14\u001b[0m,\u001b[1;36m960\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:32:15</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">025</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m18:32:15\u001b[0m,\u001b[1;36m025\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">18:32:16</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">349</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m18:32:16\u001b[0m,\u001b[1;36m349\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize HDF5 file\n",
    "results_file = '../pickle/training_4_loona_dict_results.pickle'\n",
    "\n",
    "if os.path.exists(results_file): \n",
    "\n",
    "    with open(results_file, \"rb\") as input_file:\n",
    "        all_dict_results = pickle.load(input_file)\n",
    "\n",
    "else : \n",
    "    all_dict_results = []\n",
    "\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "            \"ordinal_imputer\": name_ordinal_imputer, \n",
    "            \"continuous_imputer\": name_continuous_imputer, \n",
    "            \"model\": name_model, \"train_shape\" : (df_X.shape[0]-1, df_X.shape[1]), \n",
    "            \"test_shape\": (1, df_X.shape[1]),\n",
    "            \"test_rid\": df_all.iloc[test_indices][\"RID\"].tolist()\n",
    "        }\n",
    "\n",
    "    # Define the subset of keys you care about\n",
    "    keys_to_check = ['ordinal_imputer', 'continuous_imputer', 'model']  # or whatever subset you want\n",
    "    \n",
    "    # Check if a result in all_dict_results has the same values for just those keys\n",
    "    if any(all(result['params'].get(k) == params.get(k) for k in keys_to_check) for result in all_dict_results):\n",
    "        print(f\"Skipping existing combination (subset match): {[params[k] for k in keys_to_check]}\")\n",
    "        continue\n",
    "    \n",
    "    dict_results = {\n",
    "            \"params\": params, \n",
    "            \"imputation_time\": [],\n",
    "            \"fitting_time\": [], \n",
    "            \"results_adj\": [], \n",
    "            \"results_org\": []\n",
    "        }\n",
    "\n",
    "    for test_nloc in test_indices: \n",
    "\n",
    "        idx_train = [True for i in range(df_X.shape[0])]\n",
    "        idx_test = [False for i in range(df_X.shape[0])]\n",
    "\n",
    "        idx_test[test_nloc] = True\n",
    "        idx_train[test_nloc] = False\n",
    "\n",
    "        df_X_train = df_X.loc[idx_train]\n",
    "        df_X_test = df_X.loc[idx_test]\n",
    "\n",
    "        df_y_train = df_y.loc[idx_train]\n",
    "        df_y_test = df_y.loc[idx_test]\n",
    "\n",
    "        c_train = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_train]\n",
    "        c_test = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_test]\n",
    "\n",
    "        try: \n",
    "        \n",
    "            # Now you can call your `train_model` function with these components\n",
    "            fold_dict_results = train_imputer_model(\n",
    "                df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "                c_train, c_test,\n",
    "                ordinal_imputer_instance, name_ordinal_imputer,\n",
    "                continuous_imputer_instance, name_continuous_imputer,\n",
    "                model_instance, name_model,\n",
    "                separate_imputers=True  # Or however you want to specify\n",
    "            )\n",
    "\n",
    "            dict_results[\"imputation_time\"].append(fold_dict_results[\"imputation_time\"])  \n",
    "            dict_results[\"fitting_time\"].append(fold_dict_results[\"fitting_time\"])  \n",
    "            dict_results[\"results_adj\"].append(fold_dict_results[\"results_adj\"])  \n",
    "            dict_results[\"results_org\"].append(fold_dict_results[\"results_org\"])    \n",
    "\n",
    "        except Exception as e:  \n",
    "\n",
    "            print(e)\n",
    "            \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "    \n",
    "    # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open(results_file, 'wb') as handle:\n",
    "    pickle.dump(all_dict_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickle/training_4_loona_dict_results.pickle', \"rb\") as input_file:\n",
    "    dict_results_loo_na = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models only on MRI features to compare performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3 : Leave-one-out with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "idx_na = list(df_X.isna().any(axis=1))\n",
    "idx_nona = list(~df_X.isna().any(axis=1))\n",
    "\n",
    "# Identify unique RIDs for rows with and without missing data\n",
    "rid_na = df_all.loc[idx_na, \"RID\"].unique()\n",
    "rid_nona = df_all.loc[idx_nona, \"RID\"].unique()\n",
    "\n",
    "# Identify RIDs eligible for test set (not in train set)\n",
    "eligible_rid_na = np.setdiff1d(rid_na, rid_nona)\n",
    "\n",
    "df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]] = df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]].astype(\"category\")\n",
    "\n",
    "\n",
    "single_measurement_rids = (\n",
    "    df_all[df_all[\"RID\"].isin(eligible_rid_na)]\n",
    "    .groupby(\"RID\")\n",
    "    .filter(lambda x: len(x) == 1)\n",
    "    .RID.unique()\n",
    ")\n",
    "\n",
    "test_rids = np.random.choice(single_measurement_rids, size=20, replace=False)\n",
    "\n",
    "# Create boolean mask for the test set\n",
    "idx_test = df_all[\"RID\"].isin(test_rids)\n",
    "\n",
    "# Create boolean mask for the train set (all not in test)\n",
    "idx_train = ~idx_test\n",
    "\n",
    "print(idx_test.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: LinearRegression\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskElasticNet\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskElasticNet_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskLasso\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskLasso_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: RandomForestRegressor\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: XGBoostRegressor\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: XGBoostRegressor_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: TabNetRegressor_default\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: TabNetRegressor_custom\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: PLSRegression_4_components\n",
      "Combinations of preprocessing and models to test : 11\n"
     ]
    }
   ],
   "source": [
    "random_state=42\n",
    "n_imputation_iter = 10\n",
    "\n",
    "# Define hyperparameters\n",
    "gain_parameters = {\n",
    "    'hint_rate': 0.9,\n",
    "    'alpha': 100,\n",
    "    'iterations': 1000\n",
    "}\n",
    "\n",
    "# Continuous Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "continuous_imputer_list = [\n",
    "    (\"NoImputer\", KNNImputer(n_neighbors=1)),\n",
    "]\n",
    "\n",
    "# Ordinal Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "ordinal_imputer_list = [\n",
    "    (\"NoImputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "]\n",
    "\n",
    "# Predictive Models List (list of tuples with unique strings and corresponding instances)\n",
    "predictive_models_list = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"MultiTaskElasticNet\", MultiTaskElasticNet()),\n",
    "    (\"MultiTaskElasticNet_tuned\", MultiTaskElasticNet({'alpha': 0.01, 'l1_ratio': 0.01})),\n",
    "    (\"MultiTaskLasso\", MultiTaskLasso()),\n",
    "    (\"MultiTaskLasso_tuned\", MultiTaskLasso({'alpha': 0.001})),\n",
    "    (\"RandomForestRegressor\", RandomForestRegressor()),\n",
    "    (\"XGBoostRegressor\", XGBoostRegressor()),\n",
    "    (\"XGBoostRegressor_tuned\", XGBoostRegressor({'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105})),\n",
    "    (\"TabNetRegressor_default\", TabNetModelWrapper(n_a=8, n_d=8)),\n",
    "    (\"TabNetRegressor_custom\", TabNetModelWrapper(n_a=32, n_d=32)),\n",
    "    (\"PLSRegression_4_components\", PLSRegression(n_components=4))\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "# Display all combinations\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    print(f\"Continuous Imputer: {continuous_imputer[0]}, Ordinal Imputer: {ordinal_imputer[0]}, Model: {model[0]}\")\n",
    "\n",
    "print(f\"Combinations of preprocessing and models to test : {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = [i for i, val in enumerate(idx_test) if val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = '../pickle/training_4_loona_dict_results.pickle'\n",
    "\n",
    "with open(results_file, \"rb\") as input_file:\n",
    "    all_dict_results = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "test_rids = all_dict_results[0][\"params\"][\"test_rid\"]\n",
    "\n",
    "# Create boolean mask for the test set\n",
    "idx_test = df_all[\"RID\"].isin(test_rids)\n",
    "\n",
    "# Create boolean mask for the train set (all not in test)\n",
    "idx_train = ~idx_test\n",
    "\n",
    "print(idx_test.sum())\n",
    "\n",
    "test_indices = [i for i, val in enumerate(idx_test) if val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([results[\"params\"][\"test_rid\"] == test_rids for results in all_dict_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_dict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34,\n",
       " 369,\n",
       " 455,\n",
       " 501,\n",
       " 530,\n",
       " 551,\n",
       " 908,\n",
       " 1075,\n",
       " 1838,\n",
       " 2033,\n",
       " 2076,\n",
       " 2294,\n",
       " 2346,\n",
       " 2369,\n",
       " 2620,\n",
       " 2709,\n",
       " 2715,\n",
       " 2721,\n",
       " 2813,\n",
       " 2868]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2218423288.py, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[26], line 39\u001b[0;36m\u001b[0m\n\u001b[0;31m    df_X_train = df_X[].loc[idx_train]\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "            \"ordinal_imputer\": name_ordinal_imputer, \n",
    "            \"continuous_imputer\": name_continuous_imputer, \n",
    "            \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "            \"test_shape\": df_X_test.shape,\n",
    "            \"test_rid\": list(df_all.iloc[test_indices])\n",
    "        }\n",
    "\n",
    "    \n",
    "    # Define the subset of keys you care about\n",
    "    keys_to_check = ['ordinal_imputer', 'continuous_imputer', 'model']  # or whatever subset you want\n",
    "    \n",
    "    # Check if a result in all_dict_results has the same values for just those keys\n",
    "    if any(all(result['params'].get(k) == params.get(k) for k in keys_to_check) for result in all_dict_results):\n",
    "        print(f\"Skipping existing combination (subset match): {[params[k] for k in keys_to_check]}\")\n",
    "        continue\n",
    "    \n",
    "    dict_results = {\n",
    "            \"params\": params, \n",
    "            \"imputation_time\": [],\n",
    "            \"fitting_time\": [], \n",
    "            \"results_adj\": [], \n",
    "            \"results_org\": []\n",
    "        }\n",
    "\n",
    "    for test_nloc in test_indices: \n",
    "\n",
    "        idx_train = [True for i in range(df_X.shape[0])]\n",
    "        idx_test = [False for i in range(df_X.shape[0])]\n",
    "\n",
    "        idx_test[test_nloc] = True\n",
    "        idx_train[test_nloc] = False\n",
    "\n",
    "        df_X_train = df_X[].loc[idx_train]\n",
    "        df_X_test = df_X[select_MRIthickness].loc[idx_test]\n",
    "\n",
    "        df_y_train = df_y.loc[idx_train]\n",
    "        df_y_test = df_y.loc[idx_test]\n",
    "\n",
    "        c_train = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_train]\n",
    "        c_test = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_test]\n",
    "\n",
    "        try: \n",
    "        \n",
    "            # Now you can call your `train_model` function with these components\n",
    "            fold_dict_results = train_imputer_model(\n",
    "                df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "                c_train, c_test,\n",
    "                ordinal_imputer_instance, name_ordinal_imputer,\n",
    "                continuous_imputer_instance, name_continuous_imputer,\n",
    "                model_instance, name_model,\n",
    "                separate_imputers=True  # Or however you want to specify\n",
    "            )\n",
    "\n",
    "            dict_results[\"imputation_time\"].append(fold_dict_results[\"imputation_time\"])  \n",
    "            dict_results[\"fitting_time\"].append(fold_dict_results[\"fitting_time\"])  \n",
    "            dict_results[\"results_adj\"].append(fold_dict_results[\"results_adj\"])  \n",
    "            dict_results[\"results_org\"].append(fold_dict_results[\"results_org\"])    \n",
    "\n",
    "        except Exception as e:  \n",
    "\n",
    "            print(e)\n",
    "            \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "    \n",
    "    # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_dict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('..pickle/training_4_loona_dict_results.pickle', \"rb\") as input_file:\n",
    "    dict_results_loo_na = pickle.load(input_file)\n",
    "\n",
    "dict_results_loo_na_copy_failed = dict_results_loo_na"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
