{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.serialization.safe_globals at 0x79c485b63190>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "from itertools import product\n",
    "import warnings\n",
    "\n",
    "# System path modification\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer, MissingIndicator\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, KFold, StratifiedKFold, GroupKFold, StratifiedGroupKFold, LeaveOneOut, cross_validate, cross_val_score\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Lasso, LassoCV, MultiTaskLasso, MultiTaskLassoCV,\n",
    "    ElasticNet, ElasticNetCV, MultiTaskElasticNet, MultiTaskElasticNetCV\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Statistic imports \n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.special import kl_div\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Specialized imputation and visualization packages\n",
    "import miceforest as mf\n",
    "import missingno as msno\n",
    "#from missforest import MissForest\n",
    "#import magic\n",
    "from src.gain import *\n",
    "\n",
    "# Custom modules\n",
    "from src.train import *\n",
    "from src.functions import *\n",
    "from src.plots import *\n",
    "from src.dataset import *\n",
    "from src.multixgboost import *\n",
    "from src.wrapper import *\n",
    "from src.debug import *\n",
    "\n",
    "# Visualizatiokn \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning and machine learning specific \n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "from pytorch_tabular.models import (\n",
    "    GatedAdditiveTreeEnsembleConfig,\n",
    "    DANetConfig,\n",
    "    TabTransformerConfig,\n",
    "    TabNetModelConfig,\n",
    ")\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Print CUDA availability for PyTorch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "torch.serialization.safe_globals([DictConfig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_pickle_data_palettes()\n",
    "\n",
    "results_pickle_folder = \"../pickle/\"\n",
    "\n",
    "# Unpack data\n",
    "df_X, df_y, df_all, df_FinalCombination = data[\"df_X\"], data[\"df_y\"], data[\"df_all\"], data[\"df_FinalCombination\"]\n",
    "dict_select = data[\"dict_select\"]\n",
    "\n",
    "# Unpack colormaps\n",
    "full_palette, gender_palette, dx_palette = data[\"colormaps\"].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "3609    2002\n",
      "5631    4167\n",
      "5662    4176\n",
      "5780    4215\n",
      "5950    4349\n",
      "6069    4292\n",
      "6077    4453\n",
      "6085    4489\n",
      "6224    4505\n",
      "6400    4576\n",
      "6429    4300\n",
      "7021    2374\n",
      "7192    4179\n",
      "Name: RID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "idx_train = list(df_X.isna().any(axis=1))\n",
    "idx_test = list(~df_X.isna().any(axis=1))\n",
    "\n",
    "set_intersect_rid = set(df_all[idx_train].RID).intersection(set(df_all[idx_test].RID))\n",
    "intersect_rid_idx = df_all.RID.isin(set_intersect_rid)\n",
    "\n",
    "for i, bool_test in enumerate(idx_test): \n",
    "    if intersect_rid_idx.iloc[i] & bool_test:\n",
    "        idx_test[i] = False\n",
    "        idx_train[i] = True\n",
    "\n",
    "print(sum(idx_test))\n",
    "\n",
    "print(df_all[idx_test].RID)\n",
    "\n",
    "df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]] = df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]].astype(\"category\")\n",
    "\n",
    "test_indices = [i for i, val in enumerate(idx_test) if val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave-One-Complete-Out (LOCO-CV)\n",
    "\n",
    "## All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "\n",
    "# Continuous Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "continuous_imputer_list = [\n",
    "    (\"KNNImputer\", KNNImputer(n_neighbors=1)),\n",
    "]\n",
    "\n",
    "# Ordinal Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "ordinal_imputer_list = [\n",
    "    (\"SimpleImputer_most_frequent\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "]\n",
    "\n",
    "# Predictive Models List (list of tuples with unique strings and corresponding instances)\n",
    "predictive_models_list = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"MultiTaskElasticNet\", MultiTaskElasticNet()),\n",
    "    (\"MultiTaskElasticNet_tuned\", MultiTaskElasticNet(**{'alpha': 0.01, 'l1_ratio': 0.01})),\n",
    "    (\"MultiTaskLasso\", MultiTaskLasso()),\n",
    "    (\"MultiTaskLasso_tuned\", MultiTaskLasso(**{'alpha': 0.001})),\n",
    "    (\"RandomForestRegressor\", RandomForestRegressor()),\n",
    "    (\"XGBoostRegressor\", XGBoostRegressor()),\n",
    "    (\"XGBoostRegressor_tuned\", XGBoostRegressor(**{'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105})),\n",
    "    (\"TabNetRegressor_default\", TabNetModelWrapper(n_a=8, n_d=8)),\n",
    "    (\"TabNetRegressor_custom\", TabNetModelWrapper(n_a=32, n_d=32)),\n",
    "    (\"PLSRegression_4_components\", PLSRegression(n_components=4))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = ['APOE_epsilon2', 'APOE_epsilon3', 'APOE_epsilon4']\n",
    "continuous_features = [col for col in df_X.columns if col not in ordinal_features]\n",
    "\n",
    "# Prepare Tabular configurations (shared for all PyTorch models)\n",
    "data_config = DataConfig(\n",
    "    target=df_y.columns.tolist(),\n",
    "    continuous_cols=continuous_features,\n",
    "    categorical_cols=ordinal_features\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024, max_epochs=10, auto_lr_find=False,\n",
    "    early_stopping=\"valid_loss\", early_stopping_mode=\"min\", early_stopping_patience=5,\n",
    "    checkpoints=\"valid_loss\", load_best=True, progress_bar=\"nones\",\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "head_config = LinearHeadConfig(dropout=0.1).__dict__\n",
    "\n",
    "predictive_models_list += [\n",
    "    (\"GatedAdditiveTreeEnsembleConfig_tab\", \n",
    "    TabularModelWrapper(\n",
    "        GatedAdditiveTreeEnsembleConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        gflu_stages=6,\n",
    "        gflu_dropout=0.0,\n",
    "        tree_depth=5,\n",
    "        num_trees=20,\n",
    "        chain_trees=False,\n",
    "        share_head_weights=True), data_config, trainer_config, optimizer_config \n",
    "    )),\n",
    "    (\"DANetConfig_tab\",\n",
    "    TabularModelWrapper(\n",
    "        DANetConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_layers=8,\n",
    "        k=5,\n",
    "        dropout_rate=0.1), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabTransformerConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        embedding_initialization=\"kaiming_uniform\",\n",
    "        embedding_bias=False), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabNetModelConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        n_independent=2,\n",
    "        n_shared=2), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: LinearRegression\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskElasticNet\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskElasticNet_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskLasso\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskLasso_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: RandomForestRegressor\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: XGBoostRegressor\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: XGBoostRegressor_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetRegressor_default\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetRegressor_custom\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: PLSRegression_4_components\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: GatedAdditiveTreeEnsembleConfig_tab\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: DANetConfig_tab\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabTransformerConfig_tab\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetModelConfig_tab\n",
      "Combinations of preprocessing and models to test : 15\n"
     ]
    }
   ],
   "source": [
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "# Display all combinations\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    print(f\"Continuous Imputer: {continuous_imputer[0]}, Ordinal Imputer: {ordinal_imputer[0]}, Model: {model[0]}\")\n",
    "\n",
    "print(f\"Combinations of preprocessing and models to test : {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HDF5 file\n",
    "results_file = '../pickle/training_3_loonona_dict_results.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(results_file): \n",
    "    with open(results_file, \"rb\") as input_file:\n",
    "        all_dict_results = pickle.load(input_file)\n",
    "\n",
    "else : \n",
    "    all_dict_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False : \n",
    "    params_comb = [{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'GatedAdditiveTreeEnsembleConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]},\n",
    "    {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'DANetConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]},\n",
    "    {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'TabTransformerConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]},\n",
    "    {'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'TabNetModelConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]},\n",
    "    {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'GatedAdditiveTreeEnsembleConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]},\n",
    "    {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'DANetConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]},\n",
    "    {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabTransformerConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]},\n",
    "    {'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetModelConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]}]\n",
    "\n",
    "    for params in params_comb:\n",
    "        all_dict_results = clean_dict_list(all_dict_results, remove_if_none=False, remove_key_val={\"params\": params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict_results = clean_dict_list(all_dict_results, remove_if_none=False, remove_key_val={\"fitting_time\":None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'LinearRegression']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskElasticNet']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskElasticNet_tuned']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskLasso']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'MultiTaskLasso_tuned']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'RandomForestRegressor']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'XGBoostRegressor']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'XGBoostRegressor_tuned']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'TabNetRegressor_default']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'TabNetRegressor_custom']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'PLSRegression_4_components']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'GatedAdditiveTreeEnsembleConfig_tab']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'DANetConfig_tab']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'TabTransformerConfig_tab']\n",
      "Skipping existing combination (subset match): ['SimpleImputer_most_frequent', 'KNNImputer', 'TabNetModelConfig_tab']\n"
     ]
    }
   ],
   "source": [
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "            \"ordinal_imputer\": name_ordinal_imputer, \n",
    "            \"continuous_imputer\": name_continuous_imputer, \n",
    "            \"model\": name_model, \"train_shape\" : [df_X.shape[0]-1, df_X.shape[1]],\n",
    "            \"test_shape\": [1, df_X.shape[1]]\n",
    "        }\n",
    "    \n",
    "    # Define the subset of keys you care about\n",
    "    keys_to_check = ['ordinal_imputer', 'continuous_imputer', 'model']  # or whatever subset you want\n",
    "\n",
    "    # Check if a result in all_dict_results has the same values for just those keys\n",
    "    if any(all(result['params'].get(k) == params.get(k) for k in keys_to_check) for result in all_dict_results):\n",
    "        print(f\"Skipping existing combination (subset match): {[params[k] for k in keys_to_check]}\")\n",
    "        continue\n",
    "\n",
    "    dict_results = {\n",
    "            \"params\": params, \n",
    "            \"imputation_time\": [],\n",
    "            \"fitting_time\": [], \n",
    "            \"results_adj\": [], \n",
    "            \"results_org\": []\n",
    "        }\n",
    "\n",
    "    for test_nloc in test_indices: \n",
    "        print(test_nloc)\n",
    "\n",
    "        idx_train = [True for i in range(df_X.shape[0])]\n",
    "        idx_test = [False for i in range(df_X.shape[0])]\n",
    "\n",
    "        idx_test[test_nloc] = True\n",
    "        idx_train[test_nloc] = False\n",
    "\n",
    "        df_X_train = df_X.loc[idx_train]\n",
    "        df_X_test = df_X.loc[idx_test]\n",
    "\n",
    "        df_y_train = df_y.loc[idx_train]\n",
    "        df_y_test = df_y.loc[idx_test]\n",
    "\n",
    "        c_train = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_train]\n",
    "        c_test = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_test]\n",
    "\n",
    "        try: \n",
    "        \n",
    "            # Now you can call your `train_model` function with these components\n",
    "            fold_dict_results = train_imputer_model(\n",
    "                df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "                c_train, c_test,\n",
    "                ordinal_imputer_instance, name_ordinal_imputer,\n",
    "                continuous_imputer_instance, name_continuous_imputer,\n",
    "                model_instance, name_model,\n",
    "                separate_imputers=True  # Or however you want to specify\n",
    "            )\n",
    "            \n",
    "            dict_results[\"imputation_time\"].append(fold_dict_results[\"imputation_time\"]) \n",
    "            dict_results[\"fitting_time\"].append(fold_dict_results[\"fitting_time\"])  \n",
    "            dict_results[\"results_adj\"].append(fold_dict_results[\"results_adj\"])  \n",
    "            dict_results[\"results_org\"].append(fold_dict_results[\"results_org\"])  \n",
    "\n",
    "        except Exception as e:  \n",
    "\n",
    "            print(e)\n",
    "            \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "    # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open(results_file, 'wb') as handle:\n",
    "    pickle.dump(all_dict_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../pickle/training_3_loonona_dict_results.pickle'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_file, \"rb\") as input_file:\n",
    "    dict_results_loo_nona = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_nona = pd.json_normalize(dict_results_loo_nona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train only on MRI features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = list(df_X.isna().any(axis=1))\n",
    "idx_test = list(~df_X.isna().any(axis=1))\n",
    "\n",
    "set_intersect_rid = set(df_all[idx_train].RID).intersection(set(df_all[idx_test].RID))\n",
    "intersect_rid_idx = df_all.RID.isin(set_intersect_rid)\n",
    "\n",
    "for i, bool_test in enumerate(idx_test): \n",
    "    if intersect_rid_idx.iloc[i] & bool_test:\n",
    "        idx_test[i] = False\n",
    "        idx_train[i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]] = df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train = df_X[dict_select[\"MRIth\"]].loc[idx_train]\n",
    "df_X_test = df_X[dict_select[\"MRIth\"]].loc[idx_test]\n",
    "\n",
    "df_y_train = df_y.loc[idx_train]\n",
    "df_y_test = df_y.loc[idx_test]\n",
    "\n",
    "c_train = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_train]\n",
    "c_test = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "n_imputation_iter = 10\n",
    "\n",
    "# Continuous Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "continuous_imputer_list = [\n",
    "    (\"NoImputer\", KNNImputer(n_neighbors=1)),\n",
    "\n",
    "]\n",
    "\n",
    "# Ordinal Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "ordinal_imputer_list = [\n",
    "    (\"NoImputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "]\n",
    "\n",
    "# Predictive Models List (list of tuples with unique strings and corresponding instances)\n",
    "predictive_models_list = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"MultiTaskElasticNet\", MultiTaskElasticNet()),\n",
    "    (\"MultiTaskElasticNet_tuned\", MultiTaskElasticNet(**{'alpha': 0.01, 'l1_ratio': 0.01})),\n",
    "    (\"MultiTaskLasso\", MultiTaskLasso()),\n",
    "    (\"MultiTaskLasso_tuned\", MultiTaskLasso(**{'alpha': 0.001})),\n",
    "    (\"RandomForestRegressor\", RandomForestRegressor()),\n",
    "    (\"XGBoostRegressor\", XGBoostRegressor()),\n",
    "    (\"XGBoostRegressor_tuned\", XGBoostRegressor(**{'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105})),\n",
    "    (\"TabNetRegressor_default\", TabNetModelWrapper(n_a=8, n_d=8)),\n",
    "    (\"TabNetRegressor_custom\", TabNetModelWrapper(n_a=32, n_d=32)),\n",
    "    (\"PLSRegression_4_components\", PLSRegression(n_components=4))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = ['APOE_epsilon2', 'APOE_epsilon3', 'APOE_epsilon4']\n",
    "continuous_features = [col for col in df_X_train.columns if col not in ordinal_features]\n",
    "\n",
    "# Prepare Tabular configurations (shared for all PyTorch models)\n",
    "data_config = DataConfig(\n",
    "    target=df_y_train.columns.tolist(),\n",
    "    continuous_cols=dict_select[\"MRIth\"],\n",
    "    categorical_cols=[]\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024, max_epochs=10, auto_lr_find=True,\n",
    "    early_stopping=\"valid_loss\", early_stopping_mode=\"min\", early_stopping_patience=5,\n",
    "    checkpoints=\"valid_loss\", load_best=True, progress_bar=\"nones\",\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "head_config = LinearHeadConfig(dropout=0.1).__dict__\n",
    "predictive_models_list += [\n",
    "    (\"GatedAdditiveTreeEnsembleConfig_tab\", \n",
    "    TabularModelWrapper(\n",
    "        GatedAdditiveTreeEnsembleConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        gflu_stages=6,\n",
    "        gflu_dropout=0.0,\n",
    "        tree_depth=5,\n",
    "        num_trees=20,\n",
    "        chain_trees=False,\n",
    "        share_head_weights=True), data_config, trainer_config, optimizer_config \n",
    "    )),\n",
    "    (\"DANetConfig_tab\",\n",
    "    TabularModelWrapper(\n",
    "        DANetConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_layers=8,\n",
    "        k=5,\n",
    "        dropout_rate=0.1), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabTransformerConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        embedding_initialization=\"kaiming_uniform\",\n",
    "        embedding_bias=False), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabNetModelConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        n_independent=2,\n",
    "        n_shared=2), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: LinearRegression\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskElasticNet\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskElasticNet_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskLasso\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskLasso_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: RandomForestRegressor\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: XGBoostRegressor\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: XGBoostRegressor_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: TabNetRegressor_default\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: TabNetRegressor_custom\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: PLSRegression_4_components\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: GatedAdditiveTreeEnsembleConfig_tab\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: DANetConfig_tab\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: TabTransformerConfig_tab\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: TabNetModelConfig_tab\n",
      "Combinations of preprocessing and models to test : 15\n"
     ]
    }
   ],
   "source": [
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "# Display all combinations\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    print(f\"Continuous Imputer: {continuous_imputer[0]}, Ordinal Imputer: {ordinal_imputer[0]}, Model: {model[0]}\")\n",
    "\n",
    "print(f\"Combinations of preprocessing and models to test : {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HDF5 file\n",
    "results_file = '../pickle/training_3_loonona_dict_results.pickle'\n",
    "\n",
    "with open('../pickle/training_3_loonona_dict_results.pickle', \"rb\") as input_file:\n",
    "    all_dict_results = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict_results = clean_dict_list(all_dict_results, remove_if_none=False, remove_key_val={\"fitting_time\":None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'LinearRegression', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n",
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'MultiTaskElasticNet', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n",
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'MultiTaskElasticNet_tuned', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n",
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'MultiTaskLasso', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n",
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'MultiTaskLasso_tuned', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n",
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'RandomForestRegressor', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n",
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'XGBoostRegressor', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n",
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'XGBoostRegressor_tuned', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n",
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'TabNetRegressor_default', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n",
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'TabNetRegressor_custom', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n",
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'PLSRegression_4_components', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n",
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'GatedAdditiveTreeEnsembleConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n",
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'DANetConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n",
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'TabTransformerConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n",
      "{'ordinal_imputer': 'SimpleImputer_most_frequent', 'continuous_imputer': 'KNNImputer', 'model': 'TabNetModelConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]}\n"
     ]
    }
   ],
   "source": [
    "for res in all_dict_results: \n",
    "    print(res[\"params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.debug import *\n",
    "\n",
    "rm_combinations = [{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'LinearRegression', 'train_shape': (2881, 200), 'test_shape': (13, 200)},\n",
    "{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskElasticNet', 'train_shape': (2881, 200), 'test_shape': (13, 200)},\n",
    "{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskLasso', 'train_shape': (2881, 200), 'test_shape': (13, 200)},\n",
    "{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'RandomForestRegressor', 'train_shape': (2881, 200), 'test_shape': (13, 200)},\n",
    "{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'XGBoostRegressor', 'train_shape': (2881, 200), 'test_shape': (13, 200)},\n",
    "{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'XGBoostRegressor_tuned', 'train_shape': (2881, 200), 'test_shape': (13, 200)},\n",
    "{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetRegressor_default', 'train_shape': (2881, 200), 'test_shape': (13, 200)},\n",
    "{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetRegressor_custom', 'train_shape': (2881, 200), 'test_shape': (13, 200)},\n",
    "{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'PLSRegression_4_components', 'train_shape': (2881, 200), 'test_shape': (13, 200)},\n",
    "{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskElasticNet_tuned', 'train_shape': [2893, 348], 'test_shape': [1, 348]},\n",
    "{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'MultiTaskLasso_tuned', 'train_shape': [2893, 348], 'test_shape': [1, 348]},\n",
    "{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'GatedAdditiveTreeEnsembleConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]},\n",
    "{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'DANetConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]},\n",
    "{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabTransformerConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]},\n",
    "{'ordinal_imputer': 'NoImputer', 'continuous_imputer': 'NoImputer', 'model': 'TabNetModelConfig_tab', 'train_shape': [2893, 348], 'test_shape': [1, 348]}, \n",
    "]\n",
    "if False : \n",
    "    for par in rm_combinations:\n",
    "        all_dict_results = clean_dict_list(all_dict_results, remove_if_none=False, remove_key_val={\"params\":par})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.10727 |  0:00:00s\n",
      "epoch 1  | loss: 1.81376 |  0:00:00s\n",
      "epoch 2  | loss: 1.40002 |  0:00:00s\n",
      "epoch 3  | loss: 1.10376 |  0:00:00s\n",
      "epoch 4  | loss: 1.02352 |  0:00:00s\n",
      "epoch 5  | loss: 0.97043 |  0:00:00s\n",
      "epoch 6  | loss: 0.91183 |  0:00:00s\n",
      "epoch 7  | loss: 0.89902 |  0:00:00s\n",
      "epoch 8  | loss: 0.86666 |  0:00:00s\n",
      "epoch 9  | loss: 0.82716 |  0:00:00s\n",
      "epoch 10 | loss: 0.80379 |  0:00:00s\n",
      "epoch 11 | loss: 0.79401 |  0:00:00s\n",
      "epoch 12 | loss: 0.79482 |  0:00:00s\n",
      "epoch 13 | loss: 0.77716 |  0:00:01s\n",
      "epoch 14 | loss: 0.76278 |  0:00:01s\n",
      "epoch 15 | loss: 0.74362 |  0:00:01s\n",
      "epoch 16 | loss: 0.74461 |  0:00:01s\n",
      "epoch 17 | loss: 0.72961 |  0:00:01s\n",
      "epoch 18 | loss: 0.72244 |  0:00:01s\n",
      "epoch 19 | loss: 0.70323 |  0:00:01s\n",
      "epoch 20 | loss: 0.71614 |  0:00:01s\n",
      "epoch 21 | loss: 0.70446 |  0:00:01s\n",
      "epoch 22 | loss: 0.72803 |  0:00:01s\n",
      "epoch 23 | loss: 0.72458 |  0:00:01s\n",
      "epoch 24 | loss: 0.71645 |  0:00:01s\n",
      "epoch 25 | loss: 0.69794 |  0:00:01s\n",
      "epoch 26 | loss: 0.69704 |  0:00:01s\n",
      "epoch 27 | loss: 0.69361 |  0:00:01s\n",
      "epoch 28 | loss: 0.69174 |  0:00:01s\n",
      "epoch 29 | loss: 0.68468 |  0:00:01s\n",
      "epoch 30 | loss: 0.66944 |  0:00:01s\n",
      "epoch 31 | loss: 0.67214 |  0:00:01s\n",
      "epoch 32 | loss: 0.67583 |  0:00:01s\n",
      "epoch 33 | loss: 0.66716 |  0:00:02s\n",
      "epoch 34 | loss: 0.66334 |  0:00:02s\n",
      "epoch 35 | loss: 0.6606  |  0:00:02s\n",
      "epoch 36 | loss: 0.66445 |  0:00:02s\n",
      "epoch 37 | loss: 0.65864 |  0:00:02s\n",
      "epoch 38 | loss: 0.65699 |  0:00:02s\n",
      "epoch 39 | loss: 0.66604 |  0:00:02s\n",
      "epoch 40 | loss: 0.65377 |  0:00:02s\n",
      "epoch 41 | loss: 0.64533 |  0:00:02s\n",
      "epoch 42 | loss: 0.64625 |  0:00:02s\n",
      "epoch 43 | loss: 0.64419 |  0:00:02s\n",
      "epoch 44 | loss: 0.64157 |  0:00:02s\n",
      "epoch 45 | loss: 0.63302 |  0:00:02s\n",
      "epoch 46 | loss: 0.62952 |  0:00:02s\n",
      "epoch 47 | loss: 0.62091 |  0:00:02s\n",
      "epoch 48 | loss: 0.62409 |  0:00:02s\n",
      "epoch 49 | loss: 0.6233  |  0:00:02s\n",
      "epoch 50 | loss: 0.61619 |  0:00:02s\n",
      "epoch 51 | loss: 0.61077 |  0:00:02s\n",
      "epoch 52 | loss: 0.61618 |  0:00:02s\n",
      "epoch 53 | loss: 0.60186 |  0:00:02s\n",
      "epoch 54 | loss: 0.60865 |  0:00:02s\n",
      "epoch 55 | loss: 0.61152 |  0:00:03s\n",
      "epoch 56 | loss: 0.60383 |  0:00:03s\n",
      "epoch 57 | loss: 0.59917 |  0:00:03s\n",
      "epoch 58 | loss: 0.59786 |  0:00:03s\n",
      "epoch 59 | loss: 0.59041 |  0:00:03s\n",
      "epoch 60 | loss: 0.59098 |  0:00:03s\n",
      "epoch 61 | loss: 0.59438 |  0:00:03s\n",
      "epoch 62 | loss: 0.58536 |  0:00:03s\n",
      "epoch 63 | loss: 0.58445 |  0:00:03s\n",
      "epoch 64 | loss: 0.58633 |  0:00:03s\n",
      "epoch 65 | loss: 0.59245 |  0:00:03s\n",
      "epoch 66 | loss: 0.58223 |  0:00:03s\n",
      "epoch 67 | loss: 0.57634 |  0:00:03s\n",
      "epoch 68 | loss: 0.5911  |  0:00:03s\n",
      "epoch 69 | loss: 0.58081 |  0:00:03s\n",
      "epoch 70 | loss: 0.58173 |  0:00:03s\n",
      "epoch 71 | loss: 0.57844 |  0:00:03s\n",
      "epoch 72 | loss: 0.57845 |  0:00:03s\n",
      "epoch 73 | loss: 0.57741 |  0:00:03s\n",
      "epoch 74 | loss: 0.57134 |  0:00:03s\n",
      "epoch 75 | loss: 0.57551 |  0:00:03s\n",
      "epoch 76 | loss: 0.58357 |  0:00:03s\n",
      "epoch 77 | loss: 0.58332 |  0:00:04s\n",
      "epoch 78 | loss: 0.57899 |  0:00:04s\n",
      "epoch 79 | loss: 0.57436 |  0:00:04s\n",
      "epoch 80 | loss: 0.57996 |  0:00:04s\n",
      "epoch 81 | loss: 0.5639  |  0:00:04s\n",
      "epoch 82 | loss: 0.55915 |  0:00:04s\n",
      "epoch 83 | loss: 0.55802 |  0:00:04s\n",
      "epoch 84 | loss: 0.55843 |  0:00:04s\n",
      "epoch 85 | loss: 0.55629 |  0:00:04s\n",
      "epoch 86 | loss: 0.55241 |  0:00:04s\n",
      "epoch 87 | loss: 0.55312 |  0:00:04s\n",
      "epoch 88 | loss: 0.55277 |  0:00:04s\n",
      "epoch 89 | loss: 0.55622 |  0:00:04s\n",
      "epoch 90 | loss: 0.56237 |  0:00:04s\n",
      "epoch 91 | loss: 0.55782 |  0:00:04s\n",
      "epoch 92 | loss: 0.55274 |  0:00:04s\n",
      "epoch 93 | loss: 0.55053 |  0:00:04s\n",
      "epoch 94 | loss: 0.54504 |  0:00:04s\n",
      "epoch 95 | loss: 0.54422 |  0:00:04s\n",
      "epoch 96 | loss: 0.54183 |  0:00:04s\n",
      "epoch 97 | loss: 0.56032 |  0:00:04s\n",
      "epoch 98 | loss: 0.54053 |  0:00:04s\n",
      "epoch 99 | loss: 0.56149 |  0:00:05s\n",
      "epoch 100| loss: 0.57692 |  0:00:05s\n",
      "epoch 101| loss: 0.58133 |  0:00:05s\n",
      "epoch 102| loss: 0.56638 |  0:00:05s\n",
      "epoch 103| loss: 0.55534 |  0:00:05s\n",
      "epoch 104| loss: 0.57364 |  0:00:05s\n",
      "epoch 105| loss: 0.55764 |  0:00:05s\n",
      "epoch 106| loss: 0.55141 |  0:00:05s\n",
      "epoch 107| loss: 0.54821 |  0:00:05s\n",
      "epoch 108| loss: 0.54823 |  0:00:05s\n",
      "epoch 109| loss: 0.53885 |  0:00:05s\n",
      "epoch 110| loss: 0.5387  |  0:00:05s\n",
      "epoch 111| loss: 0.53012 |  0:00:05s\n",
      "epoch 112| loss: 0.53647 |  0:00:05s\n",
      "epoch 113| loss: 0.52862 |  0:00:05s\n",
      "epoch 114| loss: 0.52731 |  0:00:05s\n",
      "epoch 115| loss: 0.52311 |  0:00:05s\n",
      "epoch 116| loss: 0.51487 |  0:00:05s\n",
      "epoch 117| loss: 0.51878 |  0:00:05s\n",
      "epoch 118| loss: 0.51739 |  0:00:05s\n",
      "epoch 119| loss: 0.52118 |  0:00:05s\n",
      "epoch 120| loss: 0.52062 |  0:00:05s\n",
      "epoch 121| loss: 0.52398 |  0:00:05s\n",
      "epoch 122| loss: 0.52062 |  0:00:06s\n",
      "epoch 123| loss: 0.51862 |  0:00:06s\n",
      "epoch 124| loss: 0.52113 |  0:00:06s\n",
      "epoch 125| loss: 0.5196  |  0:00:06s\n",
      "epoch 126| loss: 0.50567 |  0:00:06s\n",
      "epoch 127| loss: 0.50697 |  0:00:06s\n",
      "epoch 128| loss: 0.51666 |  0:00:06s\n",
      "epoch 129| loss: 0.50417 |  0:00:06s\n",
      "epoch 130| loss: 0.50477 |  0:00:06s\n",
      "epoch 131| loss: 0.50106 |  0:00:06s\n",
      "epoch 132| loss: 0.50245 |  0:00:06s\n",
      "epoch 133| loss: 0.49708 |  0:00:06s\n",
      "epoch 134| loss: 0.49706 |  0:00:06s\n",
      "epoch 135| loss: 0.48899 |  0:00:06s\n",
      "epoch 136| loss: 0.49991 |  0:00:06s\n",
      "epoch 137| loss: 0.49843 |  0:00:06s\n",
      "epoch 138| loss: 0.50201 |  0:00:06s\n",
      "epoch 139| loss: 0.49119 |  0:00:06s\n",
      "epoch 140| loss: 0.49351 |  0:00:06s\n",
      "epoch 141| loss: 0.50573 |  0:00:06s\n",
      "epoch 142| loss: 0.50631 |  0:00:06s\n",
      "epoch 143| loss: 0.50355 |  0:00:06s\n",
      "epoch 144| loss: 0.50176 |  0:00:07s\n",
      "epoch 145| loss: 0.491   |  0:00:07s\n",
      "epoch 146| loss: 0.49505 |  0:00:07s\n",
      "epoch 147| loss: 0.49273 |  0:00:07s\n",
      "epoch 148| loss: 0.48668 |  0:00:07s\n",
      "epoch 149| loss: 0.48989 |  0:00:07s\n",
      "epoch 150| loss: 0.48481 |  0:00:07s\n",
      "epoch 151| loss: 0.48526 |  0:00:07s\n",
      "epoch 152| loss: 0.47924 |  0:00:07s\n",
      "epoch 153| loss: 0.47945 |  0:00:07s\n",
      "epoch 154| loss: 0.47222 |  0:00:07s\n",
      "epoch 155| loss: 0.47777 |  0:00:07s\n",
      "epoch 156| loss: 0.47214 |  0:00:07s\n",
      "epoch 157| loss: 0.47374 |  0:00:07s\n",
      "epoch 158| loss: 0.46716 |  0:00:07s\n",
      "epoch 159| loss: 0.46853 |  0:00:07s\n",
      "epoch 160| loss: 0.46926 |  0:00:07s\n",
      "epoch 161| loss: 0.46729 |  0:00:07s\n",
      "epoch 162| loss: 0.46919 |  0:00:07s\n",
      "epoch 163| loss: 0.4658  |  0:00:07s\n",
      "epoch 164| loss: 0.45886 |  0:00:07s\n",
      "epoch 165| loss: 0.46534 |  0:00:08s\n",
      "epoch 166| loss: 0.46896 |  0:00:08s\n",
      "epoch 167| loss: 0.46279 |  0:00:08s\n",
      "epoch 168| loss: 0.46146 |  0:00:08s\n",
      "epoch 169| loss: 0.45222 |  0:00:08s\n",
      "epoch 170| loss: 0.4541  |  0:00:08s\n",
      "epoch 171| loss: 0.4494  |  0:00:08s\n",
      "epoch 172| loss: 0.45131 |  0:00:08s\n",
      "epoch 173| loss: 0.45515 |  0:00:08s\n",
      "epoch 174| loss: 0.45909 |  0:00:08s\n",
      "epoch 175| loss: 0.4476  |  0:00:08s\n",
      "epoch 176| loss: 0.46047 |  0:00:08s\n",
      "epoch 177| loss: 0.46057 |  0:00:08s\n",
      "epoch 178| loss: 0.44505 |  0:00:08s\n",
      "epoch 179| loss: 0.44503 |  0:00:08s\n",
      "epoch 180| loss: 0.4476  |  0:00:08s\n",
      "epoch 181| loss: 0.43944 |  0:00:08s\n",
      "epoch 182| loss: 0.44231 |  0:00:08s\n",
      "epoch 183| loss: 0.44669 |  0:00:08s\n",
      "epoch 184| loss: 0.44002 |  0:00:09s\n",
      "epoch 185| loss: 0.44027 |  0:00:09s\n",
      "epoch 186| loss: 0.44622 |  0:00:09s\n",
      "epoch 187| loss: 0.43779 |  0:00:09s\n",
      "epoch 188| loss: 0.43893 |  0:00:09s\n",
      "epoch 189| loss: 0.43406 |  0:00:09s\n",
      "epoch 190| loss: 0.43674 |  0:00:09s\n",
      "epoch 191| loss: 0.43716 |  0:00:09s\n",
      "epoch 192| loss: 0.45805 |  0:00:09s\n",
      "epoch 193| loss: 0.45094 |  0:00:09s\n",
      "epoch 194| loss: 0.45074 |  0:00:09s\n",
      "epoch 195| loss: 0.4528  |  0:00:09s\n",
      "epoch 196| loss: 0.448   |  0:00:09s\n",
      "epoch 197| loss: 0.44806 |  0:00:09s\n",
      "epoch 198| loss: 0.44231 |  0:00:09s\n",
      "epoch 199| loss: 0.43502 |  0:00:09s\n",
      "epoch 200| loss: 0.43284 |  0:00:09s\n",
      "epoch 201| loss: 0.43934 |  0:00:09s\n",
      "epoch 202| loss: 0.43715 |  0:00:09s\n",
      "epoch 203| loss: 0.4372  |  0:00:09s\n",
      "epoch 204| loss: 0.43494 |  0:00:09s\n",
      "epoch 205| loss: 0.43703 |  0:00:09s\n",
      "epoch 206| loss: 0.43113 |  0:00:10s\n",
      "epoch 207| loss: 0.42905 |  0:00:10s\n",
      "epoch 208| loss: 0.42849 |  0:00:10s\n",
      "epoch 209| loss: 0.43457 |  0:00:10s\n",
      "epoch 210| loss: 0.42982 |  0:00:10s\n",
      "epoch 211| loss: 0.42431 |  0:00:10s\n",
      "epoch 212| loss: 0.42439 |  0:00:10s\n",
      "epoch 213| loss: 0.43014 |  0:00:10s\n",
      "epoch 214| loss: 0.41815 |  0:00:10s\n",
      "epoch 215| loss: 0.42128 |  0:00:10s\n",
      "epoch 216| loss: 0.41919 |  0:00:10s\n",
      "epoch 217| loss: 0.41709 |  0:00:10s\n",
      "epoch 218| loss: 0.40918 |  0:00:10s\n",
      "epoch 219| loss: 0.41329 |  0:00:10s\n",
      "epoch 220| loss: 0.42149 |  0:00:10s\n",
      "epoch 221| loss: 0.41115 |  0:00:10s\n",
      "epoch 222| loss: 0.41525 |  0:00:10s\n",
      "epoch 223| loss: 0.40977 |  0:00:10s\n",
      "epoch 224| loss: 0.41006 |  0:00:10s\n",
      "epoch 225| loss: 0.42195 |  0:00:10s\n",
      "epoch 226| loss: 0.41967 |  0:00:10s\n",
      "epoch 227| loss: 0.41834 |  0:00:10s\n",
      "epoch 228| loss: 0.41878 |  0:00:10s\n",
      "epoch 229| loss: 0.41454 |  0:00:11s\n",
      "epoch 230| loss: 0.4203  |  0:00:11s\n",
      "epoch 231| loss: 0.41827 |  0:00:11s\n",
      "epoch 232| loss: 0.41426 |  0:00:11s\n",
      "epoch 233| loss: 0.41341 |  0:00:11s\n",
      "epoch 234| loss: 0.41504 |  0:00:11s\n",
      "epoch 235| loss: 0.41382 |  0:00:11s\n",
      "epoch 236| loss: 0.41632 |  0:00:11s\n",
      "epoch 237| loss: 0.40699 |  0:00:11s\n",
      "epoch 238| loss: 0.41648 |  0:00:11s\n",
      "epoch 239| loss: 0.40644 |  0:00:11s\n",
      "epoch 240| loss: 0.41151 |  0:00:11s\n",
      "epoch 241| loss: 0.40366 |  0:00:11s\n",
      "epoch 242| loss: 0.40778 |  0:00:11s\n",
      "epoch 243| loss: 0.39894 |  0:00:11s\n",
      "epoch 244| loss: 0.39578 |  0:00:11s\n",
      "epoch 245| loss: 0.39322 |  0:00:11s\n",
      "epoch 246| loss: 0.38947 |  0:00:11s\n",
      "epoch 247| loss: 0.39362 |  0:00:11s\n",
      "epoch 248| loss: 0.39321 |  0:00:11s\n",
      "epoch 249| loss: 0.39319 |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.21332 |  0:00:00s\n",
      "epoch 1  | loss: 1.74399 |  0:00:00s\n",
      "epoch 2  | loss: 1.27526 |  0:00:00s\n",
      "epoch 3  | loss: 1.11502 |  0:00:00s\n",
      "epoch 4  | loss: 1.01473 |  0:00:00s\n",
      "epoch 5  | loss: 0.94666 |  0:00:00s\n",
      "epoch 6  | loss: 0.91709 |  0:00:00s\n",
      "epoch 7  | loss: 0.9093  |  0:00:00s\n",
      "epoch 8  | loss: 0.89101 |  0:00:00s\n",
      "epoch 9  | loss: 0.8755  |  0:00:00s\n",
      "epoch 10 | loss: 0.84547 |  0:00:00s\n",
      "epoch 11 | loss: 0.81191 |  0:00:00s\n",
      "epoch 12 | loss: 0.78913 |  0:00:00s\n",
      "epoch 13 | loss: 0.7813  |  0:00:00s\n",
      "epoch 14 | loss: 0.75617 |  0:00:00s\n",
      "epoch 15 | loss: 0.76141 |  0:00:00s\n",
      "epoch 16 | loss: 0.7441  |  0:00:00s\n",
      "epoch 17 | loss: 0.73146 |  0:00:00s\n",
      "epoch 18 | loss: 0.73093 |  0:00:00s\n",
      "epoch 19 | loss: 0.73338 |  0:00:00s\n",
      "epoch 20 | loss: 0.72826 |  0:00:00s\n",
      "epoch 21 | loss: 0.71175 |  0:00:01s\n",
      "epoch 22 | loss: 0.6976  |  0:00:01s\n",
      "epoch 23 | loss: 0.69698 |  0:00:01s\n",
      "epoch 24 | loss: 0.69846 |  0:00:01s\n",
      "epoch 25 | loss: 0.68915 |  0:00:01s\n",
      "epoch 26 | loss: 0.68334 |  0:00:01s\n",
      "epoch 27 | loss: 0.67305 |  0:00:01s\n",
      "epoch 28 | loss: 0.67746 |  0:00:01s\n",
      "epoch 29 | loss: 0.67015 |  0:00:01s\n",
      "epoch 30 | loss: 0.66555 |  0:00:01s\n",
      "epoch 31 | loss: 0.66023 |  0:00:01s\n",
      "epoch 32 | loss: 0.65139 |  0:00:01s\n",
      "epoch 33 | loss: 0.64648 |  0:00:01s\n",
      "epoch 34 | loss: 0.64848 |  0:00:01s\n",
      "epoch 35 | loss: 0.64393 |  0:00:01s\n",
      "epoch 36 | loss: 0.63925 |  0:00:01s\n",
      "epoch 37 | loss: 0.64044 |  0:00:01s\n",
      "epoch 38 | loss: 0.63902 |  0:00:01s\n",
      "epoch 39 | loss: 0.63563 |  0:00:01s\n",
      "epoch 40 | loss: 0.62866 |  0:00:01s\n",
      "epoch 41 | loss: 0.63193 |  0:00:01s\n",
      "epoch 42 | loss: 0.62997 |  0:00:01s\n",
      "epoch 43 | loss: 0.62828 |  0:00:02s\n",
      "epoch 44 | loss: 0.62296 |  0:00:02s\n",
      "epoch 45 | loss: 0.62777 |  0:00:02s\n",
      "epoch 46 | loss: 0.60954 |  0:00:02s\n",
      "epoch 47 | loss: 0.62959 |  0:00:02s\n",
      "epoch 48 | loss: 0.61447 |  0:00:02s\n",
      "epoch 49 | loss: 0.62416 |  0:00:02s\n",
      "epoch 50 | loss: 0.61346 |  0:00:02s\n",
      "epoch 51 | loss: 0.61531 |  0:00:02s\n",
      "epoch 52 | loss: 0.60532 |  0:00:02s\n",
      "epoch 53 | loss: 0.60403 |  0:00:02s\n",
      "epoch 54 | loss: 0.60301 |  0:00:02s\n",
      "epoch 55 | loss: 0.6086  |  0:00:02s\n",
      "epoch 56 | loss: 0.59678 |  0:00:02s\n",
      "epoch 57 | loss: 0.58794 |  0:00:02s\n",
      "epoch 58 | loss: 0.58315 |  0:00:02s\n",
      "epoch 59 | loss: 0.58055 |  0:00:02s\n",
      "epoch 60 | loss: 0.57904 |  0:00:02s\n",
      "epoch 61 | loss: 0.57996 |  0:00:02s\n",
      "epoch 62 | loss: 0.58103 |  0:00:02s\n",
      "epoch 63 | loss: 0.58108 |  0:00:02s\n",
      "epoch 64 | loss: 0.58514 |  0:00:02s\n",
      "epoch 65 | loss: 0.57626 |  0:00:02s\n",
      "epoch 66 | loss: 0.56813 |  0:00:03s\n",
      "epoch 67 | loss: 0.57981 |  0:00:03s\n",
      "epoch 68 | loss: 0.56928 |  0:00:03s\n",
      "epoch 69 | loss: 0.56715 |  0:00:03s\n",
      "epoch 70 | loss: 0.56997 |  0:00:03s\n",
      "epoch 71 | loss: 0.56333 |  0:00:03s\n",
      "epoch 72 | loss: 0.56185 |  0:00:03s\n",
      "epoch 73 | loss: 0.55177 |  0:00:03s\n",
      "epoch 74 | loss: 0.55775 |  0:00:03s\n",
      "epoch 75 | loss: 0.54938 |  0:00:03s\n",
      "epoch 76 | loss: 0.55908 |  0:00:03s\n",
      "epoch 77 | loss: 0.55538 |  0:00:03s\n",
      "epoch 78 | loss: 0.54704 |  0:00:03s\n",
      "epoch 79 | loss: 0.54142 |  0:00:03s\n",
      "epoch 80 | loss: 0.55225 |  0:00:03s\n",
      "epoch 81 | loss: 0.54905 |  0:00:03s\n",
      "epoch 82 | loss: 0.54485 |  0:00:03s\n",
      "epoch 83 | loss: 0.54897 |  0:00:03s\n",
      "epoch 84 | loss: 0.54858 |  0:00:03s\n",
      "epoch 85 | loss: 0.55137 |  0:00:03s\n",
      "epoch 86 | loss: 0.53835 |  0:00:03s\n",
      "epoch 87 | loss: 0.54665 |  0:00:03s\n",
      "epoch 88 | loss: 0.54344 |  0:00:04s\n",
      "epoch 89 | loss: 0.54874 |  0:00:04s\n",
      "epoch 90 | loss: 0.54238 |  0:00:04s\n",
      "epoch 91 | loss: 0.53716 |  0:00:04s\n",
      "epoch 92 | loss: 0.53439 |  0:00:04s\n",
      "epoch 93 | loss: 0.53509 |  0:00:04s\n",
      "epoch 94 | loss: 0.51904 |  0:00:04s\n",
      "epoch 95 | loss: 0.5311  |  0:00:04s\n",
      "epoch 96 | loss: 0.51734 |  0:00:04s\n",
      "epoch 97 | loss: 0.52471 |  0:00:04s\n",
      "epoch 98 | loss: 0.51968 |  0:00:04s\n",
      "epoch 99 | loss: 0.52512 |  0:00:04s\n",
      "epoch 100| loss: 0.51425 |  0:00:04s\n",
      "epoch 101| loss: 0.51297 |  0:00:04s\n",
      "epoch 102| loss: 0.51173 |  0:00:04s\n",
      "epoch 103| loss: 0.50902 |  0:00:04s\n",
      "epoch 104| loss: 0.50322 |  0:00:04s\n",
      "epoch 105| loss: 0.50033 |  0:00:04s\n",
      "epoch 106| loss: 0.49705 |  0:00:04s\n",
      "epoch 107| loss: 0.4993  |  0:00:04s\n",
      "epoch 108| loss: 0.49684 |  0:00:04s\n",
      "epoch 109| loss: 0.49207 |  0:00:04s\n",
      "epoch 110| loss: 0.49333 |  0:00:05s\n",
      "epoch 111| loss: 0.48878 |  0:00:05s\n",
      "epoch 112| loss: 0.49227 |  0:00:05s\n",
      "epoch 113| loss: 0.4879  |  0:00:05s\n",
      "epoch 114| loss: 0.48497 |  0:00:05s\n",
      "epoch 115| loss: 0.48923 |  0:00:05s\n",
      "epoch 116| loss: 0.48029 |  0:00:05s\n",
      "epoch 117| loss: 0.48308 |  0:00:05s\n",
      "epoch 118| loss: 0.47293 |  0:00:05s\n",
      "epoch 119| loss: 0.47758 |  0:00:05s\n",
      "epoch 120| loss: 0.469   |  0:00:05s\n",
      "epoch 121| loss: 0.46323 |  0:00:05s\n",
      "epoch 122| loss: 0.46951 |  0:00:05s\n",
      "epoch 123| loss: 0.47008 |  0:00:05s\n",
      "epoch 124| loss: 0.46344 |  0:00:05s\n",
      "epoch 125| loss: 0.46258 |  0:00:05s\n",
      "epoch 126| loss: 0.46212 |  0:00:05s\n",
      "epoch 127| loss: 0.45971 |  0:00:05s\n",
      "epoch 128| loss: 0.46686 |  0:00:05s\n",
      "epoch 129| loss: 0.46578 |  0:00:06s\n",
      "epoch 130| loss: 0.46025 |  0:00:06s\n",
      "epoch 131| loss: 0.45785 |  0:00:06s\n",
      "epoch 132| loss: 0.45126 |  0:00:06s\n",
      "epoch 133| loss: 0.45561 |  0:00:06s\n",
      "epoch 134| loss: 0.45098 |  0:00:06s\n",
      "epoch 135| loss: 0.44835 |  0:00:06s\n",
      "epoch 136| loss: 0.44751 |  0:00:06s\n",
      "epoch 137| loss: 0.44728 |  0:00:06s\n",
      "epoch 138| loss: 0.44602 |  0:00:06s\n",
      "epoch 139| loss: 0.44609 |  0:00:06s\n",
      "epoch 140| loss: 0.43642 |  0:00:06s\n",
      "epoch 141| loss: 0.43569 |  0:00:06s\n",
      "epoch 142| loss: 0.43548 |  0:00:06s\n",
      "epoch 143| loss: 0.43317 |  0:00:06s\n",
      "epoch 144| loss: 0.4303  |  0:00:06s\n",
      "epoch 145| loss: 0.43486 |  0:00:06s\n",
      "epoch 146| loss: 0.42748 |  0:00:06s\n",
      "epoch 147| loss: 0.43242 |  0:00:06s\n",
      "epoch 148| loss: 0.43202 |  0:00:06s\n",
      "epoch 149| loss: 0.42666 |  0:00:06s\n",
      "epoch 150| loss: 0.42352 |  0:00:06s\n",
      "epoch 151| loss: 0.42823 |  0:00:07s\n",
      "epoch 152| loss: 0.42885 |  0:00:07s\n",
      "epoch 153| loss: 0.42681 |  0:00:07s\n",
      "epoch 154| loss: 0.41822 |  0:00:07s\n",
      "epoch 155| loss: 0.41967 |  0:00:07s\n",
      "epoch 156| loss: 0.42833 |  0:00:07s\n",
      "epoch 157| loss: 0.42781 |  0:00:07s\n",
      "epoch 158| loss: 0.41832 |  0:00:07s\n",
      "epoch 159| loss: 0.41401 |  0:00:07s\n",
      "epoch 160| loss: 0.4181  |  0:00:07s\n",
      "epoch 161| loss: 0.40876 |  0:00:07s\n",
      "epoch 162| loss: 0.41791 |  0:00:07s\n",
      "epoch 163| loss: 0.41407 |  0:00:07s\n",
      "epoch 164| loss: 0.42292 |  0:00:07s\n",
      "epoch 165| loss: 0.42    |  0:00:07s\n",
      "epoch 166| loss: 0.4182  |  0:00:07s\n",
      "epoch 167| loss: 0.41053 |  0:00:07s\n",
      "epoch 168| loss: 0.41274 |  0:00:07s\n",
      "epoch 169| loss: 0.4127  |  0:00:07s\n",
      "epoch 170| loss: 0.40969 |  0:00:07s\n",
      "epoch 171| loss: 0.40587 |  0:00:07s\n",
      "epoch 172| loss: 0.40122 |  0:00:07s\n",
      "epoch 173| loss: 0.40413 |  0:00:08s\n",
      "epoch 174| loss: 0.41318 |  0:00:08s\n",
      "epoch 175| loss: 0.40207 |  0:00:08s\n",
      "epoch 176| loss: 0.41411 |  0:00:08s\n",
      "epoch 177| loss: 0.40635 |  0:00:08s\n",
      "epoch 178| loss: 0.41014 |  0:00:08s\n",
      "epoch 179| loss: 0.41803 |  0:00:08s\n",
      "epoch 180| loss: 0.42809 |  0:00:08s\n",
      "epoch 181| loss: 0.42547 |  0:00:08s\n",
      "epoch 182| loss: 0.42075 |  0:00:08s\n",
      "epoch 183| loss: 0.41667 |  0:00:08s\n",
      "epoch 184| loss: 0.41269 |  0:00:08s\n",
      "epoch 185| loss: 0.40976 |  0:00:08s\n",
      "epoch 186| loss: 0.40569 |  0:00:08s\n",
      "epoch 187| loss: 0.40823 |  0:00:08s\n",
      "epoch 188| loss: 0.39983 |  0:00:08s\n",
      "epoch 189| loss: 0.38744 |  0:00:08s\n",
      "epoch 190| loss: 0.39523 |  0:00:08s\n",
      "epoch 191| loss: 0.386   |  0:00:08s\n",
      "epoch 192| loss: 0.39409 |  0:00:08s\n",
      "epoch 193| loss: 0.40319 |  0:00:08s\n",
      "epoch 194| loss: 0.40534 |  0:00:08s\n",
      "epoch 195| loss: 0.39559 |  0:00:08s\n",
      "epoch 196| loss: 0.40065 |  0:00:09s\n",
      "epoch 197| loss: 0.39551 |  0:00:09s\n",
      "epoch 198| loss: 0.39546 |  0:00:09s\n",
      "epoch 199| loss: 0.3896  |  0:00:09s\n",
      "epoch 200| loss: 0.38968 |  0:00:09s\n",
      "epoch 201| loss: 0.38846 |  0:00:09s\n",
      "epoch 202| loss: 0.38553 |  0:00:09s\n",
      "epoch 203| loss: 0.3898  |  0:00:09s\n",
      "epoch 204| loss: 0.38716 |  0:00:09s\n",
      "epoch 205| loss: 0.38795 |  0:00:09s\n",
      "epoch 206| loss: 0.3794  |  0:00:09s\n",
      "epoch 207| loss: 0.38348 |  0:00:09s\n",
      "epoch 208| loss: 0.38581 |  0:00:09s\n",
      "epoch 209| loss: 0.38229 |  0:00:09s\n",
      "epoch 210| loss: 0.3882  |  0:00:09s\n",
      "epoch 211| loss: 0.37858 |  0:00:09s\n",
      "epoch 212| loss: 0.37731 |  0:00:09s\n",
      "epoch 213| loss: 0.38571 |  0:00:09s\n",
      "epoch 214| loss: 0.37793 |  0:00:09s\n",
      "epoch 215| loss: 0.3707  |  0:00:09s\n",
      "epoch 216| loss: 0.37735 |  0:00:09s\n",
      "epoch 217| loss: 0.37627 |  0:00:09s\n",
      "epoch 218| loss: 0.37267 |  0:00:10s\n",
      "epoch 219| loss: 0.37597 |  0:00:10s\n",
      "epoch 220| loss: 0.37719 |  0:00:10s\n",
      "epoch 221| loss: 0.37775 |  0:00:10s\n",
      "epoch 222| loss: 0.36601 |  0:00:10s\n",
      "epoch 223| loss: 0.37064 |  0:00:10s\n",
      "epoch 224| loss: 0.363   |  0:00:10s\n",
      "epoch 225| loss: 0.37128 |  0:00:10s\n",
      "epoch 226| loss: 0.36799 |  0:00:10s\n",
      "epoch 227| loss: 0.36785 |  0:00:10s\n",
      "epoch 228| loss: 0.36791 |  0:00:10s\n",
      "epoch 229| loss: 0.3681  |  0:00:10s\n",
      "epoch 230| loss: 0.37533 |  0:00:10s\n",
      "epoch 231| loss: 0.37241 |  0:00:10s\n",
      "epoch 232| loss: 0.3715  |  0:00:10s\n",
      "epoch 233| loss: 0.38047 |  0:00:10s\n",
      "epoch 234| loss: 0.37278 |  0:00:10s\n",
      "epoch 235| loss: 0.36941 |  0:00:10s\n",
      "epoch 236| loss: 0.37401 |  0:00:10s\n",
      "epoch 237| loss: 0.36542 |  0:00:10s\n",
      "epoch 238| loss: 0.37624 |  0:00:10s\n",
      "epoch 239| loss: 0.37048 |  0:00:10s\n",
      "epoch 240| loss: 0.38164 |  0:00:11s\n",
      "epoch 241| loss: 0.38452 |  0:00:11s\n",
      "epoch 242| loss: 0.38588 |  0:00:11s\n",
      "epoch 243| loss: 0.38509 |  0:00:11s\n",
      "epoch 244| loss: 0.37713 |  0:00:11s\n",
      "epoch 245| loss: 0.38453 |  0:00:11s\n",
      "epoch 246| loss: 0.37652 |  0:00:11s\n",
      "epoch 247| loss: 0.3739  |  0:00:11s\n",
      "epoch 248| loss: 0.37087 |  0:00:11s\n",
      "epoch 249| loss: 0.36762 |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.11549 |  0:00:00s\n",
      "epoch 1  | loss: 1.76977 |  0:00:00s\n",
      "epoch 2  | loss: 1.36711 |  0:00:00s\n",
      "epoch 3  | loss: 1.16814 |  0:00:00s\n",
      "epoch 4  | loss: 1.04897 |  0:00:00s\n",
      "epoch 5  | loss: 0.97017 |  0:00:00s\n",
      "epoch 6  | loss: 0.91049 |  0:00:00s\n",
      "epoch 7  | loss: 0.9003  |  0:00:00s\n",
      "epoch 8  | loss: 0.86913 |  0:00:00s\n",
      "epoch 9  | loss: 0.84258 |  0:00:00s\n",
      "epoch 10 | loss: 0.80999 |  0:00:00s\n",
      "epoch 11 | loss: 0.78886 |  0:00:00s\n",
      "epoch 12 | loss: 0.77433 |  0:00:00s\n",
      "epoch 13 | loss: 0.75659 |  0:00:00s\n",
      "epoch 14 | loss: 0.7482  |  0:00:00s\n",
      "epoch 15 | loss: 0.72838 |  0:00:00s\n",
      "epoch 16 | loss: 0.71792 |  0:00:00s\n",
      "epoch 17 | loss: 0.70968 |  0:00:00s\n",
      "epoch 18 | loss: 0.71262 |  0:00:01s\n",
      "epoch 19 | loss: 0.7012  |  0:00:01s\n",
      "epoch 20 | loss: 0.69986 |  0:00:01s\n",
      "epoch 21 | loss: 0.69412 |  0:00:01s\n",
      "epoch 22 | loss: 0.67818 |  0:00:01s\n",
      "epoch 23 | loss: 0.68774 |  0:00:01s\n",
      "epoch 24 | loss: 0.68286 |  0:00:01s\n",
      "epoch 25 | loss: 0.676   |  0:00:01s\n",
      "epoch 26 | loss: 0.67234 |  0:00:01s\n",
      "epoch 27 | loss: 0.66234 |  0:00:01s\n",
      "epoch 28 | loss: 0.65435 |  0:00:01s\n",
      "epoch 29 | loss: 0.65548 |  0:00:01s\n",
      "epoch 30 | loss: 0.65835 |  0:00:01s\n",
      "epoch 31 | loss: 0.66599 |  0:00:01s\n",
      "epoch 32 | loss: 0.65003 |  0:00:01s\n",
      "epoch 33 | loss: 0.64457 |  0:00:01s\n",
      "epoch 34 | loss: 0.64421 |  0:00:01s\n",
      "epoch 35 | loss: 0.63128 |  0:00:01s\n",
      "epoch 36 | loss: 0.64042 |  0:00:01s\n",
      "epoch 37 | loss: 0.63346 |  0:00:01s\n",
      "epoch 38 | loss: 0.62462 |  0:00:01s\n",
      "epoch 39 | loss: 0.62187 |  0:00:01s\n",
      "epoch 40 | loss: 0.62479 |  0:00:02s\n",
      "epoch 41 | loss: 0.62125 |  0:00:02s\n",
      "epoch 42 | loss: 0.61898 |  0:00:02s\n",
      "epoch 43 | loss: 0.616   |  0:00:02s\n",
      "epoch 44 | loss: 0.61572 |  0:00:02s\n",
      "epoch 45 | loss: 0.6132  |  0:00:02s\n",
      "epoch 46 | loss: 0.6199  |  0:00:02s\n",
      "epoch 47 | loss: 0.60563 |  0:00:02s\n",
      "epoch 48 | loss: 0.60668 |  0:00:02s\n",
      "epoch 49 | loss: 0.61279 |  0:00:02s\n",
      "epoch 50 | loss: 0.60846 |  0:00:02s\n",
      "epoch 51 | loss: 0.60471 |  0:00:02s\n",
      "epoch 52 | loss: 0.60351 |  0:00:02s\n",
      "epoch 53 | loss: 0.61462 |  0:00:02s\n",
      "epoch 54 | loss: 0.61718 |  0:00:02s\n",
      "epoch 55 | loss: 0.61546 |  0:00:02s\n",
      "epoch 56 | loss: 0.60856 |  0:00:02s\n",
      "epoch 57 | loss: 0.60675 |  0:00:02s\n",
      "epoch 58 | loss: 0.5929  |  0:00:02s\n",
      "epoch 59 | loss: 0.58957 |  0:00:02s\n",
      "epoch 60 | loss: 0.59397 |  0:00:02s\n",
      "epoch 61 | loss: 0.59223 |  0:00:02s\n",
      "epoch 62 | loss: 0.59255 |  0:00:03s\n",
      "epoch 63 | loss: 0.59471 |  0:00:03s\n",
      "epoch 64 | loss: 0.59662 |  0:00:03s\n",
      "epoch 65 | loss: 0.5821  |  0:00:03s\n",
      "epoch 66 | loss: 0.58595 |  0:00:03s\n",
      "epoch 67 | loss: 0.58348 |  0:00:03s\n",
      "epoch 68 | loss: 0.58659 |  0:00:03s\n",
      "epoch 69 | loss: 0.57826 |  0:00:03s\n",
      "epoch 70 | loss: 0.57464 |  0:00:03s\n",
      "epoch 71 | loss: 0.56669 |  0:00:03s\n",
      "epoch 72 | loss: 0.56559 |  0:00:03s\n",
      "epoch 73 | loss: 0.56122 |  0:00:03s\n",
      "epoch 74 | loss: 0.56396 |  0:00:03s\n",
      "epoch 75 | loss: 0.56563 |  0:00:03s\n",
      "epoch 76 | loss: 0.57109 |  0:00:03s\n",
      "epoch 77 | loss: 0.57124 |  0:00:03s\n",
      "epoch 78 | loss: 0.5574  |  0:00:03s\n",
      "epoch 79 | loss: 0.55314 |  0:00:03s\n",
      "epoch 80 | loss: 0.55555 |  0:00:03s\n",
      "epoch 81 | loss: 0.5508  |  0:00:03s\n",
      "epoch 82 | loss: 0.55356 |  0:00:03s\n",
      "epoch 83 | loss: 0.55701 |  0:00:04s\n",
      "epoch 84 | loss: 0.55038 |  0:00:04s\n",
      "epoch 85 | loss: 0.54719 |  0:00:04s\n",
      "epoch 86 | loss: 0.54124 |  0:00:04s\n",
      "epoch 87 | loss: 0.55112 |  0:00:04s\n",
      "epoch 88 | loss: 0.54286 |  0:00:04s\n",
      "epoch 89 | loss: 0.537   |  0:00:04s\n",
      "epoch 90 | loss: 0.54113 |  0:00:04s\n",
      "epoch 91 | loss: 0.53548 |  0:00:04s\n",
      "epoch 92 | loss: 0.52432 |  0:00:04s\n",
      "epoch 93 | loss: 0.54531 |  0:00:04s\n",
      "epoch 94 | loss: 0.52713 |  0:00:04s\n",
      "epoch 95 | loss: 0.53011 |  0:00:04s\n",
      "epoch 96 | loss: 0.53197 |  0:00:04s\n",
      "epoch 97 | loss: 0.53133 |  0:00:04s\n",
      "epoch 98 | loss: 0.52633 |  0:00:04s\n",
      "epoch 99 | loss: 0.52168 |  0:00:04s\n",
      "epoch 100| loss: 0.51623 |  0:00:04s\n",
      "epoch 101| loss: 0.52047 |  0:00:04s\n",
      "epoch 102| loss: 0.50621 |  0:00:04s\n",
      "epoch 103| loss: 0.51303 |  0:00:04s\n",
      "epoch 104| loss: 0.51333 |  0:00:05s\n",
      "epoch 105| loss: 0.50924 |  0:00:05s\n",
      "epoch 106| loss: 0.51543 |  0:00:05s\n",
      "epoch 107| loss: 0.50544 |  0:00:05s\n",
      "epoch 108| loss: 0.50365 |  0:00:05s\n",
      "epoch 109| loss: 0.50211 |  0:00:05s\n",
      "epoch 110| loss: 0.49997 |  0:00:05s\n",
      "epoch 111| loss: 0.49381 |  0:00:05s\n",
      "epoch 112| loss: 0.49604 |  0:00:05s\n",
      "epoch 113| loss: 0.48994 |  0:00:05s\n",
      "epoch 114| loss: 0.4897  |  0:00:05s\n",
      "epoch 115| loss: 0.49493 |  0:00:05s\n",
      "epoch 116| loss: 0.48654 |  0:00:05s\n",
      "epoch 117| loss: 0.48744 |  0:00:05s\n",
      "epoch 118| loss: 0.49286 |  0:00:05s\n",
      "epoch 119| loss: 0.49792 |  0:00:05s\n",
      "epoch 120| loss: 0.48323 |  0:00:05s\n",
      "epoch 121| loss: 0.47941 |  0:00:05s\n",
      "epoch 122| loss: 0.48082 |  0:00:05s\n",
      "epoch 123| loss: 0.49074 |  0:00:05s\n",
      "epoch 124| loss: 0.48777 |  0:00:05s\n",
      "epoch 125| loss: 0.48601 |  0:00:05s\n",
      "epoch 126| loss: 0.48695 |  0:00:05s\n",
      "epoch 127| loss: 0.49588 |  0:00:06s\n",
      "epoch 128| loss: 0.50363 |  0:00:06s\n",
      "epoch 129| loss: 0.49111 |  0:00:06s\n",
      "epoch 130| loss: 0.48984 |  0:00:06s\n",
      "epoch 131| loss: 0.49323 |  0:00:06s\n",
      "epoch 132| loss: 0.48449 |  0:00:06s\n",
      "epoch 133| loss: 0.48195 |  0:00:06s\n",
      "epoch 134| loss: 0.48063 |  0:00:06s\n",
      "epoch 135| loss: 0.48127 |  0:00:06s\n",
      "epoch 136| loss: 0.47819 |  0:00:06s\n",
      "epoch 137| loss: 0.47312 |  0:00:06s\n",
      "epoch 138| loss: 0.47778 |  0:00:06s\n",
      "epoch 139| loss: 0.46751 |  0:00:06s\n",
      "epoch 140| loss: 0.45568 |  0:00:06s\n",
      "epoch 141| loss: 0.46094 |  0:00:06s\n",
      "epoch 142| loss: 0.45596 |  0:00:06s\n",
      "epoch 143| loss: 0.46027 |  0:00:06s\n",
      "epoch 144| loss: 0.47912 |  0:00:06s\n",
      "epoch 145| loss: 0.48579 |  0:00:06s\n",
      "epoch 146| loss: 0.49232 |  0:00:06s\n",
      "epoch 147| loss: 0.4773  |  0:00:06s\n",
      "epoch 148| loss: 0.47747 |  0:00:06s\n",
      "epoch 149| loss: 0.47653 |  0:00:07s\n",
      "epoch 150| loss: 0.47024 |  0:00:07s\n",
      "epoch 151| loss: 0.46614 |  0:00:07s\n",
      "epoch 152| loss: 0.45921 |  0:00:07s\n",
      "epoch 153| loss: 0.46335 |  0:00:07s\n",
      "epoch 154| loss: 0.46276 |  0:00:07s\n",
      "epoch 155| loss: 0.47993 |  0:00:07s\n",
      "epoch 156| loss: 0.48647 |  0:00:07s\n",
      "epoch 157| loss: 0.48489 |  0:00:07s\n",
      "epoch 158| loss: 0.46767 |  0:00:07s\n",
      "epoch 159| loss: 0.4638  |  0:00:07s\n",
      "epoch 160| loss: 0.45667 |  0:00:07s\n",
      "epoch 161| loss: 0.45248 |  0:00:07s\n",
      "epoch 162| loss: 0.4555  |  0:00:07s\n",
      "epoch 163| loss: 0.44721 |  0:00:07s\n",
      "epoch 164| loss: 0.44761 |  0:00:07s\n",
      "epoch 165| loss: 0.44161 |  0:00:07s\n",
      "epoch 166| loss: 0.44471 |  0:00:07s\n",
      "epoch 167| loss: 0.43825 |  0:00:07s\n",
      "epoch 168| loss: 0.44347 |  0:00:07s\n",
      "epoch 169| loss: 0.43108 |  0:00:07s\n",
      "epoch 170| loss: 0.43506 |  0:00:07s\n",
      "epoch 171| loss: 0.4253  |  0:00:08s\n",
      "epoch 172| loss: 0.42742 |  0:00:08s\n",
      "epoch 173| loss: 0.43382 |  0:00:08s\n",
      "epoch 174| loss: 0.43186 |  0:00:08s\n",
      "epoch 175| loss: 0.43464 |  0:00:08s\n",
      "epoch 176| loss: 0.437   |  0:00:08s\n",
      "epoch 177| loss: 0.43846 |  0:00:08s\n",
      "epoch 178| loss: 0.43462 |  0:00:08s\n",
      "epoch 179| loss: 0.4305  |  0:00:08s\n",
      "epoch 180| loss: 0.43895 |  0:00:08s\n",
      "epoch 181| loss: 0.43112 |  0:00:08s\n",
      "epoch 182| loss: 0.4279  |  0:00:08s\n",
      "epoch 183| loss: 0.43241 |  0:00:08s\n",
      "epoch 184| loss: 0.44056 |  0:00:08s\n",
      "epoch 185| loss: 0.43705 |  0:00:08s\n",
      "epoch 186| loss: 0.43334 |  0:00:08s\n",
      "epoch 187| loss: 0.43325 |  0:00:08s\n",
      "epoch 188| loss: 0.4279  |  0:00:08s\n",
      "epoch 189| loss: 0.42405 |  0:00:08s\n",
      "epoch 190| loss: 0.42174 |  0:00:08s\n",
      "epoch 191| loss: 0.42283 |  0:00:08s\n",
      "epoch 192| loss: 0.43205 |  0:00:09s\n",
      "epoch 193| loss: 0.41966 |  0:00:09s\n",
      "epoch 194| loss: 0.4222  |  0:00:09s\n",
      "epoch 195| loss: 0.41793 |  0:00:09s\n",
      "epoch 196| loss: 0.41111 |  0:00:09s\n",
      "epoch 197| loss: 0.41216 |  0:00:09s\n",
      "epoch 198| loss: 0.4124  |  0:00:09s\n",
      "epoch 199| loss: 0.41448 |  0:00:09s\n",
      "epoch 200| loss: 0.41073 |  0:00:09s\n",
      "epoch 201| loss: 0.41501 |  0:00:09s\n",
      "epoch 202| loss: 0.40717 |  0:00:09s\n",
      "epoch 203| loss: 0.40797 |  0:00:09s\n",
      "epoch 204| loss: 0.40552 |  0:00:09s\n",
      "epoch 205| loss: 0.40731 |  0:00:09s\n",
      "epoch 206| loss: 0.41281 |  0:00:09s\n",
      "epoch 207| loss: 0.41265 |  0:00:09s\n",
      "epoch 208| loss: 0.40194 |  0:00:09s\n",
      "epoch 209| loss: 0.40649 |  0:00:09s\n",
      "epoch 210| loss: 0.40771 |  0:00:09s\n",
      "epoch 211| loss: 0.40892 |  0:00:09s\n",
      "epoch 212| loss: 0.40564 |  0:00:10s\n",
      "epoch 213| loss: 0.41211 |  0:00:10s\n",
      "epoch 214| loss: 0.41373 |  0:00:10s\n",
      "epoch 215| loss: 0.41218 |  0:00:10s\n",
      "epoch 216| loss: 0.41909 |  0:00:10s\n",
      "epoch 217| loss: 0.41454 |  0:00:10s\n",
      "epoch 218| loss: 0.41153 |  0:00:10s\n",
      "epoch 219| loss: 0.40941 |  0:00:10s\n",
      "epoch 220| loss: 0.4085  |  0:00:10s\n",
      "epoch 221| loss: 0.40633 |  0:00:10s\n",
      "epoch 222| loss: 0.39645 |  0:00:10s\n",
      "epoch 223| loss: 0.39719 |  0:00:10s\n",
      "epoch 224| loss: 0.38604 |  0:00:10s\n",
      "epoch 225| loss: 0.40014 |  0:00:10s\n",
      "epoch 226| loss: 0.39465 |  0:00:10s\n",
      "epoch 227| loss: 0.39981 |  0:00:10s\n",
      "epoch 228| loss: 0.39605 |  0:00:10s\n",
      "epoch 229| loss: 0.3935  |  0:00:10s\n",
      "epoch 230| loss: 0.40168 |  0:00:10s\n",
      "epoch 231| loss: 0.3947  |  0:00:10s\n",
      "epoch 232| loss: 0.39825 |  0:00:10s\n",
      "epoch 233| loss: 0.3902  |  0:00:11s\n",
      "epoch 234| loss: 0.39598 |  0:00:11s\n",
      "epoch 235| loss: 0.39971 |  0:00:11s\n",
      "epoch 236| loss: 0.39451 |  0:00:11s\n",
      "epoch 237| loss: 0.38929 |  0:00:11s\n",
      "epoch 238| loss: 0.39289 |  0:00:11s\n",
      "epoch 239| loss: 0.3886  |  0:00:11s\n",
      "epoch 240| loss: 0.39089 |  0:00:11s\n",
      "epoch 241| loss: 0.38521 |  0:00:11s\n",
      "epoch 242| loss: 0.38092 |  0:00:11s\n",
      "epoch 243| loss: 0.38414 |  0:00:11s\n",
      "epoch 244| loss: 0.38452 |  0:00:11s\n",
      "epoch 245| loss: 0.38173 |  0:00:11s\n",
      "epoch 246| loss: 0.37575 |  0:00:11s\n",
      "epoch 247| loss: 0.37819 |  0:00:11s\n",
      "epoch 248| loss: 0.37381 |  0:00:11s\n",
      "epoch 249| loss: 0.3734  |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.22802 |  0:00:00s\n",
      "epoch 1  | loss: 1.83761 |  0:00:00s\n",
      "epoch 2  | loss: 1.32039 |  0:00:00s\n",
      "epoch 3  | loss: 1.15251 |  0:00:00s\n",
      "epoch 4  | loss: 1.00359 |  0:00:00s\n",
      "epoch 5  | loss: 0.9251  |  0:00:00s\n",
      "epoch 6  | loss: 0.86421 |  0:00:00s\n",
      "epoch 7  | loss: 0.82235 |  0:00:00s\n",
      "epoch 8  | loss: 0.78594 |  0:00:00s\n",
      "epoch 9  | loss: 0.7574  |  0:00:00s\n",
      "epoch 10 | loss: 0.74605 |  0:00:00s\n",
      "epoch 11 | loss: 0.72838 |  0:00:00s\n",
      "epoch 12 | loss: 0.72697 |  0:00:00s\n",
      "epoch 13 | loss: 0.7286  |  0:00:00s\n",
      "epoch 14 | loss: 0.7074  |  0:00:00s\n",
      "epoch 15 | loss: 0.7055  |  0:00:00s\n",
      "epoch 16 | loss: 0.69635 |  0:00:00s\n",
      "epoch 17 | loss: 0.69737 |  0:00:00s\n",
      "epoch 18 | loss: 0.70477 |  0:00:00s\n",
      "epoch 19 | loss: 0.7039  |  0:00:00s\n",
      "epoch 20 | loss: 0.69235 |  0:00:00s\n",
      "epoch 21 | loss: 0.68905 |  0:00:01s\n",
      "epoch 22 | loss: 0.68357 |  0:00:01s\n",
      "epoch 23 | loss: 0.684   |  0:00:01s\n",
      "epoch 24 | loss: 0.67937 |  0:00:01s\n",
      "epoch 25 | loss: 0.68151 |  0:00:01s\n",
      "epoch 26 | loss: 0.67528 |  0:00:01s\n",
      "epoch 27 | loss: 0.67256 |  0:00:01s\n",
      "epoch 28 | loss: 0.67505 |  0:00:01s\n",
      "epoch 29 | loss: 0.66044 |  0:00:01s\n",
      "epoch 30 | loss: 0.66179 |  0:00:01s\n",
      "epoch 31 | loss: 0.65398 |  0:00:01s\n",
      "epoch 32 | loss: 0.66515 |  0:00:01s\n",
      "epoch 33 | loss: 0.66754 |  0:00:01s\n",
      "epoch 34 | loss: 0.67457 |  0:00:01s\n",
      "epoch 35 | loss: 0.6645  |  0:00:01s\n",
      "epoch 36 | loss: 0.67354 |  0:00:01s\n",
      "epoch 37 | loss: 0.66433 |  0:00:01s\n",
      "epoch 38 | loss: 0.65205 |  0:00:01s\n",
      "epoch 39 | loss: 0.65738 |  0:00:01s\n",
      "epoch 40 | loss: 0.64733 |  0:00:01s\n",
      "epoch 41 | loss: 0.64334 |  0:00:01s\n",
      "epoch 42 | loss: 0.64863 |  0:00:01s\n",
      "epoch 43 | loss: 0.64126 |  0:00:02s\n",
      "epoch 44 | loss: 0.642   |  0:00:02s\n",
      "epoch 45 | loss: 0.63985 |  0:00:02s\n",
      "epoch 46 | loss: 0.64532 |  0:00:02s\n",
      "epoch 47 | loss: 0.64873 |  0:00:02s\n",
      "epoch 48 | loss: 0.64554 |  0:00:02s\n",
      "epoch 49 | loss: 0.63921 |  0:00:02s\n",
      "epoch 50 | loss: 0.63815 |  0:00:02s\n",
      "epoch 51 | loss: 0.64403 |  0:00:02s\n",
      "epoch 52 | loss: 0.62705 |  0:00:02s\n",
      "epoch 53 | loss: 0.63044 |  0:00:02s\n",
      "epoch 54 | loss: 0.62221 |  0:00:02s\n",
      "epoch 55 | loss: 0.634   |  0:00:02s\n",
      "epoch 56 | loss: 0.61341 |  0:00:02s\n",
      "epoch 57 | loss: 0.61587 |  0:00:02s\n",
      "epoch 58 | loss: 0.61187 |  0:00:02s\n",
      "epoch 59 | loss: 0.60608 |  0:00:02s\n",
      "epoch 60 | loss: 0.61173 |  0:00:02s\n",
      "epoch 61 | loss: 0.62456 |  0:00:02s\n",
      "epoch 62 | loss: 0.60305 |  0:00:02s\n",
      "epoch 63 | loss: 0.60113 |  0:00:02s\n",
      "epoch 64 | loss: 0.60405 |  0:00:02s\n",
      "epoch 65 | loss: 0.59458 |  0:00:03s\n",
      "epoch 66 | loss: 0.5862  |  0:00:03s\n",
      "epoch 67 | loss: 0.59323 |  0:00:03s\n",
      "epoch 68 | loss: 0.58854 |  0:00:03s\n",
      "epoch 69 | loss: 0.58737 |  0:00:03s\n",
      "epoch 70 | loss: 0.57668 |  0:00:03s\n",
      "epoch 71 | loss: 0.59057 |  0:00:03s\n",
      "epoch 72 | loss: 0.57478 |  0:00:03s\n",
      "epoch 73 | loss: 0.57498 |  0:00:03s\n",
      "epoch 74 | loss: 0.58117 |  0:00:03s\n",
      "epoch 75 | loss: 0.57919 |  0:00:03s\n",
      "epoch 76 | loss: 0.58708 |  0:00:03s\n",
      "epoch 77 | loss: 0.58718 |  0:00:03s\n",
      "epoch 78 | loss: 0.57717 |  0:00:03s\n",
      "epoch 79 | loss: 0.57817 |  0:00:03s\n",
      "epoch 80 | loss: 0.57917 |  0:00:03s\n",
      "epoch 81 | loss: 0.57307 |  0:00:03s\n",
      "epoch 82 | loss: 0.5678  |  0:00:03s\n",
      "epoch 83 | loss: 0.56971 |  0:00:03s\n",
      "epoch 84 | loss: 0.56111 |  0:00:03s\n",
      "epoch 85 | loss: 0.56762 |  0:00:03s\n",
      "epoch 86 | loss: 0.56516 |  0:00:03s\n",
      "epoch 87 | loss: 0.56866 |  0:00:04s\n",
      "epoch 88 | loss: 0.56492 |  0:00:04s\n",
      "epoch 89 | loss: 0.55672 |  0:00:04s\n",
      "epoch 90 | loss: 0.56061 |  0:00:04s\n",
      "epoch 91 | loss: 0.55797 |  0:00:04s\n",
      "epoch 92 | loss: 0.55725 |  0:00:04s\n",
      "epoch 93 | loss: 0.55542 |  0:00:04s\n",
      "epoch 94 | loss: 0.55051 |  0:00:04s\n",
      "epoch 95 | loss: 0.54637 |  0:00:04s\n",
      "epoch 96 | loss: 0.54993 |  0:00:04s\n",
      "epoch 97 | loss: 0.54539 |  0:00:04s\n",
      "epoch 98 | loss: 0.54826 |  0:00:04s\n",
      "epoch 99 | loss: 0.54614 |  0:00:04s\n",
      "epoch 100| loss: 0.5363  |  0:00:04s\n",
      "epoch 101| loss: 0.54224 |  0:00:04s\n",
      "epoch 102| loss: 0.54448 |  0:00:04s\n",
      "epoch 103| loss: 0.53764 |  0:00:04s\n",
      "epoch 104| loss: 0.54623 |  0:00:04s\n",
      "epoch 105| loss: 0.53849 |  0:00:04s\n",
      "epoch 106| loss: 0.54637 |  0:00:04s\n",
      "epoch 107| loss: 0.53011 |  0:00:04s\n",
      "epoch 108| loss: 0.52923 |  0:00:04s\n",
      "epoch 109| loss: 0.53114 |  0:00:05s\n",
      "epoch 110| loss: 0.52572 |  0:00:05s\n",
      "epoch 111| loss: 0.52423 |  0:00:05s\n",
      "epoch 112| loss: 0.52046 |  0:00:05s\n",
      "epoch 113| loss: 0.51647 |  0:00:05s\n",
      "epoch 114| loss: 0.51376 |  0:00:05s\n",
      "epoch 115| loss: 0.51999 |  0:00:05s\n",
      "epoch 116| loss: 0.51027 |  0:00:05s\n",
      "epoch 117| loss: 0.5103  |  0:00:05s\n",
      "epoch 118| loss: 0.51782 |  0:00:05s\n",
      "epoch 119| loss: 0.51365 |  0:00:05s\n",
      "epoch 120| loss: 0.51112 |  0:00:05s\n",
      "epoch 121| loss: 0.51264 |  0:00:05s\n",
      "epoch 122| loss: 0.50847 |  0:00:05s\n",
      "epoch 123| loss: 0.50549 |  0:00:05s\n",
      "epoch 124| loss: 0.50417 |  0:00:05s\n",
      "epoch 125| loss: 0.50854 |  0:00:05s\n",
      "epoch 126| loss: 0.50635 |  0:00:05s\n",
      "epoch 127| loss: 0.50709 |  0:00:05s\n",
      "epoch 128| loss: 0.50144 |  0:00:05s\n",
      "epoch 129| loss: 0.49907 |  0:00:05s\n",
      "epoch 130| loss: 0.49025 |  0:00:05s\n",
      "epoch 131| loss: 0.49543 |  0:00:06s\n",
      "epoch 132| loss: 0.48731 |  0:00:06s\n",
      "epoch 133| loss: 0.48969 |  0:00:06s\n",
      "epoch 134| loss: 0.48824 |  0:00:06s\n",
      "epoch 135| loss: 0.48711 |  0:00:06s\n",
      "epoch 136| loss: 0.49157 |  0:00:06s\n",
      "epoch 137| loss: 0.48194 |  0:00:06s\n",
      "epoch 138| loss: 0.48579 |  0:00:06s\n",
      "epoch 139| loss: 0.47804 |  0:00:06s\n",
      "epoch 140| loss: 0.47836 |  0:00:06s\n",
      "epoch 141| loss: 0.47913 |  0:00:06s\n",
      "epoch 142| loss: 0.47755 |  0:00:06s\n",
      "epoch 143| loss: 0.47678 |  0:00:06s\n",
      "epoch 144| loss: 0.4758  |  0:00:06s\n",
      "epoch 145| loss: 0.48223 |  0:00:06s\n",
      "epoch 146| loss: 0.49203 |  0:00:06s\n",
      "epoch 147| loss: 0.48075 |  0:00:06s\n",
      "epoch 148| loss: 0.47744 |  0:00:06s\n",
      "epoch 149| loss: 0.47856 |  0:00:06s\n",
      "epoch 150| loss: 0.47662 |  0:00:06s\n",
      "epoch 151| loss: 0.48883 |  0:00:06s\n",
      "epoch 152| loss: 0.50684 |  0:00:06s\n",
      "epoch 153| loss: 0.52138 |  0:00:07s\n",
      "epoch 154| loss: 0.51533 |  0:00:07s\n",
      "epoch 155| loss: 0.50807 |  0:00:07s\n",
      "epoch 156| loss: 0.51028 |  0:00:07s\n",
      "epoch 157| loss: 0.50779 |  0:00:07s\n",
      "epoch 158| loss: 0.50826 |  0:00:07s\n",
      "epoch 159| loss: 0.51016 |  0:00:07s\n",
      "epoch 160| loss: 0.50465 |  0:00:07s\n",
      "epoch 161| loss: 0.50096 |  0:00:07s\n",
      "epoch 162| loss: 0.50911 |  0:00:07s\n",
      "epoch 163| loss: 0.5055  |  0:00:07s\n",
      "epoch 164| loss: 0.49224 |  0:00:07s\n",
      "epoch 165| loss: 0.49529 |  0:00:07s\n",
      "epoch 166| loss: 0.4895  |  0:00:07s\n",
      "epoch 167| loss: 0.48226 |  0:00:07s\n",
      "epoch 168| loss: 0.48627 |  0:00:07s\n",
      "epoch 169| loss: 0.491   |  0:00:07s\n",
      "epoch 170| loss: 0.50826 |  0:00:07s\n",
      "epoch 171| loss: 0.49743 |  0:00:07s\n",
      "epoch 172| loss: 0.49218 |  0:00:07s\n",
      "epoch 173| loss: 0.49913 |  0:00:07s\n",
      "epoch 174| loss: 0.49193 |  0:00:07s\n",
      "epoch 175| loss: 0.48287 |  0:00:07s\n",
      "epoch 176| loss: 0.48624 |  0:00:08s\n",
      "epoch 177| loss: 0.47809 |  0:00:08s\n",
      "epoch 178| loss: 0.47738 |  0:00:08s\n",
      "epoch 179| loss: 0.47789 |  0:00:08s\n",
      "epoch 180| loss: 0.4939  |  0:00:08s\n",
      "epoch 181| loss: 0.48725 |  0:00:08s\n",
      "epoch 182| loss: 0.48689 |  0:00:08s\n",
      "epoch 183| loss: 0.47964 |  0:00:08s\n",
      "epoch 184| loss: 0.49106 |  0:00:08s\n",
      "epoch 185| loss: 0.48981 |  0:00:08s\n",
      "epoch 186| loss: 0.49682 |  0:00:08s\n",
      "epoch 187| loss: 0.48881 |  0:00:08s\n",
      "epoch 188| loss: 0.48985 |  0:00:08s\n",
      "epoch 189| loss: 0.49281 |  0:00:08s\n",
      "epoch 190| loss: 0.49137 |  0:00:08s\n",
      "epoch 191| loss: 0.50521 |  0:00:08s\n",
      "epoch 192| loss: 0.53193 |  0:00:08s\n",
      "epoch 193| loss: 0.5119  |  0:00:08s\n",
      "epoch 194| loss: 0.51622 |  0:00:08s\n",
      "epoch 195| loss: 0.51121 |  0:00:09s\n",
      "epoch 196| loss: 0.51193 |  0:00:09s\n",
      "epoch 197| loss: 0.51342 |  0:00:09s\n",
      "epoch 198| loss: 0.52288 |  0:00:09s\n",
      "epoch 199| loss: 0.53158 |  0:00:09s\n",
      "epoch 200| loss: 0.51663 |  0:00:09s\n",
      "epoch 201| loss: 0.52018 |  0:00:09s\n",
      "epoch 202| loss: 0.53079 |  0:00:09s\n",
      "epoch 203| loss: 0.53133 |  0:00:09s\n",
      "epoch 204| loss: 0.52686 |  0:00:09s\n",
      "epoch 205| loss: 0.51298 |  0:00:09s\n",
      "epoch 206| loss: 0.49945 |  0:00:09s\n",
      "epoch 207| loss: 0.4942  |  0:00:09s\n",
      "epoch 208| loss: 0.4942  |  0:00:09s\n",
      "epoch 209| loss: 0.48241 |  0:00:09s\n",
      "epoch 210| loss: 0.50084 |  0:00:09s\n",
      "epoch 211| loss: 0.48839 |  0:00:09s\n",
      "epoch 212| loss: 0.48021 |  0:00:09s\n",
      "epoch 213| loss: 0.48072 |  0:00:09s\n",
      "epoch 214| loss: 0.47765 |  0:00:09s\n",
      "epoch 215| loss: 0.47843 |  0:00:09s\n",
      "epoch 216| loss: 0.47311 |  0:00:10s\n",
      "epoch 217| loss: 0.47429 |  0:00:10s\n",
      "epoch 218| loss: 0.45878 |  0:00:10s\n",
      "epoch 219| loss: 0.46336 |  0:00:10s\n",
      "epoch 220| loss: 0.45935 |  0:00:10s\n",
      "epoch 221| loss: 0.461   |  0:00:10s\n",
      "epoch 222| loss: 0.44863 |  0:00:10s\n",
      "epoch 223| loss: 0.45188 |  0:00:10s\n",
      "epoch 224| loss: 0.44141 |  0:00:10s\n",
      "epoch 225| loss: 0.46073 |  0:00:10s\n",
      "epoch 226| loss: 0.45753 |  0:00:10s\n",
      "epoch 227| loss: 0.46822 |  0:00:10s\n",
      "epoch 228| loss: 0.45733 |  0:00:10s\n",
      "epoch 229| loss: 0.4634  |  0:00:10s\n",
      "epoch 230| loss: 0.46358 |  0:00:10s\n",
      "epoch 231| loss: 0.45001 |  0:00:10s\n",
      "epoch 232| loss: 0.44867 |  0:00:10s\n",
      "epoch 233| loss: 0.44597 |  0:00:10s\n",
      "epoch 234| loss: 0.44319 |  0:00:10s\n",
      "epoch 235| loss: 0.45313 |  0:00:10s\n",
      "epoch 236| loss: 0.44413 |  0:00:10s\n",
      "epoch 237| loss: 0.44813 |  0:00:10s\n",
      "epoch 238| loss: 0.44549 |  0:00:11s\n",
      "epoch 239| loss: 0.46162 |  0:00:11s\n",
      "epoch 240| loss: 0.44881 |  0:00:11s\n",
      "epoch 241| loss: 0.4502  |  0:00:11s\n",
      "epoch 242| loss: 0.44869 |  0:00:11s\n",
      "epoch 243| loss: 0.44227 |  0:00:11s\n",
      "epoch 244| loss: 0.4344  |  0:00:11s\n",
      "epoch 245| loss: 0.44387 |  0:00:11s\n",
      "epoch 246| loss: 0.44055 |  0:00:11s\n",
      "epoch 247| loss: 0.44146 |  0:00:11s\n",
      "epoch 248| loss: 0.44287 |  0:00:11s\n",
      "epoch 249| loss: 0.43783 |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.10369 |  0:00:00s\n",
      "epoch 1  | loss: 1.65715 |  0:00:00s\n",
      "epoch 2  | loss: 1.34694 |  0:00:00s\n",
      "epoch 3  | loss: 1.13244 |  0:00:00s\n",
      "epoch 4  | loss: 1.00112 |  0:00:00s\n",
      "epoch 5  | loss: 0.94893 |  0:00:00s\n",
      "epoch 6  | loss: 0.89712 |  0:00:00s\n",
      "epoch 7  | loss: 0.88462 |  0:00:00s\n",
      "epoch 8  | loss: 0.85776 |  0:00:00s\n",
      "epoch 9  | loss: 0.84799 |  0:00:00s\n",
      "epoch 10 | loss: 0.82162 |  0:00:00s\n",
      "epoch 11 | loss: 0.79818 |  0:00:00s\n",
      "epoch 12 | loss: 0.80092 |  0:00:00s\n",
      "epoch 13 | loss: 0.76437 |  0:00:00s\n",
      "epoch 14 | loss: 0.74978 |  0:00:00s\n",
      "epoch 15 | loss: 0.74847 |  0:00:00s\n",
      "epoch 16 | loss: 0.73747 |  0:00:00s\n",
      "epoch 17 | loss: 0.72555 |  0:00:00s\n",
      "epoch 18 | loss: 0.71975 |  0:00:00s\n",
      "epoch 19 | loss: 0.71878 |  0:00:00s\n",
      "epoch 20 | loss: 0.70492 |  0:00:00s\n",
      "epoch 21 | loss: 0.71076 |  0:00:01s\n",
      "epoch 22 | loss: 0.70268 |  0:00:01s\n",
      "epoch 23 | loss: 0.70175 |  0:00:01s\n",
      "epoch 24 | loss: 0.68768 |  0:00:01s\n",
      "epoch 25 | loss: 0.69462 |  0:00:01s\n",
      "epoch 26 | loss: 0.68536 |  0:00:01s\n",
      "epoch 27 | loss: 0.69375 |  0:00:01s\n",
      "epoch 28 | loss: 0.6843  |  0:00:01s\n",
      "epoch 29 | loss: 0.67878 |  0:00:01s\n",
      "epoch 30 | loss: 0.66978 |  0:00:01s\n",
      "epoch 31 | loss: 0.67368 |  0:00:01s\n",
      "epoch 32 | loss: 0.67025 |  0:00:01s\n",
      "epoch 33 | loss: 0.65684 |  0:00:01s\n",
      "epoch 34 | loss: 0.65462 |  0:00:01s\n",
      "epoch 35 | loss: 0.64717 |  0:00:01s\n",
      "epoch 36 | loss: 0.64874 |  0:00:01s\n",
      "epoch 37 | loss: 0.64701 |  0:00:01s\n",
      "epoch 38 | loss: 0.64295 |  0:00:01s\n",
      "epoch 39 | loss: 0.64192 |  0:00:01s\n",
      "epoch 40 | loss: 0.63724 |  0:00:01s\n",
      "epoch 41 | loss: 0.64594 |  0:00:02s\n",
      "epoch 42 | loss: 0.6396  |  0:00:02s\n",
      "epoch 43 | loss: 0.64846 |  0:00:02s\n",
      "epoch 44 | loss: 0.631   |  0:00:02s\n",
      "epoch 45 | loss: 0.63176 |  0:00:02s\n",
      "epoch 46 | loss: 0.63293 |  0:00:02s\n",
      "epoch 47 | loss: 0.63008 |  0:00:02s\n",
      "epoch 48 | loss: 0.62151 |  0:00:02s\n",
      "epoch 49 | loss: 0.631   |  0:00:02s\n",
      "epoch 50 | loss: 0.61205 |  0:00:02s\n",
      "epoch 51 | loss: 0.61089 |  0:00:02s\n",
      "epoch 52 | loss: 0.61167 |  0:00:02s\n",
      "epoch 53 | loss: 0.61263 |  0:00:02s\n",
      "epoch 54 | loss: 0.61449 |  0:00:02s\n",
      "epoch 55 | loss: 0.6121  |  0:00:02s\n",
      "epoch 56 | loss: 0.60135 |  0:00:02s\n",
      "epoch 57 | loss: 0.60756 |  0:00:02s\n",
      "epoch 58 | loss: 0.60718 |  0:00:02s\n",
      "epoch 59 | loss: 0.59194 |  0:00:02s\n",
      "epoch 60 | loss: 0.59995 |  0:00:02s\n",
      "epoch 61 | loss: 0.59485 |  0:00:02s\n",
      "epoch 62 | loss: 0.61275 |  0:00:02s\n",
      "epoch 63 | loss: 0.59482 |  0:00:03s\n",
      "epoch 64 | loss: 0.59579 |  0:00:03s\n",
      "epoch 65 | loss: 0.58814 |  0:00:03s\n",
      "epoch 66 | loss: 0.58391 |  0:00:03s\n",
      "epoch 67 | loss: 0.60221 |  0:00:03s\n",
      "epoch 68 | loss: 0.57898 |  0:00:03s\n",
      "epoch 69 | loss: 0.58461 |  0:00:03s\n",
      "epoch 70 | loss: 0.58038 |  0:00:03s\n",
      "epoch 71 | loss: 0.5768  |  0:00:03s\n",
      "epoch 72 | loss: 0.57139 |  0:00:03s\n",
      "epoch 73 | loss: 0.57196 |  0:00:03s\n",
      "epoch 74 | loss: 0.57239 |  0:00:03s\n",
      "epoch 75 | loss: 0.57037 |  0:00:03s\n",
      "epoch 76 | loss: 0.57977 |  0:00:03s\n",
      "epoch 77 | loss: 0.58812 |  0:00:03s\n",
      "epoch 78 | loss: 0.57291 |  0:00:03s\n",
      "epoch 79 | loss: 0.57484 |  0:00:03s\n",
      "epoch 80 | loss: 0.57671 |  0:00:04s\n",
      "epoch 81 | loss: 0.56272 |  0:00:04s\n",
      "epoch 82 | loss: 0.55939 |  0:00:04s\n",
      "epoch 83 | loss: 0.56517 |  0:00:04s\n",
      "epoch 84 | loss: 0.55912 |  0:00:04s\n",
      "epoch 85 | loss: 0.55681 |  0:00:04s\n",
      "epoch 86 | loss: 0.55217 |  0:00:04s\n",
      "epoch 87 | loss: 0.55426 |  0:00:04s\n",
      "epoch 88 | loss: 0.54416 |  0:00:04s\n",
      "epoch 89 | loss: 0.54449 |  0:00:04s\n",
      "epoch 90 | loss: 0.54584 |  0:00:04s\n",
      "epoch 91 | loss: 0.55036 |  0:00:04s\n",
      "epoch 92 | loss: 0.54569 |  0:00:04s\n",
      "epoch 93 | loss: 0.54684 |  0:00:04s\n",
      "epoch 94 | loss: 0.53998 |  0:00:04s\n",
      "epoch 95 | loss: 0.54242 |  0:00:04s\n",
      "epoch 96 | loss: 0.54034 |  0:00:04s\n",
      "epoch 97 | loss: 0.54329 |  0:00:04s\n",
      "epoch 98 | loss: 0.53964 |  0:00:04s\n",
      "epoch 99 | loss: 0.53434 |  0:00:05s\n",
      "epoch 100| loss: 0.53557 |  0:00:05s\n",
      "epoch 101| loss: 0.52794 |  0:00:05s\n",
      "epoch 102| loss: 0.5322  |  0:00:05s\n",
      "epoch 103| loss: 0.52287 |  0:00:05s\n",
      "epoch 104| loss: 0.51994 |  0:00:05s\n",
      "epoch 105| loss: 0.52242 |  0:00:05s\n",
      "epoch 106| loss: 0.52653 |  0:00:05s\n",
      "epoch 107| loss: 0.52203 |  0:00:05s\n",
      "epoch 108| loss: 0.5244  |  0:00:05s\n",
      "epoch 109| loss: 0.52274 |  0:00:05s\n",
      "epoch 110| loss: 0.51311 |  0:00:05s\n",
      "epoch 111| loss: 0.51719 |  0:00:05s\n",
      "epoch 112| loss: 0.50976 |  0:00:05s\n",
      "epoch 113| loss: 0.51593 |  0:00:05s\n",
      "epoch 114| loss: 0.51186 |  0:00:05s\n",
      "epoch 115| loss: 0.51214 |  0:00:05s\n",
      "epoch 116| loss: 0.50698 |  0:00:05s\n",
      "epoch 117| loss: 0.49825 |  0:00:05s\n",
      "epoch 118| loss: 0.50793 |  0:00:06s\n",
      "epoch 119| loss: 0.50638 |  0:00:06s\n",
      "epoch 120| loss: 0.50083 |  0:00:06s\n",
      "epoch 121| loss: 0.49515 |  0:00:06s\n",
      "epoch 122| loss: 0.502   |  0:00:06s\n",
      "epoch 123| loss: 0.49332 |  0:00:06s\n",
      "epoch 124| loss: 0.4991  |  0:00:06s\n",
      "epoch 125| loss: 0.4917  |  0:00:06s\n",
      "epoch 126| loss: 0.49438 |  0:00:06s\n",
      "epoch 127| loss: 0.4968  |  0:00:06s\n",
      "epoch 128| loss: 0.50005 |  0:00:06s\n",
      "epoch 129| loss: 0.49823 |  0:00:06s\n",
      "epoch 130| loss: 0.49974 |  0:00:06s\n",
      "epoch 131| loss: 0.50176 |  0:00:06s\n",
      "epoch 132| loss: 0.49304 |  0:00:06s\n",
      "epoch 133| loss: 0.48695 |  0:00:06s\n",
      "epoch 134| loss: 0.48768 |  0:00:06s\n",
      "epoch 135| loss: 0.48119 |  0:00:06s\n",
      "epoch 136| loss: 0.48485 |  0:00:06s\n",
      "epoch 137| loss: 0.48973 |  0:00:06s\n",
      "epoch 138| loss: 0.48797 |  0:00:07s\n",
      "epoch 139| loss: 0.48703 |  0:00:07s\n",
      "epoch 140| loss: 0.47922 |  0:00:07s\n",
      "epoch 141| loss: 0.48184 |  0:00:07s\n",
      "epoch 142| loss: 0.47582 |  0:00:07s\n",
      "epoch 143| loss: 0.47739 |  0:00:07s\n",
      "epoch 144| loss: 0.47033 |  0:00:07s\n",
      "epoch 145| loss: 0.47907 |  0:00:07s\n",
      "epoch 146| loss: 0.47939 |  0:00:07s\n",
      "epoch 147| loss: 0.46783 |  0:00:07s\n",
      "epoch 148| loss: 0.47382 |  0:00:07s\n",
      "epoch 149| loss: 0.47709 |  0:00:07s\n",
      "epoch 150| loss: 0.47867 |  0:00:07s\n",
      "epoch 151| loss: 0.48345 |  0:00:07s\n",
      "epoch 152| loss: 0.48133 |  0:00:07s\n",
      "epoch 153| loss: 0.47194 |  0:00:07s\n",
      "epoch 154| loss: 0.46681 |  0:00:07s\n",
      "epoch 155| loss: 0.4692  |  0:00:07s\n",
      "epoch 156| loss: 0.46922 |  0:00:07s\n",
      "epoch 157| loss: 0.47031 |  0:00:07s\n",
      "epoch 158| loss: 0.45762 |  0:00:08s\n",
      "epoch 159| loss: 0.46355 |  0:00:08s\n",
      "epoch 160| loss: 0.45802 |  0:00:08s\n",
      "epoch 161| loss: 0.45493 |  0:00:08s\n",
      "epoch 162| loss: 0.46301 |  0:00:08s\n",
      "epoch 163| loss: 0.45319 |  0:00:08s\n",
      "epoch 164| loss: 0.45701 |  0:00:08s\n",
      "epoch 165| loss: 0.45102 |  0:00:08s\n",
      "epoch 166| loss: 0.44688 |  0:00:08s\n",
      "epoch 167| loss: 0.44439 |  0:00:08s\n",
      "epoch 168| loss: 0.44229 |  0:00:08s\n",
      "epoch 169| loss: 0.44375 |  0:00:08s\n",
      "epoch 170| loss: 0.44936 |  0:00:08s\n",
      "epoch 171| loss: 0.44737 |  0:00:08s\n",
      "epoch 172| loss: 0.45106 |  0:00:08s\n",
      "epoch 173| loss: 0.4605  |  0:00:08s\n",
      "epoch 174| loss: 0.45417 |  0:00:08s\n",
      "epoch 175| loss: 0.44826 |  0:00:08s\n",
      "epoch 176| loss: 0.45029 |  0:00:08s\n",
      "epoch 177| loss: 0.44987 |  0:00:08s\n",
      "epoch 178| loss: 0.43897 |  0:00:09s\n",
      "epoch 179| loss: 0.44343 |  0:00:09s\n",
      "epoch 180| loss: 0.45477 |  0:00:09s\n",
      "epoch 181| loss: 0.44773 |  0:00:09s\n",
      "epoch 182| loss: 0.44837 |  0:00:09s\n",
      "epoch 183| loss: 0.44795 |  0:00:09s\n",
      "epoch 184| loss: 0.45039 |  0:00:09s\n",
      "epoch 185| loss: 0.45864 |  0:00:09s\n",
      "epoch 186| loss: 0.46736 |  0:00:09s\n",
      "epoch 187| loss: 0.46649 |  0:00:09s\n",
      "epoch 188| loss: 0.45579 |  0:00:09s\n",
      "epoch 189| loss: 0.4488  |  0:00:09s\n",
      "epoch 190| loss: 0.441   |  0:00:09s\n",
      "epoch 191| loss: 0.44188 |  0:00:09s\n",
      "epoch 192| loss: 0.44835 |  0:00:09s\n",
      "epoch 193| loss: 0.44028 |  0:00:09s\n",
      "epoch 194| loss: 0.43651 |  0:00:09s\n",
      "epoch 195| loss: 0.43371 |  0:00:09s\n",
      "epoch 196| loss: 0.44524 |  0:00:09s\n",
      "epoch 197| loss: 0.447   |  0:00:09s\n",
      "epoch 198| loss: 0.43894 |  0:00:09s\n",
      "epoch 199| loss: 0.43267 |  0:00:10s\n",
      "epoch 200| loss: 0.43323 |  0:00:10s\n",
      "epoch 201| loss: 0.43276 |  0:00:10s\n",
      "epoch 202| loss: 0.43287 |  0:00:10s\n",
      "epoch 203| loss: 0.42833 |  0:00:10s\n",
      "epoch 204| loss: 0.43023 |  0:00:10s\n",
      "epoch 205| loss: 0.43435 |  0:00:10s\n",
      "epoch 206| loss: 0.42238 |  0:00:10s\n",
      "epoch 207| loss: 0.42407 |  0:00:10s\n",
      "epoch 208| loss: 0.41521 |  0:00:10s\n",
      "epoch 209| loss: 0.41605 |  0:00:10s\n",
      "epoch 210| loss: 0.41795 |  0:00:10s\n",
      "epoch 211| loss: 0.411   |  0:00:10s\n",
      "epoch 212| loss: 0.40768 |  0:00:10s\n",
      "epoch 213| loss: 0.41223 |  0:00:10s\n",
      "epoch 214| loss: 0.41409 |  0:00:10s\n",
      "epoch 215| loss: 0.41288 |  0:00:10s\n",
      "epoch 216| loss: 0.41308 |  0:00:10s\n",
      "epoch 217| loss: 0.41653 |  0:00:10s\n",
      "epoch 218| loss: 0.4189  |  0:00:10s\n",
      "epoch 219| loss: 0.42104 |  0:00:11s\n",
      "epoch 220| loss: 0.4159  |  0:00:11s\n",
      "epoch 221| loss: 0.41558 |  0:00:11s\n",
      "epoch 222| loss: 0.40749 |  0:00:11s\n",
      "epoch 223| loss: 0.40598 |  0:00:11s\n",
      "epoch 224| loss: 0.40228 |  0:00:11s\n",
      "epoch 225| loss: 0.40888 |  0:00:11s\n",
      "epoch 226| loss: 0.41046 |  0:00:11s\n",
      "epoch 227| loss: 0.41072 |  0:00:11s\n",
      "epoch 228| loss: 0.40288 |  0:00:11s\n",
      "epoch 229| loss: 0.40456 |  0:00:11s\n",
      "epoch 230| loss: 0.40798 |  0:00:11s\n",
      "epoch 231| loss: 0.39988 |  0:00:11s\n",
      "epoch 232| loss: 0.40124 |  0:00:11s\n",
      "epoch 233| loss: 0.39467 |  0:00:11s\n",
      "epoch 234| loss: 0.39583 |  0:00:11s\n",
      "epoch 235| loss: 0.38883 |  0:00:11s\n",
      "epoch 236| loss: 0.39154 |  0:00:11s\n",
      "epoch 237| loss: 0.39441 |  0:00:11s\n",
      "epoch 238| loss: 0.40218 |  0:00:11s\n",
      "epoch 239| loss: 0.40518 |  0:00:11s\n",
      "epoch 240| loss: 0.40409 |  0:00:12s\n",
      "epoch 241| loss: 0.40142 |  0:00:12s\n",
      "epoch 242| loss: 0.3943  |  0:00:12s\n",
      "epoch 243| loss: 0.39672 |  0:00:12s\n",
      "epoch 244| loss: 0.39587 |  0:00:12s\n",
      "epoch 245| loss: 0.39902 |  0:00:12s\n",
      "epoch 246| loss: 0.41109 |  0:00:12s\n",
      "epoch 247| loss: 0.41564 |  0:00:12s\n",
      "epoch 248| loss: 0.42287 |  0:00:12s\n",
      "epoch 249| loss: 0.42134 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.09335 |  0:00:00s\n",
      "epoch 1  | loss: 1.79665 |  0:00:00s\n",
      "epoch 2  | loss: 1.30054 |  0:00:00s\n",
      "epoch 3  | loss: 1.11686 |  0:00:00s\n",
      "epoch 4  | loss: 1.02626 |  0:00:00s\n",
      "epoch 5  | loss: 0.95732 |  0:00:00s\n",
      "epoch 6  | loss: 0.92083 |  0:00:00s\n",
      "epoch 7  | loss: 0.88657 |  0:00:00s\n",
      "epoch 8  | loss: 0.86544 |  0:00:00s\n",
      "epoch 9  | loss: 0.84645 |  0:00:00s\n",
      "epoch 10 | loss: 0.81514 |  0:00:00s\n",
      "epoch 11 | loss: 0.78198 |  0:00:00s\n",
      "epoch 12 | loss: 0.76867 |  0:00:00s\n",
      "epoch 13 | loss: 0.76932 |  0:00:00s\n",
      "epoch 14 | loss: 0.77019 |  0:00:00s\n",
      "epoch 15 | loss: 0.75795 |  0:00:00s\n",
      "epoch 16 | loss: 0.75606 |  0:00:00s\n",
      "epoch 17 | loss: 0.73757 |  0:00:01s\n",
      "epoch 18 | loss: 0.73926 |  0:00:01s\n",
      "epoch 19 | loss: 0.71513 |  0:00:01s\n",
      "epoch 20 | loss: 0.70987 |  0:00:01s\n",
      "epoch 21 | loss: 0.70286 |  0:00:01s\n",
      "epoch 22 | loss: 0.70101 |  0:00:01s\n",
      "epoch 23 | loss: 0.692   |  0:00:01s\n",
      "epoch 24 | loss: 0.68933 |  0:00:01s\n",
      "epoch 25 | loss: 0.68135 |  0:00:01s\n",
      "epoch 26 | loss: 0.67625 |  0:00:01s\n",
      "epoch 27 | loss: 0.67279 |  0:00:01s\n",
      "epoch 28 | loss: 0.67923 |  0:00:01s\n",
      "epoch 29 | loss: 0.66846 |  0:00:01s\n",
      "epoch 30 | loss: 0.66816 |  0:00:01s\n",
      "epoch 31 | loss: 0.66117 |  0:00:01s\n",
      "epoch 32 | loss: 0.66605 |  0:00:01s\n",
      "epoch 33 | loss: 0.66009 |  0:00:01s\n",
      "epoch 34 | loss: 0.64767 |  0:00:01s\n",
      "epoch 35 | loss: 0.66688 |  0:00:01s\n",
      "epoch 36 | loss: 0.66274 |  0:00:01s\n",
      "epoch 37 | loss: 0.65122 |  0:00:02s\n",
      "epoch 38 | loss: 0.64606 |  0:00:02s\n",
      "epoch 39 | loss: 0.64536 |  0:00:02s\n",
      "epoch 40 | loss: 0.64345 |  0:00:02s\n",
      "epoch 41 | loss: 0.64003 |  0:00:02s\n",
      "epoch 42 | loss: 0.63726 |  0:00:02s\n",
      "epoch 43 | loss: 0.6395  |  0:00:02s\n",
      "epoch 44 | loss: 0.63116 |  0:00:02s\n",
      "epoch 45 | loss: 0.6266  |  0:00:02s\n",
      "epoch 46 | loss: 0.62211 |  0:00:02s\n",
      "epoch 47 | loss: 0.61812 |  0:00:02s\n",
      "epoch 48 | loss: 0.62053 |  0:00:02s\n",
      "epoch 49 | loss: 0.61883 |  0:00:02s\n",
      "epoch 50 | loss: 0.61512 |  0:00:02s\n",
      "epoch 51 | loss: 0.6185  |  0:00:02s\n",
      "epoch 52 | loss: 0.59988 |  0:00:02s\n",
      "epoch 53 | loss: 0.61463 |  0:00:02s\n",
      "epoch 54 | loss: 0.59792 |  0:00:02s\n",
      "epoch 55 | loss: 0.60177 |  0:00:02s\n",
      "epoch 56 | loss: 0.60334 |  0:00:02s\n",
      "epoch 57 | loss: 0.6012  |  0:00:02s\n",
      "epoch 58 | loss: 0.59622 |  0:00:03s\n",
      "epoch 59 | loss: 0.60263 |  0:00:03s\n",
      "epoch 60 | loss: 0.59753 |  0:00:03s\n",
      "epoch 61 | loss: 0.60205 |  0:00:03s\n",
      "epoch 62 | loss: 0.60533 |  0:00:03s\n",
      "epoch 63 | loss: 0.60781 |  0:00:03s\n",
      "epoch 64 | loss: 0.60587 |  0:00:03s\n",
      "epoch 65 | loss: 0.60642 |  0:00:03s\n",
      "epoch 66 | loss: 0.596   |  0:00:03s\n",
      "epoch 67 | loss: 0.59394 |  0:00:03s\n",
      "epoch 68 | loss: 0.58938 |  0:00:03s\n",
      "epoch 69 | loss: 0.59282 |  0:00:03s\n",
      "epoch 70 | loss: 0.58052 |  0:00:03s\n",
      "epoch 71 | loss: 0.58334 |  0:00:03s\n",
      "epoch 72 | loss: 0.57993 |  0:00:03s\n",
      "epoch 73 | loss: 0.57667 |  0:00:03s\n",
      "epoch 74 | loss: 0.5808  |  0:00:03s\n",
      "epoch 75 | loss: 0.56743 |  0:00:03s\n",
      "epoch 76 | loss: 0.56512 |  0:00:03s\n",
      "epoch 77 | loss: 0.57663 |  0:00:03s\n",
      "epoch 78 | loss: 0.57031 |  0:00:03s\n",
      "epoch 79 | loss: 0.56495 |  0:00:04s\n",
      "epoch 80 | loss: 0.56411 |  0:00:04s\n",
      "epoch 81 | loss: 0.55359 |  0:00:04s\n",
      "epoch 82 | loss: 0.55742 |  0:00:04s\n",
      "epoch 83 | loss: 0.54568 |  0:00:04s\n",
      "epoch 84 | loss: 0.55634 |  0:00:04s\n",
      "epoch 85 | loss: 0.55356 |  0:00:04s\n",
      "epoch 86 | loss: 0.54746 |  0:00:04s\n",
      "epoch 87 | loss: 0.55843 |  0:00:04s\n",
      "epoch 88 | loss: 0.55184 |  0:00:04s\n",
      "epoch 89 | loss: 0.55458 |  0:00:04s\n",
      "epoch 90 | loss: 0.55509 |  0:00:04s\n",
      "epoch 91 | loss: 0.54976 |  0:00:04s\n",
      "epoch 92 | loss: 0.55677 |  0:00:04s\n",
      "epoch 93 | loss: 0.55311 |  0:00:04s\n",
      "epoch 94 | loss: 0.55773 |  0:00:04s\n",
      "epoch 95 | loss: 0.55523 |  0:00:04s\n",
      "epoch 96 | loss: 0.54237 |  0:00:04s\n",
      "epoch 97 | loss: 0.55078 |  0:00:04s\n",
      "epoch 98 | loss: 0.55278 |  0:00:04s\n",
      "epoch 99 | loss: 0.56258 |  0:00:04s\n",
      "epoch 100| loss: 0.55173 |  0:00:04s\n",
      "epoch 101| loss: 0.55984 |  0:00:05s\n",
      "epoch 102| loss: 0.54936 |  0:00:05s\n",
      "epoch 103| loss: 0.54714 |  0:00:05s\n",
      "epoch 104| loss: 0.54271 |  0:00:05s\n",
      "epoch 105| loss: 0.54652 |  0:00:05s\n",
      "epoch 106| loss: 0.54342 |  0:00:05s\n",
      "epoch 107| loss: 0.55193 |  0:00:05s\n",
      "epoch 108| loss: 0.54604 |  0:00:05s\n",
      "epoch 109| loss: 0.53766 |  0:00:05s\n",
      "epoch 110| loss: 0.5417  |  0:00:05s\n",
      "epoch 111| loss: 0.53703 |  0:00:05s\n",
      "epoch 112| loss: 0.53961 |  0:00:05s\n",
      "epoch 113| loss: 0.54888 |  0:00:05s\n",
      "epoch 114| loss: 0.53125 |  0:00:05s\n",
      "epoch 115| loss: 0.54093 |  0:00:05s\n",
      "epoch 116| loss: 0.53722 |  0:00:05s\n",
      "epoch 117| loss: 0.5284  |  0:00:05s\n",
      "epoch 118| loss: 0.53469 |  0:00:05s\n",
      "epoch 119| loss: 0.52702 |  0:00:05s\n",
      "epoch 120| loss: 0.52446 |  0:00:05s\n",
      "epoch 121| loss: 0.52587 |  0:00:05s\n",
      "epoch 122| loss: 0.53125 |  0:00:05s\n",
      "epoch 123| loss: 0.52034 |  0:00:06s\n",
      "epoch 124| loss: 0.5203  |  0:00:06s\n",
      "epoch 125| loss: 0.51852 |  0:00:06s\n",
      "epoch 126| loss: 0.51275 |  0:00:06s\n",
      "epoch 127| loss: 0.51449 |  0:00:06s\n",
      "epoch 128| loss: 0.50675 |  0:00:06s\n",
      "epoch 129| loss: 0.51139 |  0:00:06s\n",
      "epoch 130| loss: 0.5156  |  0:00:06s\n",
      "epoch 131| loss: 0.50827 |  0:00:06s\n",
      "epoch 132| loss: 0.49593 |  0:00:06s\n",
      "epoch 133| loss: 0.50679 |  0:00:06s\n",
      "epoch 134| loss: 0.50812 |  0:00:06s\n",
      "epoch 135| loss: 0.50854 |  0:00:06s\n",
      "epoch 136| loss: 0.52084 |  0:00:06s\n",
      "epoch 137| loss: 0.5178  |  0:00:06s\n",
      "epoch 138| loss: 0.50301 |  0:00:06s\n",
      "epoch 139| loss: 0.50917 |  0:00:06s\n",
      "epoch 140| loss: 0.52023 |  0:00:06s\n",
      "epoch 141| loss: 0.51005 |  0:00:06s\n",
      "epoch 142| loss: 0.51092 |  0:00:06s\n",
      "epoch 143| loss: 0.51632 |  0:00:06s\n",
      "epoch 144| loss: 0.51954 |  0:00:06s\n",
      "epoch 145| loss: 0.51449 |  0:00:07s\n",
      "epoch 146| loss: 0.51059 |  0:00:07s\n",
      "epoch 147| loss: 0.51747 |  0:00:07s\n",
      "epoch 148| loss: 0.50237 |  0:00:07s\n",
      "epoch 149| loss: 0.49977 |  0:00:07s\n",
      "epoch 150| loss: 0.49397 |  0:00:07s\n",
      "epoch 151| loss: 0.49225 |  0:00:07s\n",
      "epoch 152| loss: 0.48846 |  0:00:07s\n",
      "epoch 153| loss: 0.49091 |  0:00:07s\n",
      "epoch 154| loss: 0.48891 |  0:00:07s\n",
      "epoch 155| loss: 0.49028 |  0:00:07s\n",
      "epoch 156| loss: 0.49563 |  0:00:07s\n",
      "epoch 157| loss: 0.49428 |  0:00:07s\n",
      "epoch 158| loss: 0.48831 |  0:00:07s\n",
      "epoch 159| loss: 0.49319 |  0:00:07s\n",
      "epoch 160| loss: 0.4921  |  0:00:07s\n",
      "epoch 161| loss: 0.48868 |  0:00:07s\n",
      "epoch 162| loss: 0.4931  |  0:00:07s\n",
      "epoch 163| loss: 0.47832 |  0:00:07s\n",
      "epoch 164| loss: 0.48427 |  0:00:07s\n",
      "epoch 165| loss: 0.47784 |  0:00:07s\n",
      "epoch 166| loss: 0.47969 |  0:00:07s\n",
      "epoch 167| loss: 0.4729  |  0:00:08s\n",
      "epoch 168| loss: 0.47493 |  0:00:08s\n",
      "epoch 169| loss: 0.47396 |  0:00:08s\n",
      "epoch 170| loss: 0.47957 |  0:00:08s\n",
      "epoch 171| loss: 0.47746 |  0:00:08s\n",
      "epoch 172| loss: 0.47508 |  0:00:08s\n",
      "epoch 173| loss: 0.47926 |  0:00:08s\n",
      "epoch 174| loss: 0.46989 |  0:00:08s\n",
      "epoch 175| loss: 0.47491 |  0:00:08s\n",
      "epoch 176| loss: 0.47683 |  0:00:08s\n",
      "epoch 177| loss: 0.47348 |  0:00:08s\n",
      "epoch 178| loss: 0.46864 |  0:00:08s\n",
      "epoch 179| loss: 0.46701 |  0:00:08s\n",
      "epoch 180| loss: 0.47075 |  0:00:08s\n",
      "epoch 181| loss: 0.46147 |  0:00:08s\n",
      "epoch 182| loss: 0.46518 |  0:00:08s\n",
      "epoch 183| loss: 0.47282 |  0:00:08s\n",
      "epoch 184| loss: 0.47382 |  0:00:08s\n",
      "epoch 185| loss: 0.47613 |  0:00:09s\n",
      "epoch 186| loss: 0.47157 |  0:00:09s\n",
      "epoch 187| loss: 0.47805 |  0:00:09s\n",
      "epoch 188| loss: 0.48132 |  0:00:09s\n",
      "epoch 189| loss: 0.46751 |  0:00:09s\n",
      "epoch 190| loss: 0.47631 |  0:00:09s\n",
      "epoch 191| loss: 0.48383 |  0:00:09s\n",
      "epoch 192| loss: 0.49423 |  0:00:09s\n",
      "epoch 193| loss: 0.49105 |  0:00:09s\n",
      "epoch 194| loss: 0.47816 |  0:00:09s\n",
      "epoch 195| loss: 0.47235 |  0:00:09s\n",
      "epoch 196| loss: 0.46922 |  0:00:09s\n",
      "epoch 197| loss: 0.47407 |  0:00:09s\n",
      "epoch 198| loss: 0.46189 |  0:00:09s\n",
      "epoch 199| loss: 0.45997 |  0:00:09s\n",
      "epoch 200| loss: 0.47209 |  0:00:09s\n",
      "epoch 201| loss: 0.4635  |  0:00:09s\n",
      "epoch 202| loss: 0.47328 |  0:00:09s\n",
      "epoch 203| loss: 0.46848 |  0:00:09s\n",
      "epoch 204| loss: 0.47988 |  0:00:09s\n",
      "epoch 205| loss: 0.4728  |  0:00:10s\n",
      "epoch 206| loss: 0.47146 |  0:00:10s\n",
      "epoch 207| loss: 0.46432 |  0:00:10s\n",
      "epoch 208| loss: 0.46399 |  0:00:10s\n",
      "epoch 209| loss: 0.46276 |  0:00:10s\n",
      "epoch 210| loss: 0.46859 |  0:00:10s\n",
      "epoch 211| loss: 0.473   |  0:00:10s\n",
      "epoch 212| loss: 0.48024 |  0:00:10s\n",
      "epoch 213| loss: 0.48557 |  0:00:10s\n",
      "epoch 214| loss: 0.47576 |  0:00:10s\n",
      "epoch 215| loss: 0.47255 |  0:00:10s\n",
      "epoch 216| loss: 0.46839 |  0:00:10s\n",
      "epoch 217| loss: 0.45428 |  0:00:10s\n",
      "epoch 218| loss: 0.452   |  0:00:10s\n",
      "epoch 219| loss: 0.44963 |  0:00:10s\n",
      "epoch 220| loss: 0.45832 |  0:00:10s\n",
      "epoch 221| loss: 0.4439  |  0:00:10s\n",
      "epoch 222| loss: 0.44421 |  0:00:10s\n",
      "epoch 223| loss: 0.45066 |  0:00:10s\n",
      "epoch 224| loss: 0.43986 |  0:00:10s\n",
      "epoch 225| loss: 0.45022 |  0:00:11s\n",
      "epoch 226| loss: 0.43637 |  0:00:11s\n",
      "epoch 227| loss: 0.43987 |  0:00:11s\n",
      "epoch 228| loss: 0.43011 |  0:00:11s\n",
      "epoch 229| loss: 0.43302 |  0:00:11s\n",
      "epoch 230| loss: 0.43981 |  0:00:11s\n",
      "epoch 231| loss: 0.43865 |  0:00:11s\n",
      "epoch 232| loss: 0.43091 |  0:00:11s\n",
      "epoch 233| loss: 0.43859 |  0:00:11s\n",
      "epoch 234| loss: 0.42765 |  0:00:11s\n",
      "epoch 235| loss: 0.43588 |  0:00:11s\n",
      "epoch 236| loss: 0.4289  |  0:00:11s\n",
      "epoch 237| loss: 0.42676 |  0:00:11s\n",
      "epoch 238| loss: 0.43292 |  0:00:11s\n",
      "epoch 239| loss: 0.43023 |  0:00:11s\n",
      "epoch 240| loss: 0.42834 |  0:00:11s\n",
      "epoch 241| loss: 0.42445 |  0:00:11s\n",
      "epoch 242| loss: 0.42267 |  0:00:11s\n",
      "epoch 243| loss: 0.42939 |  0:00:11s\n",
      "epoch 244| loss: 0.42696 |  0:00:11s\n",
      "epoch 245| loss: 0.41563 |  0:00:11s\n",
      "epoch 246| loss: 0.4191  |  0:00:11s\n",
      "epoch 247| loss: 0.4225  |  0:00:11s\n",
      "epoch 248| loss: 0.42206 |  0:00:12s\n",
      "epoch 249| loss: 0.42164 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.1083  |  0:00:00s\n",
      "epoch 1  | loss: 1.76697 |  0:00:00s\n",
      "epoch 2  | loss: 1.32472 |  0:00:00s\n",
      "epoch 3  | loss: 1.14016 |  0:00:00s\n",
      "epoch 4  | loss: 1.03809 |  0:00:00s\n",
      "epoch 5  | loss: 0.94733 |  0:00:00s\n",
      "epoch 6  | loss: 0.93782 |  0:00:00s\n",
      "epoch 7  | loss: 0.89979 |  0:00:00s\n",
      "epoch 8  | loss: 0.88106 |  0:00:00s\n",
      "epoch 9  | loss: 0.85355 |  0:00:00s\n",
      "epoch 10 | loss: 0.83657 |  0:00:00s\n",
      "epoch 11 | loss: 0.80136 |  0:00:00s\n",
      "epoch 12 | loss: 0.79955 |  0:00:00s\n",
      "epoch 13 | loss: 0.7746  |  0:00:00s\n",
      "epoch 14 | loss: 0.76775 |  0:00:00s\n",
      "epoch 15 | loss: 0.77119 |  0:00:00s\n",
      "epoch 16 | loss: 0.75583 |  0:00:00s\n",
      "epoch 17 | loss: 0.7429  |  0:00:00s\n",
      "epoch 18 | loss: 0.72835 |  0:00:00s\n",
      "epoch 19 | loss: 0.72317 |  0:00:00s\n",
      "epoch 20 | loss: 0.71693 |  0:00:00s\n",
      "epoch 21 | loss: 0.71214 |  0:00:01s\n",
      "epoch 22 | loss: 0.69435 |  0:00:01s\n",
      "epoch 23 | loss: 0.68989 |  0:00:01s\n",
      "epoch 24 | loss: 0.68945 |  0:00:01s\n",
      "epoch 25 | loss: 0.68516 |  0:00:01s\n",
      "epoch 26 | loss: 0.6863  |  0:00:01s\n",
      "epoch 27 | loss: 0.68066 |  0:00:01s\n",
      "epoch 28 | loss: 0.68244 |  0:00:01s\n",
      "epoch 29 | loss: 0.69497 |  0:00:01s\n",
      "epoch 30 | loss: 0.68059 |  0:00:01s\n",
      "epoch 31 | loss: 0.66447 |  0:00:01s\n",
      "epoch 32 | loss: 0.67048 |  0:00:01s\n",
      "epoch 33 | loss: 0.66504 |  0:00:01s\n",
      "epoch 34 | loss: 0.66473 |  0:00:01s\n",
      "epoch 35 | loss: 0.65307 |  0:00:01s\n",
      "epoch 36 | loss: 0.65697 |  0:00:01s\n",
      "epoch 37 | loss: 0.65492 |  0:00:01s\n",
      "epoch 38 | loss: 0.64578 |  0:00:01s\n",
      "epoch 39 | loss: 0.64151 |  0:00:01s\n",
      "epoch 40 | loss: 0.6366  |  0:00:01s\n",
      "epoch 41 | loss: 0.64709 |  0:00:01s\n",
      "epoch 42 | loss: 0.63258 |  0:00:01s\n",
      "epoch 43 | loss: 0.63329 |  0:00:02s\n",
      "epoch 44 | loss: 0.62999 |  0:00:02s\n",
      "epoch 45 | loss: 0.62874 |  0:00:02s\n",
      "epoch 46 | loss: 0.62332 |  0:00:02s\n",
      "epoch 47 | loss: 0.61561 |  0:00:02s\n",
      "epoch 48 | loss: 0.61959 |  0:00:02s\n",
      "epoch 49 | loss: 0.6149  |  0:00:02s\n",
      "epoch 50 | loss: 0.60735 |  0:00:02s\n",
      "epoch 51 | loss: 0.60916 |  0:00:02s\n",
      "epoch 52 | loss: 0.60746 |  0:00:02s\n",
      "epoch 53 | loss: 0.61406 |  0:00:02s\n",
      "epoch 54 | loss: 0.6172  |  0:00:02s\n",
      "epoch 55 | loss: 0.61623 |  0:00:02s\n",
      "epoch 56 | loss: 0.62438 |  0:00:02s\n",
      "epoch 57 | loss: 0.6195  |  0:00:02s\n",
      "epoch 58 | loss: 0.61871 |  0:00:02s\n",
      "epoch 59 | loss: 0.60634 |  0:00:02s\n",
      "epoch 60 | loss: 0.61351 |  0:00:02s\n",
      "epoch 61 | loss: 0.60894 |  0:00:02s\n",
      "epoch 62 | loss: 0.60729 |  0:00:02s\n",
      "epoch 63 | loss: 0.60424 |  0:00:02s\n",
      "epoch 64 | loss: 0.59092 |  0:00:03s\n",
      "epoch 65 | loss: 0.5809  |  0:00:03s\n",
      "epoch 66 | loss: 0.58342 |  0:00:03s\n",
      "epoch 67 | loss: 0.59334 |  0:00:03s\n",
      "epoch 68 | loss: 0.59133 |  0:00:03s\n",
      "epoch 69 | loss: 0.59229 |  0:00:03s\n",
      "epoch 70 | loss: 0.58328 |  0:00:03s\n",
      "epoch 71 | loss: 0.58854 |  0:00:03s\n",
      "epoch 72 | loss: 0.57952 |  0:00:03s\n",
      "epoch 73 | loss: 0.58231 |  0:00:03s\n",
      "epoch 74 | loss: 0.57784 |  0:00:03s\n",
      "epoch 75 | loss: 0.57113 |  0:00:03s\n",
      "epoch 76 | loss: 0.56818 |  0:00:03s\n",
      "epoch 77 | loss: 0.563   |  0:00:03s\n",
      "epoch 78 | loss: 0.55929 |  0:00:03s\n",
      "epoch 79 | loss: 0.55406 |  0:00:03s\n",
      "epoch 80 | loss: 0.56359 |  0:00:03s\n",
      "epoch 81 | loss: 0.54415 |  0:00:03s\n",
      "epoch 82 | loss: 0.55206 |  0:00:03s\n",
      "epoch 83 | loss: 0.54524 |  0:00:03s\n",
      "epoch 84 | loss: 0.54432 |  0:00:03s\n",
      "epoch 85 | loss: 0.54633 |  0:00:04s\n",
      "epoch 86 | loss: 0.54405 |  0:00:04s\n",
      "epoch 87 | loss: 0.54371 |  0:00:04s\n",
      "epoch 88 | loss: 0.53201 |  0:00:04s\n",
      "epoch 89 | loss: 0.52992 |  0:00:04s\n",
      "epoch 90 | loss: 0.53022 |  0:00:04s\n",
      "epoch 91 | loss: 0.53292 |  0:00:04s\n",
      "epoch 92 | loss: 0.53349 |  0:00:04s\n",
      "epoch 93 | loss: 0.53266 |  0:00:04s\n",
      "epoch 94 | loss: 0.527   |  0:00:04s\n",
      "epoch 95 | loss: 0.51949 |  0:00:04s\n",
      "epoch 96 | loss: 0.51589 |  0:00:04s\n",
      "epoch 97 | loss: 0.52051 |  0:00:04s\n",
      "epoch 98 | loss: 0.51277 |  0:00:04s\n",
      "epoch 99 | loss: 0.52347 |  0:00:04s\n",
      "epoch 100| loss: 0.51162 |  0:00:04s\n",
      "epoch 101| loss: 0.51548 |  0:00:04s\n",
      "epoch 102| loss: 0.50792 |  0:00:04s\n",
      "epoch 103| loss: 0.49886 |  0:00:04s\n",
      "epoch 104| loss: 0.49919 |  0:00:04s\n",
      "epoch 105| loss: 0.50619 |  0:00:04s\n",
      "epoch 106| loss: 0.50649 |  0:00:04s\n",
      "epoch 107| loss: 0.50093 |  0:00:05s\n",
      "epoch 108| loss: 0.49996 |  0:00:05s\n",
      "epoch 109| loss: 0.49851 |  0:00:05s\n",
      "epoch 110| loss: 0.5001  |  0:00:05s\n",
      "epoch 111| loss: 0.49784 |  0:00:05s\n",
      "epoch 112| loss: 0.48579 |  0:00:05s\n",
      "epoch 113| loss: 0.49078 |  0:00:05s\n",
      "epoch 114| loss: 0.48718 |  0:00:05s\n",
      "epoch 115| loss: 0.49824 |  0:00:05s\n",
      "epoch 116| loss: 0.49654 |  0:00:05s\n",
      "epoch 117| loss: 0.49573 |  0:00:05s\n",
      "epoch 118| loss: 0.48986 |  0:00:05s\n",
      "epoch 119| loss: 0.49143 |  0:00:05s\n",
      "epoch 120| loss: 0.48322 |  0:00:05s\n",
      "epoch 121| loss: 0.48292 |  0:00:05s\n",
      "epoch 122| loss: 0.49334 |  0:00:05s\n",
      "epoch 123| loss: 0.48691 |  0:00:05s\n",
      "epoch 124| loss: 0.48716 |  0:00:05s\n",
      "epoch 125| loss: 0.48889 |  0:00:05s\n",
      "epoch 126| loss: 0.48344 |  0:00:06s\n",
      "epoch 127| loss: 0.48037 |  0:00:06s\n",
      "epoch 128| loss: 0.48306 |  0:00:06s\n",
      "epoch 129| loss: 0.4893  |  0:00:06s\n",
      "epoch 130| loss: 0.4866  |  0:00:06s\n",
      "epoch 131| loss: 0.47366 |  0:00:06s\n",
      "epoch 132| loss: 0.46503 |  0:00:06s\n",
      "epoch 133| loss: 0.46621 |  0:00:06s\n",
      "epoch 134| loss: 0.46101 |  0:00:06s\n",
      "epoch 135| loss: 0.47142 |  0:00:06s\n",
      "epoch 136| loss: 0.46675 |  0:00:06s\n",
      "epoch 137| loss: 0.46614 |  0:00:06s\n",
      "epoch 138| loss: 0.47011 |  0:00:06s\n",
      "epoch 139| loss: 0.46568 |  0:00:06s\n",
      "epoch 140| loss: 0.46046 |  0:00:06s\n",
      "epoch 141| loss: 0.4592  |  0:00:06s\n",
      "epoch 142| loss: 0.45765 |  0:00:06s\n",
      "epoch 143| loss: 0.45232 |  0:00:06s\n",
      "epoch 144| loss: 0.4527  |  0:00:06s\n",
      "epoch 145| loss: 0.44945 |  0:00:06s\n",
      "epoch 146| loss: 0.45498 |  0:00:06s\n",
      "epoch 147| loss: 0.45275 |  0:00:06s\n",
      "epoch 148| loss: 0.44489 |  0:00:06s\n",
      "epoch 149| loss: 0.44984 |  0:00:07s\n",
      "epoch 150| loss: 0.4463  |  0:00:07s\n",
      "epoch 151| loss: 0.45187 |  0:00:07s\n",
      "epoch 152| loss: 0.4476  |  0:00:07s\n",
      "epoch 153| loss: 0.44152 |  0:00:07s\n",
      "epoch 154| loss: 0.44385 |  0:00:07s\n",
      "epoch 155| loss: 0.43739 |  0:00:07s\n",
      "epoch 156| loss: 0.44929 |  0:00:07s\n",
      "epoch 157| loss: 0.44896 |  0:00:07s\n",
      "epoch 158| loss: 0.43371 |  0:00:07s\n",
      "epoch 159| loss: 0.43513 |  0:00:07s\n",
      "epoch 160| loss: 0.42882 |  0:00:07s\n",
      "epoch 161| loss: 0.42784 |  0:00:07s\n",
      "epoch 162| loss: 0.4418  |  0:00:07s\n",
      "epoch 163| loss: 0.42731 |  0:00:07s\n",
      "epoch 164| loss: 0.43623 |  0:00:07s\n",
      "epoch 165| loss: 0.42447 |  0:00:07s\n",
      "epoch 166| loss: 0.43153 |  0:00:07s\n",
      "epoch 167| loss: 0.4207  |  0:00:07s\n",
      "epoch 168| loss: 0.42401 |  0:00:07s\n",
      "epoch 169| loss: 0.43038 |  0:00:07s\n",
      "epoch 170| loss: 0.42871 |  0:00:07s\n",
      "epoch 171| loss: 0.4225  |  0:00:08s\n",
      "epoch 172| loss: 0.42982 |  0:00:08s\n",
      "epoch 173| loss: 0.42291 |  0:00:08s\n",
      "epoch 174| loss: 0.42588 |  0:00:08s\n",
      "epoch 175| loss: 0.41881 |  0:00:08s\n",
      "epoch 176| loss: 0.42987 |  0:00:08s\n",
      "epoch 177| loss: 0.4245  |  0:00:08s\n",
      "epoch 178| loss: 0.41946 |  0:00:08s\n",
      "epoch 179| loss: 0.42559 |  0:00:08s\n",
      "epoch 180| loss: 0.43935 |  0:00:08s\n",
      "epoch 181| loss: 0.42699 |  0:00:08s\n",
      "epoch 182| loss: 0.43504 |  0:00:08s\n",
      "epoch 183| loss: 0.4312  |  0:00:08s\n",
      "epoch 184| loss: 0.42549 |  0:00:08s\n",
      "epoch 185| loss: 0.4257  |  0:00:08s\n",
      "epoch 186| loss: 0.42102 |  0:00:08s\n",
      "epoch 187| loss: 0.4319  |  0:00:08s\n",
      "epoch 188| loss: 0.42519 |  0:00:08s\n",
      "epoch 189| loss: 0.41704 |  0:00:08s\n",
      "epoch 190| loss: 0.42761 |  0:00:08s\n",
      "epoch 191| loss: 0.43159 |  0:00:08s\n",
      "epoch 192| loss: 0.44982 |  0:00:08s\n",
      "epoch 193| loss: 0.44919 |  0:00:09s\n",
      "epoch 194| loss: 0.4431  |  0:00:09s\n",
      "epoch 195| loss: 0.44686 |  0:00:09s\n",
      "epoch 196| loss: 0.45603 |  0:00:09s\n",
      "epoch 197| loss: 0.45061 |  0:00:09s\n",
      "epoch 198| loss: 0.45055 |  0:00:09s\n",
      "epoch 199| loss: 0.43597 |  0:00:09s\n",
      "epoch 200| loss: 0.43753 |  0:00:09s\n",
      "epoch 201| loss: 0.43123 |  0:00:09s\n",
      "epoch 202| loss: 0.44245 |  0:00:09s\n",
      "epoch 203| loss: 0.4298  |  0:00:09s\n",
      "epoch 204| loss: 0.43453 |  0:00:09s\n",
      "epoch 205| loss: 0.4272  |  0:00:09s\n",
      "epoch 206| loss: 0.41802 |  0:00:09s\n",
      "epoch 207| loss: 0.42354 |  0:00:09s\n",
      "epoch 208| loss: 0.42335 |  0:00:09s\n",
      "epoch 209| loss: 0.41234 |  0:00:09s\n",
      "epoch 210| loss: 0.41673 |  0:00:09s\n",
      "epoch 211| loss: 0.41685 |  0:00:09s\n",
      "epoch 212| loss: 0.42443 |  0:00:09s\n",
      "epoch 213| loss: 0.42162 |  0:00:09s\n",
      "epoch 214| loss: 0.41351 |  0:00:09s\n",
      "epoch 215| loss: 0.41406 |  0:00:10s\n",
      "epoch 216| loss: 0.41893 |  0:00:10s\n",
      "epoch 217| loss: 0.40849 |  0:00:10s\n",
      "epoch 218| loss: 0.40857 |  0:00:10s\n",
      "epoch 219| loss: 0.40558 |  0:00:10s\n",
      "epoch 220| loss: 0.41778 |  0:00:10s\n",
      "epoch 221| loss: 0.40155 |  0:00:10s\n",
      "epoch 222| loss: 0.40582 |  0:00:10s\n",
      "epoch 223| loss: 0.40308 |  0:00:10s\n",
      "epoch 224| loss: 0.39614 |  0:00:10s\n",
      "epoch 225| loss: 0.40921 |  0:00:10s\n",
      "epoch 226| loss: 0.40059 |  0:00:10s\n",
      "epoch 227| loss: 0.40444 |  0:00:10s\n",
      "epoch 228| loss: 0.40612 |  0:00:10s\n",
      "epoch 229| loss: 0.40639 |  0:00:10s\n",
      "epoch 230| loss: 0.42104 |  0:00:10s\n",
      "epoch 231| loss: 0.41902 |  0:00:10s\n",
      "epoch 232| loss: 0.41447 |  0:00:10s\n",
      "epoch 233| loss: 0.41293 |  0:00:10s\n",
      "epoch 234| loss: 0.41061 |  0:00:10s\n",
      "epoch 235| loss: 0.40931 |  0:00:10s\n",
      "epoch 236| loss: 0.41594 |  0:00:10s\n",
      "epoch 237| loss: 0.413   |  0:00:11s\n",
      "epoch 238| loss: 0.41557 |  0:00:11s\n",
      "epoch 239| loss: 0.40221 |  0:00:11s\n",
      "epoch 240| loss: 0.40109 |  0:00:11s\n",
      "epoch 241| loss: 0.39786 |  0:00:11s\n",
      "epoch 242| loss: 0.40458 |  0:00:11s\n",
      "epoch 243| loss: 0.40444 |  0:00:11s\n",
      "epoch 244| loss: 0.40598 |  0:00:11s\n",
      "epoch 245| loss: 0.4028  |  0:00:11s\n",
      "epoch 246| loss: 0.39765 |  0:00:11s\n",
      "epoch 247| loss: 0.39279 |  0:00:11s\n",
      "epoch 248| loss: 0.40035 |  0:00:11s\n",
      "epoch 249| loss: 0.4022  |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.08547 |  0:00:00s\n",
      "epoch 1  | loss: 1.82628 |  0:00:00s\n",
      "epoch 2  | loss: 1.30046 |  0:00:00s\n",
      "epoch 3  | loss: 1.11482 |  0:00:00s\n",
      "epoch 4  | loss: 1.00093 |  0:00:00s\n",
      "epoch 5  | loss: 0.95832 |  0:00:00s\n",
      "epoch 6  | loss: 0.89901 |  0:00:00s\n",
      "epoch 7  | loss: 0.87016 |  0:00:00s\n",
      "epoch 8  | loss: 0.84628 |  0:00:00s\n",
      "epoch 9  | loss: 0.82048 |  0:00:00s\n",
      "epoch 10 | loss: 0.79145 |  0:00:00s\n",
      "epoch 11 | loss: 0.77473 |  0:00:00s\n",
      "epoch 12 | loss: 0.77184 |  0:00:00s\n",
      "epoch 13 | loss: 0.7512  |  0:00:00s\n",
      "epoch 14 | loss: 0.72758 |  0:00:00s\n",
      "epoch 15 | loss: 0.72138 |  0:00:00s\n",
      "epoch 16 | loss: 0.70718 |  0:00:00s\n",
      "epoch 17 | loss: 0.70844 |  0:00:00s\n",
      "epoch 18 | loss: 0.70185 |  0:00:00s\n",
      "epoch 19 | loss: 0.69913 |  0:00:00s\n",
      "epoch 20 | loss: 0.69866 |  0:00:00s\n",
      "epoch 21 | loss: 0.69738 |  0:00:00s\n",
      "epoch 22 | loss: 0.70511 |  0:00:01s\n",
      "epoch 23 | loss: 0.68917 |  0:00:01s\n",
      "epoch 24 | loss: 0.68636 |  0:00:01s\n",
      "epoch 25 | loss: 0.68086 |  0:00:01s\n",
      "epoch 26 | loss: 0.68037 |  0:00:01s\n",
      "epoch 27 | loss: 0.67065 |  0:00:01s\n",
      "epoch 28 | loss: 0.67508 |  0:00:01s\n",
      "epoch 29 | loss: 0.66918 |  0:00:01s\n",
      "epoch 30 | loss: 0.65771 |  0:00:01s\n",
      "epoch 31 | loss: 0.64719 |  0:00:01s\n",
      "epoch 32 | loss: 0.65056 |  0:00:01s\n",
      "epoch 33 | loss: 0.64364 |  0:00:01s\n",
      "epoch 34 | loss: 0.63337 |  0:00:01s\n",
      "epoch 35 | loss: 0.63331 |  0:00:01s\n",
      "epoch 36 | loss: 0.63825 |  0:00:01s\n",
      "epoch 37 | loss: 0.63176 |  0:00:01s\n",
      "epoch 38 | loss: 0.62518 |  0:00:01s\n",
      "epoch 39 | loss: 0.62141 |  0:00:01s\n",
      "epoch 40 | loss: 0.61597 |  0:00:01s\n",
      "epoch 41 | loss: 0.62536 |  0:00:01s\n",
      "epoch 42 | loss: 0.61741 |  0:00:01s\n",
      "epoch 43 | loss: 0.61505 |  0:00:01s\n",
      "epoch 44 | loss: 0.61657 |  0:00:02s\n",
      "epoch 45 | loss: 0.61651 |  0:00:02s\n",
      "epoch 46 | loss: 0.60166 |  0:00:02s\n",
      "epoch 47 | loss: 0.61192 |  0:00:02s\n",
      "epoch 48 | loss: 0.60378 |  0:00:02s\n",
      "epoch 49 | loss: 0.60312 |  0:00:02s\n",
      "epoch 50 | loss: 0.59534 |  0:00:02s\n",
      "epoch 51 | loss: 0.59168 |  0:00:02s\n",
      "epoch 52 | loss: 0.58858 |  0:00:02s\n",
      "epoch 53 | loss: 0.59278 |  0:00:02s\n",
      "epoch 54 | loss: 0.59348 |  0:00:02s\n",
      "epoch 55 | loss: 0.59705 |  0:00:02s\n",
      "epoch 56 | loss: 0.58945 |  0:00:02s\n",
      "epoch 57 | loss: 0.59378 |  0:00:02s\n",
      "epoch 58 | loss: 0.58503 |  0:00:02s\n",
      "epoch 59 | loss: 0.58303 |  0:00:02s\n",
      "epoch 60 | loss: 0.59094 |  0:00:02s\n",
      "epoch 61 | loss: 0.59232 |  0:00:02s\n",
      "epoch 62 | loss: 0.59004 |  0:00:02s\n",
      "epoch 63 | loss: 0.58528 |  0:00:03s\n",
      "epoch 64 | loss: 0.58146 |  0:00:03s\n",
      "epoch 65 | loss: 0.57415 |  0:00:03s\n",
      "epoch 66 | loss: 0.57358 |  0:00:03s\n",
      "epoch 67 | loss: 0.57683 |  0:00:03s\n",
      "epoch 68 | loss: 0.57807 |  0:00:03s\n",
      "epoch 69 | loss: 0.57042 |  0:00:03s\n",
      "epoch 70 | loss: 0.57418 |  0:00:03s\n",
      "epoch 71 | loss: 0.56223 |  0:00:03s\n",
      "epoch 72 | loss: 0.56661 |  0:00:03s\n",
      "epoch 73 | loss: 0.5651  |  0:00:03s\n",
      "epoch 74 | loss: 0.57336 |  0:00:03s\n",
      "epoch 75 | loss: 0.56828 |  0:00:03s\n",
      "epoch 76 | loss: 0.55683 |  0:00:03s\n",
      "epoch 77 | loss: 0.56201 |  0:00:03s\n",
      "epoch 78 | loss: 0.55577 |  0:00:03s\n",
      "epoch 79 | loss: 0.54891 |  0:00:03s\n",
      "epoch 80 | loss: 0.55235 |  0:00:03s\n",
      "epoch 81 | loss: 0.55075 |  0:00:03s\n",
      "epoch 82 | loss: 0.54689 |  0:00:03s\n",
      "epoch 83 | loss: 0.54742 |  0:00:03s\n",
      "epoch 84 | loss: 0.55186 |  0:00:03s\n",
      "epoch 85 | loss: 0.54652 |  0:00:04s\n",
      "epoch 86 | loss: 0.55151 |  0:00:04s\n",
      "epoch 87 | loss: 0.54954 |  0:00:04s\n",
      "epoch 88 | loss: 0.54219 |  0:00:04s\n",
      "epoch 89 | loss: 0.53829 |  0:00:04s\n",
      "epoch 90 | loss: 0.54005 |  0:00:04s\n",
      "epoch 91 | loss: 0.54803 |  0:00:04s\n",
      "epoch 92 | loss: 0.53876 |  0:00:04s\n",
      "epoch 93 | loss: 0.5442  |  0:00:04s\n",
      "epoch 94 | loss: 0.54015 |  0:00:04s\n",
      "epoch 95 | loss: 0.54472 |  0:00:04s\n",
      "epoch 96 | loss: 0.5451  |  0:00:04s\n",
      "epoch 97 | loss: 0.53326 |  0:00:04s\n",
      "epoch 98 | loss: 0.53224 |  0:00:04s\n",
      "epoch 99 | loss: 0.53875 |  0:00:04s\n",
      "epoch 100| loss: 0.51983 |  0:00:04s\n",
      "epoch 101| loss: 0.52697 |  0:00:04s\n",
      "epoch 102| loss: 0.53059 |  0:00:04s\n",
      "epoch 103| loss: 0.52537 |  0:00:04s\n",
      "epoch 104| loss: 0.53736 |  0:00:04s\n",
      "epoch 105| loss: 0.52705 |  0:00:04s\n",
      "epoch 106| loss: 0.53704 |  0:00:04s\n",
      "epoch 107| loss: 0.53002 |  0:00:05s\n",
      "epoch 108| loss: 0.54571 |  0:00:05s\n",
      "epoch 109| loss: 0.53845 |  0:00:05s\n",
      "epoch 110| loss: 0.53633 |  0:00:05s\n",
      "epoch 111| loss: 0.53638 |  0:00:05s\n",
      "epoch 112| loss: 0.53771 |  0:00:05s\n",
      "epoch 113| loss: 0.53386 |  0:00:05s\n",
      "epoch 114| loss: 0.53628 |  0:00:05s\n",
      "epoch 115| loss: 0.5298  |  0:00:05s\n",
      "epoch 116| loss: 0.52536 |  0:00:05s\n",
      "epoch 117| loss: 0.52113 |  0:00:05s\n",
      "epoch 118| loss: 0.51431 |  0:00:05s\n",
      "epoch 119| loss: 0.52311 |  0:00:05s\n",
      "epoch 120| loss: 0.51076 |  0:00:05s\n",
      "epoch 121| loss: 0.51819 |  0:00:05s\n",
      "epoch 122| loss: 0.51349 |  0:00:05s\n",
      "epoch 123| loss: 0.5099  |  0:00:05s\n",
      "epoch 124| loss: 0.50079 |  0:00:05s\n",
      "epoch 125| loss: 0.50844 |  0:00:05s\n",
      "epoch 126| loss: 0.50732 |  0:00:05s\n",
      "epoch 127| loss: 0.50167 |  0:00:05s\n",
      "epoch 128| loss: 0.49772 |  0:00:05s\n",
      "epoch 129| loss: 0.50255 |  0:00:05s\n",
      "epoch 130| loss: 0.50032 |  0:00:06s\n",
      "epoch 131| loss: 0.49648 |  0:00:06s\n",
      "epoch 132| loss: 0.48561 |  0:00:06s\n",
      "epoch 133| loss: 0.48953 |  0:00:06s\n",
      "epoch 134| loss: 0.49759 |  0:00:06s\n",
      "epoch 135| loss: 0.49052 |  0:00:06s\n",
      "epoch 136| loss: 0.49282 |  0:00:06s\n",
      "epoch 137| loss: 0.49556 |  0:00:06s\n",
      "epoch 138| loss: 0.47933 |  0:00:06s\n",
      "epoch 139| loss: 0.48637 |  0:00:06s\n",
      "epoch 140| loss: 0.48355 |  0:00:06s\n",
      "epoch 141| loss: 0.49092 |  0:00:06s\n",
      "epoch 142| loss: 0.48498 |  0:00:06s\n",
      "epoch 143| loss: 0.49449 |  0:00:06s\n",
      "epoch 144| loss: 0.49831 |  0:00:06s\n",
      "epoch 145| loss: 0.49832 |  0:00:06s\n",
      "epoch 146| loss: 0.49777 |  0:00:06s\n",
      "epoch 147| loss: 0.49427 |  0:00:06s\n",
      "epoch 148| loss: 0.50191 |  0:00:06s\n",
      "epoch 149| loss: 0.49719 |  0:00:06s\n",
      "epoch 150| loss: 0.49676 |  0:00:06s\n",
      "epoch 151| loss: 0.49056 |  0:00:06s\n",
      "epoch 152| loss: 0.49025 |  0:00:07s\n",
      "epoch 153| loss: 0.48437 |  0:00:07s\n",
      "epoch 154| loss: 0.47541 |  0:00:07s\n",
      "epoch 155| loss: 0.48128 |  0:00:07s\n",
      "epoch 156| loss: 0.48154 |  0:00:07s\n",
      "epoch 157| loss: 0.48507 |  0:00:07s\n",
      "epoch 158| loss: 0.47486 |  0:00:07s\n",
      "epoch 159| loss: 0.46992 |  0:00:07s\n",
      "epoch 160| loss: 0.46666 |  0:00:07s\n",
      "epoch 161| loss: 0.46826 |  0:00:07s\n",
      "epoch 162| loss: 0.48512 |  0:00:07s\n",
      "epoch 163| loss: 0.46598 |  0:00:07s\n",
      "epoch 164| loss: 0.46456 |  0:00:07s\n",
      "epoch 165| loss: 0.46562 |  0:00:07s\n",
      "epoch 166| loss: 0.48002 |  0:00:07s\n",
      "epoch 167| loss: 0.46539 |  0:00:07s\n",
      "epoch 168| loss: 0.4561  |  0:00:07s\n",
      "epoch 169| loss: 0.46584 |  0:00:07s\n",
      "epoch 170| loss: 0.46121 |  0:00:07s\n",
      "epoch 171| loss: 0.45522 |  0:00:07s\n",
      "epoch 172| loss: 0.45909 |  0:00:07s\n",
      "epoch 173| loss: 0.47426 |  0:00:07s\n",
      "epoch 174| loss: 0.46902 |  0:00:08s\n",
      "epoch 175| loss: 0.46355 |  0:00:08s\n",
      "epoch 176| loss: 0.46549 |  0:00:08s\n",
      "epoch 177| loss: 0.45982 |  0:00:08s\n",
      "epoch 178| loss: 0.45999 |  0:00:08s\n",
      "epoch 179| loss: 0.45779 |  0:00:08s\n",
      "epoch 180| loss: 0.46016 |  0:00:08s\n",
      "epoch 181| loss: 0.45536 |  0:00:08s\n",
      "epoch 182| loss: 0.45385 |  0:00:08s\n",
      "epoch 183| loss: 0.45064 |  0:00:08s\n",
      "epoch 184| loss: 0.45267 |  0:00:08s\n",
      "epoch 185| loss: 0.4531  |  0:00:08s\n",
      "epoch 186| loss: 0.44671 |  0:00:08s\n",
      "epoch 187| loss: 0.44847 |  0:00:08s\n",
      "epoch 188| loss: 0.45256 |  0:00:08s\n",
      "epoch 189| loss: 0.44241 |  0:00:08s\n",
      "epoch 190| loss: 0.43588 |  0:00:08s\n",
      "epoch 191| loss: 0.45413 |  0:00:08s\n",
      "epoch 192| loss: 0.45178 |  0:00:08s\n",
      "epoch 193| loss: 0.44876 |  0:00:08s\n",
      "epoch 194| loss: 0.44127 |  0:00:08s\n",
      "epoch 195| loss: 0.44428 |  0:00:08s\n",
      "epoch 196| loss: 0.4428  |  0:00:09s\n",
      "epoch 197| loss: 0.43594 |  0:00:09s\n",
      "epoch 198| loss: 0.43793 |  0:00:09s\n",
      "epoch 199| loss: 0.43327 |  0:00:09s\n",
      "epoch 200| loss: 0.44276 |  0:00:09s\n",
      "epoch 201| loss: 0.43506 |  0:00:09s\n",
      "epoch 202| loss: 0.44599 |  0:00:09s\n",
      "epoch 203| loss: 0.44623 |  0:00:09s\n",
      "epoch 204| loss: 0.44267 |  0:00:09s\n",
      "epoch 205| loss: 0.43837 |  0:00:09s\n",
      "epoch 206| loss: 0.43902 |  0:00:09s\n",
      "epoch 207| loss: 0.43577 |  0:00:09s\n",
      "epoch 208| loss: 0.43042 |  0:00:09s\n",
      "epoch 209| loss: 0.43286 |  0:00:09s\n",
      "epoch 210| loss: 0.42702 |  0:00:09s\n",
      "epoch 211| loss: 0.42237 |  0:00:09s\n",
      "epoch 212| loss: 0.43355 |  0:00:09s\n",
      "epoch 213| loss: 0.44016 |  0:00:09s\n",
      "epoch 214| loss: 0.43446 |  0:00:09s\n",
      "epoch 215| loss: 0.43132 |  0:00:09s\n",
      "epoch 216| loss: 0.43408 |  0:00:09s\n",
      "epoch 217| loss: 0.41793 |  0:00:09s\n",
      "epoch 218| loss: 0.42543 |  0:00:10s\n",
      "epoch 219| loss: 0.42206 |  0:00:10s\n",
      "epoch 220| loss: 0.43015 |  0:00:10s\n",
      "epoch 221| loss: 0.42438 |  0:00:10s\n",
      "epoch 222| loss: 0.41992 |  0:00:10s\n",
      "epoch 223| loss: 0.41749 |  0:00:10s\n",
      "epoch 224| loss: 0.41276 |  0:00:10s\n",
      "epoch 225| loss: 0.42527 |  0:00:10s\n",
      "epoch 226| loss: 0.42474 |  0:00:10s\n",
      "epoch 227| loss: 0.41942 |  0:00:10s\n",
      "epoch 228| loss: 0.41148 |  0:00:10s\n",
      "epoch 229| loss: 0.42007 |  0:00:10s\n",
      "epoch 230| loss: 0.42121 |  0:00:10s\n",
      "epoch 231| loss: 0.41975 |  0:00:10s\n",
      "epoch 232| loss: 0.42969 |  0:00:10s\n",
      "epoch 233| loss: 0.43242 |  0:00:10s\n",
      "epoch 234| loss: 0.42427 |  0:00:10s\n",
      "epoch 235| loss: 0.42815 |  0:00:10s\n",
      "epoch 236| loss: 0.43421 |  0:00:10s\n",
      "epoch 237| loss: 0.41473 |  0:00:10s\n",
      "epoch 238| loss: 0.42146 |  0:00:10s\n",
      "epoch 239| loss: 0.41874 |  0:00:10s\n",
      "epoch 240| loss: 0.42454 |  0:00:11s\n",
      "epoch 241| loss: 0.41388 |  0:00:11s\n",
      "epoch 242| loss: 0.41547 |  0:00:11s\n",
      "epoch 243| loss: 0.41264 |  0:00:11s\n",
      "epoch 244| loss: 0.41391 |  0:00:11s\n",
      "epoch 245| loss: 0.41144 |  0:00:11s\n",
      "epoch 246| loss: 0.40908 |  0:00:11s\n",
      "epoch 247| loss: 0.40231 |  0:00:11s\n",
      "epoch 248| loss: 0.40896 |  0:00:11s\n",
      "epoch 249| loss: 0.40615 |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.00051 |  0:00:00s\n",
      "epoch 1  | loss: 1.76471 |  0:00:00s\n",
      "epoch 2  | loss: 1.34646 |  0:00:00s\n",
      "epoch 3  | loss: 1.10716 |  0:00:00s\n",
      "epoch 4  | loss: 0.98875 |  0:00:00s\n",
      "epoch 5  | loss: 0.92667 |  0:00:00s\n",
      "epoch 6  | loss: 0.88609 |  0:00:00s\n",
      "epoch 7  | loss: 0.85575 |  0:00:00s\n",
      "epoch 8  | loss: 0.82198 |  0:00:00s\n",
      "epoch 9  | loss: 0.79484 |  0:00:00s\n",
      "epoch 10 | loss: 0.791   |  0:00:00s\n",
      "epoch 11 | loss: 0.76251 |  0:00:00s\n",
      "epoch 12 | loss: 0.75082 |  0:00:00s\n",
      "epoch 13 | loss: 0.73588 |  0:00:00s\n",
      "epoch 14 | loss: 0.74424 |  0:00:00s\n",
      "epoch 15 | loss: 0.73209 |  0:00:00s\n",
      "epoch 16 | loss: 0.71495 |  0:00:00s\n",
      "epoch 17 | loss: 0.70257 |  0:00:00s\n",
      "epoch 18 | loss: 0.70147 |  0:00:00s\n",
      "epoch 19 | loss: 0.69636 |  0:00:00s\n",
      "epoch 20 | loss: 0.68661 |  0:00:00s\n",
      "epoch 21 | loss: 0.67573 |  0:00:00s\n",
      "epoch 22 | loss: 0.68124 |  0:00:01s\n",
      "epoch 23 | loss: 0.67812 |  0:00:01s\n",
      "epoch 24 | loss: 0.67247 |  0:00:01s\n",
      "epoch 25 | loss: 0.66258 |  0:00:01s\n",
      "epoch 26 | loss: 0.64363 |  0:00:01s\n",
      "epoch 27 | loss: 0.64696 |  0:00:01s\n",
      "epoch 28 | loss: 0.63782 |  0:00:01s\n",
      "epoch 29 | loss: 0.63713 |  0:00:01s\n",
      "epoch 30 | loss: 0.64163 |  0:00:01s\n",
      "epoch 31 | loss: 0.63716 |  0:00:01s\n",
      "epoch 32 | loss: 0.63394 |  0:00:01s\n",
      "epoch 33 | loss: 0.62836 |  0:00:01s\n",
      "epoch 34 | loss: 0.63233 |  0:00:01s\n",
      "epoch 35 | loss: 0.6246  |  0:00:01s\n",
      "epoch 36 | loss: 0.61987 |  0:00:01s\n",
      "epoch 37 | loss: 0.62099 |  0:00:01s\n",
      "epoch 38 | loss: 0.61429 |  0:00:01s\n",
      "epoch 39 | loss: 0.61468 |  0:00:01s\n",
      "epoch 40 | loss: 0.62319 |  0:00:01s\n",
      "epoch 41 | loss: 0.61907 |  0:00:01s\n",
      "epoch 42 | loss: 0.63075 |  0:00:01s\n",
      "epoch 43 | loss: 0.62958 |  0:00:01s\n",
      "epoch 44 | loss: 0.62136 |  0:00:02s\n",
      "epoch 45 | loss: 0.63046 |  0:00:02s\n",
      "epoch 46 | loss: 0.61806 |  0:00:02s\n",
      "epoch 47 | loss: 0.61459 |  0:00:02s\n",
      "epoch 48 | loss: 0.59902 |  0:00:02s\n",
      "epoch 49 | loss: 0.60134 |  0:00:02s\n",
      "epoch 50 | loss: 0.59833 |  0:00:02s\n",
      "epoch 51 | loss: 0.59933 |  0:00:02s\n",
      "epoch 52 | loss: 0.59618 |  0:00:02s\n",
      "epoch 53 | loss: 0.59707 |  0:00:02s\n",
      "epoch 54 | loss: 0.59446 |  0:00:02s\n",
      "epoch 55 | loss: 0.60397 |  0:00:02s\n",
      "epoch 56 | loss: 0.59291 |  0:00:02s\n",
      "epoch 57 | loss: 0.58961 |  0:00:02s\n",
      "epoch 58 | loss: 0.58012 |  0:00:02s\n",
      "epoch 59 | loss: 0.57371 |  0:00:02s\n",
      "epoch 60 | loss: 0.5819  |  0:00:02s\n",
      "epoch 61 | loss: 0.58315 |  0:00:02s\n",
      "epoch 62 | loss: 0.57529 |  0:00:02s\n",
      "epoch 63 | loss: 0.57336 |  0:00:02s\n",
      "epoch 64 | loss: 0.57474 |  0:00:02s\n",
      "epoch 65 | loss: 0.56174 |  0:00:03s\n",
      "epoch 66 | loss: 0.56633 |  0:00:03s\n",
      "epoch 67 | loss: 0.57072 |  0:00:03s\n",
      "epoch 68 | loss: 0.56249 |  0:00:03s\n",
      "epoch 69 | loss: 0.56166 |  0:00:03s\n",
      "epoch 70 | loss: 0.55696 |  0:00:03s\n",
      "epoch 71 | loss: 0.55575 |  0:00:03s\n",
      "epoch 72 | loss: 0.55869 |  0:00:03s\n",
      "epoch 73 | loss: 0.55232 |  0:00:03s\n",
      "epoch 74 | loss: 0.54967 |  0:00:03s\n",
      "epoch 75 | loss: 0.54766 |  0:00:03s\n",
      "epoch 76 | loss: 0.5482  |  0:00:03s\n",
      "epoch 77 | loss: 0.5557  |  0:00:03s\n",
      "epoch 78 | loss: 0.54156 |  0:00:03s\n",
      "epoch 79 | loss: 0.5392  |  0:00:03s\n",
      "epoch 80 | loss: 0.54326 |  0:00:03s\n",
      "epoch 81 | loss: 0.5323  |  0:00:03s\n",
      "epoch 82 | loss: 0.53278 |  0:00:03s\n",
      "epoch 83 | loss: 0.53154 |  0:00:03s\n",
      "epoch 84 | loss: 0.525   |  0:00:03s\n",
      "epoch 85 | loss: 0.52723 |  0:00:04s\n",
      "epoch 86 | loss: 0.5236  |  0:00:04s\n",
      "epoch 87 | loss: 0.52477 |  0:00:04s\n",
      "epoch 88 | loss: 0.52293 |  0:00:04s\n",
      "epoch 89 | loss: 0.51906 |  0:00:04s\n",
      "epoch 90 | loss: 0.51543 |  0:00:04s\n",
      "epoch 91 | loss: 0.52341 |  0:00:04s\n",
      "epoch 92 | loss: 0.52155 |  0:00:04s\n",
      "epoch 93 | loss: 0.5203  |  0:00:04s\n",
      "epoch 94 | loss: 0.51965 |  0:00:04s\n",
      "epoch 95 | loss: 0.52416 |  0:00:04s\n",
      "epoch 96 | loss: 0.51792 |  0:00:04s\n",
      "epoch 97 | loss: 0.52098 |  0:00:04s\n",
      "epoch 98 | loss: 0.52523 |  0:00:04s\n",
      "epoch 99 | loss: 0.53111 |  0:00:04s\n",
      "epoch 100| loss: 0.51464 |  0:00:04s\n",
      "epoch 101| loss: 0.51839 |  0:00:04s\n",
      "epoch 102| loss: 0.52241 |  0:00:04s\n",
      "epoch 103| loss: 0.51092 |  0:00:04s\n",
      "epoch 104| loss: 0.51239 |  0:00:04s\n",
      "epoch 105| loss: 0.50912 |  0:00:04s\n",
      "epoch 106| loss: 0.50866 |  0:00:05s\n",
      "epoch 107| loss: 0.50588 |  0:00:05s\n",
      "epoch 108| loss: 0.51132 |  0:00:05s\n",
      "epoch 109| loss: 0.49999 |  0:00:05s\n",
      "epoch 110| loss: 0.494   |  0:00:05s\n",
      "epoch 111| loss: 0.49935 |  0:00:05s\n",
      "epoch 112| loss: 0.49356 |  0:00:05s\n",
      "epoch 113| loss: 0.49686 |  0:00:05s\n",
      "epoch 114| loss: 0.48926 |  0:00:05s\n",
      "epoch 115| loss: 0.49024 |  0:00:05s\n",
      "epoch 116| loss: 0.47978 |  0:00:05s\n",
      "epoch 117| loss: 0.4839  |  0:00:05s\n",
      "epoch 118| loss: 0.48744 |  0:00:05s\n",
      "epoch 119| loss: 0.48443 |  0:00:05s\n",
      "epoch 120| loss: 0.47037 |  0:00:05s\n",
      "epoch 121| loss: 0.47591 |  0:00:05s\n",
      "epoch 122| loss: 0.47402 |  0:00:05s\n",
      "epoch 123| loss: 0.47222 |  0:00:05s\n",
      "epoch 124| loss: 0.47598 |  0:00:05s\n",
      "epoch 125| loss: 0.46881 |  0:00:05s\n",
      "epoch 126| loss: 0.46985 |  0:00:05s\n",
      "epoch 127| loss: 0.46818 |  0:00:05s\n",
      "epoch 128| loss: 0.46739 |  0:00:06s\n",
      "epoch 129| loss: 0.46947 |  0:00:06s\n",
      "epoch 130| loss: 0.46114 |  0:00:06s\n",
      "epoch 131| loss: 0.45703 |  0:00:06s\n",
      "epoch 132| loss: 0.46367 |  0:00:06s\n",
      "epoch 133| loss: 0.46195 |  0:00:06s\n",
      "epoch 134| loss: 0.45505 |  0:00:06s\n",
      "epoch 135| loss: 0.45561 |  0:00:06s\n",
      "epoch 136| loss: 0.463   |  0:00:06s\n",
      "epoch 137| loss: 0.45553 |  0:00:06s\n",
      "epoch 138| loss: 0.46705 |  0:00:06s\n",
      "epoch 139| loss: 0.46727 |  0:00:06s\n",
      "epoch 140| loss: 0.45193 |  0:00:06s\n",
      "epoch 141| loss: 0.45629 |  0:00:06s\n",
      "epoch 142| loss: 0.4459  |  0:00:06s\n",
      "epoch 143| loss: 0.45359 |  0:00:06s\n",
      "epoch 144| loss: 0.45363 |  0:00:06s\n",
      "epoch 145| loss: 0.44671 |  0:00:06s\n",
      "epoch 146| loss: 0.4508  |  0:00:06s\n",
      "epoch 147| loss: 0.45044 |  0:00:06s\n",
      "epoch 148| loss: 0.4467  |  0:00:06s\n",
      "epoch 149| loss: 0.45774 |  0:00:07s\n",
      "epoch 150| loss: 0.44063 |  0:00:07s\n",
      "epoch 151| loss: 0.44268 |  0:00:07s\n",
      "epoch 152| loss: 0.44495 |  0:00:07s\n",
      "epoch 153| loss: 0.44282 |  0:00:07s\n",
      "epoch 154| loss: 0.43608 |  0:00:07s\n",
      "epoch 155| loss: 0.4341  |  0:00:07s\n",
      "epoch 156| loss: 0.44073 |  0:00:07s\n",
      "epoch 157| loss: 0.44428 |  0:00:07s\n",
      "epoch 158| loss: 0.43571 |  0:00:07s\n",
      "epoch 159| loss: 0.42677 |  0:00:07s\n",
      "epoch 160| loss: 0.42661 |  0:00:07s\n",
      "epoch 161| loss: 0.42103 |  0:00:07s\n",
      "epoch 162| loss: 0.43701 |  0:00:07s\n",
      "epoch 163| loss: 0.42655 |  0:00:07s\n",
      "epoch 164| loss: 0.42756 |  0:00:07s\n",
      "epoch 165| loss: 0.42269 |  0:00:07s\n",
      "epoch 166| loss: 0.41582 |  0:00:07s\n",
      "epoch 167| loss: 0.42154 |  0:00:08s\n",
      "epoch 168| loss: 0.42483 |  0:00:08s\n",
      "epoch 169| loss: 0.41738 |  0:00:08s\n",
      "epoch 170| loss: 0.42514 |  0:00:08s\n",
      "epoch 171| loss: 0.41282 |  0:00:08s\n",
      "epoch 172| loss: 0.41551 |  0:00:08s\n",
      "epoch 173| loss: 0.42243 |  0:00:08s\n",
      "epoch 174| loss: 0.41307 |  0:00:08s\n",
      "epoch 175| loss: 0.40513 |  0:00:08s\n",
      "epoch 176| loss: 0.41466 |  0:00:08s\n",
      "epoch 177| loss: 0.41395 |  0:00:08s\n",
      "epoch 178| loss: 0.40964 |  0:00:08s\n",
      "epoch 179| loss: 0.4172  |  0:00:08s\n",
      "epoch 180| loss: 0.41402 |  0:00:08s\n",
      "epoch 181| loss: 0.40578 |  0:00:08s\n",
      "epoch 182| loss: 0.40476 |  0:00:08s\n",
      "epoch 183| loss: 0.39892 |  0:00:08s\n",
      "epoch 184| loss: 0.39753 |  0:00:08s\n",
      "epoch 185| loss: 0.39973 |  0:00:08s\n",
      "epoch 186| loss: 0.41022 |  0:00:08s\n",
      "epoch 187| loss: 0.41518 |  0:00:08s\n",
      "epoch 188| loss: 0.40687 |  0:00:08s\n",
      "epoch 189| loss: 0.40068 |  0:00:09s\n",
      "epoch 190| loss: 0.39848 |  0:00:09s\n",
      "epoch 191| loss: 0.39973 |  0:00:09s\n",
      "epoch 192| loss: 0.40382 |  0:00:09s\n",
      "epoch 193| loss: 0.39591 |  0:00:09s\n",
      "epoch 194| loss: 0.39482 |  0:00:09s\n",
      "epoch 195| loss: 0.39274 |  0:00:09s\n",
      "epoch 196| loss: 0.39523 |  0:00:09s\n",
      "epoch 197| loss: 0.39224 |  0:00:09s\n",
      "epoch 198| loss: 0.3929  |  0:00:09s\n",
      "epoch 199| loss: 0.38244 |  0:00:09s\n",
      "epoch 200| loss: 0.39312 |  0:00:09s\n",
      "epoch 201| loss: 0.39073 |  0:00:09s\n",
      "epoch 202| loss: 0.39185 |  0:00:09s\n",
      "epoch 203| loss: 0.38723 |  0:00:09s\n",
      "epoch 204| loss: 0.38615 |  0:00:09s\n",
      "epoch 205| loss: 0.38567 |  0:00:09s\n",
      "epoch 206| loss: 0.38594 |  0:00:09s\n",
      "epoch 207| loss: 0.39739 |  0:00:09s\n",
      "epoch 208| loss: 0.38688 |  0:00:09s\n",
      "epoch 209| loss: 0.39498 |  0:00:09s\n",
      "epoch 210| loss: 0.39401 |  0:00:09s\n",
      "epoch 211| loss: 0.39161 |  0:00:10s\n",
      "epoch 212| loss: 0.39155 |  0:00:10s\n",
      "epoch 213| loss: 0.38828 |  0:00:10s\n",
      "epoch 214| loss: 0.39134 |  0:00:10s\n",
      "epoch 215| loss: 0.38887 |  0:00:10s\n",
      "epoch 216| loss: 0.3842  |  0:00:10s\n",
      "epoch 217| loss: 0.38637 |  0:00:10s\n",
      "epoch 218| loss: 0.38535 |  0:00:10s\n",
      "epoch 219| loss: 0.39687 |  0:00:10s\n",
      "epoch 220| loss: 0.39415 |  0:00:10s\n",
      "epoch 221| loss: 0.38697 |  0:00:10s\n",
      "epoch 222| loss: 0.38249 |  0:00:10s\n",
      "epoch 223| loss: 0.39102 |  0:00:10s\n",
      "epoch 224| loss: 0.41284 |  0:00:10s\n",
      "epoch 225| loss: 0.41383 |  0:00:10s\n",
      "epoch 226| loss: 0.40144 |  0:00:10s\n",
      "epoch 227| loss: 0.40658 |  0:00:10s\n",
      "epoch 228| loss: 0.38726 |  0:00:10s\n",
      "epoch 229| loss: 0.38692 |  0:00:10s\n",
      "epoch 230| loss: 0.38903 |  0:00:10s\n",
      "epoch 231| loss: 0.39528 |  0:00:10s\n",
      "epoch 232| loss: 0.38817 |  0:00:10s\n",
      "epoch 233| loss: 0.37665 |  0:00:11s\n",
      "epoch 234| loss: 0.37662 |  0:00:11s\n",
      "epoch 235| loss: 0.37617 |  0:00:11s\n",
      "epoch 236| loss: 0.37675 |  0:00:11s\n",
      "epoch 237| loss: 0.37    |  0:00:11s\n",
      "epoch 238| loss: 0.3719  |  0:00:11s\n",
      "epoch 239| loss: 0.36636 |  0:00:11s\n",
      "epoch 240| loss: 0.37351 |  0:00:11s\n",
      "epoch 241| loss: 0.3692  |  0:00:11s\n",
      "epoch 242| loss: 0.37438 |  0:00:11s\n",
      "epoch 243| loss: 0.36915 |  0:00:11s\n",
      "epoch 244| loss: 0.3668  |  0:00:11s\n",
      "epoch 245| loss: 0.36361 |  0:00:11s\n",
      "epoch 246| loss: 0.37978 |  0:00:11s\n",
      "epoch 247| loss: 0.38481 |  0:00:11s\n",
      "epoch 248| loss: 0.37843 |  0:00:11s\n",
      "epoch 249| loss: 0.37742 |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.11425 |  0:00:00s\n",
      "epoch 1  | loss: 1.68068 |  0:00:00s\n",
      "epoch 2  | loss: 1.30632 |  0:00:00s\n",
      "epoch 3  | loss: 1.08927 |  0:00:00s\n",
      "epoch 4  | loss: 1.02816 |  0:00:00s\n",
      "epoch 5  | loss: 0.96178 |  0:00:00s\n",
      "epoch 6  | loss: 0.90537 |  0:00:00s\n",
      "epoch 7  | loss: 0.86009 |  0:00:00s\n",
      "epoch 8  | loss: 0.81521 |  0:00:00s\n",
      "epoch 9  | loss: 0.78072 |  0:00:00s\n",
      "epoch 10 | loss: 0.77099 |  0:00:00s\n",
      "epoch 11 | loss: 0.75651 |  0:00:00s\n",
      "epoch 12 | loss: 0.74997 |  0:00:00s\n",
      "epoch 13 | loss: 0.73306 |  0:00:00s\n",
      "epoch 14 | loss: 0.72202 |  0:00:00s\n",
      "epoch 15 | loss: 0.72133 |  0:00:00s\n",
      "epoch 16 | loss: 0.7083  |  0:00:00s\n",
      "epoch 17 | loss: 0.70184 |  0:00:00s\n",
      "epoch 18 | loss: 0.69968 |  0:00:00s\n",
      "epoch 19 | loss: 0.70091 |  0:00:00s\n",
      "epoch 20 | loss: 0.70351 |  0:00:00s\n",
      "epoch 21 | loss: 0.70598 |  0:00:01s\n",
      "epoch 22 | loss: 0.70122 |  0:00:01s\n",
      "epoch 23 | loss: 0.69468 |  0:00:01s\n",
      "epoch 24 | loss: 0.68494 |  0:00:01s\n",
      "epoch 25 | loss: 0.67963 |  0:00:01s\n",
      "epoch 26 | loss: 0.67227 |  0:00:01s\n",
      "epoch 27 | loss: 0.66473 |  0:00:01s\n",
      "epoch 28 | loss: 0.67769 |  0:00:01s\n",
      "epoch 29 | loss: 0.66172 |  0:00:01s\n",
      "epoch 30 | loss: 0.66906 |  0:00:01s\n",
      "epoch 31 | loss: 0.6777  |  0:00:01s\n",
      "epoch 32 | loss: 0.67823 |  0:00:01s\n",
      "epoch 33 | loss: 0.67367 |  0:00:01s\n",
      "epoch 34 | loss: 0.68121 |  0:00:01s\n",
      "epoch 35 | loss: 0.68685 |  0:00:01s\n",
      "epoch 36 | loss: 0.68275 |  0:00:01s\n",
      "epoch 37 | loss: 0.67503 |  0:00:01s\n",
      "epoch 38 | loss: 0.67667 |  0:00:01s\n",
      "epoch 39 | loss: 0.66771 |  0:00:01s\n",
      "epoch 40 | loss: 0.65082 |  0:00:01s\n",
      "epoch 41 | loss: 0.64909 |  0:00:01s\n",
      "epoch 42 | loss: 0.65086 |  0:00:01s\n",
      "epoch 43 | loss: 0.65242 |  0:00:01s\n",
      "epoch 44 | loss: 0.64512 |  0:00:02s\n",
      "epoch 45 | loss: 0.63661 |  0:00:02s\n",
      "epoch 46 | loss: 0.63208 |  0:00:02s\n",
      "epoch 47 | loss: 0.62269 |  0:00:02s\n",
      "epoch 48 | loss: 0.61798 |  0:00:02s\n",
      "epoch 49 | loss: 0.61937 |  0:00:02s\n",
      "epoch 50 | loss: 0.61263 |  0:00:02s\n",
      "epoch 51 | loss: 0.60989 |  0:00:02s\n",
      "epoch 52 | loss: 0.61204 |  0:00:02s\n",
      "epoch 53 | loss: 0.61296 |  0:00:02s\n",
      "epoch 54 | loss: 0.60947 |  0:00:02s\n",
      "epoch 55 | loss: 0.60185 |  0:00:02s\n",
      "epoch 56 | loss: 0.60253 |  0:00:02s\n",
      "epoch 57 | loss: 0.60452 |  0:00:02s\n",
      "epoch 58 | loss: 0.606   |  0:00:02s\n",
      "epoch 59 | loss: 0.59065 |  0:00:02s\n",
      "epoch 60 | loss: 0.59278 |  0:00:02s\n",
      "epoch 61 | loss: 0.5953  |  0:00:02s\n",
      "epoch 62 | loss: 0.5886  |  0:00:02s\n",
      "epoch 63 | loss: 0.589   |  0:00:02s\n",
      "epoch 64 | loss: 0.589   |  0:00:02s\n",
      "epoch 65 | loss: 0.58197 |  0:00:02s\n",
      "epoch 66 | loss: 0.5808  |  0:00:03s\n",
      "epoch 67 | loss: 0.58985 |  0:00:03s\n",
      "epoch 68 | loss: 0.58231 |  0:00:03s\n",
      "epoch 69 | loss: 0.5745  |  0:00:03s\n",
      "epoch 70 | loss: 0.5711  |  0:00:03s\n",
      "epoch 71 | loss: 0.5711  |  0:00:03s\n",
      "epoch 72 | loss: 0.57251 |  0:00:03s\n",
      "epoch 73 | loss: 0.57315 |  0:00:03s\n",
      "epoch 74 | loss: 0.57947 |  0:00:03s\n",
      "epoch 75 | loss: 0.57049 |  0:00:03s\n",
      "epoch 76 | loss: 0.5717  |  0:00:03s\n",
      "epoch 77 | loss: 0.57039 |  0:00:03s\n",
      "epoch 78 | loss: 0.55941 |  0:00:03s\n",
      "epoch 79 | loss: 0.56097 |  0:00:03s\n",
      "epoch 80 | loss: 0.56692 |  0:00:03s\n",
      "epoch 81 | loss: 0.5562  |  0:00:03s\n",
      "epoch 82 | loss: 0.55348 |  0:00:03s\n",
      "epoch 83 | loss: 0.55669 |  0:00:03s\n",
      "epoch 84 | loss: 0.55222 |  0:00:03s\n",
      "epoch 85 | loss: 0.55298 |  0:00:03s\n",
      "epoch 86 | loss: 0.54966 |  0:00:03s\n",
      "epoch 87 | loss: 0.54286 |  0:00:03s\n",
      "epoch 88 | loss: 0.55082 |  0:00:04s\n",
      "epoch 89 | loss: 0.54362 |  0:00:04s\n",
      "epoch 90 | loss: 0.54212 |  0:00:04s\n",
      "epoch 91 | loss: 0.54068 |  0:00:04s\n",
      "epoch 92 | loss: 0.53862 |  0:00:04s\n",
      "epoch 93 | loss: 0.54834 |  0:00:04s\n",
      "epoch 94 | loss: 0.537   |  0:00:04s\n",
      "epoch 95 | loss: 0.5462  |  0:00:04s\n",
      "epoch 96 | loss: 0.54285 |  0:00:04s\n",
      "epoch 97 | loss: 0.54301 |  0:00:04s\n",
      "epoch 98 | loss: 0.53395 |  0:00:04s\n",
      "epoch 99 | loss: 0.53984 |  0:00:04s\n",
      "epoch 100| loss: 0.53076 |  0:00:04s\n",
      "epoch 101| loss: 0.53793 |  0:00:04s\n",
      "epoch 102| loss: 0.52796 |  0:00:04s\n",
      "epoch 103| loss: 0.52227 |  0:00:04s\n",
      "epoch 104| loss: 0.5269  |  0:00:04s\n",
      "epoch 105| loss: 0.53015 |  0:00:04s\n",
      "epoch 106| loss: 0.52411 |  0:00:04s\n",
      "epoch 107| loss: 0.52454 |  0:00:05s\n",
      "epoch 108| loss: 0.51824 |  0:00:05s\n",
      "epoch 109| loss: 0.51756 |  0:00:05s\n",
      "epoch 110| loss: 0.51751 |  0:00:05s\n",
      "epoch 111| loss: 0.50951 |  0:00:05s\n",
      "epoch 112| loss: 0.51073 |  0:00:05s\n",
      "epoch 113| loss: 0.5088  |  0:00:05s\n",
      "epoch 114| loss: 0.50258 |  0:00:05s\n",
      "epoch 115| loss: 0.51339 |  0:00:05s\n",
      "epoch 116| loss: 0.50808 |  0:00:05s\n",
      "epoch 117| loss: 0.50367 |  0:00:05s\n",
      "epoch 118| loss: 0.50354 |  0:00:05s\n",
      "epoch 119| loss: 0.50341 |  0:00:05s\n",
      "epoch 120| loss: 0.50028 |  0:00:05s\n",
      "epoch 121| loss: 0.49978 |  0:00:05s\n",
      "epoch 122| loss: 0.49662 |  0:00:05s\n",
      "epoch 123| loss: 0.49033 |  0:00:05s\n",
      "epoch 124| loss: 0.48856 |  0:00:05s\n",
      "epoch 125| loss: 0.48849 |  0:00:05s\n",
      "epoch 126| loss: 0.47989 |  0:00:05s\n",
      "epoch 127| loss: 0.48944 |  0:00:05s\n",
      "epoch 128| loss: 0.48467 |  0:00:05s\n",
      "epoch 129| loss: 0.47965 |  0:00:06s\n",
      "epoch 130| loss: 0.47563 |  0:00:06s\n",
      "epoch 131| loss: 0.48833 |  0:00:06s\n",
      "epoch 132| loss: 0.49049 |  0:00:06s\n",
      "epoch 133| loss: 0.48379 |  0:00:06s\n",
      "epoch 134| loss: 0.47909 |  0:00:06s\n",
      "epoch 135| loss: 0.49147 |  0:00:06s\n",
      "epoch 136| loss: 0.50157 |  0:00:06s\n",
      "epoch 137| loss: 0.50111 |  0:00:06s\n",
      "epoch 138| loss: 0.50472 |  0:00:06s\n",
      "epoch 139| loss: 0.50744 |  0:00:06s\n",
      "epoch 140| loss: 0.49498 |  0:00:06s\n",
      "epoch 141| loss: 0.50106 |  0:00:06s\n",
      "epoch 142| loss: 0.49369 |  0:00:06s\n",
      "epoch 143| loss: 0.4894  |  0:00:06s\n",
      "epoch 144| loss: 0.47648 |  0:00:06s\n",
      "epoch 145| loss: 0.47508 |  0:00:06s\n",
      "epoch 146| loss: 0.47553 |  0:00:06s\n",
      "epoch 147| loss: 0.48332 |  0:00:06s\n",
      "epoch 148| loss: 0.46541 |  0:00:06s\n",
      "epoch 149| loss: 0.47329 |  0:00:06s\n",
      "epoch 150| loss: 0.46626 |  0:00:06s\n",
      "epoch 151| loss: 0.47089 |  0:00:07s\n",
      "epoch 152| loss: 0.47051 |  0:00:07s\n",
      "epoch 153| loss: 0.46158 |  0:00:07s\n",
      "epoch 154| loss: 0.45003 |  0:00:07s\n",
      "epoch 155| loss: 0.45687 |  0:00:07s\n",
      "epoch 156| loss: 0.45486 |  0:00:07s\n",
      "epoch 157| loss: 0.45831 |  0:00:07s\n",
      "epoch 158| loss: 0.45059 |  0:00:07s\n",
      "epoch 159| loss: 0.4514  |  0:00:07s\n",
      "epoch 160| loss: 0.45402 |  0:00:07s\n",
      "epoch 161| loss: 0.44807 |  0:00:07s\n",
      "epoch 162| loss: 0.44634 |  0:00:07s\n",
      "epoch 163| loss: 0.44432 |  0:00:07s\n",
      "epoch 164| loss: 0.44797 |  0:00:07s\n",
      "epoch 165| loss: 0.43832 |  0:00:07s\n",
      "epoch 166| loss: 0.44275 |  0:00:07s\n",
      "epoch 167| loss: 0.43922 |  0:00:07s\n",
      "epoch 168| loss: 0.446   |  0:00:07s\n",
      "epoch 169| loss: 0.45101 |  0:00:07s\n",
      "epoch 170| loss: 0.44165 |  0:00:07s\n",
      "epoch 171| loss: 0.44053 |  0:00:07s\n",
      "epoch 172| loss: 0.43546 |  0:00:07s\n",
      "epoch 173| loss: 0.44488 |  0:00:08s\n",
      "epoch 174| loss: 0.44013 |  0:00:08s\n",
      "epoch 175| loss: 0.44409 |  0:00:08s\n",
      "epoch 176| loss: 0.45248 |  0:00:08s\n",
      "epoch 177| loss: 0.43792 |  0:00:08s\n",
      "epoch 178| loss: 0.4323  |  0:00:08s\n",
      "epoch 179| loss: 0.43023 |  0:00:08s\n",
      "epoch 180| loss: 0.43438 |  0:00:08s\n",
      "epoch 181| loss: 0.44664 |  0:00:08s\n",
      "epoch 182| loss: 0.46368 |  0:00:08s\n",
      "epoch 183| loss: 0.45321 |  0:00:08s\n",
      "epoch 184| loss: 0.4475  |  0:00:08s\n",
      "epoch 185| loss: 0.449   |  0:00:08s\n",
      "epoch 186| loss: 0.44453 |  0:00:08s\n",
      "epoch 187| loss: 0.4516  |  0:00:08s\n",
      "epoch 188| loss: 0.44639 |  0:00:08s\n",
      "epoch 189| loss: 0.44101 |  0:00:08s\n",
      "epoch 190| loss: 0.43763 |  0:00:08s\n",
      "epoch 191| loss: 0.43627 |  0:00:08s\n",
      "epoch 192| loss: 0.43917 |  0:00:08s\n",
      "epoch 193| loss: 0.43124 |  0:00:08s\n",
      "epoch 194| loss: 0.42754 |  0:00:09s\n",
      "epoch 195| loss: 0.42942 |  0:00:09s\n",
      "epoch 196| loss: 0.43105 |  0:00:09s\n",
      "epoch 197| loss: 0.42225 |  0:00:09s\n",
      "epoch 198| loss: 0.42735 |  0:00:09s\n",
      "epoch 199| loss: 0.41645 |  0:00:09s\n",
      "epoch 200| loss: 0.41797 |  0:00:09s\n",
      "epoch 201| loss: 0.41221 |  0:00:09s\n",
      "epoch 202| loss: 0.42313 |  0:00:09s\n",
      "epoch 203| loss: 0.44223 |  0:00:09s\n",
      "epoch 204| loss: 0.44533 |  0:00:09s\n",
      "epoch 205| loss: 0.43783 |  0:00:09s\n",
      "epoch 206| loss: 0.43103 |  0:00:09s\n",
      "epoch 207| loss: 0.4309  |  0:00:09s\n",
      "epoch 208| loss: 0.43308 |  0:00:09s\n",
      "epoch 209| loss: 0.4314  |  0:00:09s\n",
      "epoch 210| loss: 0.4347  |  0:00:09s\n",
      "epoch 211| loss: 0.42738 |  0:00:09s\n",
      "epoch 212| loss: 0.43138 |  0:00:09s\n",
      "epoch 213| loss: 0.42695 |  0:00:09s\n",
      "epoch 214| loss: 0.42828 |  0:00:10s\n",
      "epoch 215| loss: 0.42427 |  0:00:10s\n",
      "epoch 216| loss: 0.42074 |  0:00:10s\n",
      "epoch 217| loss: 0.41116 |  0:00:10s\n",
      "epoch 218| loss: 0.41124 |  0:00:10s\n",
      "epoch 219| loss: 0.40951 |  0:00:10s\n",
      "epoch 220| loss: 0.41041 |  0:00:10s\n",
      "epoch 221| loss: 0.40578 |  0:00:10s\n",
      "epoch 222| loss: 0.40719 |  0:00:10s\n",
      "epoch 223| loss: 0.40541 |  0:00:10s\n",
      "epoch 224| loss: 0.39708 |  0:00:10s\n",
      "epoch 225| loss: 0.41419 |  0:00:10s\n",
      "epoch 226| loss: 0.40815 |  0:00:10s\n",
      "epoch 227| loss: 0.39936 |  0:00:10s\n",
      "epoch 228| loss: 0.39852 |  0:00:10s\n",
      "epoch 229| loss: 0.40209 |  0:00:10s\n",
      "epoch 230| loss: 0.40424 |  0:00:10s\n",
      "epoch 231| loss: 0.40543 |  0:00:10s\n",
      "epoch 232| loss: 0.41101 |  0:00:10s\n",
      "epoch 233| loss: 0.41181 |  0:00:10s\n",
      "epoch 234| loss: 0.40884 |  0:00:10s\n",
      "epoch 235| loss: 0.40878 |  0:00:10s\n",
      "epoch 236| loss: 0.42266 |  0:00:11s\n",
      "epoch 237| loss: 0.42702 |  0:00:11s\n",
      "epoch 238| loss: 0.43367 |  0:00:11s\n",
      "epoch 239| loss: 0.4223  |  0:00:11s\n",
      "epoch 240| loss: 0.42584 |  0:00:11s\n",
      "epoch 241| loss: 0.42275 |  0:00:11s\n",
      "epoch 242| loss: 0.41694 |  0:00:11s\n",
      "epoch 243| loss: 0.42167 |  0:00:11s\n",
      "epoch 244| loss: 0.41632 |  0:00:11s\n",
      "epoch 245| loss: 0.40985 |  0:00:11s\n",
      "epoch 246| loss: 0.40208 |  0:00:11s\n",
      "epoch 247| loss: 0.40561 |  0:00:11s\n",
      "epoch 248| loss: 0.40735 |  0:00:11s\n",
      "epoch 249| loss: 0.4058  |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.00545 |  0:00:00s\n",
      "epoch 1  | loss: 1.71778 |  0:00:00s\n",
      "epoch 2  | loss: 1.28728 |  0:00:00s\n",
      "epoch 3  | loss: 1.10646 |  0:00:00s\n",
      "epoch 4  | loss: 0.98149 |  0:00:00s\n",
      "epoch 5  | loss: 0.93691 |  0:00:00s\n",
      "epoch 6  | loss: 0.90277 |  0:00:00s\n",
      "epoch 7  | loss: 0.85293 |  0:00:00s\n",
      "epoch 8  | loss: 0.8252  |  0:00:00s\n",
      "epoch 9  | loss: 0.79244 |  0:00:00s\n",
      "epoch 10 | loss: 0.78589 |  0:00:00s\n",
      "epoch 11 | loss: 0.76674 |  0:00:00s\n",
      "epoch 12 | loss: 0.75393 |  0:00:00s\n",
      "epoch 13 | loss: 0.73592 |  0:00:00s\n",
      "epoch 14 | loss: 0.72271 |  0:00:00s\n",
      "epoch 15 | loss: 0.71964 |  0:00:00s\n",
      "epoch 16 | loss: 0.71931 |  0:00:00s\n",
      "epoch 17 | loss: 0.70888 |  0:00:00s\n",
      "epoch 18 | loss: 0.72179 |  0:00:00s\n",
      "epoch 19 | loss: 0.69663 |  0:00:01s\n",
      "epoch 20 | loss: 0.70325 |  0:00:01s\n",
      "epoch 21 | loss: 0.69684 |  0:00:01s\n",
      "epoch 22 | loss: 0.67461 |  0:00:01s\n",
      "epoch 23 | loss: 0.66547 |  0:00:01s\n",
      "epoch 24 | loss: 0.66189 |  0:00:01s\n",
      "epoch 25 | loss: 0.6697  |  0:00:01s\n",
      "epoch 26 | loss: 0.66772 |  0:00:01s\n",
      "epoch 27 | loss: 0.66603 |  0:00:01s\n",
      "epoch 28 | loss: 0.65771 |  0:00:01s\n",
      "epoch 29 | loss: 0.6541  |  0:00:01s\n",
      "epoch 30 | loss: 0.6498  |  0:00:01s\n",
      "epoch 31 | loss: 0.64868 |  0:00:01s\n",
      "epoch 32 | loss: 0.64762 |  0:00:01s\n",
      "epoch 33 | loss: 0.63812 |  0:00:01s\n",
      "epoch 34 | loss: 0.64604 |  0:00:01s\n",
      "epoch 35 | loss: 0.63619 |  0:00:02s\n",
      "epoch 36 | loss: 0.63967 |  0:00:02s\n",
      "epoch 37 | loss: 0.64246 |  0:00:02s\n",
      "epoch 38 | loss: 0.62573 |  0:00:02s\n",
      "epoch 39 | loss: 0.63117 |  0:00:02s\n",
      "epoch 40 | loss: 0.61501 |  0:00:02s\n",
      "epoch 41 | loss: 0.61947 |  0:00:02s\n",
      "epoch 42 | loss: 0.6119  |  0:00:02s\n",
      "epoch 43 | loss: 0.61645 |  0:00:02s\n",
      "epoch 44 | loss: 0.6054  |  0:00:02s\n",
      "epoch 45 | loss: 0.60647 |  0:00:02s\n",
      "epoch 46 | loss: 0.60168 |  0:00:02s\n",
      "epoch 47 | loss: 0.60723 |  0:00:02s\n",
      "epoch 48 | loss: 0.60181 |  0:00:02s\n",
      "epoch 49 | loss: 0.5965  |  0:00:02s\n",
      "epoch 50 | loss: 0.59766 |  0:00:02s\n",
      "epoch 51 | loss: 0.59164 |  0:00:02s\n",
      "epoch 52 | loss: 0.58943 |  0:00:02s\n",
      "epoch 53 | loss: 0.59378 |  0:00:02s\n",
      "epoch 54 | loss: 0.59398 |  0:00:02s\n",
      "epoch 55 | loss: 0.59143 |  0:00:03s\n",
      "epoch 56 | loss: 0.58312 |  0:00:03s\n",
      "epoch 57 | loss: 0.58972 |  0:00:03s\n",
      "epoch 58 | loss: 0.58792 |  0:00:03s\n",
      "epoch 59 | loss: 0.57287 |  0:00:03s\n",
      "epoch 60 | loss: 0.58081 |  0:00:03s\n",
      "epoch 61 | loss: 0.58543 |  0:00:03s\n",
      "epoch 62 | loss: 0.57928 |  0:00:03s\n",
      "epoch 63 | loss: 0.5788  |  0:00:03s\n",
      "epoch 64 | loss: 0.57862 |  0:00:03s\n",
      "epoch 65 | loss: 0.5694  |  0:00:03s\n",
      "epoch 66 | loss: 0.56549 |  0:00:03s\n",
      "epoch 67 | loss: 0.56998 |  0:00:03s\n",
      "epoch 68 | loss: 0.56248 |  0:00:03s\n",
      "epoch 69 | loss: 0.5759  |  0:00:03s\n",
      "epoch 70 | loss: 0.5704  |  0:00:03s\n",
      "epoch 71 | loss: 0.57054 |  0:00:03s\n",
      "epoch 72 | loss: 0.55428 |  0:00:03s\n",
      "epoch 73 | loss: 0.57224 |  0:00:03s\n",
      "epoch 74 | loss: 0.57207 |  0:00:04s\n",
      "epoch 75 | loss: 0.56726 |  0:00:04s\n",
      "epoch 76 | loss: 0.56801 |  0:00:04s\n",
      "epoch 77 | loss: 0.56825 |  0:00:04s\n",
      "epoch 78 | loss: 0.55238 |  0:00:04s\n",
      "epoch 79 | loss: 0.55159 |  0:00:04s\n",
      "epoch 80 | loss: 0.55001 |  0:00:04s\n",
      "epoch 81 | loss: 0.54932 |  0:00:04s\n",
      "epoch 82 | loss: 0.55297 |  0:00:04s\n",
      "epoch 83 | loss: 0.55859 |  0:00:04s\n",
      "epoch 84 | loss: 0.55933 |  0:00:04s\n",
      "epoch 85 | loss: 0.55397 |  0:00:04s\n",
      "epoch 86 | loss: 0.54412 |  0:00:04s\n",
      "epoch 87 | loss: 0.54246 |  0:00:04s\n",
      "epoch 88 | loss: 0.5482  |  0:00:04s\n",
      "epoch 89 | loss: 0.5432  |  0:00:04s\n",
      "epoch 90 | loss: 0.53411 |  0:00:04s\n",
      "epoch 91 | loss: 0.53401 |  0:00:04s\n",
      "epoch 92 | loss: 0.53155 |  0:00:04s\n",
      "epoch 93 | loss: 0.53395 |  0:00:05s\n",
      "epoch 94 | loss: 0.53541 |  0:00:05s\n",
      "epoch 95 | loss: 0.52976 |  0:00:05s\n",
      "epoch 96 | loss: 0.52521 |  0:00:05s\n",
      "epoch 97 | loss: 0.52356 |  0:00:05s\n",
      "epoch 98 | loss: 0.52197 |  0:00:05s\n",
      "epoch 99 | loss: 0.52468 |  0:00:05s\n",
      "epoch 100| loss: 0.51924 |  0:00:05s\n",
      "epoch 101| loss: 0.53273 |  0:00:05s\n",
      "epoch 102| loss: 0.53217 |  0:00:05s\n",
      "epoch 103| loss: 0.51972 |  0:00:05s\n",
      "epoch 104| loss: 0.5241  |  0:00:05s\n",
      "epoch 105| loss: 0.53392 |  0:00:05s\n",
      "epoch 106| loss: 0.51929 |  0:00:05s\n",
      "epoch 107| loss: 0.51697 |  0:00:05s\n",
      "epoch 108| loss: 0.51993 |  0:00:05s\n",
      "epoch 109| loss: 0.5157  |  0:00:05s\n",
      "epoch 110| loss: 0.50673 |  0:00:05s\n",
      "epoch 111| loss: 0.51378 |  0:00:05s\n",
      "epoch 112| loss: 0.50496 |  0:00:05s\n",
      "epoch 113| loss: 0.5136  |  0:00:06s\n",
      "epoch 114| loss: 0.50387 |  0:00:06s\n",
      "epoch 115| loss: 0.51399 |  0:00:06s\n",
      "epoch 116| loss: 0.51375 |  0:00:06s\n",
      "epoch 117| loss: 0.51221 |  0:00:06s\n",
      "epoch 118| loss: 0.51842 |  0:00:06s\n",
      "epoch 119| loss: 0.5196  |  0:00:06s\n",
      "epoch 120| loss: 0.50693 |  0:00:06s\n",
      "epoch 121| loss: 0.50868 |  0:00:06s\n",
      "epoch 122| loss: 0.51039 |  0:00:06s\n",
      "epoch 123| loss: 0.49924 |  0:00:06s\n",
      "epoch 124| loss: 0.4872  |  0:00:06s\n",
      "epoch 125| loss: 0.49185 |  0:00:06s\n",
      "epoch 126| loss: 0.49113 |  0:00:06s\n",
      "epoch 127| loss: 0.49987 |  0:00:06s\n",
      "epoch 128| loss: 0.48758 |  0:00:06s\n",
      "epoch 129| loss: 0.49301 |  0:00:06s\n",
      "epoch 130| loss: 0.48189 |  0:00:06s\n",
      "epoch 131| loss: 0.47771 |  0:00:06s\n",
      "epoch 132| loss: 0.48269 |  0:00:06s\n",
      "epoch 133| loss: 0.48153 |  0:00:07s\n",
      "epoch 134| loss: 0.4769  |  0:00:07s\n",
      "epoch 135| loss: 0.47977 |  0:00:07s\n",
      "epoch 136| loss: 0.48087 |  0:00:07s\n",
      "epoch 137| loss: 0.47567 |  0:00:07s\n",
      "epoch 138| loss: 0.47885 |  0:00:07s\n",
      "epoch 139| loss: 0.46821 |  0:00:07s\n",
      "epoch 140| loss: 0.47541 |  0:00:07s\n",
      "epoch 141| loss: 0.47188 |  0:00:07s\n",
      "epoch 142| loss: 0.46815 |  0:00:07s\n",
      "epoch 143| loss: 0.45661 |  0:00:07s\n",
      "epoch 144| loss: 0.4575  |  0:00:07s\n",
      "epoch 145| loss: 0.46084 |  0:00:07s\n",
      "epoch 146| loss: 0.4576  |  0:00:07s\n",
      "epoch 147| loss: 0.46291 |  0:00:07s\n",
      "epoch 148| loss: 0.45801 |  0:00:07s\n",
      "epoch 149| loss: 0.45929 |  0:00:07s\n",
      "epoch 150| loss: 0.4571  |  0:00:07s\n",
      "epoch 151| loss: 0.46752 |  0:00:07s\n",
      "epoch 152| loss: 0.46937 |  0:00:07s\n",
      "epoch 153| loss: 0.46155 |  0:00:08s\n",
      "epoch 154| loss: 0.4611  |  0:00:08s\n",
      "epoch 155| loss: 0.46497 |  0:00:08s\n",
      "epoch 156| loss: 0.46835 |  0:00:08s\n",
      "epoch 157| loss: 0.45771 |  0:00:08s\n",
      "epoch 158| loss: 0.45381 |  0:00:08s\n",
      "epoch 159| loss: 0.45533 |  0:00:08s\n",
      "epoch 160| loss: 0.45771 |  0:00:08s\n",
      "epoch 161| loss: 0.45051 |  0:00:08s\n",
      "epoch 162| loss: 0.46154 |  0:00:08s\n",
      "epoch 163| loss: 0.45664 |  0:00:08s\n",
      "epoch 164| loss: 0.45126 |  0:00:08s\n",
      "epoch 165| loss: 0.45285 |  0:00:08s\n",
      "epoch 166| loss: 0.45149 |  0:00:08s\n",
      "epoch 167| loss: 0.44962 |  0:00:08s\n",
      "epoch 168| loss: 0.4556  |  0:00:08s\n",
      "epoch 169| loss: 0.44812 |  0:00:08s\n",
      "epoch 170| loss: 0.45203 |  0:00:08s\n",
      "epoch 171| loss: 0.44929 |  0:00:08s\n",
      "epoch 172| loss: 0.45318 |  0:00:09s\n",
      "epoch 173| loss: 0.45309 |  0:00:09s\n",
      "epoch 174| loss: 0.44091 |  0:00:09s\n",
      "epoch 175| loss: 0.44483 |  0:00:09s\n",
      "epoch 176| loss: 0.44688 |  0:00:09s\n",
      "epoch 177| loss: 0.443   |  0:00:09s\n",
      "epoch 178| loss: 0.43958 |  0:00:09s\n",
      "epoch 179| loss: 0.43811 |  0:00:09s\n",
      "epoch 180| loss: 0.44215 |  0:00:09s\n",
      "epoch 181| loss: 0.42966 |  0:00:09s\n",
      "epoch 182| loss: 0.43381 |  0:00:09s\n",
      "epoch 183| loss: 0.42783 |  0:00:09s\n",
      "epoch 184| loss: 0.42269 |  0:00:09s\n",
      "epoch 185| loss: 0.42291 |  0:00:09s\n",
      "epoch 186| loss: 0.41899 |  0:00:09s\n",
      "epoch 187| loss: 0.4231  |  0:00:09s\n",
      "epoch 188| loss: 0.42708 |  0:00:09s\n",
      "epoch 189| loss: 0.42099 |  0:00:09s\n",
      "epoch 190| loss: 0.42317 |  0:00:09s\n",
      "epoch 191| loss: 0.42029 |  0:00:10s\n",
      "epoch 192| loss: 0.42247 |  0:00:10s\n",
      "epoch 193| loss: 0.42544 |  0:00:10s\n",
      "epoch 194| loss: 0.41785 |  0:00:10s\n",
      "epoch 195| loss: 0.41222 |  0:00:10s\n",
      "epoch 196| loss: 0.41958 |  0:00:10s\n",
      "epoch 197| loss: 0.41914 |  0:00:10s\n",
      "epoch 198| loss: 0.41977 |  0:00:10s\n",
      "epoch 199| loss: 0.41046 |  0:00:10s\n",
      "epoch 200| loss: 0.41575 |  0:00:10s\n",
      "epoch 201| loss: 0.40569 |  0:00:10s\n",
      "epoch 202| loss: 0.41317 |  0:00:10s\n",
      "epoch 203| loss: 0.41008 |  0:00:10s\n",
      "epoch 204| loss: 0.40483 |  0:00:10s\n",
      "epoch 205| loss: 0.40876 |  0:00:10s\n",
      "epoch 206| loss: 0.41132 |  0:00:10s\n",
      "epoch 207| loss: 0.4103  |  0:00:10s\n",
      "epoch 208| loss: 0.39719 |  0:00:10s\n",
      "epoch 209| loss: 0.39663 |  0:00:10s\n",
      "epoch 210| loss: 0.40955 |  0:00:10s\n",
      "epoch 211| loss: 0.39567 |  0:00:11s\n",
      "epoch 212| loss: 0.39733 |  0:00:11s\n",
      "epoch 213| loss: 0.3965  |  0:00:11s\n",
      "epoch 214| loss: 0.39345 |  0:00:11s\n",
      "epoch 215| loss: 0.38575 |  0:00:11s\n",
      "epoch 216| loss: 0.39184 |  0:00:11s\n",
      "epoch 217| loss: 0.39439 |  0:00:11s\n",
      "epoch 218| loss: 0.38424 |  0:00:11s\n",
      "epoch 219| loss: 0.38464 |  0:00:11s\n",
      "epoch 220| loss: 0.39932 |  0:00:11s\n",
      "epoch 221| loss: 0.38654 |  0:00:11s\n",
      "epoch 222| loss: 0.3867  |  0:00:11s\n",
      "epoch 223| loss: 0.40125 |  0:00:11s\n",
      "epoch 224| loss: 0.38303 |  0:00:11s\n",
      "epoch 225| loss: 0.38999 |  0:00:11s\n",
      "epoch 226| loss: 0.38956 |  0:00:11s\n",
      "epoch 227| loss: 0.39119 |  0:00:11s\n",
      "epoch 228| loss: 0.38725 |  0:00:11s\n",
      "epoch 229| loss: 0.38397 |  0:00:11s\n",
      "epoch 230| loss: 0.38932 |  0:00:11s\n",
      "epoch 231| loss: 0.39075 |  0:00:12s\n",
      "epoch 232| loss: 0.38743 |  0:00:12s\n",
      "epoch 233| loss: 0.38406 |  0:00:12s\n",
      "epoch 234| loss: 0.37267 |  0:00:12s\n",
      "epoch 235| loss: 0.37838 |  0:00:12s\n",
      "epoch 236| loss: 0.38243 |  0:00:12s\n",
      "epoch 237| loss: 0.37144 |  0:00:12s\n",
      "epoch 238| loss: 0.37761 |  0:00:12s\n",
      "epoch 239| loss: 0.37598 |  0:00:12s\n",
      "epoch 240| loss: 0.37798 |  0:00:12s\n",
      "epoch 241| loss: 0.37153 |  0:00:12s\n",
      "epoch 242| loss: 0.37644 |  0:00:12s\n",
      "epoch 243| loss: 0.37735 |  0:00:12s\n",
      "epoch 244| loss: 0.36996 |  0:00:12s\n",
      "epoch 245| loss: 0.3795  |  0:00:12s\n",
      "epoch 246| loss: 0.36605 |  0:00:12s\n",
      "epoch 247| loss: 0.3814  |  0:00:12s\n",
      "epoch 248| loss: 0.38055 |  0:00:12s\n",
      "epoch 249| loss: 0.38103 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.06506 |  0:00:00s\n",
      "epoch 1  | loss: 1.69517 |  0:00:00s\n",
      "epoch 2  | loss: 1.2678  |  0:00:00s\n",
      "epoch 3  | loss: 1.11123 |  0:00:00s\n",
      "epoch 4  | loss: 1.00135 |  0:00:00s\n",
      "epoch 5  | loss: 0.94278 |  0:00:00s\n",
      "epoch 6  | loss: 0.92401 |  0:00:00s\n",
      "epoch 7  | loss: 0.89422 |  0:00:00s\n",
      "epoch 8  | loss: 0.87732 |  0:00:00s\n",
      "epoch 9  | loss: 0.86625 |  0:00:00s\n",
      "epoch 10 | loss: 0.85995 |  0:00:00s\n",
      "epoch 11 | loss: 0.82949 |  0:00:00s\n",
      "epoch 12 | loss: 0.81703 |  0:00:00s\n",
      "epoch 13 | loss: 0.79931 |  0:00:00s\n",
      "epoch 14 | loss: 0.77298 |  0:00:00s\n",
      "epoch 15 | loss: 0.73948 |  0:00:00s\n",
      "epoch 16 | loss: 0.73973 |  0:00:00s\n",
      "epoch 17 | loss: 0.72864 |  0:00:00s\n",
      "epoch 18 | loss: 0.7345  |  0:00:00s\n",
      "epoch 19 | loss: 0.71561 |  0:00:00s\n",
      "epoch 20 | loss: 0.70582 |  0:00:00s\n",
      "epoch 21 | loss: 0.69181 |  0:00:00s\n",
      "epoch 22 | loss: 0.69406 |  0:00:01s\n",
      "epoch 23 | loss: 0.69159 |  0:00:01s\n",
      "epoch 24 | loss: 0.69432 |  0:00:01s\n",
      "epoch 25 | loss: 0.69274 |  0:00:01s\n",
      "epoch 26 | loss: 0.68674 |  0:00:01s\n",
      "epoch 27 | loss: 0.67772 |  0:00:01s\n",
      "epoch 28 | loss: 0.67318 |  0:00:01s\n",
      "epoch 29 | loss: 0.67556 |  0:00:01s\n",
      "epoch 30 | loss: 0.66586 |  0:00:01s\n",
      "epoch 31 | loss: 0.66401 |  0:00:01s\n",
      "epoch 32 | loss: 0.67627 |  0:00:01s\n",
      "epoch 33 | loss: 0.66289 |  0:00:01s\n",
      "epoch 34 | loss: 0.66254 |  0:00:01s\n",
      "epoch 35 | loss: 0.6593  |  0:00:01s\n",
      "epoch 36 | loss: 0.66796 |  0:00:01s\n",
      "epoch 37 | loss: 0.65692 |  0:00:01s\n",
      "epoch 38 | loss: 0.65682 |  0:00:01s\n",
      "epoch 39 | loss: 0.64896 |  0:00:01s\n",
      "epoch 40 | loss: 0.65346 |  0:00:01s\n",
      "epoch 41 | loss: 0.64788 |  0:00:01s\n",
      "epoch 42 | loss: 0.66103 |  0:00:01s\n",
      "epoch 43 | loss: 0.65996 |  0:00:01s\n",
      "epoch 44 | loss: 0.64198 |  0:00:02s\n",
      "epoch 45 | loss: 0.63785 |  0:00:02s\n",
      "epoch 46 | loss: 0.64518 |  0:00:02s\n",
      "epoch 47 | loss: 0.63454 |  0:00:02s\n",
      "epoch 48 | loss: 0.63736 |  0:00:02s\n",
      "epoch 49 | loss: 0.633   |  0:00:02s\n",
      "epoch 50 | loss: 0.6304  |  0:00:02s\n",
      "epoch 51 | loss: 0.63413 |  0:00:02s\n",
      "epoch 52 | loss: 0.62491 |  0:00:02s\n",
      "epoch 53 | loss: 0.62267 |  0:00:02s\n",
      "epoch 54 | loss: 0.62575 |  0:00:02s\n",
      "epoch 55 | loss: 0.61916 |  0:00:02s\n",
      "epoch 56 | loss: 0.6182  |  0:00:02s\n",
      "epoch 57 | loss: 0.62324 |  0:00:02s\n",
      "epoch 58 | loss: 0.61626 |  0:00:02s\n",
      "epoch 59 | loss: 0.61613 |  0:00:02s\n",
      "epoch 60 | loss: 0.62482 |  0:00:02s\n",
      "epoch 61 | loss: 0.63301 |  0:00:02s\n",
      "epoch 62 | loss: 0.62174 |  0:00:02s\n",
      "epoch 63 | loss: 0.61791 |  0:00:02s\n",
      "epoch 64 | loss: 0.63713 |  0:00:02s\n",
      "epoch 65 | loss: 0.62674 |  0:00:03s\n",
      "epoch 66 | loss: 0.61693 |  0:00:03s\n",
      "epoch 67 | loss: 0.61247 |  0:00:03s\n",
      "epoch 68 | loss: 0.60961 |  0:00:03s\n",
      "epoch 69 | loss: 0.60262 |  0:00:03s\n",
      "epoch 70 | loss: 0.60929 |  0:00:03s\n",
      "epoch 71 | loss: 0.59796 |  0:00:03s\n",
      "epoch 72 | loss: 0.59681 |  0:00:03s\n",
      "epoch 73 | loss: 0.60657 |  0:00:03s\n",
      "epoch 74 | loss: 0.60097 |  0:00:03s\n",
      "epoch 75 | loss: 0.59685 |  0:00:03s\n",
      "epoch 76 | loss: 0.59999 |  0:00:03s\n",
      "epoch 77 | loss: 0.60238 |  0:00:03s\n",
      "epoch 78 | loss: 0.60124 |  0:00:03s\n",
      "epoch 79 | loss: 0.60047 |  0:00:03s\n",
      "epoch 80 | loss: 0.59614 |  0:00:03s\n",
      "epoch 81 | loss: 0.59529 |  0:00:03s\n",
      "epoch 82 | loss: 0.59183 |  0:00:03s\n",
      "epoch 83 | loss: 0.59549 |  0:00:03s\n",
      "epoch 84 | loss: 0.58309 |  0:00:04s\n",
      "epoch 85 | loss: 0.58689 |  0:00:04s\n",
      "epoch 86 | loss: 0.58291 |  0:00:04s\n",
      "epoch 87 | loss: 0.57032 |  0:00:04s\n",
      "epoch 88 | loss: 0.57901 |  0:00:04s\n",
      "epoch 89 | loss: 0.58179 |  0:00:04s\n",
      "epoch 90 | loss: 0.57756 |  0:00:04s\n",
      "epoch 91 | loss: 0.5795  |  0:00:04s\n",
      "epoch 92 | loss: 0.5854  |  0:00:04s\n",
      "epoch 93 | loss: 0.58178 |  0:00:04s\n",
      "epoch 94 | loss: 0.57981 |  0:00:04s\n",
      "epoch 95 | loss: 0.58284 |  0:00:04s\n",
      "epoch 96 | loss: 0.5754  |  0:00:04s\n",
      "epoch 97 | loss: 0.57961 |  0:00:04s\n",
      "epoch 98 | loss: 0.57722 |  0:00:04s\n",
      "epoch 99 | loss: 0.57667 |  0:00:04s\n",
      "epoch 100| loss: 0.56935 |  0:00:04s\n",
      "epoch 101| loss: 0.57519 |  0:00:04s\n",
      "epoch 102| loss: 0.57919 |  0:00:04s\n",
      "epoch 103| loss: 0.56353 |  0:00:05s\n",
      "epoch 104| loss: 0.58305 |  0:00:05s\n",
      "epoch 105| loss: 0.57428 |  0:00:05s\n",
      "epoch 106| loss: 0.5791  |  0:00:05s\n",
      "epoch 107| loss: 0.57706 |  0:00:05s\n",
      "epoch 108| loss: 0.5737  |  0:00:05s\n",
      "epoch 109| loss: 0.57384 |  0:00:05s\n",
      "epoch 110| loss: 0.57588 |  0:00:05s\n",
      "epoch 111| loss: 0.57819 |  0:00:05s\n",
      "epoch 112| loss: 0.58633 |  0:00:05s\n",
      "epoch 113| loss: 0.5744  |  0:00:05s\n",
      "epoch 114| loss: 0.572   |  0:00:05s\n",
      "epoch 115| loss: 0.57356 |  0:00:05s\n",
      "epoch 116| loss: 0.57995 |  0:00:05s\n",
      "epoch 117| loss: 0.56964 |  0:00:05s\n",
      "epoch 118| loss: 0.56484 |  0:00:05s\n",
      "epoch 119| loss: 0.56167 |  0:00:05s\n",
      "epoch 120| loss: 0.55671 |  0:00:05s\n",
      "epoch 121| loss: 0.56348 |  0:00:05s\n",
      "epoch 122| loss: 0.55929 |  0:00:05s\n",
      "epoch 123| loss: 0.56656 |  0:00:05s\n",
      "epoch 124| loss: 0.55828 |  0:00:06s\n",
      "epoch 125| loss: 0.56366 |  0:00:06s\n",
      "epoch 126| loss: 0.55222 |  0:00:06s\n",
      "epoch 127| loss: 0.55797 |  0:00:06s\n",
      "epoch 128| loss: 0.55347 |  0:00:06s\n",
      "epoch 129| loss: 0.55874 |  0:00:06s\n",
      "epoch 130| loss: 0.54944 |  0:00:06s\n",
      "epoch 131| loss: 0.55607 |  0:00:06s\n",
      "epoch 132| loss: 0.54923 |  0:00:06s\n",
      "epoch 133| loss: 0.54819 |  0:00:06s\n",
      "epoch 134| loss: 0.54856 |  0:00:06s\n",
      "epoch 135| loss: 0.55271 |  0:00:06s\n",
      "epoch 136| loss: 0.55015 |  0:00:06s\n",
      "epoch 137| loss: 0.5507  |  0:00:06s\n",
      "epoch 138| loss: 0.54753 |  0:00:06s\n",
      "epoch 139| loss: 0.54712 |  0:00:06s\n",
      "epoch 140| loss: 0.55146 |  0:00:06s\n",
      "epoch 141| loss: 0.54987 |  0:00:06s\n",
      "epoch 142| loss: 0.55419 |  0:00:06s\n",
      "epoch 143| loss: 0.54678 |  0:00:06s\n",
      "epoch 144| loss: 0.54985 |  0:00:06s\n",
      "epoch 145| loss: 0.54543 |  0:00:07s\n",
      "epoch 146| loss: 0.53826 |  0:00:07s\n",
      "epoch 147| loss: 0.54275 |  0:00:07s\n",
      "epoch 148| loss: 0.53659 |  0:00:07s\n",
      "epoch 149| loss: 0.53536 |  0:00:07s\n",
      "epoch 150| loss: 0.5338  |  0:00:07s\n",
      "epoch 151| loss: 0.53526 |  0:00:07s\n",
      "epoch 152| loss: 0.54004 |  0:00:07s\n",
      "epoch 153| loss: 0.53968 |  0:00:07s\n",
      "epoch 154| loss: 0.5268  |  0:00:07s\n",
      "epoch 155| loss: 0.5295  |  0:00:07s\n",
      "epoch 156| loss: 0.53019 |  0:00:07s\n",
      "epoch 157| loss: 0.53634 |  0:00:07s\n",
      "epoch 158| loss: 0.52546 |  0:00:07s\n",
      "epoch 159| loss: 0.53853 |  0:00:07s\n",
      "epoch 160| loss: 0.53212 |  0:00:07s\n",
      "epoch 161| loss: 0.53255 |  0:00:07s\n",
      "epoch 162| loss: 0.5326  |  0:00:07s\n",
      "epoch 163| loss: 0.52909 |  0:00:07s\n",
      "epoch 164| loss: 0.52829 |  0:00:07s\n",
      "epoch 165| loss: 0.52001 |  0:00:07s\n",
      "epoch 166| loss: 0.52967 |  0:00:08s\n",
      "epoch 167| loss: 0.52493 |  0:00:08s\n",
      "epoch 168| loss: 0.52865 |  0:00:08s\n",
      "epoch 169| loss: 0.51699 |  0:00:08s\n",
      "epoch 170| loss: 0.51633 |  0:00:08s\n",
      "epoch 171| loss: 0.50821 |  0:00:08s\n",
      "epoch 172| loss: 0.5216  |  0:00:08s\n",
      "epoch 173| loss: 0.52723 |  0:00:08s\n",
      "epoch 174| loss: 0.53089 |  0:00:08s\n",
      "epoch 175| loss: 0.51545 |  0:00:08s\n",
      "epoch 176| loss: 0.52324 |  0:00:08s\n",
      "epoch 177| loss: 0.50961 |  0:00:08s\n",
      "epoch 178| loss: 0.5179  |  0:00:08s\n",
      "epoch 179| loss: 0.50976 |  0:00:08s\n",
      "epoch 180| loss: 0.51815 |  0:00:08s\n",
      "epoch 181| loss: 0.50964 |  0:00:08s\n",
      "epoch 182| loss: 0.50926 |  0:00:08s\n",
      "epoch 183| loss: 0.50907 |  0:00:08s\n",
      "epoch 184| loss: 0.50783 |  0:00:08s\n",
      "epoch 185| loss: 0.5192  |  0:00:08s\n",
      "epoch 186| loss: 0.52176 |  0:00:08s\n",
      "epoch 187| loss: 0.52036 |  0:00:08s\n",
      "epoch 188| loss: 0.50941 |  0:00:08s\n",
      "epoch 189| loss: 0.50328 |  0:00:09s\n",
      "epoch 190| loss: 0.50187 |  0:00:09s\n",
      "epoch 191| loss: 0.49886 |  0:00:09s\n",
      "epoch 192| loss: 0.50964 |  0:00:09s\n",
      "epoch 193| loss: 0.51505 |  0:00:09s\n",
      "epoch 194| loss: 0.5207  |  0:00:09s\n",
      "epoch 195| loss: 0.51459 |  0:00:09s\n",
      "epoch 196| loss: 0.52319 |  0:00:09s\n",
      "epoch 197| loss: 0.51752 |  0:00:09s\n",
      "epoch 198| loss: 0.5284  |  0:00:09s\n",
      "epoch 199| loss: 0.51995 |  0:00:09s\n",
      "epoch 200| loss: 0.53539 |  0:00:09s\n",
      "epoch 201| loss: 0.53263 |  0:00:09s\n",
      "epoch 202| loss: 0.52834 |  0:00:09s\n",
      "epoch 203| loss: 0.52372 |  0:00:09s\n",
      "epoch 204| loss: 0.51785 |  0:00:09s\n",
      "epoch 205| loss: 0.51564 |  0:00:09s\n",
      "epoch 206| loss: 0.51087 |  0:00:10s\n",
      "epoch 207| loss: 0.51563 |  0:00:10s\n",
      "epoch 208| loss: 0.507   |  0:00:10s\n",
      "epoch 209| loss: 0.50358 |  0:00:10s\n",
      "epoch 210| loss: 0.51535 |  0:00:10s\n",
      "epoch 211| loss: 0.50499 |  0:00:10s\n",
      "epoch 212| loss: 0.50397 |  0:00:10s\n",
      "epoch 213| loss: 0.50421 |  0:00:10s\n",
      "epoch 214| loss: 0.49352 |  0:00:10s\n",
      "epoch 215| loss: 0.50278 |  0:00:10s\n",
      "epoch 216| loss: 0.50238 |  0:00:10s\n",
      "epoch 217| loss: 0.50509 |  0:00:10s\n",
      "epoch 218| loss: 0.51113 |  0:00:10s\n",
      "epoch 219| loss: 0.50793 |  0:00:10s\n",
      "epoch 220| loss: 0.51384 |  0:00:10s\n",
      "epoch 221| loss: 0.50091 |  0:00:10s\n",
      "epoch 222| loss: 0.49404 |  0:00:10s\n",
      "epoch 223| loss: 0.5018  |  0:00:10s\n",
      "epoch 224| loss: 0.49187 |  0:00:10s\n",
      "epoch 225| loss: 0.49905 |  0:00:11s\n",
      "epoch 226| loss: 0.50279 |  0:00:11s\n",
      "epoch 227| loss: 0.50334 |  0:00:11s\n",
      "epoch 228| loss: 0.49982 |  0:00:11s\n",
      "epoch 229| loss: 0.50282 |  0:00:11s\n",
      "epoch 230| loss: 0.49709 |  0:00:11s\n",
      "epoch 231| loss: 0.50042 |  0:00:11s\n",
      "epoch 232| loss: 0.50495 |  0:00:11s\n",
      "epoch 233| loss: 0.50777 |  0:00:11s\n",
      "epoch 234| loss: 0.50267 |  0:00:11s\n",
      "epoch 235| loss: 0.50165 |  0:00:11s\n",
      "epoch 236| loss: 0.49917 |  0:00:11s\n",
      "epoch 237| loss: 0.48967 |  0:00:11s\n",
      "epoch 238| loss: 0.4904  |  0:00:11s\n",
      "epoch 239| loss: 0.48499 |  0:00:11s\n",
      "epoch 240| loss: 0.48454 |  0:00:11s\n",
      "epoch 241| loss: 0.48119 |  0:00:11s\n",
      "epoch 242| loss: 0.48592 |  0:00:11s\n",
      "epoch 243| loss: 0.49457 |  0:00:11s\n",
      "epoch 244| loss: 0.48401 |  0:00:12s\n",
      "epoch 245| loss: 0.48093 |  0:00:12s\n",
      "epoch 246| loss: 0.47491 |  0:00:12s\n",
      "epoch 247| loss: 0.47695 |  0:00:12s\n",
      "epoch 248| loss: 0.4824  |  0:00:12s\n",
      "epoch 249| loss: 0.47773 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.96173 |  0:00:00s\n",
      "epoch 1  | loss: 1.7274  |  0:00:00s\n",
      "epoch 2  | loss: 1.33408 |  0:00:00s\n",
      "epoch 3  | loss: 1.1371  |  0:00:00s\n",
      "epoch 4  | loss: 1.03854 |  0:00:00s\n",
      "epoch 5  | loss: 0.97704 |  0:00:00s\n",
      "epoch 6  | loss: 0.92519 |  0:00:00s\n",
      "epoch 7  | loss: 0.8904  |  0:00:00s\n",
      "epoch 8  | loss: 0.87267 |  0:00:00s\n",
      "epoch 9  | loss: 0.85248 |  0:00:00s\n",
      "epoch 10 | loss: 0.83977 |  0:00:00s\n",
      "epoch 11 | loss: 0.82122 |  0:00:00s\n",
      "epoch 12 | loss: 0.78864 |  0:00:00s\n",
      "epoch 13 | loss: 0.79545 |  0:00:00s\n",
      "epoch 14 | loss: 0.77988 |  0:00:00s\n",
      "epoch 15 | loss: 0.77376 |  0:00:00s\n",
      "epoch 16 | loss: 0.75211 |  0:00:00s\n",
      "epoch 17 | loss: 0.74622 |  0:00:00s\n",
      "epoch 18 | loss: 0.72201 |  0:00:00s\n",
      "epoch 19 | loss: 0.72192 |  0:00:01s\n",
      "epoch 20 | loss: 0.71102 |  0:00:01s\n",
      "epoch 21 | loss: 0.70454 |  0:00:01s\n",
      "epoch 22 | loss: 0.70549 |  0:00:01s\n",
      "epoch 23 | loss: 0.7063  |  0:00:01s\n",
      "epoch 24 | loss: 0.7101  |  0:00:01s\n",
      "epoch 25 | loss: 0.70035 |  0:00:01s\n",
      "epoch 26 | loss: 0.69825 |  0:00:01s\n",
      "epoch 27 | loss: 0.69188 |  0:00:01s\n",
      "epoch 28 | loss: 0.68374 |  0:00:01s\n",
      "epoch 29 | loss: 0.6829  |  0:00:01s\n",
      "epoch 30 | loss: 0.68747 |  0:00:01s\n",
      "epoch 31 | loss: 0.66001 |  0:00:01s\n",
      "epoch 32 | loss: 0.65886 |  0:00:01s\n",
      "epoch 33 | loss: 0.66184 |  0:00:01s\n",
      "epoch 34 | loss: 0.66006 |  0:00:01s\n",
      "epoch 35 | loss: 0.66428 |  0:00:01s\n",
      "epoch 36 | loss: 0.66576 |  0:00:01s\n",
      "epoch 37 | loss: 0.66495 |  0:00:01s\n",
      "epoch 38 | loss: 0.64973 |  0:00:02s\n",
      "epoch 39 | loss: 0.64858 |  0:00:02s\n",
      "epoch 40 | loss: 0.63754 |  0:00:02s\n",
      "epoch 41 | loss: 0.63733 |  0:00:02s\n",
      "epoch 42 | loss: 0.63204 |  0:00:02s\n",
      "epoch 43 | loss: 0.63463 |  0:00:02s\n",
      "epoch 44 | loss: 0.62646 |  0:00:02s\n",
      "epoch 45 | loss: 0.62357 |  0:00:02s\n",
      "epoch 46 | loss: 0.61685 |  0:00:02s\n",
      "epoch 47 | loss: 0.6077  |  0:00:02s\n",
      "epoch 48 | loss: 0.61828 |  0:00:02s\n",
      "epoch 49 | loss: 0.61565 |  0:00:02s\n",
      "epoch 50 | loss: 0.60571 |  0:00:02s\n",
      "epoch 51 | loss: 0.6127  |  0:00:02s\n",
      "epoch 52 | loss: 0.61828 |  0:00:02s\n",
      "epoch 53 | loss: 0.61272 |  0:00:02s\n",
      "epoch 54 | loss: 0.61905 |  0:00:02s\n",
      "epoch 55 | loss: 0.6122  |  0:00:02s\n",
      "epoch 56 | loss: 0.61457 |  0:00:02s\n",
      "epoch 57 | loss: 0.61661 |  0:00:03s\n",
      "epoch 58 | loss: 0.61493 |  0:00:03s\n",
      "epoch 59 | loss: 0.6029  |  0:00:03s\n",
      "epoch 60 | loss: 0.61047 |  0:00:03s\n",
      "epoch 61 | loss: 0.60009 |  0:00:03s\n",
      "epoch 62 | loss: 0.60791 |  0:00:03s\n",
      "epoch 63 | loss: 0.59768 |  0:00:03s\n",
      "epoch 64 | loss: 0.61084 |  0:00:03s\n",
      "epoch 65 | loss: 0.59764 |  0:00:03s\n",
      "epoch 66 | loss: 0.60042 |  0:00:03s\n",
      "epoch 67 | loss: 0.6058  |  0:00:03s\n",
      "epoch 68 | loss: 0.60486 |  0:00:03s\n",
      "epoch 69 | loss: 0.60533 |  0:00:03s\n",
      "epoch 70 | loss: 0.60038 |  0:00:03s\n",
      "epoch 71 | loss: 0.59729 |  0:00:03s\n",
      "epoch 72 | loss: 0.59485 |  0:00:03s\n",
      "epoch 73 | loss: 0.59706 |  0:00:03s\n",
      "epoch 74 | loss: 0.59296 |  0:00:03s\n",
      "epoch 75 | loss: 0.5885  |  0:00:03s\n",
      "epoch 76 | loss: 0.5891  |  0:00:03s\n",
      "epoch 77 | loss: 0.58005 |  0:00:04s\n",
      "epoch 78 | loss: 0.58998 |  0:00:04s\n",
      "epoch 79 | loss: 0.57786 |  0:00:04s\n",
      "epoch 80 | loss: 0.5827  |  0:00:04s\n",
      "epoch 81 | loss: 0.56873 |  0:00:04s\n",
      "epoch 82 | loss: 0.56573 |  0:00:04s\n",
      "epoch 83 | loss: 0.57053 |  0:00:04s\n",
      "epoch 84 | loss: 0.56871 |  0:00:04s\n",
      "epoch 85 | loss: 0.56271 |  0:00:04s\n",
      "epoch 86 | loss: 0.56349 |  0:00:04s\n",
      "epoch 87 | loss: 0.55581 |  0:00:04s\n",
      "epoch 88 | loss: 0.56123 |  0:00:04s\n",
      "epoch 89 | loss: 0.56191 |  0:00:04s\n",
      "epoch 90 | loss: 0.55806 |  0:00:04s\n",
      "epoch 91 | loss: 0.55087 |  0:00:04s\n",
      "epoch 92 | loss: 0.55528 |  0:00:04s\n",
      "epoch 93 | loss: 0.55176 |  0:00:04s\n",
      "epoch 94 | loss: 0.55841 |  0:00:04s\n",
      "epoch 95 | loss: 0.55334 |  0:00:04s\n",
      "epoch 96 | loss: 0.54835 |  0:00:04s\n",
      "epoch 97 | loss: 0.53923 |  0:00:04s\n",
      "epoch 98 | loss: 0.54041 |  0:00:04s\n",
      "epoch 99 | loss: 0.55215 |  0:00:05s\n",
      "epoch 100| loss: 0.53923 |  0:00:05s\n",
      "epoch 101| loss: 0.5412  |  0:00:05s\n",
      "epoch 102| loss: 0.54177 |  0:00:05s\n",
      "epoch 103| loss: 0.53744 |  0:00:05s\n",
      "epoch 104| loss: 0.54963 |  0:00:05s\n",
      "epoch 105| loss: 0.53287 |  0:00:05s\n",
      "epoch 106| loss: 0.5364  |  0:00:05s\n",
      "epoch 107| loss: 0.53211 |  0:00:05s\n",
      "epoch 108| loss: 0.53507 |  0:00:05s\n",
      "epoch 109| loss: 0.53199 |  0:00:05s\n",
      "epoch 110| loss: 0.53056 |  0:00:05s\n",
      "epoch 111| loss: 0.53298 |  0:00:05s\n",
      "epoch 112| loss: 0.53668 |  0:00:05s\n",
      "epoch 113| loss: 0.53749 |  0:00:05s\n",
      "epoch 114| loss: 0.52832 |  0:00:05s\n",
      "epoch 115| loss: 0.53271 |  0:00:05s\n",
      "epoch 116| loss: 0.52821 |  0:00:05s\n",
      "epoch 117| loss: 0.52053 |  0:00:05s\n",
      "epoch 118| loss: 0.51217 |  0:00:05s\n",
      "epoch 119| loss: 0.5196  |  0:00:05s\n",
      "epoch 120| loss: 0.52214 |  0:00:06s\n",
      "epoch 121| loss: 0.52796 |  0:00:06s\n",
      "epoch 122| loss: 0.54161 |  0:00:06s\n",
      "epoch 123| loss: 0.54367 |  0:00:06s\n",
      "epoch 124| loss: 0.54269 |  0:00:06s\n",
      "epoch 125| loss: 0.53724 |  0:00:06s\n",
      "epoch 126| loss: 0.53564 |  0:00:06s\n",
      "epoch 127| loss: 0.53966 |  0:00:06s\n",
      "epoch 128| loss: 0.53028 |  0:00:06s\n",
      "epoch 129| loss: 0.52477 |  0:00:06s\n",
      "epoch 130| loss: 0.51654 |  0:00:06s\n",
      "epoch 131| loss: 0.51701 |  0:00:06s\n",
      "epoch 132| loss: 0.52122 |  0:00:06s\n",
      "epoch 133| loss: 0.51196 |  0:00:06s\n",
      "epoch 134| loss: 0.51123 |  0:00:06s\n",
      "epoch 135| loss: 0.51024 |  0:00:06s\n",
      "epoch 136| loss: 0.51495 |  0:00:06s\n",
      "epoch 137| loss: 0.51413 |  0:00:06s\n",
      "epoch 138| loss: 0.50874 |  0:00:06s\n",
      "epoch 139| loss: 0.50653 |  0:00:07s\n",
      "epoch 140| loss: 0.50069 |  0:00:07s\n",
      "epoch 141| loss: 0.50643 |  0:00:07s\n",
      "epoch 142| loss: 0.49557 |  0:00:07s\n",
      "epoch 143| loss: 0.49421 |  0:00:07s\n",
      "epoch 144| loss: 0.51418 |  0:00:07s\n",
      "epoch 145| loss: 0.51641 |  0:00:07s\n",
      "epoch 146| loss: 0.52213 |  0:00:07s\n",
      "epoch 147| loss: 0.52596 |  0:00:07s\n",
      "epoch 148| loss: 0.51334 |  0:00:07s\n",
      "epoch 149| loss: 0.51516 |  0:00:07s\n",
      "epoch 150| loss: 0.50274 |  0:00:07s\n",
      "epoch 151| loss: 0.49741 |  0:00:07s\n",
      "epoch 152| loss: 0.50524 |  0:00:07s\n",
      "epoch 153| loss: 0.48941 |  0:00:07s\n",
      "epoch 154| loss: 0.48941 |  0:00:07s\n",
      "epoch 155| loss: 0.48828 |  0:00:07s\n",
      "epoch 156| loss: 0.48667 |  0:00:07s\n",
      "epoch 157| loss: 0.49155 |  0:00:07s\n",
      "epoch 158| loss: 0.48234 |  0:00:08s\n",
      "epoch 159| loss: 0.48782 |  0:00:08s\n",
      "epoch 160| loss: 0.47439 |  0:00:08s\n",
      "epoch 161| loss: 0.48025 |  0:00:08s\n",
      "epoch 162| loss: 0.48479 |  0:00:08s\n",
      "epoch 163| loss: 0.48518 |  0:00:08s\n",
      "epoch 164| loss: 0.49395 |  0:00:08s\n",
      "epoch 165| loss: 0.4825  |  0:00:08s\n",
      "epoch 166| loss: 0.49269 |  0:00:08s\n",
      "epoch 167| loss: 0.48316 |  0:00:08s\n",
      "epoch 168| loss: 0.48384 |  0:00:08s\n",
      "epoch 169| loss: 0.47382 |  0:00:08s\n",
      "epoch 170| loss: 0.46801 |  0:00:08s\n",
      "epoch 171| loss: 0.46061 |  0:00:08s\n",
      "epoch 172| loss: 0.46937 |  0:00:08s\n",
      "epoch 173| loss: 0.46733 |  0:00:08s\n",
      "epoch 174| loss: 0.46842 |  0:00:08s\n",
      "epoch 175| loss: 0.45681 |  0:00:08s\n",
      "epoch 176| loss: 0.46212 |  0:00:08s\n",
      "epoch 177| loss: 0.45502 |  0:00:08s\n",
      "epoch 178| loss: 0.45563 |  0:00:09s\n",
      "epoch 179| loss: 0.44916 |  0:00:09s\n",
      "epoch 180| loss: 0.457   |  0:00:09s\n",
      "epoch 181| loss: 0.45152 |  0:00:09s\n",
      "epoch 182| loss: 0.4465  |  0:00:09s\n",
      "epoch 183| loss: 0.44334 |  0:00:09s\n",
      "epoch 184| loss: 0.4449  |  0:00:09s\n",
      "epoch 185| loss: 0.44807 |  0:00:09s\n",
      "epoch 186| loss: 0.4484  |  0:00:09s\n",
      "epoch 187| loss: 0.45934 |  0:00:09s\n",
      "epoch 188| loss: 0.45805 |  0:00:09s\n",
      "epoch 189| loss: 0.45397 |  0:00:09s\n",
      "epoch 190| loss: 0.44733 |  0:00:09s\n",
      "epoch 191| loss: 0.45431 |  0:00:09s\n",
      "epoch 192| loss: 0.46352 |  0:00:09s\n",
      "epoch 193| loss: 0.46674 |  0:00:09s\n",
      "epoch 194| loss: 0.46052 |  0:00:09s\n",
      "epoch 195| loss: 0.45506 |  0:00:09s\n",
      "epoch 196| loss: 0.45852 |  0:00:09s\n",
      "epoch 197| loss: 0.45467 |  0:00:10s\n",
      "epoch 198| loss: 0.4483  |  0:00:10s\n",
      "epoch 199| loss: 0.44655 |  0:00:10s\n",
      "epoch 200| loss: 0.44436 |  0:00:10s\n",
      "epoch 201| loss: 0.44279 |  0:00:10s\n",
      "epoch 202| loss: 0.44633 |  0:00:10s\n",
      "epoch 203| loss: 0.44059 |  0:00:10s\n",
      "epoch 204| loss: 0.43735 |  0:00:10s\n",
      "epoch 205| loss: 0.44729 |  0:00:10s\n",
      "epoch 206| loss: 0.45098 |  0:00:10s\n",
      "epoch 207| loss: 0.43973 |  0:00:10s\n",
      "epoch 208| loss: 0.43851 |  0:00:10s\n",
      "epoch 209| loss: 0.44138 |  0:00:10s\n",
      "epoch 210| loss: 0.44237 |  0:00:10s\n",
      "epoch 211| loss: 0.43381 |  0:00:10s\n",
      "epoch 212| loss: 0.43359 |  0:00:10s\n",
      "epoch 213| loss: 0.4378  |  0:00:10s\n",
      "epoch 214| loss: 0.43801 |  0:00:10s\n",
      "epoch 215| loss: 0.43989 |  0:00:10s\n",
      "epoch 216| loss: 0.43279 |  0:00:11s\n",
      "epoch 217| loss: 0.4325  |  0:00:11s\n",
      "epoch 218| loss: 0.43005 |  0:00:11s\n",
      "epoch 219| loss: 0.42571 |  0:00:11s\n",
      "epoch 220| loss: 0.42945 |  0:00:11s\n",
      "epoch 221| loss: 0.42231 |  0:00:11s\n",
      "epoch 222| loss: 0.41747 |  0:00:11s\n",
      "epoch 223| loss: 0.4222  |  0:00:11s\n",
      "epoch 224| loss: 0.42369 |  0:00:11s\n",
      "epoch 225| loss: 0.42858 |  0:00:11s\n",
      "epoch 226| loss: 0.43059 |  0:00:11s\n",
      "epoch 227| loss: 0.43018 |  0:00:11s\n",
      "epoch 228| loss: 0.43262 |  0:00:11s\n",
      "epoch 229| loss: 0.42365 |  0:00:11s\n",
      "epoch 230| loss: 0.42675 |  0:00:11s\n",
      "epoch 231| loss: 0.4234  |  0:00:11s\n",
      "epoch 232| loss: 0.41725 |  0:00:11s\n",
      "epoch 233| loss: 0.42258 |  0:00:11s\n",
      "epoch 234| loss: 0.41042 |  0:00:11s\n",
      "epoch 235| loss: 0.41656 |  0:00:12s\n",
      "epoch 236| loss: 0.41135 |  0:00:12s\n",
      "epoch 237| loss: 0.40743 |  0:00:12s\n",
      "epoch 238| loss: 0.41173 |  0:00:12s\n",
      "epoch 239| loss: 0.41737 |  0:00:12s\n",
      "epoch 240| loss: 0.41297 |  0:00:12s\n",
      "epoch 241| loss: 0.40717 |  0:00:12s\n",
      "epoch 242| loss: 0.42054 |  0:00:12s\n",
      "epoch 243| loss: 0.43107 |  0:00:12s\n",
      "epoch 244| loss: 0.43315 |  0:00:12s\n",
      "epoch 245| loss: 0.42597 |  0:00:12s\n",
      "epoch 246| loss: 0.42012 |  0:00:12s\n",
      "epoch 247| loss: 0.41827 |  0:00:12s\n",
      "epoch 248| loss: 0.42028 |  0:00:12s\n",
      "epoch 249| loss: 0.40845 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.29683 |  0:00:00s\n",
      "epoch 1  | loss: 2.21727 |  0:00:00s\n",
      "epoch 2  | loss: 1.49121 |  0:00:00s\n",
      "epoch 3  | loss: 1.2367  |  0:00:00s\n",
      "epoch 4  | loss: 1.17934 |  0:00:00s\n",
      "epoch 5  | loss: 1.02842 |  0:00:00s\n",
      "epoch 6  | loss: 0.94218 |  0:00:00s\n",
      "epoch 7  | loss: 0.88467 |  0:00:00s\n",
      "epoch 8  | loss: 0.84898 |  0:00:00s\n",
      "epoch 9  | loss: 0.81409 |  0:00:00s\n",
      "epoch 10 | loss: 0.78359 |  0:00:00s\n",
      "epoch 11 | loss: 0.75824 |  0:00:00s\n",
      "epoch 12 | loss: 0.73657 |  0:00:00s\n",
      "epoch 13 | loss: 0.73128 |  0:00:00s\n",
      "epoch 14 | loss: 0.70811 |  0:00:00s\n",
      "epoch 15 | loss: 0.70112 |  0:00:00s\n",
      "epoch 16 | loss: 0.68672 |  0:00:00s\n",
      "epoch 17 | loss: 0.67042 |  0:00:00s\n",
      "epoch 18 | loss: 0.66112 |  0:00:00s\n",
      "epoch 19 | loss: 0.6596  |  0:00:01s\n",
      "epoch 20 | loss: 0.65813 |  0:00:01s\n",
      "epoch 21 | loss: 0.63441 |  0:00:01s\n",
      "epoch 22 | loss: 0.63688 |  0:00:01s\n",
      "epoch 23 | loss: 0.63725 |  0:00:01s\n",
      "epoch 24 | loss: 0.62737 |  0:00:01s\n",
      "epoch 25 | loss: 0.62547 |  0:00:01s\n",
      "epoch 26 | loss: 0.62323 |  0:00:01s\n",
      "epoch 27 | loss: 0.62546 |  0:00:01s\n",
      "epoch 28 | loss: 0.61795 |  0:00:01s\n",
      "epoch 29 | loss: 0.61716 |  0:00:01s\n",
      "epoch 30 | loss: 0.62142 |  0:00:01s\n",
      "epoch 31 | loss: 0.61708 |  0:00:01s\n",
      "epoch 32 | loss: 0.59931 |  0:00:01s\n",
      "epoch 33 | loss: 0.61275 |  0:00:01s\n",
      "epoch 34 | loss: 0.59834 |  0:00:01s\n",
      "epoch 35 | loss: 0.60762 |  0:00:01s\n",
      "epoch 36 | loss: 0.60008 |  0:00:02s\n",
      "epoch 37 | loss: 0.59453 |  0:00:02s\n",
      "epoch 38 | loss: 0.59371 |  0:00:02s\n",
      "epoch 39 | loss: 0.59041 |  0:00:02s\n",
      "epoch 40 | loss: 0.59104 |  0:00:02s\n",
      "epoch 41 | loss: 0.58499 |  0:00:02s\n",
      "epoch 42 | loss: 0.58381 |  0:00:02s\n",
      "epoch 43 | loss: 0.58256 |  0:00:02s\n",
      "epoch 44 | loss: 0.57326 |  0:00:02s\n",
      "epoch 45 | loss: 0.56496 |  0:00:02s\n",
      "epoch 46 | loss: 0.57158 |  0:00:02s\n",
      "epoch 47 | loss: 0.55144 |  0:00:02s\n",
      "epoch 48 | loss: 0.55396 |  0:00:02s\n",
      "epoch 49 | loss: 0.54413 |  0:00:02s\n",
      "epoch 50 | loss: 0.53733 |  0:00:02s\n",
      "epoch 51 | loss: 0.54705 |  0:00:02s\n",
      "epoch 52 | loss: 0.53766 |  0:00:02s\n",
      "epoch 53 | loss: 0.53823 |  0:00:02s\n",
      "epoch 54 | loss: 0.53651 |  0:00:03s\n",
      "epoch 55 | loss: 0.53494 |  0:00:03s\n",
      "epoch 56 | loss: 0.53383 |  0:00:03s\n",
      "epoch 57 | loss: 0.52713 |  0:00:03s\n",
      "epoch 58 | loss: 0.5258  |  0:00:03s\n",
      "epoch 59 | loss: 0.52903 |  0:00:03s\n",
      "epoch 60 | loss: 0.52446 |  0:00:03s\n",
      "epoch 61 | loss: 0.5159  |  0:00:03s\n",
      "epoch 62 | loss: 0.51248 |  0:00:03s\n",
      "epoch 63 | loss: 0.5092  |  0:00:03s\n",
      "epoch 64 | loss: 0.50705 |  0:00:03s\n",
      "epoch 65 | loss: 0.50883 |  0:00:03s\n",
      "epoch 66 | loss: 0.50359 |  0:00:03s\n",
      "epoch 67 | loss: 0.49379 |  0:00:03s\n",
      "epoch 68 | loss: 0.48759 |  0:00:03s\n",
      "epoch 69 | loss: 0.49419 |  0:00:03s\n",
      "epoch 70 | loss: 0.49018 |  0:00:03s\n",
      "epoch 71 | loss: 0.49017 |  0:00:03s\n",
      "epoch 72 | loss: 0.49532 |  0:00:03s\n",
      "epoch 73 | loss: 0.49836 |  0:00:04s\n",
      "epoch 74 | loss: 0.50175 |  0:00:04s\n",
      "epoch 75 | loss: 0.48882 |  0:00:04s\n",
      "epoch 76 | loss: 0.49759 |  0:00:04s\n",
      "epoch 77 | loss: 0.48293 |  0:00:04s\n",
      "epoch 78 | loss: 0.50873 |  0:00:04s\n",
      "epoch 79 | loss: 0.50248 |  0:00:04s\n",
      "epoch 80 | loss: 0.49791 |  0:00:04s\n",
      "epoch 81 | loss: 0.49908 |  0:00:04s\n",
      "epoch 82 | loss: 0.4995  |  0:00:04s\n",
      "epoch 83 | loss: 0.49803 |  0:00:04s\n",
      "epoch 84 | loss: 0.509   |  0:00:04s\n",
      "epoch 85 | loss: 0.4927  |  0:00:04s\n",
      "epoch 86 | loss: 0.49654 |  0:00:04s\n",
      "epoch 87 | loss: 0.48924 |  0:00:04s\n",
      "epoch 88 | loss: 0.49156 |  0:00:04s\n",
      "epoch 89 | loss: 0.49041 |  0:00:04s\n",
      "epoch 90 | loss: 0.49623 |  0:00:04s\n",
      "epoch 91 | loss: 0.48553 |  0:00:04s\n",
      "epoch 92 | loss: 0.47776 |  0:00:05s\n",
      "epoch 93 | loss: 0.47856 |  0:00:05s\n",
      "epoch 94 | loss: 0.47667 |  0:00:05s\n",
      "epoch 95 | loss: 0.46531 |  0:00:05s\n",
      "epoch 96 | loss: 0.46992 |  0:00:05s\n",
      "epoch 97 | loss: 0.46345 |  0:00:05s\n",
      "epoch 98 | loss: 0.46819 |  0:00:05s\n",
      "epoch 99 | loss: 0.46519 |  0:00:05s\n",
      "epoch 100| loss: 0.45987 |  0:00:05s\n",
      "epoch 101| loss: 0.46076 |  0:00:05s\n",
      "epoch 102| loss: 0.45581 |  0:00:05s\n",
      "epoch 103| loss: 0.44603 |  0:00:05s\n",
      "epoch 104| loss: 0.44609 |  0:00:05s\n",
      "epoch 105| loss: 0.44899 |  0:00:05s\n",
      "epoch 106| loss: 0.44252 |  0:00:05s\n",
      "epoch 107| loss: 0.44044 |  0:00:05s\n",
      "epoch 108| loss: 0.44175 |  0:00:05s\n",
      "epoch 109| loss: 0.44004 |  0:00:05s\n",
      "epoch 110| loss: 0.4412  |  0:00:05s\n",
      "epoch 111| loss: 0.44739 |  0:00:05s\n",
      "epoch 112| loss: 0.43976 |  0:00:05s\n",
      "epoch 113| loss: 0.43709 |  0:00:06s\n",
      "epoch 114| loss: 0.43523 |  0:00:06s\n",
      "epoch 115| loss: 0.4305  |  0:00:06s\n",
      "epoch 116| loss: 0.42994 |  0:00:06s\n",
      "epoch 117| loss: 0.43013 |  0:00:06s\n",
      "epoch 118| loss: 0.41947 |  0:00:06s\n",
      "epoch 119| loss: 0.4194  |  0:00:06s\n",
      "epoch 120| loss: 0.41478 |  0:00:06s\n",
      "epoch 121| loss: 0.4183  |  0:00:06s\n",
      "epoch 122| loss: 0.42278 |  0:00:06s\n",
      "epoch 123| loss: 0.41872 |  0:00:06s\n",
      "epoch 124| loss: 0.41604 |  0:00:06s\n",
      "epoch 125| loss: 0.40786 |  0:00:06s\n",
      "epoch 126| loss: 0.40542 |  0:00:06s\n",
      "epoch 127| loss: 0.40289 |  0:00:06s\n",
      "epoch 128| loss: 0.40451 |  0:00:06s\n",
      "epoch 129| loss: 0.40234 |  0:00:06s\n",
      "epoch 130| loss: 0.40371 |  0:00:06s\n",
      "epoch 131| loss: 0.40081 |  0:00:06s\n",
      "epoch 132| loss: 0.39888 |  0:00:06s\n",
      "epoch 133| loss: 0.39865 |  0:00:06s\n",
      "epoch 134| loss: 0.39413 |  0:00:06s\n",
      "epoch 135| loss: 0.3909  |  0:00:07s\n",
      "epoch 136| loss: 0.38517 |  0:00:07s\n",
      "epoch 137| loss: 0.38533 |  0:00:07s\n",
      "epoch 138| loss: 0.3828  |  0:00:07s\n",
      "epoch 139| loss: 0.38608 |  0:00:07s\n",
      "epoch 140| loss: 0.38215 |  0:00:07s\n",
      "epoch 141| loss: 0.37963 |  0:00:07s\n",
      "epoch 142| loss: 0.38699 |  0:00:07s\n",
      "epoch 143| loss: 0.38861 |  0:00:07s\n",
      "epoch 144| loss: 0.38335 |  0:00:07s\n",
      "epoch 145| loss: 0.37875 |  0:00:07s\n",
      "epoch 146| loss: 0.37672 |  0:00:07s\n",
      "epoch 147| loss: 0.37961 |  0:00:07s\n",
      "epoch 148| loss: 0.36947 |  0:00:07s\n",
      "epoch 149| loss: 0.36485 |  0:00:07s\n",
      "epoch 150| loss: 0.38348 |  0:00:07s\n",
      "epoch 151| loss: 0.3856  |  0:00:07s\n",
      "epoch 152| loss: 0.38954 |  0:00:07s\n",
      "epoch 153| loss: 0.38722 |  0:00:07s\n",
      "epoch 154| loss: 0.3802  |  0:00:07s\n",
      "epoch 155| loss: 0.38032 |  0:00:07s\n",
      "epoch 156| loss: 0.38673 |  0:00:07s\n",
      "epoch 157| loss: 0.38657 |  0:00:07s\n",
      "epoch 158| loss: 0.38812 |  0:00:08s\n",
      "epoch 159| loss: 0.38999 |  0:00:08s\n",
      "epoch 160| loss: 0.3901  |  0:00:08s\n",
      "epoch 161| loss: 0.38249 |  0:00:08s\n",
      "epoch 162| loss: 0.3856  |  0:00:08s\n",
      "epoch 163| loss: 0.38597 |  0:00:08s\n",
      "epoch 164| loss: 0.37862 |  0:00:08s\n",
      "epoch 165| loss: 0.38426 |  0:00:08s\n",
      "epoch 166| loss: 0.38258 |  0:00:08s\n",
      "epoch 167| loss: 0.37182 |  0:00:08s\n",
      "epoch 168| loss: 0.37018 |  0:00:08s\n",
      "epoch 169| loss: 0.36122 |  0:00:08s\n",
      "epoch 170| loss: 0.36594 |  0:00:08s\n",
      "epoch 171| loss: 0.38115 |  0:00:08s\n",
      "epoch 172| loss: 0.36879 |  0:00:08s\n",
      "epoch 173| loss: 0.36666 |  0:00:08s\n",
      "epoch 174| loss: 0.36851 |  0:00:08s\n",
      "epoch 175| loss: 0.36849 |  0:00:08s\n",
      "epoch 176| loss: 0.37047 |  0:00:08s\n",
      "epoch 177| loss: 0.3576  |  0:00:08s\n",
      "epoch 178| loss: 0.3496  |  0:00:08s\n",
      "epoch 179| loss: 0.35544 |  0:00:08s\n",
      "epoch 180| loss: 0.36177 |  0:00:08s\n",
      "epoch 181| loss: 0.36013 |  0:00:09s\n",
      "epoch 182| loss: 0.36248 |  0:00:09s\n",
      "epoch 183| loss: 0.35462 |  0:00:09s\n",
      "epoch 184| loss: 0.37186 |  0:00:09s\n",
      "epoch 185| loss: 0.34674 |  0:00:09s\n",
      "epoch 186| loss: 0.35409 |  0:00:09s\n",
      "epoch 187| loss: 0.34825 |  0:00:09s\n",
      "epoch 188| loss: 0.34607 |  0:00:09s\n",
      "epoch 189| loss: 0.34089 |  0:00:09s\n",
      "epoch 190| loss: 0.34318 |  0:00:09s\n",
      "epoch 191| loss: 0.33902 |  0:00:09s\n",
      "epoch 192| loss: 0.33205 |  0:00:09s\n",
      "epoch 193| loss: 0.3292  |  0:00:09s\n",
      "epoch 194| loss: 0.32841 |  0:00:09s\n",
      "epoch 195| loss: 0.32921 |  0:00:09s\n",
      "epoch 196| loss: 0.33702 |  0:00:09s\n",
      "epoch 197| loss: 0.34183 |  0:00:09s\n",
      "epoch 198| loss: 0.34598 |  0:00:09s\n",
      "epoch 199| loss: 0.33981 |  0:00:09s\n",
      "epoch 200| loss: 0.33471 |  0:00:09s\n",
      "epoch 201| loss: 0.3218  |  0:00:09s\n",
      "epoch 202| loss: 0.32367 |  0:00:09s\n",
      "epoch 203| loss: 0.32321 |  0:00:10s\n",
      "epoch 204| loss: 0.31935 |  0:00:10s\n",
      "epoch 205| loss: 0.32285 |  0:00:10s\n",
      "epoch 206| loss: 0.321   |  0:00:10s\n",
      "epoch 207| loss: 0.3107  |  0:00:10s\n",
      "epoch 208| loss: 0.30566 |  0:00:10s\n",
      "epoch 209| loss: 0.3077  |  0:00:10s\n",
      "epoch 210| loss: 0.30458 |  0:00:10s\n",
      "epoch 211| loss: 0.30835 |  0:00:10s\n",
      "epoch 212| loss: 0.30393 |  0:00:10s\n",
      "epoch 213| loss: 0.31644 |  0:00:10s\n",
      "epoch 214| loss: 0.30894 |  0:00:10s\n",
      "epoch 215| loss: 0.30127 |  0:00:10s\n",
      "epoch 216| loss: 0.30417 |  0:00:10s\n",
      "epoch 217| loss: 0.30029 |  0:00:10s\n",
      "epoch 218| loss: 0.29659 |  0:00:10s\n",
      "epoch 219| loss: 0.29736 |  0:00:10s\n",
      "epoch 220| loss: 0.29419 |  0:00:10s\n",
      "epoch 221| loss: 0.29043 |  0:00:11s\n",
      "epoch 222| loss: 0.2902  |  0:00:11s\n",
      "epoch 223| loss: 0.29148 |  0:00:11s\n",
      "epoch 224| loss: 0.29317 |  0:00:11s\n",
      "epoch 225| loss: 0.28875 |  0:00:11s\n",
      "epoch 226| loss: 0.2894  |  0:00:11s\n",
      "epoch 227| loss: 0.28723 |  0:00:11s\n",
      "epoch 228| loss: 0.2881  |  0:00:11s\n",
      "epoch 229| loss: 0.2886  |  0:00:11s\n",
      "epoch 230| loss: 0.28606 |  0:00:11s\n",
      "epoch 231| loss: 0.28486 |  0:00:11s\n",
      "epoch 232| loss: 0.28206 |  0:00:11s\n",
      "epoch 233| loss: 0.28406 |  0:00:11s\n",
      "epoch 234| loss: 0.27997 |  0:00:11s\n",
      "epoch 235| loss: 0.28216 |  0:00:11s\n",
      "epoch 236| loss: 0.28428 |  0:00:11s\n",
      "epoch 237| loss: 0.2767  |  0:00:11s\n",
      "epoch 238| loss: 0.27831 |  0:00:11s\n",
      "epoch 239| loss: 0.27987 |  0:00:11s\n",
      "epoch 240| loss: 0.28346 |  0:00:11s\n",
      "epoch 241| loss: 0.28126 |  0:00:11s\n",
      "epoch 242| loss: 0.27975 |  0:00:11s\n",
      "epoch 243| loss: 0.27703 |  0:00:12s\n",
      "epoch 244| loss: 0.27595 |  0:00:12s\n",
      "epoch 245| loss: 0.27366 |  0:00:12s\n",
      "epoch 246| loss: 0.27854 |  0:00:12s\n",
      "epoch 247| loss: 0.27485 |  0:00:12s\n",
      "epoch 248| loss: 0.28165 |  0:00:12s\n",
      "epoch 249| loss: 0.2733  |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.30907 |  0:00:00s\n",
      "epoch 1  | loss: 2.16057 |  0:00:00s\n",
      "epoch 2  | loss: 1.58308 |  0:00:00s\n",
      "epoch 3  | loss: 1.29525 |  0:00:00s\n",
      "epoch 4  | loss: 1.08903 |  0:00:00s\n",
      "epoch 5  | loss: 0.95421 |  0:00:00s\n",
      "epoch 6  | loss: 0.91011 |  0:00:00s\n",
      "epoch 7  | loss: 0.86277 |  0:00:00s\n",
      "epoch 8  | loss: 0.79716 |  0:00:00s\n",
      "epoch 9  | loss: 0.76777 |  0:00:00s\n",
      "epoch 10 | loss: 0.73808 |  0:00:00s\n",
      "epoch 11 | loss: 0.70718 |  0:00:00s\n",
      "epoch 12 | loss: 0.70446 |  0:00:00s\n",
      "epoch 13 | loss: 0.68987 |  0:00:00s\n",
      "epoch 14 | loss: 0.67269 |  0:00:00s\n",
      "epoch 15 | loss: 0.67146 |  0:00:00s\n",
      "epoch 16 | loss: 0.66417 |  0:00:00s\n",
      "epoch 17 | loss: 0.66444 |  0:00:00s\n",
      "epoch 18 | loss: 0.65106 |  0:00:00s\n",
      "epoch 19 | loss: 0.64332 |  0:00:00s\n",
      "epoch 20 | loss: 0.63488 |  0:00:00s\n",
      "epoch 21 | loss: 0.63665 |  0:00:01s\n",
      "epoch 22 | loss: 0.61706 |  0:00:01s\n",
      "epoch 23 | loss: 0.61577 |  0:00:01s\n",
      "epoch 24 | loss: 0.60476 |  0:00:01s\n",
      "epoch 25 | loss: 0.59835 |  0:00:01s\n",
      "epoch 26 | loss: 0.58767 |  0:00:01s\n",
      "epoch 27 | loss: 0.59052 |  0:00:01s\n",
      "epoch 28 | loss: 0.57454 |  0:00:01s\n",
      "epoch 29 | loss: 0.57173 |  0:00:01s\n",
      "epoch 30 | loss: 0.57708 |  0:00:01s\n",
      "epoch 31 | loss: 0.56835 |  0:00:01s\n",
      "epoch 32 | loss: 0.57144 |  0:00:01s\n",
      "epoch 33 | loss: 0.57973 |  0:00:01s\n",
      "epoch 34 | loss: 0.57411 |  0:00:01s\n",
      "epoch 35 | loss: 0.57971 |  0:00:01s\n",
      "epoch 36 | loss: 0.5648  |  0:00:01s\n",
      "epoch 37 | loss: 0.55789 |  0:00:01s\n",
      "epoch 38 | loss: 0.5563  |  0:00:01s\n",
      "epoch 39 | loss: 0.54273 |  0:00:01s\n",
      "epoch 40 | loss: 0.54551 |  0:00:01s\n",
      "epoch 41 | loss: 0.55467 |  0:00:01s\n",
      "epoch 42 | loss: 0.56192 |  0:00:01s\n",
      "epoch 43 | loss: 0.5575  |  0:00:02s\n",
      "epoch 44 | loss: 0.53944 |  0:00:02s\n",
      "epoch 45 | loss: 0.54619 |  0:00:02s\n",
      "epoch 46 | loss: 0.54861 |  0:00:02s\n",
      "epoch 47 | loss: 0.53694 |  0:00:02s\n",
      "epoch 48 | loss: 0.54244 |  0:00:02s\n",
      "epoch 49 | loss: 0.53441 |  0:00:02s\n",
      "epoch 50 | loss: 0.53025 |  0:00:02s\n",
      "epoch 51 | loss: 0.53698 |  0:00:02s\n",
      "epoch 52 | loss: 0.52143 |  0:00:02s\n",
      "epoch 53 | loss: 0.51974 |  0:00:02s\n",
      "epoch 54 | loss: 0.50231 |  0:00:02s\n",
      "epoch 55 | loss: 0.49774 |  0:00:02s\n",
      "epoch 56 | loss: 0.49625 |  0:00:02s\n",
      "epoch 57 | loss: 0.49259 |  0:00:02s\n",
      "epoch 58 | loss: 0.47757 |  0:00:02s\n",
      "epoch 59 | loss: 0.47618 |  0:00:02s\n",
      "epoch 60 | loss: 0.47523 |  0:00:02s\n",
      "epoch 61 | loss: 0.46929 |  0:00:02s\n",
      "epoch 62 | loss: 0.46874 |  0:00:02s\n",
      "epoch 63 | loss: 0.46449 |  0:00:02s\n",
      "epoch 64 | loss: 0.46893 |  0:00:03s\n",
      "epoch 65 | loss: 0.47262 |  0:00:03s\n",
      "epoch 66 | loss: 0.47424 |  0:00:03s\n",
      "epoch 67 | loss: 0.46461 |  0:00:03s\n",
      "epoch 68 | loss: 0.456   |  0:00:03s\n",
      "epoch 69 | loss: 0.44684 |  0:00:03s\n",
      "epoch 70 | loss: 0.45409 |  0:00:03s\n",
      "epoch 71 | loss: 0.44225 |  0:00:03s\n",
      "epoch 72 | loss: 0.45006 |  0:00:03s\n",
      "epoch 73 | loss: 0.45472 |  0:00:03s\n",
      "epoch 74 | loss: 0.43865 |  0:00:03s\n",
      "epoch 75 | loss: 0.43624 |  0:00:03s\n",
      "epoch 76 | loss: 0.4329  |  0:00:03s\n",
      "epoch 77 | loss: 0.42412 |  0:00:03s\n",
      "epoch 78 | loss: 0.42787 |  0:00:03s\n",
      "epoch 79 | loss: 0.42297 |  0:00:03s\n",
      "epoch 80 | loss: 0.42081 |  0:00:03s\n",
      "epoch 81 | loss: 0.42076 |  0:00:03s\n",
      "epoch 82 | loss: 0.41386 |  0:00:03s\n",
      "epoch 83 | loss: 0.40671 |  0:00:03s\n",
      "epoch 84 | loss: 0.40377 |  0:00:03s\n",
      "epoch 85 | loss: 0.40324 |  0:00:03s\n",
      "epoch 86 | loss: 0.41164 |  0:00:04s\n",
      "epoch 87 | loss: 0.41199 |  0:00:04s\n",
      "epoch 88 | loss: 0.41846 |  0:00:04s\n",
      "epoch 89 | loss: 0.42004 |  0:00:04s\n",
      "epoch 90 | loss: 0.40304 |  0:00:04s\n",
      "epoch 91 | loss: 0.39916 |  0:00:04s\n",
      "epoch 92 | loss: 0.40317 |  0:00:04s\n",
      "epoch 93 | loss: 0.40122 |  0:00:04s\n",
      "epoch 94 | loss: 0.39759 |  0:00:04s\n",
      "epoch 95 | loss: 0.39928 |  0:00:04s\n",
      "epoch 96 | loss: 0.39618 |  0:00:04s\n",
      "epoch 97 | loss: 0.39523 |  0:00:04s\n",
      "epoch 98 | loss: 0.39416 |  0:00:04s\n",
      "epoch 99 | loss: 0.39367 |  0:00:04s\n",
      "epoch 100| loss: 0.39129 |  0:00:04s\n",
      "epoch 101| loss: 0.39339 |  0:00:04s\n",
      "epoch 102| loss: 0.39446 |  0:00:04s\n",
      "epoch 103| loss: 0.38619 |  0:00:04s\n",
      "epoch 104| loss: 0.37518 |  0:00:04s\n",
      "epoch 105| loss: 0.37442 |  0:00:04s\n",
      "epoch 106| loss: 0.37468 |  0:00:04s\n",
      "epoch 107| loss: 0.37407 |  0:00:04s\n",
      "epoch 108| loss: 0.37276 |  0:00:05s\n",
      "epoch 109| loss: 0.37147 |  0:00:05s\n",
      "epoch 110| loss: 0.36659 |  0:00:05s\n",
      "epoch 111| loss: 0.37965 |  0:00:05s\n",
      "epoch 112| loss: 0.38266 |  0:00:05s\n",
      "epoch 113| loss: 0.38153 |  0:00:05s\n",
      "epoch 114| loss: 0.36963 |  0:00:05s\n",
      "epoch 115| loss: 0.37807 |  0:00:05s\n",
      "epoch 116| loss: 0.36956 |  0:00:05s\n",
      "epoch 117| loss: 0.36269 |  0:00:05s\n",
      "epoch 118| loss: 0.36525 |  0:00:05s\n",
      "epoch 119| loss: 0.36404 |  0:00:05s\n",
      "epoch 120| loss: 0.36195 |  0:00:05s\n",
      "epoch 121| loss: 0.36181 |  0:00:05s\n",
      "epoch 122| loss: 0.3603  |  0:00:05s\n",
      "epoch 123| loss: 0.3599  |  0:00:05s\n",
      "epoch 124| loss: 0.3632  |  0:00:05s\n",
      "epoch 125| loss: 0.35045 |  0:00:05s\n",
      "epoch 126| loss: 0.37319 |  0:00:06s\n",
      "epoch 127| loss: 0.35892 |  0:00:06s\n",
      "epoch 128| loss: 0.35856 |  0:00:06s\n",
      "epoch 129| loss: 0.35228 |  0:00:06s\n",
      "epoch 130| loss: 0.34492 |  0:00:06s\n",
      "epoch 131| loss: 0.33818 |  0:00:06s\n",
      "epoch 132| loss: 0.34571 |  0:00:06s\n",
      "epoch 133| loss: 0.34511 |  0:00:06s\n",
      "epoch 134| loss: 0.3431  |  0:00:06s\n",
      "epoch 135| loss: 0.33584 |  0:00:06s\n",
      "epoch 136| loss: 0.33164 |  0:00:06s\n",
      "epoch 137| loss: 0.32396 |  0:00:06s\n",
      "epoch 138| loss: 0.32275 |  0:00:06s\n",
      "epoch 139| loss: 0.33135 |  0:00:06s\n",
      "epoch 140| loss: 0.32974 |  0:00:06s\n",
      "epoch 141| loss: 0.32661 |  0:00:06s\n",
      "epoch 142| loss: 0.32601 |  0:00:06s\n",
      "epoch 143| loss: 0.32725 |  0:00:06s\n",
      "epoch 144| loss: 0.32273 |  0:00:06s\n",
      "epoch 145| loss: 0.32339 |  0:00:06s\n",
      "epoch 146| loss: 0.32527 |  0:00:06s\n",
      "epoch 147| loss: 0.32143 |  0:00:06s\n",
      "epoch 148| loss: 0.31675 |  0:00:07s\n",
      "epoch 149| loss: 0.31491 |  0:00:07s\n",
      "epoch 150| loss: 0.31498 |  0:00:07s\n",
      "epoch 151| loss: 0.31735 |  0:00:07s\n",
      "epoch 152| loss: 0.31567 |  0:00:07s\n",
      "epoch 153| loss: 0.3076  |  0:00:07s\n",
      "epoch 154| loss: 0.30278 |  0:00:07s\n",
      "epoch 155| loss: 0.30975 |  0:00:07s\n",
      "epoch 156| loss: 0.30265 |  0:00:07s\n",
      "epoch 157| loss: 0.30253 |  0:00:07s\n",
      "epoch 158| loss: 0.29607 |  0:00:07s\n",
      "epoch 159| loss: 0.29524 |  0:00:07s\n",
      "epoch 160| loss: 0.30425 |  0:00:07s\n",
      "epoch 161| loss: 0.29638 |  0:00:07s\n",
      "epoch 162| loss: 0.2885  |  0:00:07s\n",
      "epoch 163| loss: 0.28482 |  0:00:07s\n",
      "epoch 164| loss: 0.28961 |  0:00:07s\n",
      "epoch 165| loss: 0.28664 |  0:00:07s\n",
      "epoch 166| loss: 0.28408 |  0:00:07s\n",
      "epoch 167| loss: 0.28303 |  0:00:07s\n",
      "epoch 168| loss: 0.28461 |  0:00:07s\n",
      "epoch 169| loss: 0.28161 |  0:00:07s\n",
      "epoch 170| loss: 0.28916 |  0:00:08s\n",
      "epoch 171| loss: 0.28483 |  0:00:08s\n",
      "epoch 172| loss: 0.27825 |  0:00:08s\n",
      "epoch 173| loss: 0.28072 |  0:00:08s\n",
      "epoch 174| loss: 0.27968 |  0:00:08s\n",
      "epoch 175| loss: 0.27706 |  0:00:08s\n",
      "epoch 176| loss: 0.28358 |  0:00:08s\n",
      "epoch 177| loss: 0.28264 |  0:00:08s\n",
      "epoch 178| loss: 0.27402 |  0:00:08s\n",
      "epoch 179| loss: 0.27698 |  0:00:08s\n",
      "epoch 180| loss: 0.27845 |  0:00:08s\n",
      "epoch 181| loss: 0.27785 |  0:00:08s\n",
      "epoch 182| loss: 0.26929 |  0:00:08s\n",
      "epoch 183| loss: 0.26585 |  0:00:08s\n",
      "epoch 184| loss: 0.26656 |  0:00:08s\n",
      "epoch 185| loss: 0.26607 |  0:00:08s\n",
      "epoch 186| loss: 0.2695  |  0:00:08s\n",
      "epoch 187| loss: 0.26834 |  0:00:08s\n",
      "epoch 188| loss: 0.26362 |  0:00:08s\n",
      "epoch 189| loss: 0.26759 |  0:00:08s\n",
      "epoch 190| loss: 0.27416 |  0:00:08s\n",
      "epoch 191| loss: 0.26696 |  0:00:09s\n",
      "epoch 192| loss: 0.28011 |  0:00:09s\n",
      "epoch 193| loss: 0.28177 |  0:00:09s\n",
      "epoch 194| loss: 0.2782  |  0:00:09s\n",
      "epoch 195| loss: 0.2748  |  0:00:09s\n",
      "epoch 196| loss: 0.28114 |  0:00:09s\n",
      "epoch 197| loss: 0.27277 |  0:00:09s\n",
      "epoch 198| loss: 0.27851 |  0:00:09s\n",
      "epoch 199| loss: 0.28457 |  0:00:09s\n",
      "epoch 200| loss: 0.2972  |  0:00:09s\n",
      "epoch 201| loss: 0.29474 |  0:00:09s\n",
      "epoch 202| loss: 0.29531 |  0:00:09s\n",
      "epoch 203| loss: 0.29579 |  0:00:09s\n",
      "epoch 204| loss: 0.30895 |  0:00:09s\n",
      "epoch 205| loss: 0.33564 |  0:00:09s\n",
      "epoch 206| loss: 0.34134 |  0:00:09s\n",
      "epoch 207| loss: 0.35779 |  0:00:09s\n",
      "epoch 208| loss: 0.35733 |  0:00:09s\n",
      "epoch 209| loss: 0.34694 |  0:00:09s\n",
      "epoch 210| loss: 0.33421 |  0:00:09s\n",
      "epoch 211| loss: 0.32475 |  0:00:09s\n",
      "epoch 212| loss: 0.31805 |  0:00:10s\n",
      "epoch 213| loss: 0.32171 |  0:00:10s\n",
      "epoch 214| loss: 0.31247 |  0:00:10s\n",
      "epoch 215| loss: 0.30651 |  0:00:10s\n",
      "epoch 216| loss: 0.3059  |  0:00:10s\n",
      "epoch 217| loss: 0.30192 |  0:00:10s\n",
      "epoch 218| loss: 0.29252 |  0:00:10s\n",
      "epoch 219| loss: 0.2901  |  0:00:10s\n",
      "epoch 220| loss: 0.29171 |  0:00:10s\n",
      "epoch 221| loss: 0.28585 |  0:00:10s\n",
      "epoch 222| loss: 0.28834 |  0:00:10s\n",
      "epoch 223| loss: 0.28785 |  0:00:10s\n",
      "epoch 224| loss: 0.27664 |  0:00:10s\n",
      "epoch 225| loss: 0.27876 |  0:00:10s\n",
      "epoch 226| loss: 0.27817 |  0:00:10s\n",
      "epoch 227| loss: 0.28306 |  0:00:10s\n",
      "epoch 228| loss: 0.27468 |  0:00:10s\n",
      "epoch 229| loss: 0.26437 |  0:00:10s\n",
      "epoch 230| loss: 0.27087 |  0:00:10s\n",
      "epoch 231| loss: 0.2795  |  0:00:10s\n",
      "epoch 232| loss: 0.28044 |  0:00:11s\n",
      "epoch 233| loss: 0.2813  |  0:00:11s\n",
      "epoch 234| loss: 0.27462 |  0:00:11s\n",
      "epoch 235| loss: 0.27719 |  0:00:11s\n",
      "epoch 236| loss: 0.26957 |  0:00:11s\n",
      "epoch 237| loss: 0.26334 |  0:00:11s\n",
      "epoch 238| loss: 0.25812 |  0:00:11s\n",
      "epoch 239| loss: 0.26172 |  0:00:11s\n",
      "epoch 240| loss: 0.26601 |  0:00:11s\n",
      "epoch 241| loss: 0.2656  |  0:00:11s\n",
      "epoch 242| loss: 0.26314 |  0:00:11s\n",
      "epoch 243| loss: 0.2759  |  0:00:11s\n",
      "epoch 244| loss: 0.2836  |  0:00:11s\n",
      "epoch 245| loss: 0.28473 |  0:00:11s\n",
      "epoch 246| loss: 0.29018 |  0:00:11s\n",
      "epoch 247| loss: 0.27938 |  0:00:11s\n",
      "epoch 248| loss: 0.27395 |  0:00:11s\n",
      "epoch 249| loss: 0.27184 |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.29571 |  0:00:00s\n",
      "epoch 1  | loss: 2.34879 |  0:00:00s\n",
      "epoch 2  | loss: 1.67563 |  0:00:00s\n",
      "epoch 3  | loss: 1.37287 |  0:00:00s\n",
      "epoch 4  | loss: 1.13382 |  0:00:00s\n",
      "epoch 5  | loss: 1.03009 |  0:00:00s\n",
      "epoch 6  | loss: 0.92461 |  0:00:00s\n",
      "epoch 7  | loss: 0.89946 |  0:00:00s\n",
      "epoch 8  | loss: 0.85066 |  0:00:00s\n",
      "epoch 9  | loss: 0.80771 |  0:00:00s\n",
      "epoch 10 | loss: 0.76185 |  0:00:00s\n",
      "epoch 11 | loss: 0.75469 |  0:00:00s\n",
      "epoch 12 | loss: 0.76448 |  0:00:00s\n",
      "epoch 13 | loss: 0.72172 |  0:00:00s\n",
      "epoch 14 | loss: 0.69938 |  0:00:00s\n",
      "epoch 15 | loss: 0.68564 |  0:00:00s\n",
      "epoch 16 | loss: 0.67581 |  0:00:00s\n",
      "epoch 17 | loss: 0.67386 |  0:00:00s\n",
      "epoch 18 | loss: 0.66219 |  0:00:00s\n",
      "epoch 19 | loss: 0.65933 |  0:00:00s\n",
      "epoch 20 | loss: 0.65008 |  0:00:00s\n",
      "epoch 21 | loss: 0.64203 |  0:00:01s\n",
      "epoch 22 | loss: 0.63269 |  0:00:01s\n",
      "epoch 23 | loss: 0.62385 |  0:00:01s\n",
      "epoch 24 | loss: 0.61494 |  0:00:01s\n",
      "epoch 25 | loss: 0.61212 |  0:00:01s\n",
      "epoch 26 | loss: 0.59995 |  0:00:01s\n",
      "epoch 27 | loss: 0.58475 |  0:00:01s\n",
      "epoch 28 | loss: 0.58959 |  0:00:01s\n",
      "epoch 29 | loss: 0.57863 |  0:00:01s\n",
      "epoch 30 | loss: 0.58633 |  0:00:01s\n",
      "epoch 31 | loss: 0.58338 |  0:00:01s\n",
      "epoch 32 | loss: 0.57125 |  0:00:01s\n",
      "epoch 33 | loss: 0.5721  |  0:00:01s\n",
      "epoch 34 | loss: 0.56963 |  0:00:01s\n",
      "epoch 35 | loss: 0.56272 |  0:00:01s\n",
      "epoch 36 | loss: 0.55134 |  0:00:01s\n",
      "epoch 37 | loss: 0.5543  |  0:00:01s\n",
      "epoch 38 | loss: 0.54818 |  0:00:01s\n",
      "epoch 39 | loss: 0.53131 |  0:00:01s\n",
      "epoch 40 | loss: 0.54088 |  0:00:01s\n",
      "epoch 41 | loss: 0.52882 |  0:00:01s\n",
      "epoch 42 | loss: 0.52763 |  0:00:01s\n",
      "epoch 43 | loss: 0.52705 |  0:00:01s\n",
      "epoch 44 | loss: 0.51593 |  0:00:02s\n",
      "epoch 45 | loss: 0.52387 |  0:00:02s\n",
      "epoch 46 | loss: 0.52505 |  0:00:02s\n",
      "epoch 47 | loss: 0.53244 |  0:00:02s\n",
      "epoch 48 | loss: 0.53425 |  0:00:02s\n",
      "epoch 49 | loss: 0.52514 |  0:00:02s\n",
      "epoch 50 | loss: 0.51263 |  0:00:02s\n",
      "epoch 51 | loss: 0.51091 |  0:00:02s\n",
      "epoch 52 | loss: 0.50195 |  0:00:02s\n",
      "epoch 53 | loss: 0.49868 |  0:00:02s\n",
      "epoch 54 | loss: 0.49194 |  0:00:02s\n",
      "epoch 55 | loss: 0.48406 |  0:00:02s\n",
      "epoch 56 | loss: 0.48527 |  0:00:02s\n",
      "epoch 57 | loss: 0.4808  |  0:00:02s\n",
      "epoch 58 | loss: 0.46973 |  0:00:02s\n",
      "epoch 59 | loss: 0.46819 |  0:00:02s\n",
      "epoch 60 | loss: 0.46754 |  0:00:02s\n",
      "epoch 61 | loss: 0.46364 |  0:00:02s\n",
      "epoch 62 | loss: 0.45756 |  0:00:03s\n",
      "epoch 63 | loss: 0.45563 |  0:00:03s\n",
      "epoch 64 | loss: 0.44705 |  0:00:03s\n",
      "epoch 65 | loss: 0.44842 |  0:00:03s\n",
      "epoch 66 | loss: 0.44842 |  0:00:03s\n",
      "epoch 67 | loss: 0.44539 |  0:00:03s\n",
      "epoch 68 | loss: 0.43499 |  0:00:03s\n",
      "epoch 69 | loss: 0.43308 |  0:00:03s\n",
      "epoch 70 | loss: 0.444   |  0:00:03s\n",
      "epoch 71 | loss: 0.43566 |  0:00:03s\n",
      "epoch 72 | loss: 0.43224 |  0:00:03s\n",
      "epoch 73 | loss: 0.43308 |  0:00:03s\n",
      "epoch 74 | loss: 0.42596 |  0:00:03s\n",
      "epoch 75 | loss: 0.4329  |  0:00:03s\n",
      "epoch 76 | loss: 0.42802 |  0:00:03s\n",
      "epoch 77 | loss: 0.42039 |  0:00:03s\n",
      "epoch 78 | loss: 0.42291 |  0:00:03s\n",
      "epoch 79 | loss: 0.42843 |  0:00:03s\n",
      "epoch 80 | loss: 0.42212 |  0:00:03s\n",
      "epoch 81 | loss: 0.4202  |  0:00:03s\n",
      "epoch 82 | loss: 0.41594 |  0:00:03s\n",
      "epoch 83 | loss: 0.41349 |  0:00:03s\n",
      "epoch 84 | loss: 0.41234 |  0:00:04s\n",
      "epoch 85 | loss: 0.40832 |  0:00:04s\n",
      "epoch 86 | loss: 0.40203 |  0:00:04s\n",
      "epoch 87 | loss: 0.39218 |  0:00:04s\n",
      "epoch 88 | loss: 0.39111 |  0:00:04s\n",
      "epoch 89 | loss: 0.38714 |  0:00:04s\n",
      "epoch 90 | loss: 0.38527 |  0:00:04s\n",
      "epoch 91 | loss: 0.38564 |  0:00:04s\n",
      "epoch 92 | loss: 0.38691 |  0:00:04s\n",
      "epoch 93 | loss: 0.3792  |  0:00:04s\n",
      "epoch 94 | loss: 0.37855 |  0:00:04s\n",
      "epoch 95 | loss: 0.37753 |  0:00:04s\n",
      "epoch 96 | loss: 0.38033 |  0:00:04s\n",
      "epoch 97 | loss: 0.37368 |  0:00:04s\n",
      "epoch 98 | loss: 0.37008 |  0:00:04s\n",
      "epoch 99 | loss: 0.37057 |  0:00:04s\n",
      "epoch 100| loss: 0.36982 |  0:00:04s\n",
      "epoch 101| loss: 0.36531 |  0:00:04s\n",
      "epoch 102| loss: 0.37275 |  0:00:04s\n",
      "epoch 103| loss: 0.36427 |  0:00:04s\n",
      "epoch 104| loss: 0.35482 |  0:00:04s\n",
      "epoch 105| loss: 0.35929 |  0:00:04s\n",
      "epoch 106| loss: 0.36445 |  0:00:05s\n",
      "epoch 107| loss: 0.35794 |  0:00:05s\n",
      "epoch 108| loss: 0.34791 |  0:00:05s\n",
      "epoch 109| loss: 0.35479 |  0:00:05s\n",
      "epoch 110| loss: 0.35703 |  0:00:05s\n",
      "epoch 111| loss: 0.36697 |  0:00:05s\n",
      "epoch 112| loss: 0.36886 |  0:00:05s\n",
      "epoch 113| loss: 0.35907 |  0:00:05s\n",
      "epoch 114| loss: 0.35635 |  0:00:05s\n",
      "epoch 115| loss: 0.35371 |  0:00:05s\n",
      "epoch 116| loss: 0.34837 |  0:00:05s\n",
      "epoch 117| loss: 0.35317 |  0:00:05s\n",
      "epoch 118| loss: 0.3523  |  0:00:05s\n",
      "epoch 119| loss: 0.35531 |  0:00:05s\n",
      "epoch 120| loss: 0.34998 |  0:00:05s\n",
      "epoch 121| loss: 0.35466 |  0:00:05s\n",
      "epoch 122| loss: 0.34585 |  0:00:05s\n",
      "epoch 123| loss: 0.34235 |  0:00:05s\n",
      "epoch 124| loss: 0.34428 |  0:00:05s\n",
      "epoch 125| loss: 0.34245 |  0:00:05s\n",
      "epoch 126| loss: 0.33733 |  0:00:05s\n",
      "epoch 127| loss: 0.32843 |  0:00:05s\n",
      "epoch 128| loss: 0.32945 |  0:00:05s\n",
      "epoch 129| loss: 0.33256 |  0:00:06s\n",
      "epoch 130| loss: 0.33175 |  0:00:06s\n",
      "epoch 131| loss: 0.3332  |  0:00:06s\n",
      "epoch 132| loss: 0.32812 |  0:00:06s\n",
      "epoch 133| loss: 0.32502 |  0:00:06s\n",
      "epoch 134| loss: 0.32346 |  0:00:06s\n",
      "epoch 135| loss: 0.32086 |  0:00:06s\n",
      "epoch 136| loss: 0.31675 |  0:00:06s\n",
      "epoch 137| loss: 0.31681 |  0:00:06s\n",
      "epoch 138| loss: 0.31272 |  0:00:06s\n",
      "epoch 139| loss: 0.31943 |  0:00:06s\n",
      "epoch 140| loss: 0.32684 |  0:00:06s\n",
      "epoch 141| loss: 0.3223  |  0:00:06s\n",
      "epoch 142| loss: 0.31701 |  0:00:06s\n",
      "epoch 143| loss: 0.31857 |  0:00:06s\n",
      "epoch 144| loss: 0.31407 |  0:00:06s\n",
      "epoch 145| loss: 0.31412 |  0:00:06s\n",
      "epoch 146| loss: 0.31174 |  0:00:06s\n",
      "epoch 147| loss: 0.30392 |  0:00:06s\n",
      "epoch 148| loss: 0.30357 |  0:00:06s\n",
      "epoch 149| loss: 0.29974 |  0:00:06s\n",
      "epoch 150| loss: 0.30496 |  0:00:06s\n",
      "epoch 151| loss: 0.29573 |  0:00:07s\n",
      "epoch 152| loss: 0.29048 |  0:00:07s\n",
      "epoch 153| loss: 0.28226 |  0:00:07s\n",
      "epoch 154| loss: 0.28826 |  0:00:07s\n",
      "epoch 155| loss: 0.28693 |  0:00:07s\n",
      "epoch 156| loss: 0.28638 |  0:00:07s\n",
      "epoch 157| loss: 0.28901 |  0:00:07s\n",
      "epoch 158| loss: 0.28016 |  0:00:07s\n",
      "epoch 159| loss: 0.26914 |  0:00:07s\n",
      "epoch 160| loss: 0.28034 |  0:00:07s\n",
      "epoch 161| loss: 0.27969 |  0:00:07s\n",
      "epoch 162| loss: 0.27893 |  0:00:07s\n",
      "epoch 163| loss: 0.27798 |  0:00:07s\n",
      "epoch 164| loss: 0.27294 |  0:00:07s\n",
      "epoch 165| loss: 0.27126 |  0:00:07s\n",
      "epoch 166| loss: 0.26454 |  0:00:07s\n",
      "epoch 167| loss: 0.25916 |  0:00:07s\n",
      "epoch 168| loss: 0.26909 |  0:00:07s\n",
      "epoch 169| loss: 0.26235 |  0:00:07s\n",
      "epoch 170| loss: 0.2675  |  0:00:07s\n",
      "epoch 171| loss: 0.27359 |  0:00:07s\n",
      "epoch 172| loss: 0.26863 |  0:00:07s\n",
      "epoch 173| loss: 0.2666  |  0:00:08s\n",
      "epoch 174| loss: 0.25944 |  0:00:08s\n",
      "epoch 175| loss: 0.24989 |  0:00:08s\n",
      "epoch 176| loss: 0.25525 |  0:00:08s\n",
      "epoch 177| loss: 0.24779 |  0:00:08s\n",
      "epoch 178| loss: 0.2516  |  0:00:08s\n",
      "epoch 179| loss: 0.25071 |  0:00:08s\n",
      "epoch 180| loss: 0.25355 |  0:00:08s\n",
      "epoch 181| loss: 0.25454 |  0:00:08s\n",
      "epoch 182| loss: 0.25612 |  0:00:08s\n",
      "epoch 183| loss: 0.24904 |  0:00:08s\n",
      "epoch 184| loss: 0.24829 |  0:00:08s\n",
      "epoch 185| loss: 0.24475 |  0:00:08s\n",
      "epoch 186| loss: 0.24948 |  0:00:08s\n",
      "epoch 187| loss: 0.23988 |  0:00:08s\n",
      "epoch 188| loss: 0.24639 |  0:00:08s\n",
      "epoch 189| loss: 0.24821 |  0:00:08s\n",
      "epoch 190| loss: 0.24762 |  0:00:08s\n",
      "epoch 191| loss: 0.23517 |  0:00:08s\n",
      "epoch 192| loss: 0.24153 |  0:00:08s\n",
      "epoch 193| loss: 0.25217 |  0:00:08s\n",
      "epoch 194| loss: 0.24298 |  0:00:08s\n",
      "epoch 195| loss: 0.2387  |  0:00:09s\n",
      "epoch 196| loss: 0.24761 |  0:00:09s\n",
      "epoch 197| loss: 0.23682 |  0:00:09s\n",
      "epoch 198| loss: 0.24273 |  0:00:09s\n",
      "epoch 199| loss: 0.23808 |  0:00:09s\n",
      "epoch 200| loss: 0.23974 |  0:00:09s\n",
      "epoch 201| loss: 0.23512 |  0:00:09s\n",
      "epoch 202| loss: 0.25452 |  0:00:09s\n",
      "epoch 203| loss: 0.25467 |  0:00:09s\n",
      "epoch 204| loss: 0.24848 |  0:00:09s\n",
      "epoch 205| loss: 0.26114 |  0:00:09s\n",
      "epoch 206| loss: 0.25293 |  0:00:09s\n",
      "epoch 207| loss: 0.24133 |  0:00:09s\n",
      "epoch 208| loss: 0.24597 |  0:00:09s\n",
      "epoch 209| loss: 0.23329 |  0:00:09s\n",
      "epoch 210| loss: 0.23377 |  0:00:09s\n",
      "epoch 211| loss: 0.23712 |  0:00:09s\n",
      "epoch 212| loss: 0.23461 |  0:00:09s\n",
      "epoch 213| loss: 0.25758 |  0:00:09s\n",
      "epoch 214| loss: 0.24188 |  0:00:09s\n",
      "epoch 215| loss: 0.25372 |  0:00:09s\n",
      "epoch 216| loss: 0.25104 |  0:00:09s\n",
      "epoch 217| loss: 0.24228 |  0:00:09s\n",
      "epoch 218| loss: 0.2505  |  0:00:10s\n",
      "epoch 219| loss: 0.23879 |  0:00:10s\n",
      "epoch 220| loss: 0.24027 |  0:00:10s\n",
      "epoch 221| loss: 0.23601 |  0:00:10s\n",
      "epoch 222| loss: 0.22914 |  0:00:10s\n",
      "epoch 223| loss: 0.23619 |  0:00:10s\n",
      "epoch 224| loss: 0.23133 |  0:00:10s\n",
      "epoch 225| loss: 0.22127 |  0:00:10s\n",
      "epoch 226| loss: 0.23045 |  0:00:10s\n",
      "epoch 227| loss: 0.23159 |  0:00:10s\n",
      "epoch 228| loss: 0.23048 |  0:00:10s\n",
      "epoch 229| loss: 0.22539 |  0:00:10s\n",
      "epoch 230| loss: 0.22372 |  0:00:10s\n",
      "epoch 231| loss: 0.21867 |  0:00:10s\n",
      "epoch 232| loss: 0.21615 |  0:00:10s\n",
      "epoch 233| loss: 0.21826 |  0:00:10s\n",
      "epoch 234| loss: 0.21408 |  0:00:10s\n",
      "epoch 235| loss: 0.21753 |  0:00:10s\n",
      "epoch 236| loss: 0.21668 |  0:00:11s\n",
      "epoch 237| loss: 0.21089 |  0:00:11s\n",
      "epoch 238| loss: 0.2048  |  0:00:11s\n",
      "epoch 239| loss: 0.21715 |  0:00:11s\n",
      "epoch 240| loss: 0.21011 |  0:00:11s\n",
      "epoch 241| loss: 0.21663 |  0:00:11s\n",
      "epoch 242| loss: 0.2116  |  0:00:11s\n",
      "epoch 243| loss: 0.20601 |  0:00:11s\n",
      "epoch 244| loss: 0.20762 |  0:00:11s\n",
      "epoch 245| loss: 0.20846 |  0:00:11s\n",
      "epoch 246| loss: 0.2101  |  0:00:11s\n",
      "epoch 247| loss: 0.21145 |  0:00:11s\n",
      "epoch 248| loss: 0.2059  |  0:00:11s\n",
      "epoch 249| loss: 0.21491 |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.26739 |  0:00:00s\n",
      "epoch 1  | loss: 2.16308 |  0:00:00s\n",
      "epoch 2  | loss: 1.55036 |  0:00:00s\n",
      "epoch 3  | loss: 1.31403 |  0:00:00s\n",
      "epoch 4  | loss: 1.08474 |  0:00:00s\n",
      "epoch 5  | loss: 0.98107 |  0:00:00s\n",
      "epoch 6  | loss: 0.92977 |  0:00:00s\n",
      "epoch 7  | loss: 0.90429 |  0:00:00s\n",
      "epoch 8  | loss: 0.86339 |  0:00:00s\n",
      "epoch 9  | loss: 0.80559 |  0:00:00s\n",
      "epoch 10 | loss: 0.79045 |  0:00:00s\n",
      "epoch 11 | loss: 0.75551 |  0:00:00s\n",
      "epoch 12 | loss: 0.73722 |  0:00:00s\n",
      "epoch 13 | loss: 0.70227 |  0:00:00s\n",
      "epoch 14 | loss: 0.70065 |  0:00:00s\n",
      "epoch 15 | loss: 0.69198 |  0:00:00s\n",
      "epoch 16 | loss: 0.67823 |  0:00:00s\n",
      "epoch 17 | loss: 0.67407 |  0:00:00s\n",
      "epoch 18 | loss: 0.66299 |  0:00:00s\n",
      "epoch 19 | loss: 0.67118 |  0:00:00s\n",
      "epoch 20 | loss: 0.65594 |  0:00:00s\n",
      "epoch 21 | loss: 0.64246 |  0:00:00s\n",
      "epoch 22 | loss: 0.64269 |  0:00:01s\n",
      "epoch 23 | loss: 0.64083 |  0:00:01s\n",
      "epoch 24 | loss: 0.62976 |  0:00:01s\n",
      "epoch 25 | loss: 0.62906 |  0:00:01s\n",
      "epoch 26 | loss: 0.62267 |  0:00:01s\n",
      "epoch 27 | loss: 0.61757 |  0:00:01s\n",
      "epoch 28 | loss: 0.6158  |  0:00:01s\n",
      "epoch 29 | loss: 0.60877 |  0:00:01s\n",
      "epoch 30 | loss: 0.59934 |  0:00:01s\n",
      "epoch 31 | loss: 0.59093 |  0:00:01s\n",
      "epoch 32 | loss: 0.58788 |  0:00:01s\n",
      "epoch 33 | loss: 0.58286 |  0:00:01s\n",
      "epoch 34 | loss: 0.56804 |  0:00:01s\n",
      "epoch 35 | loss: 0.57722 |  0:00:01s\n",
      "epoch 36 | loss: 0.56032 |  0:00:01s\n",
      "epoch 37 | loss: 0.56357 |  0:00:01s\n",
      "epoch 38 | loss: 0.56836 |  0:00:01s\n",
      "epoch 39 | loss: 0.55497 |  0:00:01s\n",
      "epoch 40 | loss: 0.54817 |  0:00:01s\n",
      "epoch 41 | loss: 0.53763 |  0:00:01s\n",
      "epoch 42 | loss: 0.52901 |  0:00:01s\n",
      "epoch 43 | loss: 0.53247 |  0:00:01s\n",
      "epoch 44 | loss: 0.5315  |  0:00:02s\n",
      "epoch 45 | loss: 0.5418  |  0:00:02s\n",
      "epoch 46 | loss: 0.53935 |  0:00:02s\n",
      "epoch 47 | loss: 0.526   |  0:00:02s\n",
      "epoch 48 | loss: 0.53675 |  0:00:02s\n",
      "epoch 49 | loss: 0.52227 |  0:00:02s\n",
      "epoch 50 | loss: 0.52354 |  0:00:02s\n",
      "epoch 51 | loss: 0.52217 |  0:00:02s\n",
      "epoch 52 | loss: 0.51375 |  0:00:02s\n",
      "epoch 53 | loss: 0.51361 |  0:00:02s\n",
      "epoch 54 | loss: 0.50212 |  0:00:02s\n",
      "epoch 55 | loss: 0.50094 |  0:00:02s\n",
      "epoch 56 | loss: 0.49832 |  0:00:02s\n",
      "epoch 57 | loss: 0.4958  |  0:00:02s\n",
      "epoch 58 | loss: 0.49041 |  0:00:02s\n",
      "epoch 59 | loss: 0.48433 |  0:00:02s\n",
      "epoch 60 | loss: 0.49188 |  0:00:02s\n",
      "epoch 61 | loss: 0.48236 |  0:00:02s\n",
      "epoch 62 | loss: 0.47558 |  0:00:02s\n",
      "epoch 63 | loss: 0.47484 |  0:00:02s\n",
      "epoch 64 | loss: 0.48412 |  0:00:02s\n",
      "epoch 65 | loss: 0.47142 |  0:00:02s\n",
      "epoch 66 | loss: 0.47291 |  0:00:03s\n",
      "epoch 67 | loss: 0.4695  |  0:00:03s\n",
      "epoch 68 | loss: 0.46579 |  0:00:03s\n",
      "epoch 69 | loss: 0.46937 |  0:00:03s\n",
      "epoch 70 | loss: 0.46578 |  0:00:03s\n",
      "epoch 71 | loss: 0.45995 |  0:00:03s\n",
      "epoch 72 | loss: 0.45413 |  0:00:03s\n",
      "epoch 73 | loss: 0.46289 |  0:00:03s\n",
      "epoch 74 | loss: 0.4523  |  0:00:03s\n",
      "epoch 75 | loss: 0.44265 |  0:00:03s\n",
      "epoch 76 | loss: 0.44273 |  0:00:03s\n",
      "epoch 77 | loss: 0.43316 |  0:00:03s\n",
      "epoch 78 | loss: 0.43246 |  0:00:03s\n",
      "epoch 79 | loss: 0.4369  |  0:00:03s\n",
      "epoch 80 | loss: 0.43073 |  0:00:03s\n",
      "epoch 81 | loss: 0.43083 |  0:00:03s\n",
      "epoch 82 | loss: 0.43172 |  0:00:03s\n",
      "epoch 83 | loss: 0.43264 |  0:00:03s\n",
      "epoch 84 | loss: 0.43861 |  0:00:03s\n",
      "epoch 85 | loss: 0.41655 |  0:00:03s\n",
      "epoch 86 | loss: 0.41824 |  0:00:03s\n",
      "epoch 87 | loss: 0.41368 |  0:00:03s\n",
      "epoch 88 | loss: 0.40601 |  0:00:04s\n",
      "epoch 89 | loss: 0.40878 |  0:00:04s\n",
      "epoch 90 | loss: 0.39458 |  0:00:04s\n",
      "epoch 91 | loss: 0.40408 |  0:00:04s\n",
      "epoch 92 | loss: 0.40629 |  0:00:04s\n",
      "epoch 93 | loss: 0.40661 |  0:00:04s\n",
      "epoch 94 | loss: 0.40818 |  0:00:04s\n",
      "epoch 95 | loss: 0.40447 |  0:00:04s\n",
      "epoch 96 | loss: 0.40432 |  0:00:04s\n",
      "epoch 97 | loss: 0.41255 |  0:00:04s\n",
      "epoch 98 | loss: 0.3933  |  0:00:04s\n",
      "epoch 99 | loss: 0.39797 |  0:00:04s\n",
      "epoch 100| loss: 0.39024 |  0:00:04s\n",
      "epoch 101| loss: 0.39299 |  0:00:04s\n",
      "epoch 102| loss: 0.39109 |  0:00:04s\n",
      "epoch 103| loss: 0.38705 |  0:00:04s\n",
      "epoch 104| loss: 0.38511 |  0:00:04s\n",
      "epoch 105| loss: 0.38081 |  0:00:04s\n",
      "epoch 106| loss: 0.37893 |  0:00:04s\n",
      "epoch 107| loss: 0.37029 |  0:00:04s\n",
      "epoch 108| loss: 0.36087 |  0:00:04s\n",
      "epoch 109| loss: 0.35172 |  0:00:04s\n",
      "epoch 110| loss: 0.3707  |  0:00:05s\n",
      "epoch 111| loss: 0.35042 |  0:00:05s\n",
      "epoch 112| loss: 0.35732 |  0:00:05s\n",
      "epoch 113| loss: 0.35408 |  0:00:05s\n",
      "epoch 114| loss: 0.35277 |  0:00:05s\n",
      "epoch 115| loss: 0.35963 |  0:00:05s\n",
      "epoch 116| loss: 0.3449  |  0:00:05s\n",
      "epoch 117| loss: 0.35207 |  0:00:05s\n",
      "epoch 118| loss: 0.34722 |  0:00:05s\n",
      "epoch 119| loss: 0.34339 |  0:00:05s\n",
      "epoch 120| loss: 0.3519  |  0:00:05s\n",
      "epoch 121| loss: 0.33853 |  0:00:05s\n",
      "epoch 122| loss: 0.34075 |  0:00:05s\n",
      "epoch 123| loss: 0.33966 |  0:00:05s\n",
      "epoch 124| loss: 0.34703 |  0:00:05s\n",
      "epoch 125| loss: 0.34009 |  0:00:05s\n",
      "epoch 126| loss: 0.34494 |  0:00:05s\n",
      "epoch 127| loss: 0.33102 |  0:00:05s\n",
      "epoch 128| loss: 0.3298  |  0:00:06s\n",
      "epoch 129| loss: 0.33035 |  0:00:06s\n",
      "epoch 130| loss: 0.32209 |  0:00:06s\n",
      "epoch 131| loss: 0.33095 |  0:00:06s\n",
      "epoch 132| loss: 0.33056 |  0:00:06s\n",
      "epoch 133| loss: 0.32345 |  0:00:06s\n",
      "epoch 134| loss: 0.32738 |  0:00:06s\n",
      "epoch 135| loss: 0.32108 |  0:00:06s\n",
      "epoch 136| loss: 0.3256  |  0:00:06s\n",
      "epoch 137| loss: 0.32016 |  0:00:06s\n",
      "epoch 138| loss: 0.3366  |  0:00:06s\n",
      "epoch 139| loss: 0.34353 |  0:00:06s\n",
      "epoch 140| loss: 0.33559 |  0:00:06s\n",
      "epoch 141| loss: 0.32863 |  0:00:06s\n",
      "epoch 142| loss: 0.32946 |  0:00:06s\n",
      "epoch 143| loss: 0.33153 |  0:00:06s\n",
      "epoch 144| loss: 0.33232 |  0:00:06s\n",
      "epoch 145| loss: 0.32496 |  0:00:06s\n",
      "epoch 146| loss: 0.33394 |  0:00:06s\n",
      "epoch 147| loss: 0.32724 |  0:00:06s\n",
      "epoch 148| loss: 0.32478 |  0:00:06s\n",
      "epoch 149| loss: 0.32303 |  0:00:07s\n",
      "epoch 150| loss: 0.32697 |  0:00:07s\n",
      "epoch 151| loss: 0.31297 |  0:00:07s\n",
      "epoch 152| loss: 0.30988 |  0:00:07s\n",
      "epoch 153| loss: 0.30845 |  0:00:07s\n",
      "epoch 154| loss: 0.30491 |  0:00:07s\n",
      "epoch 155| loss: 0.31422 |  0:00:07s\n",
      "epoch 156| loss: 0.30096 |  0:00:07s\n",
      "epoch 157| loss: 0.29834 |  0:00:07s\n",
      "epoch 158| loss: 0.29625 |  0:00:07s\n",
      "epoch 159| loss: 0.29154 |  0:00:07s\n",
      "epoch 160| loss: 0.29997 |  0:00:07s\n",
      "epoch 161| loss: 0.29114 |  0:00:07s\n",
      "epoch 162| loss: 0.29661 |  0:00:07s\n",
      "epoch 163| loss: 0.28906 |  0:00:07s\n",
      "epoch 164| loss: 0.29088 |  0:00:07s\n",
      "epoch 165| loss: 0.29009 |  0:00:07s\n",
      "epoch 166| loss: 0.28991 |  0:00:07s\n",
      "epoch 167| loss: 0.2811  |  0:00:07s\n",
      "epoch 168| loss: 0.28544 |  0:00:07s\n",
      "epoch 169| loss: 0.28423 |  0:00:08s\n",
      "epoch 170| loss: 0.28293 |  0:00:08s\n",
      "epoch 171| loss: 0.28972 |  0:00:08s\n",
      "epoch 172| loss: 0.28177 |  0:00:08s\n",
      "epoch 173| loss: 0.27491 |  0:00:08s\n",
      "epoch 174| loss: 0.27393 |  0:00:08s\n",
      "epoch 175| loss: 0.27496 |  0:00:08s\n",
      "epoch 176| loss: 0.27294 |  0:00:08s\n",
      "epoch 177| loss: 0.26378 |  0:00:08s\n",
      "epoch 178| loss: 0.26274 |  0:00:08s\n",
      "epoch 179| loss: 0.28676 |  0:00:08s\n",
      "epoch 180| loss: 0.29448 |  0:00:08s\n",
      "epoch 181| loss: 0.29002 |  0:00:08s\n",
      "epoch 182| loss: 0.28801 |  0:00:08s\n",
      "epoch 183| loss: 0.28513 |  0:00:08s\n",
      "epoch 184| loss: 0.29688 |  0:00:08s\n",
      "epoch 185| loss: 0.30327 |  0:00:08s\n",
      "epoch 186| loss: 0.29082 |  0:00:08s\n",
      "epoch 187| loss: 0.2966  |  0:00:08s\n",
      "epoch 188| loss: 0.29193 |  0:00:08s\n",
      "epoch 189| loss: 0.29074 |  0:00:08s\n",
      "epoch 190| loss: 0.29411 |  0:00:09s\n",
      "epoch 191| loss: 0.29347 |  0:00:09s\n",
      "epoch 192| loss: 0.29544 |  0:00:09s\n",
      "epoch 193| loss: 0.28531 |  0:00:09s\n",
      "epoch 194| loss: 0.29172 |  0:00:09s\n",
      "epoch 195| loss: 0.27827 |  0:00:09s\n",
      "epoch 196| loss: 0.28784 |  0:00:09s\n",
      "epoch 197| loss: 0.28221 |  0:00:09s\n",
      "epoch 198| loss: 0.2905  |  0:00:09s\n",
      "epoch 199| loss: 0.2984  |  0:00:09s\n",
      "epoch 200| loss: 0.29356 |  0:00:09s\n",
      "epoch 201| loss: 0.2869  |  0:00:09s\n",
      "epoch 202| loss: 0.28835 |  0:00:09s\n",
      "epoch 203| loss: 0.28395 |  0:00:09s\n",
      "epoch 204| loss: 0.28914 |  0:00:09s\n",
      "epoch 205| loss: 0.29086 |  0:00:09s\n",
      "epoch 206| loss: 0.2839  |  0:00:09s\n",
      "epoch 207| loss: 0.27733 |  0:00:09s\n",
      "epoch 208| loss: 0.28439 |  0:00:09s\n",
      "epoch 209| loss: 0.2805  |  0:00:09s\n",
      "epoch 210| loss: 0.28386 |  0:00:10s\n",
      "epoch 211| loss: 0.27678 |  0:00:10s\n",
      "epoch 212| loss: 0.26994 |  0:00:10s\n",
      "epoch 213| loss: 0.28816 |  0:00:10s\n",
      "epoch 214| loss: 0.27128 |  0:00:10s\n",
      "epoch 215| loss: 0.26669 |  0:00:10s\n",
      "epoch 216| loss: 0.26857 |  0:00:10s\n",
      "epoch 217| loss: 0.25902 |  0:00:10s\n",
      "epoch 218| loss: 0.25166 |  0:00:10s\n",
      "epoch 219| loss: 0.26065 |  0:00:10s\n",
      "epoch 220| loss: 0.25099 |  0:00:10s\n",
      "epoch 221| loss: 0.2482  |  0:00:10s\n",
      "epoch 222| loss: 0.24807 |  0:00:10s\n",
      "epoch 223| loss: 0.25669 |  0:00:10s\n",
      "epoch 224| loss: 0.25482 |  0:00:10s\n",
      "epoch 225| loss: 0.25485 |  0:00:10s\n",
      "epoch 226| loss: 0.25745 |  0:00:10s\n",
      "epoch 227| loss: 0.25503 |  0:00:10s\n",
      "epoch 228| loss: 0.25305 |  0:00:10s\n",
      "epoch 229| loss: 0.25824 |  0:00:10s\n",
      "epoch 230| loss: 0.26147 |  0:00:10s\n",
      "epoch 231| loss: 0.27369 |  0:00:11s\n",
      "epoch 232| loss: 0.26651 |  0:00:11s\n",
      "epoch 233| loss: 0.26595 |  0:00:11s\n",
      "epoch 234| loss: 0.2624  |  0:00:11s\n",
      "epoch 235| loss: 0.26058 |  0:00:11s\n",
      "epoch 236| loss: 0.25663 |  0:00:11s\n",
      "epoch 237| loss: 0.25409 |  0:00:11s\n",
      "epoch 238| loss: 0.24791 |  0:00:11s\n",
      "epoch 239| loss: 0.25486 |  0:00:11s\n",
      "epoch 240| loss: 0.24594 |  0:00:11s\n",
      "epoch 241| loss: 0.24726 |  0:00:11s\n",
      "epoch 242| loss: 0.24821 |  0:00:11s\n",
      "epoch 243| loss: 0.24569 |  0:00:11s\n",
      "epoch 244| loss: 0.24182 |  0:00:11s\n",
      "epoch 245| loss: 0.24223 |  0:00:11s\n",
      "epoch 246| loss: 0.24614 |  0:00:11s\n",
      "epoch 247| loss: 0.24522 |  0:00:11s\n",
      "epoch 248| loss: 0.24382 |  0:00:11s\n",
      "epoch 249| loss: 0.24266 |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.2486  |  0:00:00s\n",
      "epoch 1  | loss: 2.06282 |  0:00:00s\n",
      "epoch 2  | loss: 1.44976 |  0:00:00s\n",
      "epoch 3  | loss: 1.19593 |  0:00:00s\n",
      "epoch 4  | loss: 1.00946 |  0:00:00s\n",
      "epoch 5  | loss: 0.93953 |  0:00:00s\n",
      "epoch 6  | loss: 0.86413 |  0:00:00s\n",
      "epoch 7  | loss: 0.85461 |  0:00:00s\n",
      "epoch 8  | loss: 0.81045 |  0:00:00s\n",
      "epoch 9  | loss: 0.79771 |  0:00:00s\n",
      "epoch 10 | loss: 0.77951 |  0:00:00s\n",
      "epoch 11 | loss: 0.74701 |  0:00:00s\n",
      "epoch 12 | loss: 0.73321 |  0:00:00s\n",
      "epoch 13 | loss: 0.72783 |  0:00:00s\n",
      "epoch 14 | loss: 0.70271 |  0:00:00s\n",
      "epoch 15 | loss: 0.71143 |  0:00:00s\n",
      "epoch 16 | loss: 0.69654 |  0:00:00s\n",
      "epoch 17 | loss: 0.68261 |  0:00:00s\n",
      "epoch 18 | loss: 0.68195 |  0:00:00s\n",
      "epoch 19 | loss: 0.68283 |  0:00:00s\n",
      "epoch 20 | loss: 0.65206 |  0:00:00s\n",
      "epoch 21 | loss: 0.65266 |  0:00:01s\n",
      "epoch 22 | loss: 0.63854 |  0:00:01s\n",
      "epoch 23 | loss: 0.63305 |  0:00:01s\n",
      "epoch 24 | loss: 0.64081 |  0:00:01s\n",
      "epoch 25 | loss: 0.63722 |  0:00:01s\n",
      "epoch 26 | loss: 0.6391  |  0:00:01s\n",
      "epoch 27 | loss: 0.62837 |  0:00:01s\n",
      "epoch 28 | loss: 0.61583 |  0:00:01s\n",
      "epoch 29 | loss: 0.61153 |  0:00:01s\n",
      "epoch 30 | loss: 0.6167  |  0:00:01s\n",
      "epoch 31 | loss: 0.60416 |  0:00:01s\n",
      "epoch 32 | loss: 0.60451 |  0:00:01s\n",
      "epoch 33 | loss: 0.60479 |  0:00:01s\n",
      "epoch 34 | loss: 0.59773 |  0:00:01s\n",
      "epoch 35 | loss: 0.58634 |  0:00:01s\n",
      "epoch 36 | loss: 0.58686 |  0:00:01s\n",
      "epoch 37 | loss: 0.57328 |  0:00:01s\n",
      "epoch 38 | loss: 0.56929 |  0:00:01s\n",
      "epoch 39 | loss: 0.56096 |  0:00:01s\n",
      "epoch 40 | loss: 0.56043 |  0:00:01s\n",
      "epoch 41 | loss: 0.55928 |  0:00:01s\n",
      "epoch 42 | loss: 0.559   |  0:00:01s\n",
      "epoch 43 | loss: 0.55227 |  0:00:01s\n",
      "epoch 44 | loss: 0.54885 |  0:00:02s\n",
      "epoch 45 | loss: 0.55205 |  0:00:02s\n",
      "epoch 46 | loss: 0.54311 |  0:00:02s\n",
      "epoch 47 | loss: 0.53953 |  0:00:02s\n",
      "epoch 48 | loss: 0.54193 |  0:00:02s\n",
      "epoch 49 | loss: 0.5222  |  0:00:02s\n",
      "epoch 50 | loss: 0.51628 |  0:00:02s\n",
      "epoch 51 | loss: 0.51843 |  0:00:02s\n",
      "epoch 52 | loss: 0.50972 |  0:00:02s\n",
      "epoch 53 | loss: 0.50843 |  0:00:02s\n",
      "epoch 54 | loss: 0.50961 |  0:00:02s\n",
      "epoch 55 | loss: 0.51412 |  0:00:02s\n",
      "epoch 56 | loss: 0.5015  |  0:00:02s\n",
      "epoch 57 | loss: 0.4999  |  0:00:02s\n",
      "epoch 58 | loss: 0.5035  |  0:00:02s\n",
      "epoch 59 | loss: 0.49825 |  0:00:02s\n",
      "epoch 60 | loss: 0.50489 |  0:00:02s\n",
      "epoch 61 | loss: 0.49637 |  0:00:02s\n",
      "epoch 62 | loss: 0.50116 |  0:00:02s\n",
      "epoch 63 | loss: 0.49943 |  0:00:03s\n",
      "epoch 64 | loss: 0.48503 |  0:00:03s\n",
      "epoch 65 | loss: 0.48627 |  0:00:03s\n",
      "epoch 66 | loss: 0.49936 |  0:00:03s\n",
      "epoch 67 | loss: 0.50995 |  0:00:03s\n",
      "epoch 68 | loss: 0.51124 |  0:00:03s\n",
      "epoch 69 | loss: 0.50111 |  0:00:03s\n",
      "epoch 70 | loss: 0.50119 |  0:00:03s\n",
      "epoch 71 | loss: 0.48803 |  0:00:03s\n",
      "epoch 72 | loss: 0.48629 |  0:00:03s\n",
      "epoch 73 | loss: 0.48764 |  0:00:03s\n",
      "epoch 74 | loss: 0.49189 |  0:00:03s\n",
      "epoch 75 | loss: 0.47736 |  0:00:03s\n",
      "epoch 76 | loss: 0.4706  |  0:00:03s\n",
      "epoch 77 | loss: 0.4585  |  0:00:03s\n",
      "epoch 78 | loss: 0.45933 |  0:00:03s\n",
      "epoch 79 | loss: 0.45486 |  0:00:03s\n",
      "epoch 80 | loss: 0.45863 |  0:00:03s\n",
      "epoch 81 | loss: 0.45079 |  0:00:03s\n",
      "epoch 82 | loss: 0.452   |  0:00:03s\n",
      "epoch 83 | loss: 0.44497 |  0:00:03s\n",
      "epoch 84 | loss: 0.44759 |  0:00:04s\n",
      "epoch 85 | loss: 0.43526 |  0:00:04s\n",
      "epoch 86 | loss: 0.43806 |  0:00:04s\n",
      "epoch 87 | loss: 0.4375  |  0:00:04s\n",
      "epoch 88 | loss: 0.43598 |  0:00:04s\n",
      "epoch 89 | loss: 0.44854 |  0:00:04s\n",
      "epoch 90 | loss: 0.43128 |  0:00:04s\n",
      "epoch 91 | loss: 0.42785 |  0:00:04s\n",
      "epoch 92 | loss: 0.4382  |  0:00:04s\n",
      "epoch 93 | loss: 0.42942 |  0:00:04s\n",
      "epoch 94 | loss: 0.43048 |  0:00:04s\n",
      "epoch 95 | loss: 0.42857 |  0:00:04s\n",
      "epoch 96 | loss: 0.43443 |  0:00:04s\n",
      "epoch 97 | loss: 0.43637 |  0:00:04s\n",
      "epoch 98 | loss: 0.42098 |  0:00:04s\n",
      "epoch 99 | loss: 0.42076 |  0:00:04s\n",
      "epoch 100| loss: 0.41703 |  0:00:04s\n",
      "epoch 101| loss: 0.41803 |  0:00:04s\n",
      "epoch 102| loss: 0.41604 |  0:00:04s\n",
      "epoch 103| loss: 0.40411 |  0:00:04s\n",
      "epoch 104| loss: 0.40394 |  0:00:04s\n",
      "epoch 105| loss: 0.41177 |  0:00:04s\n",
      "epoch 106| loss: 0.41556 |  0:00:05s\n",
      "epoch 107| loss: 0.41723 |  0:00:05s\n",
      "epoch 108| loss: 0.40922 |  0:00:05s\n",
      "epoch 109| loss: 0.39792 |  0:00:05s\n",
      "epoch 110| loss: 0.4021  |  0:00:05s\n",
      "epoch 111| loss: 0.40363 |  0:00:05s\n",
      "epoch 112| loss: 0.40593 |  0:00:05s\n",
      "epoch 113| loss: 0.40148 |  0:00:05s\n",
      "epoch 114| loss: 0.39449 |  0:00:05s\n",
      "epoch 115| loss: 0.39689 |  0:00:05s\n",
      "epoch 116| loss: 0.3973  |  0:00:05s\n",
      "epoch 117| loss: 0.39231 |  0:00:05s\n",
      "epoch 118| loss: 0.39039 |  0:00:05s\n",
      "epoch 119| loss: 0.37572 |  0:00:05s\n",
      "epoch 120| loss: 0.38143 |  0:00:05s\n",
      "epoch 121| loss: 0.38423 |  0:00:05s\n",
      "epoch 122| loss: 0.38321 |  0:00:05s\n",
      "epoch 123| loss: 0.37935 |  0:00:05s\n",
      "epoch 124| loss: 0.37443 |  0:00:05s\n",
      "epoch 125| loss: 0.38225 |  0:00:05s\n",
      "epoch 126| loss: 0.37725 |  0:00:05s\n",
      "epoch 127| loss: 0.3749  |  0:00:05s\n",
      "epoch 128| loss: 0.37571 |  0:00:06s\n",
      "epoch 129| loss: 0.37631 |  0:00:06s\n",
      "epoch 130| loss: 0.38246 |  0:00:06s\n",
      "epoch 131| loss: 0.37292 |  0:00:06s\n",
      "epoch 132| loss: 0.37464 |  0:00:06s\n",
      "epoch 133| loss: 0.36222 |  0:00:06s\n",
      "epoch 134| loss: 0.35628 |  0:00:06s\n",
      "epoch 135| loss: 0.36492 |  0:00:06s\n",
      "epoch 136| loss: 0.37445 |  0:00:06s\n",
      "epoch 137| loss: 0.37396 |  0:00:06s\n",
      "epoch 138| loss: 0.37547 |  0:00:06s\n",
      "epoch 139| loss: 0.37458 |  0:00:06s\n",
      "epoch 140| loss: 0.37963 |  0:00:06s\n",
      "epoch 141| loss: 0.36387 |  0:00:06s\n",
      "epoch 142| loss: 0.37403 |  0:00:06s\n",
      "epoch 143| loss: 0.37194 |  0:00:06s\n",
      "epoch 144| loss: 0.36944 |  0:00:06s\n",
      "epoch 145| loss: 0.37213 |  0:00:06s\n",
      "epoch 146| loss: 0.3618  |  0:00:06s\n",
      "epoch 147| loss: 0.35394 |  0:00:06s\n",
      "epoch 148| loss: 0.35041 |  0:00:06s\n",
      "epoch 149| loss: 0.34232 |  0:00:06s\n",
      "epoch 150| loss: 0.3449  |  0:00:07s\n",
      "epoch 151| loss: 0.34477 |  0:00:07s\n",
      "epoch 152| loss: 0.34271 |  0:00:07s\n",
      "epoch 153| loss: 0.33773 |  0:00:07s\n",
      "epoch 154| loss: 0.33939 |  0:00:07s\n",
      "epoch 155| loss: 0.33884 |  0:00:07s\n",
      "epoch 156| loss: 0.33087 |  0:00:07s\n",
      "epoch 157| loss: 0.32997 |  0:00:07s\n",
      "epoch 158| loss: 0.32173 |  0:00:07s\n",
      "epoch 159| loss: 0.31818 |  0:00:07s\n",
      "epoch 160| loss: 0.32394 |  0:00:07s\n",
      "epoch 161| loss: 0.31871 |  0:00:07s\n",
      "epoch 162| loss: 0.31413 |  0:00:07s\n",
      "epoch 163| loss: 0.31192 |  0:00:07s\n",
      "epoch 164| loss: 0.31277 |  0:00:07s\n",
      "epoch 165| loss: 0.3116  |  0:00:07s\n",
      "epoch 166| loss: 0.30861 |  0:00:07s\n",
      "epoch 167| loss: 0.30821 |  0:00:07s\n",
      "epoch 168| loss: 0.31842 |  0:00:07s\n",
      "epoch 169| loss: 0.3153  |  0:00:07s\n",
      "epoch 170| loss: 0.31992 |  0:00:07s\n",
      "epoch 171| loss: 0.30576 |  0:00:07s\n",
      "epoch 172| loss: 0.31282 |  0:00:08s\n",
      "epoch 173| loss: 0.30664 |  0:00:08s\n",
      "epoch 174| loss: 0.31087 |  0:00:08s\n",
      "epoch 175| loss: 0.30685 |  0:00:08s\n",
      "epoch 176| loss: 0.31266 |  0:00:08s\n",
      "epoch 177| loss: 0.30691 |  0:00:08s\n",
      "epoch 178| loss: 0.30405 |  0:00:08s\n",
      "epoch 179| loss: 0.30477 |  0:00:08s\n",
      "epoch 180| loss: 0.3137  |  0:00:08s\n",
      "epoch 181| loss: 0.31953 |  0:00:08s\n",
      "epoch 182| loss: 0.31387 |  0:00:08s\n",
      "epoch 183| loss: 0.32294 |  0:00:08s\n",
      "epoch 184| loss: 0.33423 |  0:00:08s\n",
      "epoch 185| loss: 0.32829 |  0:00:08s\n",
      "epoch 186| loss: 0.34033 |  0:00:08s\n",
      "epoch 187| loss: 0.33158 |  0:00:08s\n",
      "epoch 188| loss: 0.325   |  0:00:08s\n",
      "epoch 189| loss: 0.32537 |  0:00:08s\n",
      "epoch 190| loss: 0.32186 |  0:00:08s\n",
      "epoch 191| loss: 0.31216 |  0:00:08s\n",
      "epoch 192| loss: 0.31177 |  0:00:08s\n",
      "epoch 193| loss: 0.30774 |  0:00:08s\n",
      "epoch 194| loss: 0.32506 |  0:00:09s\n",
      "epoch 195| loss: 0.29844 |  0:00:09s\n",
      "epoch 196| loss: 0.30574 |  0:00:09s\n",
      "epoch 197| loss: 0.29321 |  0:00:09s\n",
      "epoch 198| loss: 0.29286 |  0:00:09s\n",
      "epoch 199| loss: 0.29553 |  0:00:09s\n",
      "epoch 200| loss: 0.29363 |  0:00:09s\n",
      "epoch 201| loss: 0.29233 |  0:00:09s\n",
      "epoch 202| loss: 0.29256 |  0:00:09s\n",
      "epoch 203| loss: 0.29032 |  0:00:09s\n",
      "epoch 204| loss: 0.28968 |  0:00:09s\n",
      "epoch 205| loss: 0.28276 |  0:00:09s\n",
      "epoch 206| loss: 0.28419 |  0:00:09s\n",
      "epoch 207| loss: 0.28398 |  0:00:09s\n",
      "epoch 208| loss: 0.28037 |  0:00:09s\n",
      "epoch 209| loss: 0.26746 |  0:00:09s\n",
      "epoch 210| loss: 0.27672 |  0:00:09s\n",
      "epoch 211| loss: 0.27771 |  0:00:09s\n",
      "epoch 212| loss: 0.27351 |  0:00:09s\n",
      "epoch 213| loss: 0.28692 |  0:00:09s\n",
      "epoch 214| loss: 0.29625 |  0:00:09s\n",
      "epoch 215| loss: 0.32982 |  0:00:09s\n",
      "epoch 216| loss: 0.31672 |  0:00:10s\n",
      "epoch 217| loss: 0.31072 |  0:00:10s\n",
      "epoch 218| loss: 0.299   |  0:00:10s\n",
      "epoch 219| loss: 0.29723 |  0:00:10s\n",
      "epoch 220| loss: 0.29488 |  0:00:10s\n",
      "epoch 221| loss: 0.28578 |  0:00:10s\n",
      "epoch 222| loss: 0.2927  |  0:00:10s\n",
      "epoch 223| loss: 0.28974 |  0:00:10s\n",
      "epoch 224| loss: 0.30272 |  0:00:10s\n",
      "epoch 225| loss: 0.30547 |  0:00:10s\n",
      "epoch 226| loss: 0.29657 |  0:00:10s\n",
      "epoch 227| loss: 0.29806 |  0:00:10s\n",
      "epoch 228| loss: 0.28889 |  0:00:10s\n",
      "epoch 229| loss: 0.28142 |  0:00:10s\n",
      "epoch 230| loss: 0.28355 |  0:00:10s\n",
      "epoch 231| loss: 0.27881 |  0:00:10s\n",
      "epoch 232| loss: 0.27745 |  0:00:10s\n",
      "epoch 233| loss: 0.27503 |  0:00:10s\n",
      "epoch 234| loss: 0.27091 |  0:00:11s\n",
      "epoch 235| loss: 0.27448 |  0:00:11s\n",
      "epoch 236| loss: 0.26686 |  0:00:11s\n",
      "epoch 237| loss: 0.26839 |  0:00:11s\n",
      "epoch 238| loss: 0.26507 |  0:00:11s\n",
      "epoch 239| loss: 0.26046 |  0:00:11s\n",
      "epoch 240| loss: 0.25326 |  0:00:11s\n",
      "epoch 241| loss: 0.26362 |  0:00:11s\n",
      "epoch 242| loss: 0.26618 |  0:00:11s\n",
      "epoch 243| loss: 0.25368 |  0:00:11s\n",
      "epoch 244| loss: 0.25123 |  0:00:11s\n",
      "epoch 245| loss: 0.25968 |  0:00:11s\n",
      "epoch 246| loss: 0.26768 |  0:00:11s\n",
      "epoch 247| loss: 0.26397 |  0:00:11s\n",
      "epoch 248| loss: 0.25994 |  0:00:11s\n",
      "epoch 249| loss: 0.26478 |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.29304 |  0:00:00s\n",
      "epoch 1  | loss: 2.09497 |  0:00:00s\n",
      "epoch 2  | loss: 1.46216 |  0:00:00s\n",
      "epoch 3  | loss: 1.17344 |  0:00:00s\n",
      "epoch 4  | loss: 1.00228 |  0:00:00s\n",
      "epoch 5  | loss: 0.93208 |  0:00:00s\n",
      "epoch 6  | loss: 0.87139 |  0:00:00s\n",
      "epoch 7  | loss: 0.84045 |  0:00:00s\n",
      "epoch 8  | loss: 0.81209 |  0:00:00s\n",
      "epoch 9  | loss: 0.7843  |  0:00:00s\n",
      "epoch 10 | loss: 0.74332 |  0:00:00s\n",
      "epoch 11 | loss: 0.74062 |  0:00:00s\n",
      "epoch 12 | loss: 0.73752 |  0:00:00s\n",
      "epoch 13 | loss: 0.738   |  0:00:00s\n",
      "epoch 14 | loss: 0.70291 |  0:00:00s\n",
      "epoch 15 | loss: 0.68456 |  0:00:00s\n",
      "epoch 16 | loss: 0.67312 |  0:00:00s\n",
      "epoch 17 | loss: 0.65493 |  0:00:00s\n",
      "epoch 18 | loss: 0.65364 |  0:00:00s\n",
      "epoch 19 | loss: 0.65234 |  0:00:00s\n",
      "epoch 20 | loss: 0.64781 |  0:00:01s\n",
      "epoch 21 | loss: 0.64139 |  0:00:01s\n",
      "epoch 22 | loss: 0.63201 |  0:00:01s\n",
      "epoch 23 | loss: 0.62466 |  0:00:01s\n",
      "epoch 24 | loss: 0.61    |  0:00:01s\n",
      "epoch 25 | loss: 0.6011  |  0:00:01s\n",
      "epoch 26 | loss: 0.59483 |  0:00:01s\n",
      "epoch 27 | loss: 0.59141 |  0:00:01s\n",
      "epoch 28 | loss: 0.58998 |  0:00:01s\n",
      "epoch 29 | loss: 0.58344 |  0:00:01s\n",
      "epoch 30 | loss: 0.57371 |  0:00:01s\n",
      "epoch 31 | loss: 0.56778 |  0:00:01s\n",
      "epoch 32 | loss: 0.56813 |  0:00:01s\n",
      "epoch 33 | loss: 0.57346 |  0:00:01s\n",
      "epoch 34 | loss: 0.57949 |  0:00:01s\n",
      "epoch 35 | loss: 0.58689 |  0:00:01s\n",
      "epoch 36 | loss: 0.57122 |  0:00:01s\n",
      "epoch 37 | loss: 0.56594 |  0:00:01s\n",
      "epoch 38 | loss: 0.56659 |  0:00:01s\n",
      "epoch 39 | loss: 0.54915 |  0:00:01s\n",
      "epoch 40 | loss: 0.55969 |  0:00:01s\n",
      "epoch 41 | loss: 0.55326 |  0:00:02s\n",
      "epoch 42 | loss: 0.54168 |  0:00:02s\n",
      "epoch 43 | loss: 0.53901 |  0:00:02s\n",
      "epoch 44 | loss: 0.53369 |  0:00:02s\n",
      "epoch 45 | loss: 0.54126 |  0:00:02s\n",
      "epoch 46 | loss: 0.52825 |  0:00:02s\n",
      "epoch 47 | loss: 0.51011 |  0:00:02s\n",
      "epoch 48 | loss: 0.5222  |  0:00:02s\n",
      "epoch 49 | loss: 0.5111  |  0:00:02s\n",
      "epoch 50 | loss: 0.51444 |  0:00:02s\n",
      "epoch 51 | loss: 0.5126  |  0:00:02s\n",
      "epoch 52 | loss: 0.50481 |  0:00:02s\n",
      "epoch 53 | loss: 0.50676 |  0:00:02s\n",
      "epoch 54 | loss: 0.50931 |  0:00:02s\n",
      "epoch 55 | loss: 0.50297 |  0:00:02s\n",
      "epoch 56 | loss: 0.50513 |  0:00:02s\n",
      "epoch 57 | loss: 0.50517 |  0:00:02s\n",
      "epoch 58 | loss: 0.49613 |  0:00:02s\n",
      "epoch 59 | loss: 0.49459 |  0:00:02s\n",
      "epoch 60 | loss: 0.49627 |  0:00:02s\n",
      "epoch 61 | loss: 0.49982 |  0:00:03s\n",
      "epoch 62 | loss: 0.49867 |  0:00:03s\n",
      "epoch 63 | loss: 0.49645 |  0:00:03s\n",
      "epoch 64 | loss: 0.48454 |  0:00:03s\n",
      "epoch 65 | loss: 0.47759 |  0:00:03s\n",
      "epoch 66 | loss: 0.47808 |  0:00:03s\n",
      "epoch 67 | loss: 0.48184 |  0:00:03s\n",
      "epoch 68 | loss: 0.47143 |  0:00:03s\n",
      "epoch 69 | loss: 0.47011 |  0:00:03s\n",
      "epoch 70 | loss: 0.45602 |  0:00:03s\n",
      "epoch 71 | loss: 0.4449  |  0:00:03s\n",
      "epoch 72 | loss: 0.44592 |  0:00:03s\n",
      "epoch 73 | loss: 0.45084 |  0:00:03s\n",
      "epoch 74 | loss: 0.444   |  0:00:03s\n",
      "epoch 75 | loss: 0.4369  |  0:00:03s\n",
      "epoch 76 | loss: 0.43076 |  0:00:03s\n",
      "epoch 77 | loss: 0.42693 |  0:00:03s\n",
      "epoch 78 | loss: 0.43006 |  0:00:03s\n",
      "epoch 79 | loss: 0.42223 |  0:00:03s\n",
      "epoch 80 | loss: 0.4249  |  0:00:03s\n",
      "epoch 81 | loss: 0.4235  |  0:00:03s\n",
      "epoch 82 | loss: 0.41749 |  0:00:03s\n",
      "epoch 83 | loss: 0.41858 |  0:00:04s\n",
      "epoch 84 | loss: 0.42893 |  0:00:04s\n",
      "epoch 85 | loss: 0.42297 |  0:00:04s\n",
      "epoch 86 | loss: 0.42771 |  0:00:04s\n",
      "epoch 87 | loss: 0.41672 |  0:00:04s\n",
      "epoch 88 | loss: 0.41965 |  0:00:04s\n",
      "epoch 89 | loss: 0.40917 |  0:00:04s\n",
      "epoch 90 | loss: 0.40222 |  0:00:04s\n",
      "epoch 91 | loss: 0.39761 |  0:00:04s\n",
      "epoch 92 | loss: 0.40956 |  0:00:04s\n",
      "epoch 93 | loss: 0.38995 |  0:00:04s\n",
      "epoch 94 | loss: 0.40154 |  0:00:04s\n",
      "epoch 95 | loss: 0.39481 |  0:00:04s\n",
      "epoch 96 | loss: 0.39401 |  0:00:04s\n",
      "epoch 97 | loss: 0.38717 |  0:00:04s\n",
      "epoch 98 | loss: 0.37223 |  0:00:04s\n",
      "epoch 99 | loss: 0.39223 |  0:00:04s\n",
      "epoch 100| loss: 0.37552 |  0:00:04s\n",
      "epoch 101| loss: 0.38866 |  0:00:04s\n",
      "epoch 102| loss: 0.3921  |  0:00:04s\n",
      "epoch 103| loss: 0.39781 |  0:00:04s\n",
      "epoch 104| loss: 0.3982  |  0:00:04s\n",
      "epoch 105| loss: 0.396   |  0:00:05s\n",
      "epoch 106| loss: 0.41994 |  0:00:05s\n",
      "epoch 107| loss: 0.4149  |  0:00:05s\n",
      "epoch 108| loss: 0.40532 |  0:00:05s\n",
      "epoch 109| loss: 0.39398 |  0:00:05s\n",
      "epoch 110| loss: 0.39535 |  0:00:05s\n",
      "epoch 111| loss: 0.39182 |  0:00:05s\n",
      "epoch 112| loss: 0.38532 |  0:00:05s\n",
      "epoch 113| loss: 0.38362 |  0:00:05s\n",
      "epoch 114| loss: 0.37151 |  0:00:05s\n",
      "epoch 115| loss: 0.37029 |  0:00:05s\n",
      "epoch 116| loss: 0.36615 |  0:00:05s\n",
      "epoch 117| loss: 0.36273 |  0:00:05s\n",
      "epoch 118| loss: 0.36397 |  0:00:05s\n",
      "epoch 119| loss: 0.36078 |  0:00:05s\n",
      "epoch 120| loss: 0.35451 |  0:00:05s\n",
      "epoch 121| loss: 0.37071 |  0:00:05s\n",
      "epoch 122| loss: 0.36094 |  0:00:05s\n",
      "epoch 123| loss: 0.35748 |  0:00:05s\n",
      "epoch 124| loss: 0.36278 |  0:00:05s\n",
      "epoch 125| loss: 0.37409 |  0:00:05s\n",
      "epoch 126| loss: 0.35632 |  0:00:05s\n",
      "epoch 127| loss: 0.36415 |  0:00:06s\n",
      "epoch 128| loss: 0.35622 |  0:00:06s\n",
      "epoch 129| loss: 0.35124 |  0:00:06s\n",
      "epoch 130| loss: 0.36196 |  0:00:06s\n",
      "epoch 131| loss: 0.36869 |  0:00:06s\n",
      "epoch 132| loss: 0.37847 |  0:00:06s\n",
      "epoch 133| loss: 0.36645 |  0:00:06s\n",
      "epoch 134| loss: 0.3718  |  0:00:06s\n",
      "epoch 135| loss: 0.3753  |  0:00:06s\n",
      "epoch 136| loss: 0.36587 |  0:00:06s\n",
      "epoch 137| loss: 0.37495 |  0:00:06s\n",
      "epoch 138| loss: 0.38076 |  0:00:06s\n",
      "epoch 139| loss: 0.38321 |  0:00:06s\n",
      "epoch 140| loss: 0.38943 |  0:00:06s\n",
      "epoch 141| loss: 0.37428 |  0:00:06s\n",
      "epoch 142| loss: 0.38431 |  0:00:06s\n",
      "epoch 143| loss: 0.38196 |  0:00:06s\n",
      "epoch 144| loss: 0.37315 |  0:00:06s\n",
      "epoch 145| loss: 0.3702  |  0:00:06s\n",
      "epoch 146| loss: 0.36353 |  0:00:07s\n",
      "epoch 147| loss: 0.36535 |  0:00:07s\n",
      "epoch 148| loss: 0.36038 |  0:00:07s\n",
      "epoch 149| loss: 0.37931 |  0:00:07s\n",
      "epoch 150| loss: 0.37418 |  0:00:07s\n",
      "epoch 151| loss: 0.3653  |  0:00:07s\n",
      "epoch 152| loss: 0.35268 |  0:00:07s\n",
      "epoch 153| loss: 0.34531 |  0:00:07s\n",
      "epoch 154| loss: 0.34556 |  0:00:07s\n",
      "epoch 155| loss: 0.35078 |  0:00:07s\n",
      "epoch 156| loss: 0.34257 |  0:00:07s\n",
      "epoch 157| loss: 0.33926 |  0:00:07s\n",
      "epoch 158| loss: 0.33251 |  0:00:07s\n",
      "epoch 159| loss: 0.32473 |  0:00:07s\n",
      "epoch 160| loss: 0.32733 |  0:00:07s\n",
      "epoch 161| loss: 0.32292 |  0:00:07s\n",
      "epoch 162| loss: 0.31937 |  0:00:07s\n",
      "epoch 163| loss: 0.31992 |  0:00:07s\n",
      "epoch 164| loss: 0.31465 |  0:00:07s\n",
      "epoch 165| loss: 0.31404 |  0:00:07s\n",
      "epoch 166| loss: 0.30888 |  0:00:07s\n",
      "epoch 167| loss: 0.31331 |  0:00:07s\n",
      "epoch 168| loss: 0.3108  |  0:00:08s\n",
      "epoch 169| loss: 0.3106  |  0:00:08s\n",
      "epoch 170| loss: 0.31791 |  0:00:08s\n",
      "epoch 171| loss: 0.3115  |  0:00:08s\n",
      "epoch 172| loss: 0.31084 |  0:00:08s\n",
      "epoch 173| loss: 0.30666 |  0:00:08s\n",
      "epoch 174| loss: 0.30302 |  0:00:08s\n",
      "epoch 175| loss: 0.30061 |  0:00:08s\n",
      "epoch 176| loss: 0.31662 |  0:00:08s\n",
      "epoch 177| loss: 0.31913 |  0:00:08s\n",
      "epoch 178| loss: 0.31414 |  0:00:08s\n",
      "epoch 179| loss: 0.32099 |  0:00:08s\n",
      "epoch 180| loss: 0.32081 |  0:00:08s\n",
      "epoch 181| loss: 0.30787 |  0:00:08s\n",
      "epoch 182| loss: 0.31232 |  0:00:08s\n",
      "epoch 183| loss: 0.31396 |  0:00:08s\n",
      "epoch 184| loss: 0.31124 |  0:00:08s\n",
      "epoch 185| loss: 0.30193 |  0:00:08s\n",
      "epoch 186| loss: 0.31083 |  0:00:08s\n",
      "epoch 187| loss: 0.31142 |  0:00:08s\n",
      "epoch 188| loss: 0.30848 |  0:00:08s\n",
      "epoch 189| loss: 0.30416 |  0:00:08s\n",
      "epoch 190| loss: 0.30591 |  0:00:09s\n",
      "epoch 191| loss: 0.29798 |  0:00:09s\n",
      "epoch 192| loss: 0.29852 |  0:00:09s\n",
      "epoch 193| loss: 0.29459 |  0:00:09s\n",
      "epoch 194| loss: 0.29599 |  0:00:09s\n",
      "epoch 195| loss: 0.2886  |  0:00:09s\n",
      "epoch 196| loss: 0.29663 |  0:00:09s\n",
      "epoch 197| loss: 0.29001 |  0:00:09s\n",
      "epoch 198| loss: 0.29543 |  0:00:09s\n",
      "epoch 199| loss: 0.29942 |  0:00:09s\n",
      "epoch 200| loss: 0.29935 |  0:00:09s\n",
      "epoch 201| loss: 0.3054  |  0:00:09s\n",
      "epoch 202| loss: 0.31136 |  0:00:09s\n",
      "epoch 203| loss: 0.33284 |  0:00:09s\n",
      "epoch 204| loss: 0.33779 |  0:00:09s\n",
      "epoch 205| loss: 0.33981 |  0:00:09s\n",
      "epoch 206| loss: 0.33816 |  0:00:09s\n",
      "epoch 207| loss: 0.33352 |  0:00:09s\n",
      "epoch 208| loss: 0.33404 |  0:00:09s\n",
      "epoch 209| loss: 0.3226  |  0:00:09s\n",
      "epoch 210| loss: 0.31882 |  0:00:10s\n",
      "epoch 211| loss: 0.3117  |  0:00:10s\n",
      "epoch 212| loss: 0.31923 |  0:00:10s\n",
      "epoch 213| loss: 0.32635 |  0:00:10s\n",
      "epoch 214| loss: 0.31911 |  0:00:10s\n",
      "epoch 215| loss: 0.32428 |  0:00:10s\n",
      "epoch 216| loss: 0.343   |  0:00:10s\n",
      "epoch 217| loss: 0.3413  |  0:00:10s\n",
      "epoch 218| loss: 0.32859 |  0:00:10s\n",
      "epoch 219| loss: 0.3239  |  0:00:10s\n",
      "epoch 220| loss: 0.31803 |  0:00:10s\n",
      "epoch 221| loss: 0.31655 |  0:00:10s\n",
      "epoch 222| loss: 0.31959 |  0:00:10s\n",
      "epoch 223| loss: 0.30608 |  0:00:10s\n",
      "epoch 224| loss: 0.31313 |  0:00:10s\n",
      "epoch 225| loss: 0.32692 |  0:00:10s\n",
      "epoch 226| loss: 0.31392 |  0:00:10s\n",
      "epoch 227| loss: 0.3038  |  0:00:10s\n",
      "epoch 228| loss: 0.30674 |  0:00:10s\n",
      "epoch 229| loss: 0.30323 |  0:00:11s\n",
      "epoch 230| loss: 0.30052 |  0:00:11s\n",
      "epoch 231| loss: 0.29424 |  0:00:11s\n",
      "epoch 232| loss: 0.29022 |  0:00:11s\n",
      "epoch 233| loss: 0.28451 |  0:00:11s\n",
      "epoch 234| loss: 0.2833  |  0:00:11s\n",
      "epoch 235| loss: 0.28661 |  0:00:11s\n",
      "epoch 236| loss: 0.28201 |  0:00:11s\n",
      "epoch 237| loss: 0.2825  |  0:00:11s\n",
      "epoch 238| loss: 0.27999 |  0:00:11s\n",
      "epoch 239| loss: 0.28085 |  0:00:11s\n",
      "epoch 240| loss: 0.27776 |  0:00:11s\n",
      "epoch 241| loss: 0.28051 |  0:00:11s\n",
      "epoch 242| loss: 0.27745 |  0:00:11s\n",
      "epoch 243| loss: 0.2707  |  0:00:11s\n",
      "epoch 244| loss: 0.26258 |  0:00:11s\n",
      "epoch 245| loss: 0.2692  |  0:00:11s\n",
      "epoch 246| loss: 0.26624 |  0:00:11s\n",
      "epoch 247| loss: 0.26501 |  0:00:11s\n",
      "epoch 248| loss: 0.26053 |  0:00:12s\n",
      "epoch 249| loss: 0.26342 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.25521 |  0:00:00s\n",
      "epoch 1  | loss: 2.03271 |  0:00:00s\n",
      "epoch 2  | loss: 1.45353 |  0:00:00s\n",
      "epoch 3  | loss: 1.16746 |  0:00:00s\n",
      "epoch 4  | loss: 1.01914 |  0:00:00s\n",
      "epoch 5  | loss: 0.9476  |  0:00:00s\n",
      "epoch 6  | loss: 0.89499 |  0:00:00s\n",
      "epoch 7  | loss: 0.82952 |  0:00:00s\n",
      "epoch 8  | loss: 0.79656 |  0:00:00s\n",
      "epoch 9  | loss: 0.77926 |  0:00:00s\n",
      "epoch 10 | loss: 0.73839 |  0:00:00s\n",
      "epoch 11 | loss: 0.73328 |  0:00:00s\n",
      "epoch 12 | loss: 0.70578 |  0:00:00s\n",
      "epoch 13 | loss: 0.69773 |  0:00:00s\n",
      "epoch 14 | loss: 0.68548 |  0:00:00s\n",
      "epoch 15 | loss: 0.67363 |  0:00:00s\n",
      "epoch 16 | loss: 0.65582 |  0:00:00s\n",
      "epoch 17 | loss: 0.6551  |  0:00:00s\n",
      "epoch 18 | loss: 0.64868 |  0:00:01s\n",
      "epoch 19 | loss: 0.64852 |  0:00:01s\n",
      "epoch 20 | loss: 0.63595 |  0:00:01s\n",
      "epoch 21 | loss: 0.63231 |  0:00:01s\n",
      "epoch 22 | loss: 0.62435 |  0:00:01s\n",
      "epoch 23 | loss: 0.61657 |  0:00:01s\n",
      "epoch 24 | loss: 0.61474 |  0:00:01s\n",
      "epoch 25 | loss: 0.61313 |  0:00:01s\n",
      "epoch 26 | loss: 0.61599 |  0:00:01s\n",
      "epoch 27 | loss: 0.62023 |  0:00:01s\n",
      "epoch 28 | loss: 0.61406 |  0:00:01s\n",
      "epoch 29 | loss: 0.61467 |  0:00:01s\n",
      "epoch 30 | loss: 0.59929 |  0:00:01s\n",
      "epoch 31 | loss: 0.59843 |  0:00:01s\n",
      "epoch 32 | loss: 0.59376 |  0:00:01s\n",
      "epoch 33 | loss: 0.59171 |  0:00:01s\n",
      "epoch 34 | loss: 0.58677 |  0:00:01s\n",
      "epoch 35 | loss: 0.58287 |  0:00:01s\n",
      "epoch 36 | loss: 0.58964 |  0:00:01s\n",
      "epoch 37 | loss: 0.58176 |  0:00:02s\n",
      "epoch 38 | loss: 0.57913 |  0:00:02s\n",
      "epoch 39 | loss: 0.56487 |  0:00:02s\n",
      "epoch 40 | loss: 0.57369 |  0:00:02s\n",
      "epoch 41 | loss: 0.56928 |  0:00:02s\n",
      "epoch 42 | loss: 0.56654 |  0:00:02s\n",
      "epoch 43 | loss: 0.55102 |  0:00:02s\n",
      "epoch 44 | loss: 0.54865 |  0:00:02s\n",
      "epoch 45 | loss: 0.55088 |  0:00:02s\n",
      "epoch 46 | loss: 0.54446 |  0:00:02s\n",
      "epoch 47 | loss: 0.53079 |  0:00:02s\n",
      "epoch 48 | loss: 0.53908 |  0:00:02s\n",
      "epoch 49 | loss: 0.53478 |  0:00:02s\n",
      "epoch 50 | loss: 0.54179 |  0:00:02s\n",
      "epoch 51 | loss: 0.54495 |  0:00:02s\n",
      "epoch 52 | loss: 0.53186 |  0:00:02s\n",
      "epoch 53 | loss: 0.52292 |  0:00:02s\n",
      "epoch 54 | loss: 0.5263  |  0:00:02s\n",
      "epoch 55 | loss: 0.52561 |  0:00:02s\n",
      "epoch 56 | loss: 0.52532 |  0:00:03s\n",
      "epoch 57 | loss: 0.51259 |  0:00:03s\n",
      "epoch 58 | loss: 0.49901 |  0:00:03s\n",
      "epoch 59 | loss: 0.50848 |  0:00:03s\n",
      "epoch 60 | loss: 0.50417 |  0:00:03s\n",
      "epoch 61 | loss: 0.49358 |  0:00:03s\n",
      "epoch 62 | loss: 0.49349 |  0:00:03s\n",
      "epoch 63 | loss: 0.48667 |  0:00:03s\n",
      "epoch 64 | loss: 0.49428 |  0:00:03s\n",
      "epoch 65 | loss: 0.4879  |  0:00:03s\n",
      "epoch 66 | loss: 0.49677 |  0:00:03s\n",
      "epoch 67 | loss: 0.5106  |  0:00:03s\n",
      "epoch 68 | loss: 0.50077 |  0:00:03s\n",
      "epoch 69 | loss: 0.5042  |  0:00:03s\n",
      "epoch 70 | loss: 0.5003  |  0:00:03s\n",
      "epoch 71 | loss: 0.49864 |  0:00:03s\n",
      "epoch 72 | loss: 0.49389 |  0:00:04s\n",
      "epoch 73 | loss: 0.49536 |  0:00:04s\n",
      "epoch 74 | loss: 0.49165 |  0:00:04s\n",
      "epoch 75 | loss: 0.48882 |  0:00:04s\n",
      "epoch 76 | loss: 0.48118 |  0:00:04s\n",
      "epoch 77 | loss: 0.47275 |  0:00:04s\n",
      "epoch 78 | loss: 0.47462 |  0:00:04s\n",
      "epoch 79 | loss: 0.48351 |  0:00:04s\n",
      "epoch 80 | loss: 0.48656 |  0:00:04s\n",
      "epoch 81 | loss: 0.48115 |  0:00:04s\n",
      "epoch 82 | loss: 0.48211 |  0:00:04s\n",
      "epoch 83 | loss: 0.48789 |  0:00:04s\n",
      "epoch 84 | loss: 0.49386 |  0:00:04s\n",
      "epoch 85 | loss: 0.49331 |  0:00:04s\n",
      "epoch 86 | loss: 0.49975 |  0:00:04s\n",
      "epoch 87 | loss: 0.50149 |  0:00:04s\n",
      "epoch 88 | loss: 0.49423 |  0:00:04s\n",
      "epoch 89 | loss: 0.49364 |  0:00:04s\n",
      "epoch 90 | loss: 0.48097 |  0:00:04s\n",
      "epoch 91 | loss: 0.48415 |  0:00:05s\n",
      "epoch 92 | loss: 0.48689 |  0:00:05s\n",
      "epoch 93 | loss: 0.46821 |  0:00:05s\n",
      "epoch 94 | loss: 0.48361 |  0:00:05s\n",
      "epoch 95 | loss: 0.4652  |  0:00:05s\n",
      "epoch 96 | loss: 0.46042 |  0:00:05s\n",
      "epoch 97 | loss: 0.46019 |  0:00:05s\n",
      "epoch 98 | loss: 0.45206 |  0:00:05s\n",
      "epoch 99 | loss: 0.44693 |  0:00:05s\n",
      "epoch 100| loss: 0.43833 |  0:00:05s\n",
      "epoch 101| loss: 0.43872 |  0:00:05s\n",
      "epoch 102| loss: 0.43781 |  0:00:05s\n",
      "epoch 103| loss: 0.43535 |  0:00:05s\n",
      "epoch 104| loss: 0.44092 |  0:00:05s\n",
      "epoch 105| loss: 0.43274 |  0:00:05s\n",
      "epoch 106| loss: 0.4543  |  0:00:05s\n",
      "epoch 107| loss: 0.44717 |  0:00:05s\n",
      "epoch 108| loss: 0.43743 |  0:00:05s\n",
      "epoch 109| loss: 0.43725 |  0:00:05s\n",
      "epoch 110| loss: 0.42883 |  0:00:06s\n",
      "epoch 111| loss: 0.42949 |  0:00:06s\n",
      "epoch 112| loss: 0.41962 |  0:00:06s\n",
      "epoch 113| loss: 0.42093 |  0:00:06s\n",
      "epoch 114| loss: 0.41489 |  0:00:06s\n",
      "epoch 115| loss: 0.40623 |  0:00:06s\n",
      "epoch 116| loss: 0.40897 |  0:00:06s\n",
      "epoch 117| loss: 0.40992 |  0:00:06s\n",
      "epoch 118| loss: 0.41057 |  0:00:06s\n",
      "epoch 119| loss: 0.40576 |  0:00:06s\n",
      "epoch 120| loss: 0.40718 |  0:00:06s\n",
      "epoch 121| loss: 0.40571 |  0:00:06s\n",
      "epoch 122| loss: 0.39775 |  0:00:06s\n",
      "epoch 123| loss: 0.3982  |  0:00:06s\n",
      "epoch 124| loss: 0.39547 |  0:00:06s\n",
      "epoch 125| loss: 0.4026  |  0:00:06s\n",
      "epoch 126| loss: 0.40394 |  0:00:06s\n",
      "epoch 127| loss: 0.39937 |  0:00:06s\n",
      "epoch 128| loss: 0.39685 |  0:00:06s\n",
      "epoch 129| loss: 0.38937 |  0:00:07s\n",
      "epoch 130| loss: 0.38975 |  0:00:07s\n",
      "epoch 131| loss: 0.38708 |  0:00:07s\n",
      "epoch 132| loss: 0.38547 |  0:00:07s\n",
      "epoch 133| loss: 0.38657 |  0:00:07s\n",
      "epoch 134| loss: 0.38855 |  0:00:07s\n",
      "epoch 135| loss: 0.3731  |  0:00:07s\n",
      "epoch 136| loss: 0.3697  |  0:00:07s\n",
      "epoch 137| loss: 0.37109 |  0:00:07s\n",
      "epoch 138| loss: 0.36873 |  0:00:07s\n",
      "epoch 139| loss: 0.36834 |  0:00:07s\n",
      "epoch 140| loss: 0.38124 |  0:00:07s\n",
      "epoch 141| loss: 0.37123 |  0:00:07s\n",
      "epoch 142| loss: 0.37261 |  0:00:07s\n",
      "epoch 143| loss: 0.3778  |  0:00:07s\n",
      "epoch 144| loss: 0.37247 |  0:00:07s\n",
      "epoch 145| loss: 0.37033 |  0:00:07s\n",
      "epoch 146| loss: 0.37821 |  0:00:07s\n",
      "epoch 147| loss: 0.36966 |  0:00:07s\n",
      "epoch 148| loss: 0.36606 |  0:00:08s\n",
      "epoch 149| loss: 0.36195 |  0:00:08s\n",
      "epoch 150| loss: 0.36605 |  0:00:08s\n",
      "epoch 151| loss: 0.35978 |  0:00:08s\n",
      "epoch 152| loss: 0.35879 |  0:00:08s\n",
      "epoch 153| loss: 0.35434 |  0:00:08s\n",
      "epoch 154| loss: 0.35699 |  0:00:08s\n",
      "epoch 155| loss: 0.36006 |  0:00:08s\n",
      "epoch 156| loss: 0.34588 |  0:00:08s\n",
      "epoch 157| loss: 0.35995 |  0:00:08s\n",
      "epoch 158| loss: 0.36321 |  0:00:08s\n",
      "epoch 159| loss: 0.35608 |  0:00:08s\n",
      "epoch 160| loss: 0.35311 |  0:00:08s\n",
      "epoch 161| loss: 0.35692 |  0:00:08s\n",
      "epoch 162| loss: 0.34767 |  0:00:08s\n",
      "epoch 163| loss: 0.34351 |  0:00:08s\n",
      "epoch 164| loss: 0.34748 |  0:00:08s\n",
      "epoch 165| loss: 0.347   |  0:00:08s\n",
      "epoch 166| loss: 0.34381 |  0:00:08s\n",
      "epoch 167| loss: 0.33879 |  0:00:09s\n",
      "epoch 168| loss: 0.34562 |  0:00:09s\n",
      "epoch 169| loss: 0.34083 |  0:00:09s\n",
      "epoch 170| loss: 0.33812 |  0:00:09s\n",
      "epoch 171| loss: 0.33539 |  0:00:09s\n",
      "epoch 172| loss: 0.3331  |  0:00:09s\n",
      "epoch 173| loss: 0.3297  |  0:00:09s\n",
      "epoch 174| loss: 0.33193 |  0:00:09s\n",
      "epoch 175| loss: 0.33215 |  0:00:09s\n",
      "epoch 176| loss: 0.34733 |  0:00:09s\n",
      "epoch 177| loss: 0.33818 |  0:00:09s\n",
      "epoch 178| loss: 0.33166 |  0:00:09s\n",
      "epoch 179| loss: 0.33344 |  0:00:09s\n",
      "epoch 180| loss: 0.32728 |  0:00:09s\n",
      "epoch 181| loss: 0.32171 |  0:00:09s\n",
      "epoch 182| loss: 0.33195 |  0:00:09s\n",
      "epoch 183| loss: 0.33785 |  0:00:09s\n",
      "epoch 184| loss: 0.3434  |  0:00:09s\n",
      "epoch 185| loss: 0.33825 |  0:00:09s\n",
      "epoch 186| loss: 0.3371  |  0:00:09s\n",
      "epoch 187| loss: 0.33694 |  0:00:10s\n",
      "epoch 188| loss: 0.32753 |  0:00:10s\n",
      "epoch 189| loss: 0.33069 |  0:00:10s\n",
      "epoch 190| loss: 0.32981 |  0:00:10s\n",
      "epoch 191| loss: 0.32111 |  0:00:10s\n",
      "epoch 192| loss: 0.31666 |  0:00:10s\n",
      "epoch 193| loss: 0.3178  |  0:00:10s\n",
      "epoch 194| loss: 0.31812 |  0:00:10s\n",
      "epoch 195| loss: 0.30214 |  0:00:10s\n",
      "epoch 196| loss: 0.31361 |  0:00:10s\n",
      "epoch 197| loss: 0.29795 |  0:00:10s\n",
      "epoch 198| loss: 0.30494 |  0:00:10s\n",
      "epoch 199| loss: 0.31036 |  0:00:10s\n",
      "epoch 200| loss: 0.30366 |  0:00:10s\n",
      "epoch 201| loss: 0.30321 |  0:00:10s\n",
      "epoch 202| loss: 0.29918 |  0:00:10s\n",
      "epoch 203| loss: 0.29399 |  0:00:10s\n",
      "epoch 204| loss: 0.29466 |  0:00:10s\n",
      "epoch 205| loss: 0.29837 |  0:00:10s\n",
      "epoch 206| loss: 0.30893 |  0:00:11s\n",
      "epoch 207| loss: 0.30166 |  0:00:11s\n",
      "epoch 208| loss: 0.29861 |  0:00:11s\n",
      "epoch 209| loss: 0.29401 |  0:00:11s\n",
      "epoch 210| loss: 0.29261 |  0:00:11s\n",
      "epoch 211| loss: 0.31403 |  0:00:11s\n",
      "epoch 212| loss: 0.31748 |  0:00:11s\n",
      "epoch 213| loss: 0.33504 |  0:00:11s\n",
      "epoch 214| loss: 0.31966 |  0:00:11s\n",
      "epoch 215| loss: 0.32426 |  0:00:11s\n",
      "epoch 216| loss: 0.32675 |  0:00:11s\n",
      "epoch 217| loss: 0.32185 |  0:00:11s\n",
      "epoch 218| loss: 0.31115 |  0:00:11s\n",
      "epoch 219| loss: 0.30512 |  0:00:11s\n",
      "epoch 220| loss: 0.30981 |  0:00:11s\n",
      "epoch 221| loss: 0.31026 |  0:00:11s\n",
      "epoch 222| loss: 0.31011 |  0:00:11s\n",
      "epoch 223| loss: 0.30323 |  0:00:11s\n",
      "epoch 224| loss: 0.29855 |  0:00:11s\n",
      "epoch 225| loss: 0.30233 |  0:00:11s\n",
      "epoch 226| loss: 0.30311 |  0:00:12s\n",
      "epoch 227| loss: 0.29199 |  0:00:12s\n",
      "epoch 228| loss: 0.3003  |  0:00:12s\n",
      "epoch 229| loss: 0.30385 |  0:00:12s\n",
      "epoch 230| loss: 0.29538 |  0:00:12s\n",
      "epoch 231| loss: 0.2874  |  0:00:12s\n",
      "epoch 232| loss: 0.2893  |  0:00:12s\n",
      "epoch 233| loss: 0.28882 |  0:00:12s\n",
      "epoch 234| loss: 0.28686 |  0:00:12s\n",
      "epoch 235| loss: 0.28366 |  0:00:12s\n",
      "epoch 236| loss: 0.28057 |  0:00:12s\n",
      "epoch 237| loss: 0.27509 |  0:00:12s\n",
      "epoch 238| loss: 0.26885 |  0:00:12s\n",
      "epoch 239| loss: 0.27371 |  0:00:12s\n",
      "epoch 240| loss: 0.27083 |  0:00:12s\n",
      "epoch 241| loss: 0.26927 |  0:00:12s\n",
      "epoch 242| loss: 0.26881 |  0:00:12s\n",
      "epoch 243| loss: 0.26171 |  0:00:12s\n",
      "epoch 244| loss: 0.26582 |  0:00:12s\n",
      "epoch 245| loss: 0.26233 |  0:00:13s\n",
      "epoch 246| loss: 0.26353 |  0:00:13s\n",
      "epoch 247| loss: 0.26606 |  0:00:13s\n",
      "epoch 248| loss: 0.25838 |  0:00:13s\n",
      "epoch 249| loss: 0.2626  |  0:00:13s\n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.13151 |  0:00:00s\n",
      "epoch 1  | loss: 2.32712 |  0:00:00s\n",
      "epoch 2  | loss: 1.77067 |  0:00:00s\n",
      "epoch 3  | loss: 1.37885 |  0:00:00s\n",
      "epoch 4  | loss: 1.12198 |  0:00:00s\n",
      "epoch 5  | loss: 0.99169 |  0:00:00s\n",
      "epoch 6  | loss: 0.88149 |  0:00:00s\n",
      "epoch 7  | loss: 0.84903 |  0:00:00s\n",
      "epoch 8  | loss: 0.818   |  0:00:00s\n",
      "epoch 9  | loss: 0.79185 |  0:00:00s\n",
      "epoch 10 | loss: 0.75085 |  0:00:00s\n",
      "epoch 11 | loss: 0.73867 |  0:00:00s\n",
      "epoch 12 | loss: 0.72169 |  0:00:00s\n",
      "epoch 13 | loss: 0.70296 |  0:00:00s\n",
      "epoch 14 | loss: 0.68238 |  0:00:00s\n",
      "epoch 15 | loss: 0.67111 |  0:00:00s\n",
      "epoch 16 | loss: 0.65123 |  0:00:00s\n",
      "epoch 17 | loss: 0.64188 |  0:00:00s\n",
      "epoch 18 | loss: 0.62413 |  0:00:00s\n",
      "epoch 19 | loss: 0.62598 |  0:00:01s\n",
      "epoch 20 | loss: 0.60966 |  0:00:01s\n",
      "epoch 21 | loss: 0.611   |  0:00:01s\n",
      "epoch 22 | loss: 0.60504 |  0:00:01s\n",
      "epoch 23 | loss: 0.6049  |  0:00:01s\n",
      "epoch 24 | loss: 0.59904 |  0:00:01s\n",
      "epoch 25 | loss: 0.58471 |  0:00:01s\n",
      "epoch 26 | loss: 0.59506 |  0:00:01s\n",
      "epoch 27 | loss: 0.58476 |  0:00:01s\n",
      "epoch 28 | loss: 0.58698 |  0:00:01s\n",
      "epoch 29 | loss: 0.58205 |  0:00:01s\n",
      "epoch 30 | loss: 0.57136 |  0:00:01s\n",
      "epoch 31 | loss: 0.5716  |  0:00:01s\n",
      "epoch 32 | loss: 0.56999 |  0:00:01s\n",
      "epoch 33 | loss: 0.57148 |  0:00:01s\n",
      "epoch 34 | loss: 0.55729 |  0:00:01s\n",
      "epoch 35 | loss: 0.55443 |  0:00:01s\n",
      "epoch 36 | loss: 0.54709 |  0:00:01s\n",
      "epoch 37 | loss: 0.54246 |  0:00:01s\n",
      "epoch 38 | loss: 0.54077 |  0:00:02s\n",
      "epoch 39 | loss: 0.52661 |  0:00:02s\n",
      "epoch 40 | loss: 0.53859 |  0:00:02s\n",
      "epoch 41 | loss: 0.53579 |  0:00:02s\n",
      "epoch 42 | loss: 0.52783 |  0:00:02s\n",
      "epoch 43 | loss: 0.51934 |  0:00:02s\n",
      "epoch 44 | loss: 0.5123  |  0:00:02s\n",
      "epoch 45 | loss: 0.52282 |  0:00:02s\n",
      "epoch 46 | loss: 0.52244 |  0:00:02s\n",
      "epoch 47 | loss: 0.51215 |  0:00:02s\n",
      "epoch 48 | loss: 0.51354 |  0:00:02s\n",
      "epoch 49 | loss: 0.51066 |  0:00:02s\n",
      "epoch 50 | loss: 0.49272 |  0:00:02s\n",
      "epoch 51 | loss: 0.508   |  0:00:02s\n",
      "epoch 52 | loss: 0.49464 |  0:00:02s\n",
      "epoch 53 | loss: 0.50851 |  0:00:02s\n",
      "epoch 54 | loss: 0.50546 |  0:00:02s\n",
      "epoch 55 | loss: 0.50162 |  0:00:02s\n",
      "epoch 56 | loss: 0.50664 |  0:00:02s\n",
      "epoch 57 | loss: 0.49223 |  0:00:02s\n",
      "epoch 58 | loss: 0.49751 |  0:00:03s\n",
      "epoch 59 | loss: 0.50724 |  0:00:03s\n",
      "epoch 60 | loss: 0.50212 |  0:00:03s\n",
      "epoch 61 | loss: 0.49883 |  0:00:03s\n",
      "epoch 62 | loss: 0.48064 |  0:00:03s\n",
      "epoch 63 | loss: 0.46955 |  0:00:03s\n",
      "epoch 64 | loss: 0.46876 |  0:00:03s\n",
      "epoch 65 | loss: 0.46269 |  0:00:03s\n",
      "epoch 66 | loss: 0.46743 |  0:00:03s\n",
      "epoch 67 | loss: 0.47798 |  0:00:03s\n",
      "epoch 68 | loss: 0.47977 |  0:00:03s\n",
      "epoch 69 | loss: 0.50211 |  0:00:03s\n",
      "epoch 70 | loss: 0.50377 |  0:00:03s\n",
      "epoch 71 | loss: 0.49073 |  0:00:03s\n",
      "epoch 72 | loss: 0.48555 |  0:00:03s\n",
      "epoch 73 | loss: 0.49009 |  0:00:03s\n",
      "epoch 74 | loss: 0.47736 |  0:00:03s\n",
      "epoch 75 | loss: 0.47491 |  0:00:03s\n",
      "epoch 76 | loss: 0.47189 |  0:00:03s\n",
      "epoch 77 | loss: 0.46684 |  0:00:04s\n",
      "epoch 78 | loss: 0.47827 |  0:00:04s\n",
      "epoch 79 | loss: 0.48033 |  0:00:04s\n",
      "epoch 80 | loss: 0.45935 |  0:00:04s\n",
      "epoch 81 | loss: 0.45427 |  0:00:04s\n",
      "epoch 82 | loss: 0.45578 |  0:00:04s\n",
      "epoch 83 | loss: 0.45506 |  0:00:04s\n",
      "epoch 84 | loss: 0.45052 |  0:00:04s\n",
      "epoch 85 | loss: 0.44698 |  0:00:04s\n",
      "epoch 86 | loss: 0.44983 |  0:00:04s\n",
      "epoch 87 | loss: 0.43293 |  0:00:04s\n",
      "epoch 88 | loss: 0.43178 |  0:00:04s\n",
      "epoch 89 | loss: 0.44465 |  0:00:04s\n",
      "epoch 90 | loss: 0.44276 |  0:00:04s\n",
      "epoch 91 | loss: 0.43099 |  0:00:04s\n",
      "epoch 92 | loss: 0.43863 |  0:00:04s\n",
      "epoch 93 | loss: 0.4284  |  0:00:04s\n",
      "epoch 94 | loss: 0.42661 |  0:00:04s\n",
      "epoch 95 | loss: 0.42327 |  0:00:04s\n",
      "epoch 96 | loss: 0.42166 |  0:00:05s\n",
      "epoch 97 | loss: 0.42026 |  0:00:05s\n",
      "epoch 98 | loss: 0.40702 |  0:00:05s\n",
      "epoch 99 | loss: 0.41048 |  0:00:05s\n",
      "epoch 100| loss: 0.41529 |  0:00:05s\n",
      "epoch 101| loss: 0.41387 |  0:00:05s\n",
      "epoch 102| loss: 0.41079 |  0:00:05s\n",
      "epoch 103| loss: 0.41173 |  0:00:05s\n",
      "epoch 104| loss: 0.41665 |  0:00:05s\n",
      "epoch 105| loss: 0.42016 |  0:00:05s\n",
      "epoch 106| loss: 0.42652 |  0:00:05s\n",
      "epoch 107| loss: 0.41527 |  0:00:05s\n",
      "epoch 108| loss: 0.41019 |  0:00:05s\n",
      "epoch 109| loss: 0.4053  |  0:00:05s\n",
      "epoch 110| loss: 0.40786 |  0:00:05s\n",
      "epoch 111| loss: 0.39453 |  0:00:05s\n",
      "epoch 112| loss: 0.39143 |  0:00:05s\n",
      "epoch 113| loss: 0.39537 |  0:00:05s\n",
      "epoch 114| loss: 0.38681 |  0:00:05s\n",
      "epoch 115| loss: 0.38612 |  0:00:06s\n",
      "epoch 116| loss: 0.37725 |  0:00:06s\n",
      "epoch 117| loss: 0.38111 |  0:00:06s\n",
      "epoch 118| loss: 0.37851 |  0:00:06s\n",
      "epoch 119| loss: 0.38303 |  0:00:06s\n",
      "epoch 120| loss: 0.37559 |  0:00:06s\n",
      "epoch 121| loss: 0.38168 |  0:00:06s\n",
      "epoch 122| loss: 0.37734 |  0:00:06s\n",
      "epoch 123| loss: 0.36943 |  0:00:06s\n",
      "epoch 124| loss: 0.38929 |  0:00:06s\n",
      "epoch 125| loss: 0.39246 |  0:00:06s\n",
      "epoch 126| loss: 0.38596 |  0:00:06s\n",
      "epoch 127| loss: 0.38878 |  0:00:06s\n",
      "epoch 128| loss: 0.3776  |  0:00:06s\n",
      "epoch 129| loss: 0.37933 |  0:00:06s\n",
      "epoch 130| loss: 0.37885 |  0:00:06s\n",
      "epoch 131| loss: 0.3724  |  0:00:06s\n",
      "epoch 132| loss: 0.37996 |  0:00:06s\n",
      "epoch 133| loss: 0.38565 |  0:00:06s\n",
      "epoch 134| loss: 0.37789 |  0:00:06s\n",
      "epoch 135| loss: 0.37609 |  0:00:07s\n",
      "epoch 136| loss: 0.38242 |  0:00:07s\n",
      "epoch 137| loss: 0.38201 |  0:00:07s\n",
      "epoch 138| loss: 0.3857  |  0:00:07s\n",
      "epoch 139| loss: 0.37448 |  0:00:07s\n",
      "epoch 140| loss: 0.37243 |  0:00:07s\n",
      "epoch 141| loss: 0.36447 |  0:00:07s\n",
      "epoch 142| loss: 0.36692 |  0:00:07s\n",
      "epoch 143| loss: 0.36562 |  0:00:07s\n",
      "epoch 144| loss: 0.36104 |  0:00:07s\n",
      "epoch 145| loss: 0.36043 |  0:00:07s\n",
      "epoch 146| loss: 0.36412 |  0:00:07s\n",
      "epoch 147| loss: 0.36409 |  0:00:07s\n",
      "epoch 148| loss: 0.35987 |  0:00:07s\n",
      "epoch 149| loss: 0.35212 |  0:00:07s\n",
      "epoch 150| loss: 0.35776 |  0:00:07s\n",
      "epoch 151| loss: 0.35962 |  0:00:08s\n",
      "epoch 152| loss: 0.35308 |  0:00:08s\n",
      "epoch 153| loss: 0.35414 |  0:00:08s\n",
      "epoch 154| loss: 0.35206 |  0:00:08s\n",
      "epoch 155| loss: 0.34829 |  0:00:08s\n",
      "epoch 156| loss: 0.34396 |  0:00:08s\n",
      "epoch 157| loss: 0.3495  |  0:00:08s\n",
      "epoch 158| loss: 0.33955 |  0:00:08s\n",
      "epoch 159| loss: 0.33901 |  0:00:08s\n",
      "epoch 160| loss: 0.34397 |  0:00:08s\n",
      "epoch 161| loss: 0.33859 |  0:00:08s\n",
      "epoch 162| loss: 0.3404  |  0:00:08s\n",
      "epoch 163| loss: 0.33785 |  0:00:08s\n",
      "epoch 164| loss: 0.33596 |  0:00:08s\n",
      "epoch 165| loss: 0.33604 |  0:00:08s\n",
      "epoch 166| loss: 0.32838 |  0:00:08s\n",
      "epoch 167| loss: 0.32974 |  0:00:08s\n",
      "epoch 168| loss: 0.34817 |  0:00:08s\n",
      "epoch 169| loss: 0.35279 |  0:00:08s\n",
      "epoch 170| loss: 0.35646 |  0:00:09s\n",
      "epoch 171| loss: 0.35398 |  0:00:09s\n",
      "epoch 172| loss: 0.34375 |  0:00:09s\n",
      "epoch 173| loss: 0.33815 |  0:00:09s\n",
      "epoch 174| loss: 0.3388  |  0:00:09s\n",
      "epoch 175| loss: 0.33365 |  0:00:09s\n",
      "epoch 176| loss: 0.33985 |  0:00:09s\n",
      "epoch 177| loss: 0.33422 |  0:00:09s\n",
      "epoch 178| loss: 0.32901 |  0:00:09s\n",
      "epoch 179| loss: 0.34635 |  0:00:09s\n",
      "epoch 180| loss: 0.34818 |  0:00:09s\n",
      "epoch 181| loss: 0.33372 |  0:00:09s\n",
      "epoch 182| loss: 0.33203 |  0:00:09s\n",
      "epoch 183| loss: 0.32769 |  0:00:09s\n",
      "epoch 184| loss: 0.32719 |  0:00:09s\n",
      "epoch 185| loss: 0.31329 |  0:00:09s\n",
      "epoch 186| loss: 0.31307 |  0:00:09s\n",
      "epoch 187| loss: 0.31032 |  0:00:09s\n",
      "epoch 188| loss: 0.30477 |  0:00:09s\n",
      "epoch 189| loss: 0.31307 |  0:00:10s\n",
      "epoch 190| loss: 0.32305 |  0:00:10s\n",
      "epoch 191| loss: 0.32728 |  0:00:10s\n",
      "epoch 192| loss: 0.32735 |  0:00:10s\n",
      "epoch 193| loss: 0.31679 |  0:00:10s\n",
      "epoch 194| loss: 0.32076 |  0:00:10s\n",
      "epoch 195| loss: 0.30255 |  0:00:10s\n",
      "epoch 196| loss: 0.31217 |  0:00:10s\n",
      "epoch 197| loss: 0.30194 |  0:00:10s\n",
      "epoch 198| loss: 0.30261 |  0:00:10s\n",
      "epoch 199| loss: 0.30884 |  0:00:10s\n",
      "epoch 200| loss: 0.29178 |  0:00:10s\n",
      "epoch 201| loss: 0.29722 |  0:00:10s\n",
      "epoch 202| loss: 0.29258 |  0:00:10s\n",
      "epoch 203| loss: 0.29433 |  0:00:10s\n",
      "epoch 204| loss: 0.30461 |  0:00:10s\n",
      "epoch 205| loss: 0.30725 |  0:00:10s\n",
      "epoch 206| loss: 0.29872 |  0:00:10s\n",
      "epoch 207| loss: 0.30223 |  0:00:10s\n",
      "epoch 208| loss: 0.29774 |  0:00:11s\n",
      "epoch 209| loss: 0.28623 |  0:00:11s\n",
      "epoch 210| loss: 0.28529 |  0:00:11s\n",
      "epoch 211| loss: 0.29315 |  0:00:11s\n",
      "epoch 212| loss: 0.28352 |  0:00:11s\n",
      "epoch 213| loss: 0.30166 |  0:00:11s\n",
      "epoch 214| loss: 0.29366 |  0:00:11s\n",
      "epoch 215| loss: 0.27949 |  0:00:11s\n",
      "epoch 216| loss: 0.28452 |  0:00:11s\n",
      "epoch 217| loss: 0.31079 |  0:00:11s\n",
      "epoch 218| loss: 0.3275  |  0:00:11s\n",
      "epoch 219| loss: 0.3087  |  0:00:11s\n",
      "epoch 220| loss: 0.302   |  0:00:11s\n",
      "epoch 221| loss: 0.29721 |  0:00:11s\n",
      "epoch 222| loss: 0.3008  |  0:00:11s\n",
      "epoch 223| loss: 0.29147 |  0:00:11s\n",
      "epoch 224| loss: 0.28825 |  0:00:11s\n",
      "epoch 225| loss: 0.28308 |  0:00:11s\n",
      "epoch 226| loss: 0.2861  |  0:00:11s\n",
      "epoch 227| loss: 0.285   |  0:00:12s\n",
      "epoch 228| loss: 0.27313 |  0:00:12s\n",
      "epoch 229| loss: 0.27843 |  0:00:12s\n",
      "epoch 230| loss: 0.26605 |  0:00:12s\n",
      "epoch 231| loss: 0.26602 |  0:00:12s\n",
      "epoch 232| loss: 0.26824 |  0:00:12s\n",
      "epoch 233| loss: 0.26481 |  0:00:12s\n",
      "epoch 234| loss: 0.26602 |  0:00:12s\n",
      "epoch 235| loss: 0.26511 |  0:00:12s\n",
      "epoch 236| loss: 0.26251 |  0:00:12s\n",
      "epoch 237| loss: 0.26582 |  0:00:12s\n",
      "epoch 238| loss: 0.25754 |  0:00:12s\n",
      "epoch 239| loss: 0.25981 |  0:00:12s\n",
      "epoch 240| loss: 0.25957 |  0:00:12s\n",
      "epoch 241| loss: 0.26828 |  0:00:12s\n",
      "epoch 242| loss: 0.26331 |  0:00:12s\n",
      "epoch 243| loss: 0.25297 |  0:00:12s\n",
      "epoch 244| loss: 0.25358 |  0:00:12s\n",
      "epoch 245| loss: 0.25557 |  0:00:12s\n",
      "epoch 246| loss: 0.24961 |  0:00:12s\n",
      "epoch 247| loss: 0.25293 |  0:00:13s\n",
      "epoch 248| loss: 0.24877 |  0:00:13s\n",
      "epoch 249| loss: 0.25001 |  0:00:13s\n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.37322 |  0:00:00s\n",
      "epoch 1  | loss: 1.9718  |  0:00:00s\n",
      "epoch 2  | loss: 1.42697 |  0:00:00s\n",
      "epoch 3  | loss: 1.11495 |  0:00:00s\n",
      "epoch 4  | loss: 0.97579 |  0:00:00s\n",
      "epoch 5  | loss: 0.91797 |  0:00:00s\n",
      "epoch 6  | loss: 0.87275 |  0:00:00s\n",
      "epoch 7  | loss: 0.82662 |  0:00:00s\n",
      "epoch 8  | loss: 0.80067 |  0:00:00s\n",
      "epoch 9  | loss: 0.78923 |  0:00:00s\n",
      "epoch 10 | loss: 0.75014 |  0:00:00s\n",
      "epoch 11 | loss: 0.73102 |  0:00:00s\n",
      "epoch 12 | loss: 0.72296 |  0:00:00s\n",
      "epoch 13 | loss: 0.73204 |  0:00:00s\n",
      "epoch 14 | loss: 0.7254  |  0:00:00s\n",
      "epoch 15 | loss: 0.69861 |  0:00:00s\n",
      "epoch 16 | loss: 0.69247 |  0:00:00s\n",
      "epoch 17 | loss: 0.67164 |  0:00:00s\n",
      "epoch 18 | loss: 0.66239 |  0:00:00s\n",
      "epoch 19 | loss: 0.66707 |  0:00:00s\n",
      "epoch 20 | loss: 0.64392 |  0:00:01s\n",
      "epoch 21 | loss: 0.63059 |  0:00:01s\n",
      "epoch 22 | loss: 0.61943 |  0:00:01s\n",
      "epoch 23 | loss: 0.62119 |  0:00:01s\n",
      "epoch 24 | loss: 0.60196 |  0:00:01s\n",
      "epoch 25 | loss: 0.59773 |  0:00:01s\n",
      "epoch 26 | loss: 0.59551 |  0:00:01s\n",
      "epoch 27 | loss: 0.59189 |  0:00:01s\n",
      "epoch 28 | loss: 0.60184 |  0:00:01s\n",
      "epoch 29 | loss: 0.60084 |  0:00:01s\n",
      "epoch 30 | loss: 0.59551 |  0:00:01s\n",
      "epoch 31 | loss: 0.5911  |  0:00:01s\n",
      "epoch 32 | loss: 0.58993 |  0:00:01s\n",
      "epoch 33 | loss: 0.57881 |  0:00:01s\n",
      "epoch 34 | loss: 0.57394 |  0:00:01s\n",
      "epoch 35 | loss: 0.56503 |  0:00:01s\n",
      "epoch 36 | loss: 0.56707 |  0:00:01s\n",
      "epoch 37 | loss: 0.56709 |  0:00:01s\n",
      "epoch 38 | loss: 0.5742  |  0:00:01s\n",
      "epoch 39 | loss: 0.56216 |  0:00:02s\n",
      "epoch 40 | loss: 0.5653  |  0:00:02s\n",
      "epoch 41 | loss: 0.56659 |  0:00:02s\n",
      "epoch 42 | loss: 0.55543 |  0:00:02s\n",
      "epoch 43 | loss: 0.55533 |  0:00:02s\n",
      "epoch 44 | loss: 0.55104 |  0:00:02s\n",
      "epoch 45 | loss: 0.54946 |  0:00:02s\n",
      "epoch 46 | loss: 0.53691 |  0:00:02s\n",
      "epoch 47 | loss: 0.52733 |  0:00:02s\n",
      "epoch 48 | loss: 0.52323 |  0:00:02s\n",
      "epoch 49 | loss: 0.51494 |  0:00:02s\n",
      "epoch 50 | loss: 0.51491 |  0:00:02s\n",
      "epoch 51 | loss: 0.50855 |  0:00:02s\n",
      "epoch 52 | loss: 0.50055 |  0:00:02s\n",
      "epoch 53 | loss: 0.49921 |  0:00:02s\n",
      "epoch 54 | loss: 0.48988 |  0:00:02s\n",
      "epoch 55 | loss: 0.4791  |  0:00:02s\n",
      "epoch 56 | loss: 0.48832 |  0:00:02s\n",
      "epoch 57 | loss: 0.48899 |  0:00:02s\n",
      "epoch 58 | loss: 0.48422 |  0:00:02s\n",
      "epoch 59 | loss: 0.47942 |  0:00:03s\n",
      "epoch 60 | loss: 0.47915 |  0:00:03s\n",
      "epoch 61 | loss: 0.47872 |  0:00:03s\n",
      "epoch 62 | loss: 0.48009 |  0:00:03s\n",
      "epoch 63 | loss: 0.46896 |  0:00:03s\n",
      "epoch 64 | loss: 0.46857 |  0:00:03s\n",
      "epoch 65 | loss: 0.4609  |  0:00:03s\n",
      "epoch 66 | loss: 0.45994 |  0:00:03s\n",
      "epoch 67 | loss: 0.46722 |  0:00:03s\n",
      "epoch 68 | loss: 0.46636 |  0:00:03s\n",
      "epoch 69 | loss: 0.45154 |  0:00:03s\n",
      "epoch 70 | loss: 0.44978 |  0:00:03s\n",
      "epoch 71 | loss: 0.45097 |  0:00:03s\n",
      "epoch 72 | loss: 0.44645 |  0:00:03s\n",
      "epoch 73 | loss: 0.46164 |  0:00:03s\n",
      "epoch 74 | loss: 0.4535  |  0:00:03s\n",
      "epoch 75 | loss: 0.45252 |  0:00:03s\n",
      "epoch 76 | loss: 0.44522 |  0:00:04s\n",
      "epoch 77 | loss: 0.44399 |  0:00:04s\n",
      "epoch 78 | loss: 0.44699 |  0:00:04s\n",
      "epoch 79 | loss: 0.44125 |  0:00:04s\n",
      "epoch 80 | loss: 0.43443 |  0:00:04s\n",
      "epoch 81 | loss: 0.42991 |  0:00:04s\n",
      "epoch 82 | loss: 0.42869 |  0:00:04s\n",
      "epoch 83 | loss: 0.41967 |  0:00:04s\n",
      "epoch 84 | loss: 0.42789 |  0:00:04s\n",
      "epoch 85 | loss: 0.41316 |  0:00:04s\n",
      "epoch 86 | loss: 0.42207 |  0:00:04s\n",
      "epoch 87 | loss: 0.4296  |  0:00:04s\n",
      "epoch 88 | loss: 0.4177  |  0:00:04s\n",
      "epoch 89 | loss: 0.41589 |  0:00:04s\n",
      "epoch 90 | loss: 0.39493 |  0:00:04s\n",
      "epoch 91 | loss: 0.40128 |  0:00:04s\n",
      "epoch 92 | loss: 0.40539 |  0:00:04s\n",
      "epoch 93 | loss: 0.39358 |  0:00:04s\n",
      "epoch 94 | loss: 0.40754 |  0:00:04s\n",
      "epoch 95 | loss: 0.40763 |  0:00:05s\n",
      "epoch 96 | loss: 0.406   |  0:00:05s\n",
      "epoch 97 | loss: 0.39793 |  0:00:05s\n",
      "epoch 98 | loss: 0.38632 |  0:00:05s\n",
      "epoch 99 | loss: 0.3982  |  0:00:05s\n",
      "epoch 100| loss: 0.38183 |  0:00:05s\n",
      "epoch 101| loss: 0.38706 |  0:00:05s\n",
      "epoch 102| loss: 0.3845  |  0:00:05s\n",
      "epoch 103| loss: 0.37802 |  0:00:05s\n",
      "epoch 104| loss: 0.37284 |  0:00:05s\n",
      "epoch 105| loss: 0.36946 |  0:00:05s\n",
      "epoch 106| loss: 0.3706  |  0:00:05s\n",
      "epoch 107| loss: 0.36563 |  0:00:05s\n",
      "epoch 108| loss: 0.37155 |  0:00:05s\n",
      "epoch 109| loss: 0.39195 |  0:00:05s\n",
      "epoch 110| loss: 0.39084 |  0:00:05s\n",
      "epoch 111| loss: 0.37603 |  0:00:05s\n",
      "epoch 112| loss: 0.38211 |  0:00:05s\n",
      "epoch 113| loss: 0.39987 |  0:00:05s\n",
      "epoch 114| loss: 0.3961  |  0:00:06s\n",
      "epoch 115| loss: 0.38917 |  0:00:06s\n",
      "epoch 116| loss: 0.39175 |  0:00:06s\n",
      "epoch 117| loss: 0.38364 |  0:00:06s\n",
      "epoch 118| loss: 0.38007 |  0:00:06s\n",
      "epoch 119| loss: 0.37182 |  0:00:06s\n",
      "epoch 120| loss: 0.36995 |  0:00:06s\n",
      "epoch 121| loss: 0.37822 |  0:00:06s\n",
      "epoch 122| loss: 0.36228 |  0:00:06s\n",
      "epoch 123| loss: 0.36713 |  0:00:06s\n",
      "epoch 124| loss: 0.37008 |  0:00:06s\n",
      "epoch 125| loss: 0.36706 |  0:00:06s\n",
      "epoch 126| loss: 0.36835 |  0:00:06s\n",
      "epoch 127| loss: 0.36163 |  0:00:06s\n",
      "epoch 128| loss: 0.364   |  0:00:06s\n",
      "epoch 129| loss: 0.36105 |  0:00:06s\n",
      "epoch 130| loss: 0.3642  |  0:00:06s\n",
      "epoch 131| loss: 0.35567 |  0:00:06s\n",
      "epoch 132| loss: 0.35624 |  0:00:06s\n",
      "epoch 133| loss: 0.34412 |  0:00:07s\n",
      "epoch 134| loss: 0.34907 |  0:00:07s\n",
      "epoch 135| loss: 0.34401 |  0:00:07s\n",
      "epoch 136| loss: 0.33827 |  0:00:07s\n",
      "epoch 137| loss: 0.32815 |  0:00:07s\n",
      "epoch 138| loss: 0.33159 |  0:00:07s\n",
      "epoch 139| loss: 0.32565 |  0:00:07s\n",
      "epoch 140| loss: 0.33247 |  0:00:07s\n",
      "epoch 141| loss: 0.33    |  0:00:07s\n",
      "epoch 142| loss: 0.32372 |  0:00:07s\n",
      "epoch 143| loss: 0.32876 |  0:00:07s\n",
      "epoch 144| loss: 0.32437 |  0:00:07s\n",
      "epoch 145| loss: 0.32019 |  0:00:07s\n",
      "epoch 146| loss: 0.32255 |  0:00:07s\n",
      "epoch 147| loss: 0.32122 |  0:00:07s\n",
      "epoch 148| loss: 0.31915 |  0:00:07s\n",
      "epoch 149| loss: 0.3284  |  0:00:07s\n",
      "epoch 150| loss: 0.34726 |  0:00:07s\n",
      "epoch 151| loss: 0.34851 |  0:00:07s\n",
      "epoch 152| loss: 0.33309 |  0:00:07s\n",
      "epoch 153| loss: 0.32397 |  0:00:08s\n",
      "epoch 154| loss: 0.3291  |  0:00:08s\n",
      "epoch 155| loss: 0.32406 |  0:00:08s\n",
      "epoch 156| loss: 0.31774 |  0:00:08s\n",
      "epoch 157| loss: 0.31712 |  0:00:08s\n",
      "epoch 158| loss: 0.3154  |  0:00:08s\n",
      "epoch 159| loss: 0.31178 |  0:00:08s\n",
      "epoch 160| loss: 0.31271 |  0:00:08s\n",
      "epoch 161| loss: 0.30576 |  0:00:08s\n",
      "epoch 162| loss: 0.30111 |  0:00:08s\n",
      "epoch 163| loss: 0.2996  |  0:00:08s\n",
      "epoch 164| loss: 0.29705 |  0:00:08s\n",
      "epoch 165| loss: 0.29529 |  0:00:08s\n",
      "epoch 166| loss: 0.29636 |  0:00:08s\n",
      "epoch 167| loss: 0.28643 |  0:00:08s\n",
      "epoch 168| loss: 0.29324 |  0:00:08s\n",
      "epoch 169| loss: 0.29084 |  0:00:08s\n",
      "epoch 170| loss: 0.29597 |  0:00:08s\n",
      "epoch 171| loss: 0.2904  |  0:00:08s\n",
      "epoch 172| loss: 0.29267 |  0:00:09s\n",
      "epoch 173| loss: 0.28699 |  0:00:09s\n",
      "epoch 174| loss: 0.27692 |  0:00:09s\n",
      "epoch 175| loss: 0.28536 |  0:00:09s\n",
      "epoch 176| loss: 0.28522 |  0:00:09s\n",
      "epoch 177| loss: 0.2789  |  0:00:09s\n",
      "epoch 178| loss: 0.27284 |  0:00:09s\n",
      "epoch 179| loss: 0.28211 |  0:00:09s\n",
      "epoch 180| loss: 0.2849  |  0:00:09s\n",
      "epoch 181| loss: 0.29169 |  0:00:09s\n",
      "epoch 182| loss: 0.30442 |  0:00:09s\n",
      "epoch 183| loss: 0.29559 |  0:00:09s\n",
      "epoch 184| loss: 0.29241 |  0:00:09s\n",
      "epoch 185| loss: 0.29497 |  0:00:09s\n",
      "epoch 186| loss: 0.29504 |  0:00:09s\n",
      "epoch 187| loss: 0.30044 |  0:00:09s\n",
      "epoch 188| loss: 0.2967  |  0:00:09s\n",
      "epoch 189| loss: 0.29658 |  0:00:09s\n",
      "epoch 190| loss: 0.29926 |  0:00:09s\n",
      "epoch 191| loss: 0.28887 |  0:00:10s\n",
      "epoch 192| loss: 0.28804 |  0:00:10s\n",
      "epoch 193| loss: 0.28896 |  0:00:10s\n",
      "epoch 194| loss: 0.29149 |  0:00:10s\n",
      "epoch 195| loss: 0.28521 |  0:00:10s\n",
      "epoch 196| loss: 0.29003 |  0:00:10s\n",
      "epoch 197| loss: 0.28231 |  0:00:10s\n",
      "epoch 198| loss: 0.28079 |  0:00:10s\n",
      "epoch 199| loss: 0.28349 |  0:00:10s\n",
      "epoch 200| loss: 0.28469 |  0:00:10s\n",
      "epoch 201| loss: 0.27623 |  0:00:10s\n",
      "epoch 202| loss: 0.26759 |  0:00:10s\n",
      "epoch 203| loss: 0.26981 |  0:00:10s\n",
      "epoch 204| loss: 0.26413 |  0:00:10s\n",
      "epoch 205| loss: 0.26921 |  0:00:10s\n",
      "epoch 206| loss: 0.27192 |  0:00:10s\n",
      "epoch 207| loss: 0.26268 |  0:00:10s\n",
      "epoch 208| loss: 0.26044 |  0:00:10s\n",
      "epoch 209| loss: 0.25378 |  0:00:10s\n",
      "epoch 210| loss: 0.25719 |  0:00:11s\n",
      "epoch 211| loss: 0.25517 |  0:00:11s\n",
      "epoch 212| loss: 0.25725 |  0:00:11s\n",
      "epoch 213| loss: 0.27252 |  0:00:11s\n",
      "epoch 214| loss: 0.25313 |  0:00:11s\n",
      "epoch 215| loss: 0.25595 |  0:00:11s\n",
      "epoch 216| loss: 0.25696 |  0:00:11s\n",
      "epoch 217| loss: 0.25175 |  0:00:11s\n",
      "epoch 218| loss: 0.25048 |  0:00:11s\n",
      "epoch 219| loss: 0.24357 |  0:00:11s\n",
      "epoch 220| loss: 0.24345 |  0:00:11s\n",
      "epoch 221| loss: 0.2434  |  0:00:11s\n",
      "epoch 222| loss: 0.24339 |  0:00:11s\n",
      "epoch 223| loss: 0.24254 |  0:00:11s\n",
      "epoch 224| loss: 0.24469 |  0:00:11s\n",
      "epoch 225| loss: 0.24089 |  0:00:11s\n",
      "epoch 226| loss: 0.24141 |  0:00:11s\n",
      "epoch 227| loss: 0.23769 |  0:00:11s\n",
      "epoch 228| loss: 0.23607 |  0:00:11s\n",
      "epoch 229| loss: 0.2335  |  0:00:12s\n",
      "epoch 230| loss: 0.22605 |  0:00:12s\n",
      "epoch 231| loss: 0.23034 |  0:00:12s\n",
      "epoch 232| loss: 0.22677 |  0:00:12s\n",
      "epoch 233| loss: 0.23    |  0:00:12s\n",
      "epoch 234| loss: 0.22643 |  0:00:12s\n",
      "epoch 235| loss: 0.22947 |  0:00:12s\n",
      "epoch 236| loss: 0.22539 |  0:00:12s\n",
      "epoch 237| loss: 0.22616 |  0:00:12s\n",
      "epoch 238| loss: 0.22505 |  0:00:12s\n",
      "epoch 239| loss: 0.22529 |  0:00:12s\n",
      "epoch 240| loss: 0.22552 |  0:00:12s\n",
      "epoch 241| loss: 0.22571 |  0:00:12s\n",
      "epoch 242| loss: 0.22623 |  0:00:12s\n",
      "epoch 243| loss: 0.21401 |  0:00:12s\n",
      "epoch 244| loss: 0.21583 |  0:00:12s\n",
      "epoch 245| loss: 0.21951 |  0:00:12s\n",
      "epoch 246| loss: 0.22066 |  0:00:12s\n",
      "epoch 247| loss: 0.22319 |  0:00:12s\n",
      "epoch 248| loss: 0.21821 |  0:00:12s\n",
      "epoch 249| loss: 0.22024 |  0:00:13s\n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.15793 |  0:00:00s\n",
      "epoch 1  | loss: 2.26904 |  0:00:00s\n",
      "epoch 2  | loss: 1.64186 |  0:00:00s\n",
      "epoch 3  | loss: 1.27474 |  0:00:00s\n",
      "epoch 4  | loss: 1.10578 |  0:00:00s\n",
      "epoch 5  | loss: 0.99496 |  0:00:00s\n",
      "epoch 6  | loss: 0.91026 |  0:00:00s\n",
      "epoch 7  | loss: 0.82682 |  0:00:00s\n",
      "epoch 8  | loss: 0.81161 |  0:00:00s\n",
      "epoch 9  | loss: 0.81233 |  0:00:00s\n",
      "epoch 10 | loss: 0.79192 |  0:00:00s\n",
      "epoch 11 | loss: 0.75969 |  0:00:00s\n",
      "epoch 12 | loss: 0.73579 |  0:00:00s\n",
      "epoch 13 | loss: 0.73912 |  0:00:00s\n",
      "epoch 14 | loss: 0.72203 |  0:00:00s\n",
      "epoch 15 | loss: 0.71143 |  0:00:00s\n",
      "epoch 16 | loss: 0.6902  |  0:00:00s\n",
      "epoch 17 | loss: 0.67038 |  0:00:00s\n",
      "epoch 18 | loss: 0.66252 |  0:00:00s\n",
      "epoch 19 | loss: 0.66223 |  0:00:00s\n",
      "epoch 20 | loss: 0.66222 |  0:00:00s\n",
      "epoch 21 | loss: 0.65558 |  0:00:00s\n",
      "epoch 22 | loss: 0.64298 |  0:00:01s\n",
      "epoch 23 | loss: 0.63014 |  0:00:01s\n",
      "epoch 24 | loss: 0.62728 |  0:00:01s\n",
      "epoch 25 | loss: 0.62789 |  0:00:01s\n",
      "epoch 26 | loss: 0.62763 |  0:00:01s\n",
      "epoch 27 | loss: 0.61018 |  0:00:01s\n",
      "epoch 28 | loss: 0.60521 |  0:00:01s\n",
      "epoch 29 | loss: 0.60703 |  0:00:01s\n",
      "epoch 30 | loss: 0.6031  |  0:00:01s\n",
      "epoch 31 | loss: 0.59793 |  0:00:01s\n",
      "epoch 32 | loss: 0.58436 |  0:00:01s\n",
      "epoch 33 | loss: 0.58758 |  0:00:01s\n",
      "epoch 34 | loss: 0.58925 |  0:00:01s\n",
      "epoch 35 | loss: 0.57443 |  0:00:01s\n",
      "epoch 36 | loss: 0.56869 |  0:00:01s\n",
      "epoch 37 | loss: 0.56346 |  0:00:01s\n",
      "epoch 38 | loss: 0.56791 |  0:00:01s\n",
      "epoch 39 | loss: 0.54828 |  0:00:01s\n",
      "epoch 40 | loss: 0.54754 |  0:00:01s\n",
      "epoch 41 | loss: 0.548   |  0:00:01s\n",
      "epoch 42 | loss: 0.54367 |  0:00:01s\n",
      "epoch 43 | loss: 0.54963 |  0:00:01s\n",
      "epoch 44 | loss: 0.53528 |  0:00:02s\n",
      "epoch 45 | loss: 0.53677 |  0:00:02s\n",
      "epoch 46 | loss: 0.53909 |  0:00:02s\n",
      "epoch 47 | loss: 0.52781 |  0:00:02s\n",
      "epoch 48 | loss: 0.51698 |  0:00:02s\n",
      "epoch 49 | loss: 0.51716 |  0:00:02s\n",
      "epoch 50 | loss: 0.51654 |  0:00:02s\n",
      "epoch 51 | loss: 0.5064  |  0:00:02s\n",
      "epoch 52 | loss: 0.5047  |  0:00:02s\n",
      "epoch 53 | loss: 0.49884 |  0:00:02s\n",
      "epoch 54 | loss: 0.50027 |  0:00:02s\n",
      "epoch 55 | loss: 0.49774 |  0:00:02s\n",
      "epoch 56 | loss: 0.49146 |  0:00:02s\n",
      "epoch 57 | loss: 0.48936 |  0:00:02s\n",
      "epoch 58 | loss: 0.48839 |  0:00:02s\n",
      "epoch 59 | loss: 0.49609 |  0:00:02s\n",
      "epoch 60 | loss: 0.48782 |  0:00:02s\n",
      "epoch 61 | loss: 0.47839 |  0:00:02s\n",
      "epoch 62 | loss: 0.47816 |  0:00:02s\n",
      "epoch 63 | loss: 0.47317 |  0:00:02s\n",
      "epoch 64 | loss: 0.47299 |  0:00:02s\n",
      "epoch 65 | loss: 0.45397 |  0:00:02s\n",
      "epoch 66 | loss: 0.47103 |  0:00:03s\n",
      "epoch 67 | loss: 0.48291 |  0:00:03s\n",
      "epoch 68 | loss: 0.4732  |  0:00:03s\n",
      "epoch 69 | loss: 0.47214 |  0:00:03s\n",
      "epoch 70 | loss: 0.4707  |  0:00:03s\n",
      "epoch 71 | loss: 0.46368 |  0:00:03s\n",
      "epoch 72 | loss: 0.45759 |  0:00:03s\n",
      "epoch 73 | loss: 0.45843 |  0:00:03s\n",
      "epoch 74 | loss: 0.45734 |  0:00:03s\n",
      "epoch 75 | loss: 0.44519 |  0:00:03s\n",
      "epoch 76 | loss: 0.45714 |  0:00:03s\n",
      "epoch 77 | loss: 0.44923 |  0:00:03s\n",
      "epoch 78 | loss: 0.43681 |  0:00:03s\n",
      "epoch 79 | loss: 0.43539 |  0:00:03s\n",
      "epoch 80 | loss: 0.43187 |  0:00:03s\n",
      "epoch 81 | loss: 0.43937 |  0:00:03s\n",
      "epoch 82 | loss: 0.42993 |  0:00:03s\n",
      "epoch 83 | loss: 0.44115 |  0:00:03s\n",
      "epoch 84 | loss: 0.4508  |  0:00:03s\n",
      "epoch 85 | loss: 0.43272 |  0:00:03s\n",
      "epoch 86 | loss: 0.43127 |  0:00:03s\n",
      "epoch 87 | loss: 0.43326 |  0:00:03s\n",
      "epoch 88 | loss: 0.42421 |  0:00:04s\n",
      "epoch 89 | loss: 0.42397 |  0:00:04s\n",
      "epoch 90 | loss: 0.41295 |  0:00:04s\n",
      "epoch 91 | loss: 0.41204 |  0:00:04s\n",
      "epoch 92 | loss: 0.4135  |  0:00:04s\n",
      "epoch 93 | loss: 0.40817 |  0:00:04s\n",
      "epoch 94 | loss: 0.40178 |  0:00:04s\n",
      "epoch 95 | loss: 0.40956 |  0:00:04s\n",
      "epoch 96 | loss: 0.40517 |  0:00:04s\n",
      "epoch 97 | loss: 0.41071 |  0:00:04s\n",
      "epoch 98 | loss: 0.39932 |  0:00:04s\n",
      "epoch 99 | loss: 0.40972 |  0:00:04s\n",
      "epoch 100| loss: 0.39449 |  0:00:04s\n",
      "epoch 101| loss: 0.39098 |  0:00:04s\n",
      "epoch 102| loss: 0.41054 |  0:00:04s\n",
      "epoch 103| loss: 0.39599 |  0:00:04s\n",
      "epoch 104| loss: 0.39233 |  0:00:04s\n",
      "epoch 105| loss: 0.38747 |  0:00:04s\n",
      "epoch 106| loss: 0.39378 |  0:00:04s\n",
      "epoch 107| loss: 0.39605 |  0:00:04s\n",
      "epoch 108| loss: 0.38871 |  0:00:04s\n",
      "epoch 109| loss: 0.38163 |  0:00:05s\n",
      "epoch 110| loss: 0.38133 |  0:00:05s\n",
      "epoch 111| loss: 0.39514 |  0:00:05s\n",
      "epoch 112| loss: 0.37991 |  0:00:05s\n",
      "epoch 113| loss: 0.38758 |  0:00:05s\n",
      "epoch 114| loss: 0.37725 |  0:00:05s\n",
      "epoch 115| loss: 0.36941 |  0:00:05s\n",
      "epoch 116| loss: 0.37084 |  0:00:05s\n",
      "epoch 117| loss: 0.38035 |  0:00:05s\n",
      "epoch 118| loss: 0.36995 |  0:00:05s\n",
      "epoch 119| loss: 0.372   |  0:00:05s\n",
      "epoch 120| loss: 0.36611 |  0:00:05s\n",
      "epoch 121| loss: 0.37603 |  0:00:05s\n",
      "epoch 122| loss: 0.37674 |  0:00:05s\n",
      "epoch 123| loss: 0.37985 |  0:00:05s\n",
      "epoch 124| loss: 0.37166 |  0:00:05s\n",
      "epoch 125| loss: 0.37293 |  0:00:05s\n",
      "epoch 126| loss: 0.38094 |  0:00:05s\n",
      "epoch 127| loss: 0.3688  |  0:00:05s\n",
      "epoch 128| loss: 0.36802 |  0:00:05s\n",
      "epoch 129| loss: 0.36269 |  0:00:06s\n",
      "epoch 130| loss: 0.36529 |  0:00:06s\n",
      "epoch 131| loss: 0.36274 |  0:00:06s\n",
      "epoch 132| loss: 0.35921 |  0:00:06s\n",
      "epoch 133| loss: 0.35141 |  0:00:06s\n",
      "epoch 134| loss: 0.34415 |  0:00:06s\n",
      "epoch 135| loss: 0.33891 |  0:00:06s\n",
      "epoch 136| loss: 0.33665 |  0:00:06s\n",
      "epoch 137| loss: 0.33297 |  0:00:06s\n",
      "epoch 138| loss: 0.34407 |  0:00:06s\n",
      "epoch 139| loss: 0.35835 |  0:00:06s\n",
      "epoch 140| loss: 0.349   |  0:00:06s\n",
      "epoch 141| loss: 0.34895 |  0:00:06s\n",
      "epoch 142| loss: 0.3403  |  0:00:06s\n",
      "epoch 143| loss: 0.33489 |  0:00:06s\n",
      "epoch 144| loss: 0.33108 |  0:00:06s\n",
      "epoch 145| loss: 0.33369 |  0:00:07s\n",
      "epoch 146| loss: 0.32986 |  0:00:07s\n",
      "epoch 147| loss: 0.3298  |  0:00:07s\n",
      "epoch 148| loss: 0.32453 |  0:00:07s\n",
      "epoch 149| loss: 0.32405 |  0:00:07s\n",
      "epoch 150| loss: 0.32849 |  0:00:07s\n",
      "epoch 151| loss: 0.32622 |  0:00:07s\n",
      "epoch 152| loss: 0.31549 |  0:00:07s\n",
      "epoch 153| loss: 0.32211 |  0:00:07s\n",
      "epoch 154| loss: 0.32969 |  0:00:07s\n",
      "epoch 155| loss: 0.32915 |  0:00:07s\n",
      "epoch 156| loss: 0.32426 |  0:00:07s\n",
      "epoch 157| loss: 0.33915 |  0:00:07s\n",
      "epoch 158| loss: 0.34501 |  0:00:07s\n",
      "epoch 159| loss: 0.34001 |  0:00:07s\n",
      "epoch 160| loss: 0.3294  |  0:00:07s\n",
      "epoch 161| loss: 0.33055 |  0:00:07s\n",
      "epoch 162| loss: 0.32805 |  0:00:07s\n",
      "epoch 163| loss: 0.3234  |  0:00:07s\n",
      "epoch 164| loss: 0.32493 |  0:00:08s\n",
      "epoch 165| loss: 0.32511 |  0:00:08s\n",
      "epoch 166| loss: 0.32847 |  0:00:08s\n",
      "epoch 167| loss: 0.32641 |  0:00:08s\n",
      "epoch 168| loss: 0.31748 |  0:00:08s\n",
      "epoch 169| loss: 0.32271 |  0:00:08s\n",
      "epoch 170| loss: 0.32564 |  0:00:08s\n",
      "epoch 171| loss: 0.32394 |  0:00:08s\n",
      "epoch 172| loss: 0.32242 |  0:00:08s\n",
      "epoch 173| loss: 0.31845 |  0:00:08s\n",
      "epoch 174| loss: 0.30364 |  0:00:08s\n",
      "epoch 175| loss: 0.30435 |  0:00:08s\n",
      "epoch 176| loss: 0.3062  |  0:00:08s\n",
      "epoch 177| loss: 0.29238 |  0:00:08s\n",
      "epoch 178| loss: 0.2968  |  0:00:08s\n",
      "epoch 179| loss: 0.30673 |  0:00:08s\n",
      "epoch 180| loss: 0.30897 |  0:00:08s\n",
      "epoch 181| loss: 0.3088  |  0:00:08s\n",
      "epoch 182| loss: 0.29644 |  0:00:08s\n",
      "epoch 183| loss: 0.29157 |  0:00:08s\n",
      "epoch 184| loss: 0.28892 |  0:00:09s\n",
      "epoch 185| loss: 0.29249 |  0:00:09s\n",
      "epoch 186| loss: 0.2877  |  0:00:09s\n",
      "epoch 187| loss: 0.28097 |  0:00:09s\n",
      "epoch 188| loss: 0.28796 |  0:00:09s\n",
      "epoch 189| loss: 0.28975 |  0:00:09s\n",
      "epoch 190| loss: 0.30666 |  0:00:09s\n",
      "epoch 191| loss: 0.29557 |  0:00:09s\n",
      "epoch 192| loss: 0.29843 |  0:00:09s\n",
      "epoch 193| loss: 0.31045 |  0:00:09s\n",
      "epoch 194| loss: 0.32147 |  0:00:09s\n",
      "epoch 195| loss: 0.32986 |  0:00:09s\n",
      "epoch 196| loss: 0.34378 |  0:00:09s\n",
      "epoch 197| loss: 0.33485 |  0:00:09s\n",
      "epoch 198| loss: 0.33613 |  0:00:09s\n",
      "epoch 199| loss: 0.33215 |  0:00:09s\n",
      "epoch 200| loss: 0.33301 |  0:00:09s\n",
      "epoch 201| loss: 0.31605 |  0:00:09s\n",
      "epoch 202| loss: 0.31775 |  0:00:09s\n",
      "epoch 203| loss: 0.31086 |  0:00:09s\n",
      "epoch 204| loss: 0.30625 |  0:00:09s\n",
      "epoch 205| loss: 0.29949 |  0:00:09s\n",
      "epoch 206| loss: 0.30375 |  0:00:10s\n",
      "epoch 207| loss: 0.29364 |  0:00:10s\n",
      "epoch 208| loss: 0.29742 |  0:00:10s\n",
      "epoch 209| loss: 0.29785 |  0:00:10s\n",
      "epoch 210| loss: 0.28833 |  0:00:10s\n",
      "epoch 211| loss: 0.28201 |  0:00:10s\n",
      "epoch 212| loss: 0.28214 |  0:00:10s\n",
      "epoch 213| loss: 0.2899  |  0:00:10s\n",
      "epoch 214| loss: 0.27665 |  0:00:10s\n",
      "epoch 215| loss: 0.27054 |  0:00:10s\n",
      "epoch 216| loss: 0.28114 |  0:00:10s\n",
      "epoch 217| loss: 0.27309 |  0:00:10s\n",
      "epoch 218| loss: 0.27551 |  0:00:10s\n",
      "epoch 219| loss: 0.27841 |  0:00:10s\n",
      "epoch 220| loss: 0.28162 |  0:00:10s\n",
      "epoch 221| loss: 0.27926 |  0:00:10s\n",
      "epoch 222| loss: 0.27282 |  0:00:10s\n",
      "epoch 223| loss: 0.28005 |  0:00:10s\n",
      "epoch 224| loss: 0.27338 |  0:00:10s\n",
      "epoch 225| loss: 0.27411 |  0:00:10s\n",
      "epoch 226| loss: 0.27145 |  0:00:10s\n",
      "epoch 227| loss: 0.26512 |  0:00:10s\n",
      "epoch 228| loss: 0.26812 |  0:00:11s\n",
      "epoch 229| loss: 0.25956 |  0:00:11s\n",
      "epoch 230| loss: 0.25888 |  0:00:11s\n",
      "epoch 231| loss: 0.25635 |  0:00:11s\n",
      "epoch 232| loss: 0.25271 |  0:00:11s\n",
      "epoch 233| loss: 0.25637 |  0:00:11s\n",
      "epoch 234| loss: 0.25405 |  0:00:11s\n",
      "epoch 235| loss: 0.25468 |  0:00:11s\n",
      "epoch 236| loss: 0.25131 |  0:00:11s\n",
      "epoch 237| loss: 0.24911 |  0:00:11s\n",
      "epoch 238| loss: 0.24478 |  0:00:11s\n",
      "epoch 239| loss: 0.2502  |  0:00:11s\n",
      "epoch 240| loss: 0.25098 |  0:00:11s\n",
      "epoch 241| loss: 0.25889 |  0:00:11s\n",
      "epoch 242| loss: 0.26485 |  0:00:11s\n",
      "epoch 243| loss: 0.25068 |  0:00:11s\n",
      "epoch 244| loss: 0.24848 |  0:00:11s\n",
      "epoch 245| loss: 0.24835 |  0:00:11s\n",
      "epoch 246| loss: 0.24864 |  0:00:11s\n",
      "epoch 247| loss: 0.24277 |  0:00:11s\n",
      "epoch 248| loss: 0.24258 |  0:00:11s\n",
      "epoch 249| loss: 0.23999 |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.30713 |  0:00:00s\n",
      "epoch 1  | loss: 2.13779 |  0:00:00s\n",
      "epoch 2  | loss: 1.58119 |  0:00:00s\n",
      "epoch 3  | loss: 1.20819 |  0:00:00s\n",
      "epoch 4  | loss: 1.08439 |  0:00:00s\n",
      "epoch 5  | loss: 0.97914 |  0:00:00s\n",
      "epoch 6  | loss: 0.92649 |  0:00:00s\n",
      "epoch 7  | loss: 0.86025 |  0:00:00s\n",
      "epoch 8  | loss: 0.82443 |  0:00:00s\n",
      "epoch 9  | loss: 0.80343 |  0:00:00s\n",
      "epoch 10 | loss: 0.78402 |  0:00:00s\n",
      "epoch 11 | loss: 0.74642 |  0:00:00s\n",
      "epoch 12 | loss: 0.72975 |  0:00:00s\n",
      "epoch 13 | loss: 0.73467 |  0:00:00s\n",
      "epoch 14 | loss: 0.7247  |  0:00:00s\n",
      "epoch 15 | loss: 0.69536 |  0:00:00s\n",
      "epoch 16 | loss: 0.68583 |  0:00:00s\n",
      "epoch 17 | loss: 0.66557 |  0:00:00s\n",
      "epoch 18 | loss: 0.6614  |  0:00:00s\n",
      "epoch 19 | loss: 0.66826 |  0:00:00s\n",
      "epoch 20 | loss: 0.65312 |  0:00:00s\n",
      "epoch 21 | loss: 0.64068 |  0:00:00s\n",
      "epoch 22 | loss: 0.63389 |  0:00:01s\n",
      "epoch 23 | loss: 0.63076 |  0:00:01s\n",
      "epoch 24 | loss: 0.63213 |  0:00:01s\n",
      "epoch 25 | loss: 0.62094 |  0:00:01s\n",
      "epoch 26 | loss: 0.62839 |  0:00:01s\n",
      "epoch 27 | loss: 0.62589 |  0:00:01s\n",
      "epoch 28 | loss: 0.61149 |  0:00:01s\n",
      "epoch 29 | loss: 0.61465 |  0:00:01s\n",
      "epoch 30 | loss: 0.59878 |  0:00:01s\n",
      "epoch 31 | loss: 0.59499 |  0:00:01s\n",
      "epoch 32 | loss: 0.58981 |  0:00:01s\n",
      "epoch 33 | loss: 0.58611 |  0:00:01s\n",
      "epoch 34 | loss: 0.5853  |  0:00:01s\n",
      "epoch 35 | loss: 0.58019 |  0:00:01s\n",
      "epoch 36 | loss: 0.57081 |  0:00:01s\n",
      "epoch 37 | loss: 0.57682 |  0:00:01s\n",
      "epoch 38 | loss: 0.56484 |  0:00:01s\n",
      "epoch 39 | loss: 0.55808 |  0:00:01s\n",
      "epoch 40 | loss: 0.55769 |  0:00:01s\n",
      "epoch 41 | loss: 0.56027 |  0:00:01s\n",
      "epoch 42 | loss: 0.55252 |  0:00:02s\n",
      "epoch 43 | loss: 0.55459 |  0:00:02s\n",
      "epoch 44 | loss: 0.54071 |  0:00:02s\n",
      "epoch 45 | loss: 0.54362 |  0:00:02s\n",
      "epoch 46 | loss: 0.5402  |  0:00:02s\n",
      "epoch 47 | loss: 0.52995 |  0:00:02s\n",
      "epoch 48 | loss: 0.53369 |  0:00:02s\n",
      "epoch 49 | loss: 0.52529 |  0:00:02s\n",
      "epoch 50 | loss: 0.52486 |  0:00:02s\n",
      "epoch 51 | loss: 0.5157  |  0:00:02s\n",
      "epoch 52 | loss: 0.5214  |  0:00:02s\n",
      "epoch 53 | loss: 0.51911 |  0:00:02s\n",
      "epoch 54 | loss: 0.52277 |  0:00:02s\n",
      "epoch 55 | loss: 0.51675 |  0:00:02s\n",
      "epoch 56 | loss: 0.51191 |  0:00:02s\n",
      "epoch 57 | loss: 0.52087 |  0:00:02s\n",
      "epoch 58 | loss: 0.50536 |  0:00:02s\n",
      "epoch 59 | loss: 0.51807 |  0:00:02s\n",
      "epoch 60 | loss: 0.50533 |  0:00:02s\n",
      "epoch 61 | loss: 0.49589 |  0:00:02s\n",
      "epoch 62 | loss: 0.49552 |  0:00:02s\n",
      "epoch 63 | loss: 0.4888  |  0:00:02s\n",
      "epoch 64 | loss: 0.48477 |  0:00:03s\n",
      "epoch 65 | loss: 0.48204 |  0:00:03s\n",
      "epoch 66 | loss: 0.47848 |  0:00:03s\n",
      "epoch 67 | loss: 0.48553 |  0:00:03s\n",
      "epoch 68 | loss: 0.46988 |  0:00:03s\n",
      "epoch 69 | loss: 0.46225 |  0:00:03s\n",
      "epoch 70 | loss: 0.45829 |  0:00:03s\n",
      "epoch 71 | loss: 0.45324 |  0:00:03s\n",
      "epoch 72 | loss: 0.45451 |  0:00:03s\n",
      "epoch 73 | loss: 0.457   |  0:00:03s\n",
      "epoch 74 | loss: 0.45038 |  0:00:03s\n",
      "epoch 75 | loss: 0.445   |  0:00:03s\n",
      "epoch 76 | loss: 0.44439 |  0:00:03s\n",
      "epoch 77 | loss: 0.43325 |  0:00:03s\n",
      "epoch 78 | loss: 0.4287  |  0:00:03s\n",
      "epoch 79 | loss: 0.4349  |  0:00:03s\n",
      "epoch 80 | loss: 0.43057 |  0:00:03s\n",
      "epoch 81 | loss: 0.43487 |  0:00:03s\n",
      "epoch 82 | loss: 0.42836 |  0:00:04s\n",
      "epoch 83 | loss: 0.42594 |  0:00:04s\n",
      "epoch 84 | loss: 0.42972 |  0:00:04s\n",
      "epoch 85 | loss: 0.43263 |  0:00:04s\n",
      "epoch 86 | loss: 0.44375 |  0:00:04s\n",
      "epoch 87 | loss: 0.43947 |  0:00:04s\n",
      "epoch 88 | loss: 0.43514 |  0:00:04s\n",
      "epoch 89 | loss: 0.43247 |  0:00:04s\n",
      "epoch 90 | loss: 0.43363 |  0:00:04s\n",
      "epoch 91 | loss: 0.43539 |  0:00:04s\n",
      "epoch 92 | loss: 0.43628 |  0:00:04s\n",
      "epoch 93 | loss: 0.42514 |  0:00:04s\n",
      "epoch 94 | loss: 0.43604 |  0:00:04s\n",
      "epoch 95 | loss: 0.43792 |  0:00:04s\n",
      "epoch 96 | loss: 0.43535 |  0:00:04s\n",
      "epoch 97 | loss: 0.42604 |  0:00:04s\n",
      "epoch 98 | loss: 0.41937 |  0:00:04s\n",
      "epoch 99 | loss: 0.4345  |  0:00:04s\n",
      "epoch 100| loss: 0.4373  |  0:00:04s\n",
      "epoch 101| loss: 0.43592 |  0:00:04s\n",
      "epoch 102| loss: 0.44303 |  0:00:04s\n",
      "epoch 103| loss: 0.43238 |  0:00:04s\n",
      "epoch 104| loss: 0.42451 |  0:00:05s\n",
      "epoch 105| loss: 0.43319 |  0:00:05s\n",
      "epoch 106| loss: 0.42782 |  0:00:05s\n",
      "epoch 107| loss: 0.4369  |  0:00:05s\n",
      "epoch 108| loss: 0.43939 |  0:00:05s\n",
      "epoch 109| loss: 0.43358 |  0:00:05s\n",
      "epoch 110| loss: 0.44351 |  0:00:05s\n",
      "epoch 111| loss: 0.44104 |  0:00:05s\n",
      "epoch 112| loss: 0.43688 |  0:00:05s\n",
      "epoch 113| loss: 0.43848 |  0:00:05s\n",
      "epoch 114| loss: 0.43342 |  0:00:05s\n",
      "epoch 115| loss: 0.42556 |  0:00:05s\n",
      "epoch 116| loss: 0.42181 |  0:00:05s\n",
      "epoch 117| loss: 0.41947 |  0:00:05s\n",
      "epoch 118| loss: 0.4136  |  0:00:05s\n",
      "epoch 119| loss: 0.40252 |  0:00:05s\n",
      "epoch 120| loss: 0.39737 |  0:00:05s\n",
      "epoch 121| loss: 0.39515 |  0:00:05s\n",
      "epoch 122| loss: 0.39549 |  0:00:05s\n",
      "epoch 123| loss: 0.39985 |  0:00:05s\n",
      "epoch 124| loss: 0.38542 |  0:00:05s\n",
      "epoch 125| loss: 0.38232 |  0:00:05s\n",
      "epoch 126| loss: 0.38899 |  0:00:06s\n",
      "epoch 127| loss: 0.38478 |  0:00:06s\n",
      "epoch 128| loss: 0.38642 |  0:00:06s\n",
      "epoch 129| loss: 0.37802 |  0:00:06s\n",
      "epoch 130| loss: 0.38014 |  0:00:06s\n",
      "epoch 131| loss: 0.38282 |  0:00:06s\n",
      "epoch 132| loss: 0.38061 |  0:00:06s\n",
      "epoch 133| loss: 0.36517 |  0:00:06s\n",
      "epoch 134| loss: 0.37117 |  0:00:06s\n",
      "epoch 135| loss: 0.37282 |  0:00:06s\n",
      "epoch 136| loss: 0.37325 |  0:00:06s\n",
      "epoch 137| loss: 0.38146 |  0:00:06s\n",
      "epoch 138| loss: 0.37802 |  0:00:06s\n",
      "epoch 139| loss: 0.38001 |  0:00:06s\n",
      "epoch 140| loss: 0.38036 |  0:00:06s\n",
      "epoch 141| loss: 0.37553 |  0:00:06s\n",
      "epoch 142| loss: 0.37305 |  0:00:06s\n",
      "epoch 143| loss: 0.36603 |  0:00:06s\n",
      "epoch 144| loss: 0.36458 |  0:00:06s\n",
      "epoch 145| loss: 0.36657 |  0:00:06s\n",
      "epoch 146| loss: 0.36159 |  0:00:06s\n",
      "epoch 147| loss: 0.36029 |  0:00:06s\n",
      "epoch 148| loss: 0.36445 |  0:00:07s\n",
      "epoch 149| loss: 0.36289 |  0:00:07s\n",
      "epoch 150| loss: 0.3612  |  0:00:07s\n",
      "epoch 151| loss: 0.36335 |  0:00:07s\n",
      "epoch 152| loss: 0.36239 |  0:00:07s\n",
      "epoch 153| loss: 0.35239 |  0:00:07s\n",
      "epoch 154| loss: 0.35529 |  0:00:07s\n",
      "epoch 155| loss: 0.36451 |  0:00:07s\n",
      "epoch 156| loss: 0.35422 |  0:00:07s\n",
      "epoch 157| loss: 0.35248 |  0:00:07s\n",
      "epoch 158| loss: 0.34857 |  0:00:07s\n",
      "epoch 159| loss: 0.34403 |  0:00:07s\n",
      "epoch 160| loss: 0.34602 |  0:00:07s\n",
      "epoch 161| loss: 0.34287 |  0:00:07s\n",
      "epoch 162| loss: 0.33568 |  0:00:07s\n",
      "epoch 163| loss: 0.33232 |  0:00:07s\n",
      "epoch 164| loss: 0.33241 |  0:00:07s\n",
      "epoch 165| loss: 0.32379 |  0:00:07s\n",
      "epoch 166| loss: 0.33445 |  0:00:07s\n",
      "epoch 167| loss: 0.33403 |  0:00:07s\n",
      "epoch 168| loss: 0.33201 |  0:00:07s\n",
      "epoch 169| loss: 0.33254 |  0:00:07s\n",
      "epoch 170| loss: 0.33453 |  0:00:08s\n",
      "epoch 171| loss: 0.3316  |  0:00:08s\n",
      "epoch 172| loss: 0.33301 |  0:00:08s\n",
      "epoch 173| loss: 0.33526 |  0:00:08s\n",
      "epoch 174| loss: 0.33071 |  0:00:08s\n",
      "epoch 175| loss: 0.32897 |  0:00:08s\n",
      "epoch 176| loss: 0.33342 |  0:00:08s\n",
      "epoch 177| loss: 0.33323 |  0:00:08s\n",
      "epoch 178| loss: 0.33039 |  0:00:08s\n",
      "epoch 179| loss: 0.33041 |  0:00:08s\n",
      "epoch 180| loss: 0.33885 |  0:00:08s\n",
      "epoch 181| loss: 0.34127 |  0:00:08s\n",
      "epoch 182| loss: 0.34929 |  0:00:08s\n",
      "epoch 183| loss: 0.35821 |  0:00:08s\n",
      "epoch 184| loss: 0.37221 |  0:00:08s\n",
      "epoch 185| loss: 0.35606 |  0:00:08s\n",
      "epoch 186| loss: 0.3581  |  0:00:08s\n",
      "epoch 187| loss: 0.35445 |  0:00:08s\n",
      "epoch 188| loss: 0.34855 |  0:00:08s\n",
      "epoch 189| loss: 0.36865 |  0:00:08s\n",
      "epoch 190| loss: 0.3792  |  0:00:08s\n",
      "epoch 191| loss: 0.3871  |  0:00:08s\n",
      "epoch 192| loss: 0.38051 |  0:00:08s\n",
      "epoch 193| loss: 0.39114 |  0:00:09s\n",
      "epoch 194| loss: 0.39019 |  0:00:09s\n",
      "epoch 195| loss: 0.37218 |  0:00:09s\n",
      "epoch 196| loss: 0.39021 |  0:00:09s\n",
      "epoch 197| loss: 0.37126 |  0:00:09s\n",
      "epoch 198| loss: 0.37479 |  0:00:09s\n",
      "epoch 199| loss: 0.37989 |  0:00:09s\n",
      "epoch 200| loss: 0.37458 |  0:00:09s\n",
      "epoch 201| loss: 0.38543 |  0:00:09s\n",
      "epoch 202| loss: 0.3869  |  0:00:09s\n",
      "epoch 203| loss: 0.38917 |  0:00:09s\n",
      "epoch 204| loss: 0.38401 |  0:00:09s\n",
      "epoch 205| loss: 0.37942 |  0:00:09s\n",
      "epoch 206| loss: 0.38878 |  0:00:09s\n",
      "epoch 207| loss: 0.37786 |  0:00:09s\n",
      "epoch 208| loss: 0.37313 |  0:00:09s\n",
      "epoch 209| loss: 0.37296 |  0:00:09s\n",
      "epoch 210| loss: 0.36403 |  0:00:09s\n",
      "epoch 211| loss: 0.37543 |  0:00:09s\n",
      "epoch 212| loss: 0.36333 |  0:00:09s\n",
      "epoch 213| loss: 0.36308 |  0:00:09s\n",
      "epoch 214| loss: 0.36016 |  0:00:09s\n",
      "epoch 215| loss: 0.3481  |  0:00:10s\n",
      "epoch 216| loss: 0.34874 |  0:00:10s\n",
      "epoch 217| loss: 0.3457  |  0:00:10s\n",
      "epoch 218| loss: 0.33992 |  0:00:10s\n",
      "epoch 219| loss: 0.33323 |  0:00:10s\n",
      "epoch 220| loss: 0.33059 |  0:00:10s\n",
      "epoch 221| loss: 0.33001 |  0:00:10s\n",
      "epoch 222| loss: 0.3188  |  0:00:10s\n",
      "epoch 223| loss: 0.3211  |  0:00:10s\n",
      "epoch 224| loss: 0.32137 |  0:00:10s\n",
      "epoch 225| loss: 0.31423 |  0:00:10s\n",
      "epoch 226| loss: 0.31205 |  0:00:10s\n",
      "epoch 227| loss: 0.31217 |  0:00:10s\n",
      "epoch 228| loss: 0.31841 |  0:00:10s\n",
      "epoch 229| loss: 0.31626 |  0:00:10s\n",
      "epoch 230| loss: 0.30769 |  0:00:10s\n",
      "epoch 231| loss: 0.31026 |  0:00:10s\n",
      "epoch 232| loss: 0.31346 |  0:00:10s\n",
      "epoch 233| loss: 0.32509 |  0:00:10s\n",
      "epoch 234| loss: 0.32899 |  0:00:10s\n",
      "epoch 235| loss: 0.33075 |  0:00:10s\n",
      "epoch 236| loss: 0.32442 |  0:00:10s\n",
      "epoch 237| loss: 0.32003 |  0:00:10s\n",
      "epoch 238| loss: 0.313   |  0:00:11s\n",
      "epoch 239| loss: 0.30782 |  0:00:11s\n",
      "epoch 240| loss: 0.30437 |  0:00:11s\n",
      "epoch 241| loss: 0.30487 |  0:00:11s\n",
      "epoch 242| loss: 0.29788 |  0:00:11s\n",
      "epoch 243| loss: 0.2926  |  0:00:11s\n",
      "epoch 244| loss: 0.28903 |  0:00:11s\n",
      "epoch 245| loss: 0.29529 |  0:00:11s\n",
      "epoch 246| loss: 0.29702 |  0:00:11s\n",
      "epoch 247| loss: 0.29581 |  0:00:11s\n",
      "epoch 248| loss: 0.2879  |  0:00:11s\n",
      "epoch 249| loss: 0.2937  |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.08616 |  0:00:00s\n",
      "epoch 1  | loss: 1.99692 |  0:00:00s\n",
      "epoch 2  | loss: 1.51921 |  0:00:00s\n",
      "epoch 3  | loss: 1.22288 |  0:00:00s\n",
      "epoch 4  | loss: 1.07843 |  0:00:00s\n",
      "epoch 5  | loss: 0.98707 |  0:00:00s\n",
      "epoch 6  | loss: 0.92166 |  0:00:00s\n",
      "epoch 7  | loss: 0.86011 |  0:00:00s\n",
      "epoch 8  | loss: 0.81934 |  0:00:00s\n",
      "epoch 9  | loss: 0.816   |  0:00:00s\n",
      "epoch 10 | loss: 0.77005 |  0:00:00s\n",
      "epoch 11 | loss: 0.75571 |  0:00:00s\n",
      "epoch 12 | loss: 0.73664 |  0:00:00s\n",
      "epoch 13 | loss: 0.71011 |  0:00:00s\n",
      "epoch 14 | loss: 0.70892 |  0:00:00s\n",
      "epoch 15 | loss: 0.69154 |  0:00:00s\n",
      "epoch 16 | loss: 0.69426 |  0:00:00s\n",
      "epoch 17 | loss: 0.68379 |  0:00:00s\n",
      "epoch 18 | loss: 0.66659 |  0:00:00s\n",
      "epoch 19 | loss: 0.68649 |  0:00:01s\n",
      "epoch 20 | loss: 0.66267 |  0:00:01s\n",
      "epoch 21 | loss: 0.65057 |  0:00:01s\n",
      "epoch 22 | loss: 0.65006 |  0:00:01s\n",
      "epoch 23 | loss: 0.65317 |  0:00:01s\n",
      "epoch 24 | loss: 0.63892 |  0:00:01s\n",
      "epoch 25 | loss: 0.64353 |  0:00:01s\n",
      "epoch 26 | loss: 0.62966 |  0:00:01s\n",
      "epoch 27 | loss: 0.62233 |  0:00:01s\n",
      "epoch 28 | loss: 0.61662 |  0:00:01s\n",
      "epoch 29 | loss: 0.61523 |  0:00:01s\n",
      "epoch 30 | loss: 0.60332 |  0:00:01s\n",
      "epoch 31 | loss: 0.60665 |  0:00:01s\n",
      "epoch 32 | loss: 0.59817 |  0:00:01s\n",
      "epoch 33 | loss: 0.58514 |  0:00:01s\n",
      "epoch 34 | loss: 0.59182 |  0:00:01s\n",
      "epoch 35 | loss: 0.58751 |  0:00:01s\n",
      "epoch 36 | loss: 0.58225 |  0:00:01s\n",
      "epoch 37 | loss: 0.57539 |  0:00:01s\n",
      "epoch 38 | loss: 0.56596 |  0:00:02s\n",
      "epoch 39 | loss: 0.56772 |  0:00:02s\n",
      "epoch 40 | loss: 0.56011 |  0:00:02s\n",
      "epoch 41 | loss: 0.55347 |  0:00:02s\n",
      "epoch 42 | loss: 0.55387 |  0:00:02s\n",
      "epoch 43 | loss: 0.55807 |  0:00:02s\n",
      "epoch 44 | loss: 0.55101 |  0:00:02s\n",
      "epoch 45 | loss: 0.5553  |  0:00:02s\n",
      "epoch 46 | loss: 0.5492  |  0:00:02s\n",
      "epoch 47 | loss: 0.54729 |  0:00:02s\n",
      "epoch 48 | loss: 0.5402  |  0:00:02s\n",
      "epoch 49 | loss: 0.54397 |  0:00:02s\n",
      "epoch 50 | loss: 0.53822 |  0:00:02s\n",
      "epoch 51 | loss: 0.51996 |  0:00:02s\n",
      "epoch 52 | loss: 0.51973 |  0:00:02s\n",
      "epoch 53 | loss: 0.52605 |  0:00:02s\n",
      "epoch 54 | loss: 0.53091 |  0:00:02s\n",
      "epoch 55 | loss: 0.51418 |  0:00:02s\n",
      "epoch 56 | loss: 0.51768 |  0:00:02s\n",
      "epoch 57 | loss: 0.51518 |  0:00:03s\n",
      "epoch 58 | loss: 0.51377 |  0:00:03s\n",
      "epoch 59 | loss: 0.51491 |  0:00:03s\n",
      "epoch 60 | loss: 0.50419 |  0:00:03s\n",
      "epoch 61 | loss: 0.49755 |  0:00:03s\n",
      "epoch 62 | loss: 0.48982 |  0:00:03s\n",
      "epoch 63 | loss: 0.48951 |  0:00:03s\n",
      "epoch 64 | loss: 0.49336 |  0:00:03s\n",
      "epoch 65 | loss: 0.48558 |  0:00:03s\n",
      "epoch 66 | loss: 0.47718 |  0:00:03s\n",
      "epoch 67 | loss: 0.48548 |  0:00:03s\n",
      "epoch 68 | loss: 0.48379 |  0:00:03s\n",
      "epoch 69 | loss: 0.48451 |  0:00:03s\n",
      "epoch 70 | loss: 0.48667 |  0:00:03s\n",
      "epoch 71 | loss: 0.48048 |  0:00:03s\n",
      "epoch 72 | loss: 0.49262 |  0:00:03s\n",
      "epoch 73 | loss: 0.49801 |  0:00:03s\n",
      "epoch 74 | loss: 0.49064 |  0:00:03s\n",
      "epoch 75 | loss: 0.49109 |  0:00:03s\n",
      "epoch 76 | loss: 0.48624 |  0:00:04s\n",
      "epoch 77 | loss: 0.46725 |  0:00:04s\n",
      "epoch 78 | loss: 0.46879 |  0:00:04s\n",
      "epoch 79 | loss: 0.46863 |  0:00:04s\n",
      "epoch 80 | loss: 0.46061 |  0:00:04s\n",
      "epoch 81 | loss: 0.46025 |  0:00:04s\n",
      "epoch 82 | loss: 0.45935 |  0:00:04s\n",
      "epoch 83 | loss: 0.46193 |  0:00:04s\n",
      "epoch 84 | loss: 0.46561 |  0:00:04s\n",
      "epoch 85 | loss: 0.4627  |  0:00:04s\n",
      "epoch 86 | loss: 0.45826 |  0:00:04s\n",
      "epoch 87 | loss: 0.45637 |  0:00:04s\n",
      "epoch 88 | loss: 0.44884 |  0:00:04s\n",
      "epoch 89 | loss: 0.44113 |  0:00:04s\n",
      "epoch 90 | loss: 0.43175 |  0:00:04s\n",
      "epoch 91 | loss: 0.43868 |  0:00:04s\n",
      "epoch 92 | loss: 0.43166 |  0:00:04s\n",
      "epoch 93 | loss: 0.42731 |  0:00:04s\n",
      "epoch 94 | loss: 0.42624 |  0:00:04s\n",
      "epoch 95 | loss: 0.42663 |  0:00:05s\n",
      "epoch 96 | loss: 0.41915 |  0:00:05s\n",
      "epoch 97 | loss: 0.41324 |  0:00:05s\n",
      "epoch 98 | loss: 0.41011 |  0:00:05s\n",
      "epoch 99 | loss: 0.41206 |  0:00:05s\n",
      "epoch 100| loss: 0.40597 |  0:00:05s\n",
      "epoch 101| loss: 0.40502 |  0:00:05s\n",
      "epoch 102| loss: 0.40335 |  0:00:05s\n",
      "epoch 103| loss: 0.40793 |  0:00:05s\n",
      "epoch 104| loss: 0.39342 |  0:00:05s\n",
      "epoch 105| loss: 0.40058 |  0:00:05s\n",
      "epoch 106| loss: 0.39846 |  0:00:05s\n",
      "epoch 107| loss: 0.40464 |  0:00:05s\n",
      "epoch 108| loss: 0.40027 |  0:00:05s\n",
      "epoch 109| loss: 0.40422 |  0:00:05s\n",
      "epoch 110| loss: 0.39797 |  0:00:05s\n",
      "epoch 111| loss: 0.39685 |  0:00:05s\n",
      "epoch 112| loss: 0.39652 |  0:00:05s\n",
      "epoch 113| loss: 0.39398 |  0:00:05s\n",
      "epoch 114| loss: 0.38075 |  0:00:05s\n",
      "epoch 115| loss: 0.38803 |  0:00:05s\n",
      "epoch 116| loss: 0.38169 |  0:00:06s\n",
      "epoch 117| loss: 0.39511 |  0:00:06s\n",
      "epoch 118| loss: 0.39264 |  0:00:06s\n",
      "epoch 119| loss: 0.3902  |  0:00:06s\n",
      "epoch 120| loss: 0.3961  |  0:00:06s\n",
      "epoch 121| loss: 0.38554 |  0:00:06s\n",
      "epoch 122| loss: 0.38045 |  0:00:06s\n",
      "epoch 123| loss: 0.38715 |  0:00:06s\n",
      "epoch 124| loss: 0.39292 |  0:00:06s\n",
      "epoch 125| loss: 0.39562 |  0:00:06s\n",
      "epoch 126| loss: 0.39354 |  0:00:06s\n",
      "epoch 127| loss: 0.38614 |  0:00:06s\n",
      "epoch 128| loss: 0.38406 |  0:00:06s\n",
      "epoch 129| loss: 0.3801  |  0:00:06s\n",
      "epoch 130| loss: 0.3781  |  0:00:06s\n",
      "epoch 131| loss: 0.37751 |  0:00:06s\n",
      "epoch 132| loss: 0.37483 |  0:00:06s\n",
      "epoch 133| loss: 0.37149 |  0:00:06s\n",
      "epoch 134| loss: 0.36729 |  0:00:06s\n",
      "epoch 135| loss: 0.36143 |  0:00:06s\n",
      "epoch 136| loss: 0.36104 |  0:00:06s\n",
      "epoch 137| loss: 0.3577  |  0:00:07s\n",
      "epoch 138| loss: 0.3615  |  0:00:07s\n",
      "epoch 139| loss: 0.35509 |  0:00:07s\n",
      "epoch 140| loss: 0.36142 |  0:00:07s\n",
      "epoch 141| loss: 0.35513 |  0:00:07s\n",
      "epoch 142| loss: 0.3468  |  0:00:07s\n",
      "epoch 143| loss: 0.34713 |  0:00:07s\n",
      "epoch 144| loss: 0.34398 |  0:00:07s\n",
      "epoch 145| loss: 0.34907 |  0:00:07s\n",
      "epoch 146| loss: 0.34366 |  0:00:07s\n",
      "epoch 147| loss: 0.33914 |  0:00:07s\n",
      "epoch 148| loss: 0.33699 |  0:00:07s\n",
      "epoch 149| loss: 0.32931 |  0:00:07s\n",
      "epoch 150| loss: 0.33362 |  0:00:07s\n",
      "epoch 151| loss: 0.33424 |  0:00:07s\n",
      "epoch 152| loss: 0.33461 |  0:00:07s\n",
      "epoch 153| loss: 0.33516 |  0:00:07s\n",
      "epoch 154| loss: 0.34614 |  0:00:07s\n",
      "epoch 155| loss: 0.36231 |  0:00:07s\n",
      "epoch 156| loss: 0.35786 |  0:00:07s\n",
      "epoch 157| loss: 0.35465 |  0:00:07s\n",
      "epoch 158| loss: 0.34475 |  0:00:08s\n",
      "epoch 159| loss: 0.3483  |  0:00:08s\n",
      "epoch 160| loss: 0.33894 |  0:00:08s\n",
      "epoch 161| loss: 0.33857 |  0:00:08s\n",
      "epoch 162| loss: 0.34392 |  0:00:08s\n",
      "epoch 163| loss: 0.33566 |  0:00:08s\n",
      "epoch 164| loss: 0.33076 |  0:00:08s\n",
      "epoch 165| loss: 0.32852 |  0:00:08s\n",
      "epoch 166| loss: 0.33604 |  0:00:08s\n",
      "epoch 167| loss: 0.33294 |  0:00:08s\n",
      "epoch 168| loss: 0.32373 |  0:00:08s\n",
      "epoch 169| loss: 0.32464 |  0:00:08s\n",
      "epoch 170| loss: 0.32145 |  0:00:08s\n",
      "epoch 171| loss: 0.32798 |  0:00:08s\n",
      "epoch 172| loss: 0.32168 |  0:00:08s\n",
      "epoch 173| loss: 0.3253  |  0:00:08s\n",
      "epoch 174| loss: 0.31638 |  0:00:08s\n",
      "epoch 175| loss: 0.31692 |  0:00:08s\n",
      "epoch 176| loss: 0.31902 |  0:00:08s\n",
      "epoch 177| loss: 0.30534 |  0:00:08s\n",
      "epoch 178| loss: 0.29903 |  0:00:08s\n",
      "epoch 179| loss: 0.31335 |  0:00:09s\n",
      "epoch 180| loss: 0.30341 |  0:00:09s\n",
      "epoch 181| loss: 0.31308 |  0:00:09s\n",
      "epoch 182| loss: 0.30399 |  0:00:09s\n",
      "epoch 183| loss: 0.30743 |  0:00:09s\n",
      "epoch 184| loss: 0.30517 |  0:00:09s\n",
      "epoch 185| loss: 0.30503 |  0:00:09s\n",
      "epoch 186| loss: 0.29693 |  0:00:09s\n",
      "epoch 187| loss: 0.3018  |  0:00:09s\n",
      "epoch 188| loss: 0.30708 |  0:00:09s\n",
      "epoch 189| loss: 0.31732 |  0:00:09s\n",
      "epoch 190| loss: 0.32049 |  0:00:09s\n",
      "epoch 191| loss: 0.31446 |  0:00:09s\n",
      "epoch 192| loss: 0.31319 |  0:00:09s\n",
      "epoch 193| loss: 0.32478 |  0:00:09s\n",
      "epoch 194| loss: 0.33138 |  0:00:09s\n",
      "epoch 195| loss: 0.32402 |  0:00:09s\n",
      "epoch 196| loss: 0.32298 |  0:00:09s\n",
      "epoch 197| loss: 0.3098  |  0:00:09s\n",
      "epoch 198| loss: 0.31211 |  0:00:09s\n",
      "epoch 199| loss: 0.30954 |  0:00:09s\n",
      "epoch 200| loss: 0.30828 |  0:00:10s\n",
      "epoch 201| loss: 0.30802 |  0:00:10s\n",
      "epoch 202| loss: 0.29822 |  0:00:10s\n",
      "epoch 203| loss: 0.29937 |  0:00:10s\n",
      "epoch 204| loss: 0.29349 |  0:00:10s\n",
      "epoch 205| loss: 0.3006  |  0:00:10s\n",
      "epoch 206| loss: 0.31119 |  0:00:10s\n",
      "epoch 207| loss: 0.30711 |  0:00:10s\n",
      "epoch 208| loss: 0.30833 |  0:00:10s\n",
      "epoch 209| loss: 0.30129 |  0:00:10s\n",
      "epoch 210| loss: 0.2942  |  0:00:10s\n",
      "epoch 211| loss: 0.292   |  0:00:10s\n",
      "epoch 212| loss: 0.29318 |  0:00:10s\n",
      "epoch 213| loss: 0.29648 |  0:00:10s\n",
      "epoch 214| loss: 0.28428 |  0:00:10s\n",
      "epoch 215| loss: 0.28626 |  0:00:10s\n",
      "epoch 216| loss: 0.29214 |  0:00:10s\n",
      "epoch 217| loss: 0.2918  |  0:00:10s\n",
      "epoch 218| loss: 0.29217 |  0:00:10s\n",
      "epoch 219| loss: 0.2874  |  0:00:11s\n",
      "epoch 220| loss: 0.2871  |  0:00:11s\n",
      "epoch 221| loss: 0.28943 |  0:00:11s\n",
      "epoch 222| loss: 0.2815  |  0:00:11s\n",
      "epoch 223| loss: 0.29426 |  0:00:11s\n",
      "epoch 224| loss: 0.29592 |  0:00:11s\n",
      "epoch 225| loss: 0.2978  |  0:00:11s\n",
      "epoch 226| loss: 0.2967  |  0:00:11s\n",
      "epoch 227| loss: 0.29798 |  0:00:11s\n",
      "epoch 228| loss: 0.29811 |  0:00:11s\n",
      "epoch 229| loss: 0.29227 |  0:00:11s\n",
      "epoch 230| loss: 0.28356 |  0:00:11s\n",
      "epoch 231| loss: 0.29011 |  0:00:11s\n",
      "epoch 232| loss: 0.28439 |  0:00:11s\n",
      "epoch 233| loss: 0.28272 |  0:00:11s\n",
      "epoch 234| loss: 0.27713 |  0:00:11s\n",
      "epoch 235| loss: 0.28031 |  0:00:11s\n",
      "epoch 236| loss: 0.27333 |  0:00:11s\n",
      "epoch 237| loss: 0.27165 |  0:00:11s\n",
      "epoch 238| loss: 0.27734 |  0:00:11s\n",
      "epoch 239| loss: 0.27376 |  0:00:12s\n",
      "epoch 240| loss: 0.26623 |  0:00:12s\n",
      "epoch 241| loss: 0.26052 |  0:00:12s\n",
      "epoch 242| loss: 0.26002 |  0:00:12s\n",
      "epoch 243| loss: 0.25252 |  0:00:12s\n",
      "epoch 244| loss: 0.25153 |  0:00:12s\n",
      "epoch 245| loss: 0.25134 |  0:00:12s\n",
      "epoch 246| loss: 0.25492 |  0:00:12s\n",
      "epoch 247| loss: 0.25407 |  0:00:12s\n",
      "epoch 248| loss: 0.25068 |  0:00:12s\n",
      "epoch 249| loss: 0.24869 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.17561 |  0:00:00s\n",
      "epoch 1  | loss: 1.96278 |  0:00:00s\n",
      "epoch 2  | loss: 1.5142  |  0:00:00s\n",
      "epoch 3  | loss: 1.14604 |  0:00:00s\n",
      "epoch 4  | loss: 0.99016 |  0:00:00s\n",
      "epoch 5  | loss: 0.92803 |  0:00:00s\n",
      "epoch 6  | loss: 0.86633 |  0:00:00s\n",
      "epoch 7  | loss: 0.83679 |  0:00:00s\n",
      "epoch 8  | loss: 0.80491 |  0:00:00s\n",
      "epoch 9  | loss: 0.75397 |  0:00:00s\n",
      "epoch 10 | loss: 0.74072 |  0:00:00s\n",
      "epoch 11 | loss: 0.71566 |  0:00:00s\n",
      "epoch 12 | loss: 0.70368 |  0:00:00s\n",
      "epoch 13 | loss: 0.69701 |  0:00:00s\n",
      "epoch 14 | loss: 0.69419 |  0:00:00s\n",
      "epoch 15 | loss: 0.68001 |  0:00:00s\n",
      "epoch 16 | loss: 0.66379 |  0:00:00s\n",
      "epoch 17 | loss: 0.64793 |  0:00:00s\n",
      "epoch 18 | loss: 0.65847 |  0:00:00s\n",
      "epoch 19 | loss: 0.66242 |  0:00:01s\n",
      "epoch 20 | loss: 0.66663 |  0:00:01s\n",
      "epoch 21 | loss: 0.661   |  0:00:01s\n",
      "epoch 22 | loss: 0.64132 |  0:00:01s\n",
      "epoch 23 | loss: 0.63772 |  0:00:01s\n",
      "epoch 24 | loss: 0.63212 |  0:00:01s\n",
      "epoch 25 | loss: 0.62681 |  0:00:01s\n",
      "epoch 26 | loss: 0.62206 |  0:00:01s\n",
      "epoch 27 | loss: 0.62014 |  0:00:01s\n",
      "epoch 28 | loss: 0.60973 |  0:00:01s\n",
      "epoch 29 | loss: 0.60399 |  0:00:01s\n",
      "epoch 30 | loss: 0.59626 |  0:00:01s\n",
      "epoch 31 | loss: 0.60579 |  0:00:01s\n",
      "epoch 32 | loss: 0.59889 |  0:00:01s\n",
      "epoch 33 | loss: 0.5896  |  0:00:01s\n",
      "epoch 34 | loss: 0.59848 |  0:00:01s\n",
      "epoch 35 | loss: 0.59395 |  0:00:01s\n",
      "epoch 36 | loss: 0.59082 |  0:00:01s\n",
      "epoch 37 | loss: 0.58563 |  0:00:01s\n",
      "epoch 38 | loss: 0.58594 |  0:00:02s\n",
      "epoch 39 | loss: 0.57288 |  0:00:02s\n",
      "epoch 40 | loss: 0.57888 |  0:00:02s\n",
      "epoch 41 | loss: 0.5678  |  0:00:02s\n",
      "epoch 42 | loss: 0.56083 |  0:00:02s\n",
      "epoch 43 | loss: 0.55176 |  0:00:02s\n",
      "epoch 44 | loss: 0.5615  |  0:00:02s\n",
      "epoch 45 | loss: 0.54305 |  0:00:02s\n",
      "epoch 46 | loss: 0.54618 |  0:00:02s\n",
      "epoch 47 | loss: 0.53067 |  0:00:02s\n",
      "epoch 48 | loss: 0.52949 |  0:00:02s\n",
      "epoch 49 | loss: 0.52526 |  0:00:02s\n",
      "epoch 50 | loss: 0.52545 |  0:00:02s\n",
      "epoch 51 | loss: 0.51957 |  0:00:02s\n",
      "epoch 52 | loss: 0.52065 |  0:00:02s\n",
      "epoch 53 | loss: 0.50999 |  0:00:02s\n",
      "epoch 54 | loss: 0.5181  |  0:00:02s\n",
      "epoch 55 | loss: 0.51432 |  0:00:02s\n",
      "epoch 56 | loss: 0.5275  |  0:00:02s\n",
      "epoch 57 | loss: 0.51784 |  0:00:03s\n",
      "epoch 58 | loss: 0.51028 |  0:00:03s\n",
      "epoch 59 | loss: 0.51226 |  0:00:03s\n",
      "epoch 60 | loss: 0.51521 |  0:00:03s\n",
      "epoch 61 | loss: 0.51147 |  0:00:03s\n",
      "epoch 62 | loss: 0.51968 |  0:00:03s\n",
      "epoch 63 | loss: 0.51441 |  0:00:03s\n",
      "epoch 64 | loss: 0.521   |  0:00:03s\n",
      "epoch 65 | loss: 0.50392 |  0:00:03s\n",
      "epoch 66 | loss: 0.50647 |  0:00:03s\n",
      "epoch 67 | loss: 0.51009 |  0:00:03s\n",
      "epoch 68 | loss: 0.5068  |  0:00:03s\n",
      "epoch 69 | loss: 0.49726 |  0:00:03s\n",
      "epoch 70 | loss: 0.49285 |  0:00:03s\n",
      "epoch 71 | loss: 0.48552 |  0:00:03s\n",
      "epoch 72 | loss: 0.48446 |  0:00:03s\n",
      "epoch 73 | loss: 0.4884  |  0:00:03s\n",
      "epoch 74 | loss: 0.47458 |  0:00:03s\n",
      "epoch 75 | loss: 0.47866 |  0:00:03s\n",
      "epoch 76 | loss: 0.47523 |  0:00:03s\n",
      "epoch 77 | loss: 0.47695 |  0:00:04s\n",
      "epoch 78 | loss: 0.47072 |  0:00:04s\n",
      "epoch 79 | loss: 0.48309 |  0:00:04s\n",
      "epoch 80 | loss: 0.47428 |  0:00:04s\n",
      "epoch 81 | loss: 0.46762 |  0:00:04s\n",
      "epoch 82 | loss: 0.46821 |  0:00:04s\n",
      "epoch 83 | loss: 0.46417 |  0:00:04s\n",
      "epoch 84 | loss: 0.45456 |  0:00:04s\n",
      "epoch 85 | loss: 0.45181 |  0:00:04s\n",
      "epoch 86 | loss: 0.45019 |  0:00:04s\n",
      "epoch 87 | loss: 0.44847 |  0:00:04s\n",
      "epoch 88 | loss: 0.44492 |  0:00:04s\n",
      "epoch 89 | loss: 0.4527  |  0:00:04s\n",
      "epoch 90 | loss: 0.44264 |  0:00:04s\n",
      "epoch 91 | loss: 0.44011 |  0:00:04s\n",
      "epoch 92 | loss: 0.45906 |  0:00:04s\n",
      "epoch 93 | loss: 0.45019 |  0:00:04s\n",
      "epoch 94 | loss: 0.4567  |  0:00:04s\n",
      "epoch 95 | loss: 0.45766 |  0:00:04s\n",
      "epoch 96 | loss: 0.44498 |  0:00:05s\n",
      "epoch 97 | loss: 0.45109 |  0:00:05s\n",
      "epoch 98 | loss: 0.44143 |  0:00:05s\n",
      "epoch 99 | loss: 0.45206 |  0:00:05s\n",
      "epoch 100| loss: 0.44142 |  0:00:05s\n",
      "epoch 101| loss: 0.44347 |  0:00:05s\n",
      "epoch 102| loss: 0.45385 |  0:00:05s\n",
      "epoch 103| loss: 0.44333 |  0:00:05s\n",
      "epoch 104| loss: 0.42773 |  0:00:05s\n",
      "epoch 105| loss: 0.43011 |  0:00:05s\n",
      "epoch 106| loss: 0.43168 |  0:00:05s\n",
      "epoch 107| loss: 0.42855 |  0:00:05s\n",
      "epoch 108| loss: 0.42821 |  0:00:05s\n",
      "epoch 109| loss: 0.43759 |  0:00:05s\n",
      "epoch 110| loss: 0.43007 |  0:00:05s\n",
      "epoch 111| loss: 0.43012 |  0:00:05s\n",
      "epoch 112| loss: 0.4298  |  0:00:05s\n",
      "epoch 113| loss: 0.42482 |  0:00:05s\n",
      "epoch 114| loss: 0.41745 |  0:00:05s\n",
      "epoch 115| loss: 0.41248 |  0:00:06s\n",
      "epoch 116| loss: 0.41052 |  0:00:06s\n",
      "epoch 117| loss: 0.41201 |  0:00:06s\n",
      "epoch 118| loss: 0.41041 |  0:00:06s\n",
      "epoch 119| loss: 0.40342 |  0:00:06s\n",
      "epoch 120| loss: 0.40342 |  0:00:06s\n",
      "epoch 121| loss: 0.39938 |  0:00:06s\n",
      "epoch 122| loss: 0.4022  |  0:00:06s\n",
      "epoch 123| loss: 0.39838 |  0:00:06s\n",
      "epoch 124| loss: 0.38998 |  0:00:06s\n",
      "epoch 125| loss: 0.39436 |  0:00:06s\n",
      "epoch 126| loss: 0.40654 |  0:00:06s\n",
      "epoch 127| loss: 0.39538 |  0:00:06s\n",
      "epoch 128| loss: 0.3937  |  0:00:06s\n",
      "epoch 129| loss: 0.39733 |  0:00:06s\n",
      "epoch 130| loss: 0.39139 |  0:00:06s\n",
      "epoch 131| loss: 0.38659 |  0:00:06s\n",
      "epoch 132| loss: 0.39147 |  0:00:06s\n",
      "epoch 133| loss: 0.38342 |  0:00:06s\n",
      "epoch 134| loss: 0.38891 |  0:00:07s\n",
      "epoch 135| loss: 0.39524 |  0:00:07s\n",
      "epoch 136| loss: 0.39719 |  0:00:07s\n",
      "epoch 137| loss: 0.38863 |  0:00:07s\n",
      "epoch 138| loss: 0.38415 |  0:00:07s\n",
      "epoch 139| loss: 0.38349 |  0:00:07s\n",
      "epoch 140| loss: 0.38968 |  0:00:07s\n",
      "epoch 141| loss: 0.38396 |  0:00:07s\n",
      "epoch 142| loss: 0.38418 |  0:00:07s\n",
      "epoch 143| loss: 0.3947  |  0:00:07s\n",
      "epoch 144| loss: 0.38312 |  0:00:07s\n",
      "epoch 145| loss: 0.38693 |  0:00:07s\n",
      "epoch 146| loss: 0.38452 |  0:00:07s\n",
      "epoch 147| loss: 0.38458 |  0:00:07s\n",
      "epoch 148| loss: 0.3743  |  0:00:07s\n",
      "epoch 149| loss: 0.37743 |  0:00:07s\n",
      "epoch 150| loss: 0.38509 |  0:00:07s\n",
      "epoch 151| loss: 0.39131 |  0:00:08s\n",
      "epoch 152| loss: 0.38573 |  0:00:08s\n",
      "epoch 153| loss: 0.37369 |  0:00:08s\n",
      "epoch 154| loss: 0.36823 |  0:00:08s\n",
      "epoch 155| loss: 0.38857 |  0:00:08s\n",
      "epoch 156| loss: 0.381   |  0:00:08s\n",
      "epoch 157| loss: 0.38564 |  0:00:08s\n",
      "epoch 158| loss: 0.38483 |  0:00:08s\n",
      "epoch 159| loss: 0.37601 |  0:00:08s\n",
      "epoch 160| loss: 0.38508 |  0:00:08s\n",
      "epoch 161| loss: 0.38088 |  0:00:08s\n",
      "epoch 162| loss: 0.37511 |  0:00:08s\n",
      "epoch 163| loss: 0.36525 |  0:00:08s\n",
      "epoch 164| loss: 0.37101 |  0:00:08s\n",
      "epoch 165| loss: 0.36811 |  0:00:08s\n",
      "epoch 166| loss: 0.36276 |  0:00:08s\n",
      "epoch 167| loss: 0.35595 |  0:00:08s\n",
      "epoch 168| loss: 0.35012 |  0:00:08s\n",
      "epoch 169| loss: 0.35034 |  0:00:08s\n",
      "epoch 170| loss: 0.34772 |  0:00:09s\n",
      "epoch 171| loss: 0.34762 |  0:00:09s\n",
      "epoch 172| loss: 0.35422 |  0:00:09s\n",
      "epoch 173| loss: 0.36041 |  0:00:09s\n",
      "epoch 174| loss: 0.35012 |  0:00:09s\n",
      "epoch 175| loss: 0.35338 |  0:00:09s\n",
      "epoch 176| loss: 0.35319 |  0:00:09s\n",
      "epoch 177| loss: 0.34337 |  0:00:09s\n",
      "epoch 178| loss: 0.33973 |  0:00:09s\n",
      "epoch 179| loss: 0.34529 |  0:00:09s\n",
      "epoch 180| loss: 0.34736 |  0:00:09s\n",
      "epoch 181| loss: 0.34299 |  0:00:09s\n",
      "epoch 182| loss: 0.3393  |  0:00:09s\n",
      "epoch 183| loss: 0.32969 |  0:00:09s\n",
      "epoch 184| loss: 0.34097 |  0:00:09s\n",
      "epoch 185| loss: 0.33258 |  0:00:09s\n",
      "epoch 186| loss: 0.32536 |  0:00:09s\n",
      "epoch 187| loss: 0.32619 |  0:00:09s\n",
      "epoch 188| loss: 0.32186 |  0:00:09s\n",
      "epoch 189| loss: 0.3237  |  0:00:10s\n",
      "epoch 190| loss: 0.32562 |  0:00:10s\n",
      "epoch 191| loss: 0.32648 |  0:00:10s\n",
      "epoch 192| loss: 0.31634 |  0:00:10s\n",
      "epoch 193| loss: 0.31721 |  0:00:10s\n",
      "epoch 194| loss: 0.31482 |  0:00:10s\n",
      "epoch 195| loss: 0.31017 |  0:00:10s\n",
      "epoch 196| loss: 0.31798 |  0:00:10s\n",
      "epoch 197| loss: 0.30401 |  0:00:10s\n",
      "epoch 198| loss: 0.3105  |  0:00:10s\n",
      "epoch 199| loss: 0.32064 |  0:00:10s\n",
      "epoch 200| loss: 0.32496 |  0:00:10s\n",
      "epoch 201| loss: 0.32828 |  0:00:10s\n",
      "epoch 202| loss: 0.33608 |  0:00:10s\n",
      "epoch 203| loss: 0.34196 |  0:00:10s\n",
      "epoch 204| loss: 0.33897 |  0:00:10s\n",
      "epoch 205| loss: 0.33254 |  0:00:10s\n",
      "epoch 206| loss: 0.33388 |  0:00:10s\n",
      "epoch 207| loss: 0.32877 |  0:00:10s\n",
      "epoch 208| loss: 0.33084 |  0:00:11s\n",
      "epoch 209| loss: 0.32437 |  0:00:11s\n",
      "epoch 210| loss: 0.32741 |  0:00:11s\n",
      "epoch 211| loss: 0.32468 |  0:00:11s\n",
      "epoch 212| loss: 0.3197  |  0:00:11s\n",
      "epoch 213| loss: 0.33104 |  0:00:11s\n",
      "epoch 214| loss: 0.31973 |  0:00:11s\n",
      "epoch 215| loss: 0.31543 |  0:00:11s\n",
      "epoch 216| loss: 0.31335 |  0:00:11s\n",
      "epoch 217| loss: 0.30104 |  0:00:11s\n",
      "epoch 218| loss: 0.31855 |  0:00:11s\n",
      "epoch 219| loss: 0.32479 |  0:00:11s\n",
      "epoch 220| loss: 0.3268  |  0:00:11s\n",
      "epoch 221| loss: 0.32224 |  0:00:11s\n",
      "epoch 222| loss: 0.32027 |  0:00:11s\n",
      "epoch 223| loss: 0.31522 |  0:00:11s\n",
      "epoch 224| loss: 0.31316 |  0:00:11s\n",
      "epoch 225| loss: 0.31854 |  0:00:11s\n",
      "epoch 226| loss: 0.30223 |  0:00:11s\n",
      "epoch 227| loss: 0.30252 |  0:00:12s\n",
      "epoch 228| loss: 0.29296 |  0:00:12s\n",
      "epoch 229| loss: 0.29526 |  0:00:12s\n",
      "epoch 230| loss: 0.29664 |  0:00:12s\n",
      "epoch 231| loss: 0.28958 |  0:00:12s\n",
      "epoch 232| loss: 0.29413 |  0:00:12s\n",
      "epoch 233| loss: 0.28904 |  0:00:12s\n",
      "epoch 234| loss: 0.28323 |  0:00:12s\n",
      "epoch 235| loss: 0.28641 |  0:00:12s\n",
      "epoch 236| loss: 0.28265 |  0:00:12s\n",
      "epoch 237| loss: 0.28182 |  0:00:12s\n",
      "epoch 238| loss: 0.28607 |  0:00:12s\n",
      "epoch 239| loss: 0.28442 |  0:00:12s\n",
      "epoch 240| loss: 0.27673 |  0:00:12s\n",
      "epoch 241| loss: 0.28134 |  0:00:12s\n",
      "epoch 242| loss: 0.28108 |  0:00:12s\n",
      "epoch 243| loss: 0.27934 |  0:00:12s\n",
      "epoch 244| loss: 0.28415 |  0:00:12s\n",
      "epoch 245| loss: 0.28668 |  0:00:12s\n",
      "epoch 246| loss: 0.28166 |  0:00:12s\n",
      "epoch 247| loss: 0.27643 |  0:00:13s\n",
      "epoch 248| loss: 0.26644 |  0:00:13s\n",
      "epoch 249| loss: 0.26896 |  0:00:13s\n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Invalid value assigned: Index is not a ListConfig, list or tuple.\n",
      "    full_key: continuous_cols\n",
      "    object_type=None\n"
     ]
    }
   ],
   "source": [
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "            \"ordinal_imputer\": name_ordinal_imputer, \n",
    "            \"continuous_imputer\": name_continuous_imputer, \n",
    "            \"model\": name_model, \"train_shape\" : [df_X.shape[0]-1, df_X.shape[1]],\n",
    "            \"test_shape\": [1, df_X.shape[1]]\n",
    "        }\n",
    "    \n",
    "    # Define the subset of keys you care about\n",
    "    keys_to_check = ['ordinal_imputer', 'continuous_imputer', 'model']  # or whatever subset you want\n",
    "\n",
    "    # Check if a result in all_dict_results has the same values for just those keys\n",
    "    if any(all(result['params'].get(k) == params.get(k) for k in keys_to_check) for result in all_dict_results):\n",
    "        print(f\"Skipping existing combination (subset match): {[params[k] for k in keys_to_check]}\")\n",
    "        continue\n",
    "\n",
    "    dict_results = {\n",
    "            \"params\": params, \n",
    "            \"imputation_time\": [],\n",
    "            \"fitting_time\": [], \n",
    "            \"results_adj\": [], \n",
    "            \"results_org\": []\n",
    "        }\n",
    "\n",
    "    for test_nloc in test_indices: \n",
    "        print(test_nloc)\n",
    "\n",
    "        idx_train = [True for i in range(df_X.shape[0])]\n",
    "        idx_test = [False for i in range(df_X.shape[0])]\n",
    "\n",
    "        idx_test[test_nloc] = True\n",
    "        idx_train[test_nloc] = False\n",
    "\n",
    "        df_X_train = df_X[dict_select[\"MRIth\"]].loc[idx_train]\n",
    "        df_X_test = df_X[dict_select[\"MRIth\"]].loc[idx_test]\n",
    "\n",
    "        df_y_train = df_y.loc[idx_train]\n",
    "        df_y_test = df_y.loc[idx_test]\n",
    "\n",
    "        c_train = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_train]\n",
    "        c_test = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_test]\n",
    "\n",
    "        try: \n",
    "        \n",
    "            # Now you can call your `train_model` function with these components\n",
    "            fold_dict_results = train_imputer_model(\n",
    "                df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "                c_train, c_test,\n",
    "                ordinal_imputer_instance, name_ordinal_imputer,\n",
    "                continuous_imputer_instance, name_continuous_imputer,\n",
    "                model_instance, name_model,\n",
    "                separate_imputers=True  # Or however you want to specify\n",
    "            )\n",
    "            \n",
    "            dict_results[\"imputation_time\"].append(fold_dict_results[\"imputation_time\"]) \n",
    "            dict_results[\"fitting_time\"].append(fold_dict_results[\"fitting_time\"])  \n",
    "            dict_results[\"results_adj\"].append(fold_dict_results[\"results_adj\"])  \n",
    "            dict_results[\"results_org\"].append(fold_dict_results[\"results_org\"])  \n",
    "\n",
    "        except Exception as e:  \n",
    "\n",
    "            print(e)\n",
    "            \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "    # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open(results_file, 'wb') as handle:\n",
    "    pickle.dump(all_dict_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_file, \"rb\") as input_file:\n",
    "    all_dict_results = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Table for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickle/training_3_loonona_dict_results.pickle', \"rb\") as input_file:\n",
    "    dict_results_loo_nona = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metric_table(\n",
    "results_list,\n",
    "    targets,\n",
    "    metric_name=\"r2\",\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=None,\n",
    "    sort_order=\"descending\"\n",
    "):\n",
    "    key = \"results_adj\" if source.lower() == \"adjusted\" else \"results_org\"\n",
    "    df_rows = []\n",
    "\n",
    "    for res in results_list:\n",
    "        result_blocks = res.get(key, [])\n",
    "        if not isinstance(result_blocks, list) or len(result_blocks) == 0:\n",
    "            continue\n",
    "\n",
    "        y_preds_all = {t: [] for t in targets}\n",
    "        y_tests_all = {t: [] for t in targets}\n",
    "\n",
    "        for fold in result_blocks:\n",
    "            y_pred = fold[\"y_pred\"]\n",
    "            y_test = fold[\"y_test\"]\n",
    "            if y_pred.shape != y_test.shape:\n",
    "                continue\n",
    "\n",
    "            for i, target in enumerate(targets):\n",
    "                if y_pred.shape[1] <= i:\n",
    "                    continue\n",
    "                y_preds_all[target].append(y_pred[0, i])\n",
    "                y_tests_all[target].append(y_test[0, i])\n",
    "\n",
    "        target_metrics = []\n",
    "        for target in targets:\n",
    "            y_preds = y_preds_all[target]\n",
    "            y_tests = y_tests_all[target]\n",
    "\n",
    "            if len(y_preds) < 2:\n",
    "                metric = float(\"nan\")\n",
    "            else:\n",
    "                if metric_name == \"r2\":\n",
    "                    metric = r2_score(y_tests, y_preds)\n",
    "                elif metric_name == \"mae\":\n",
    "                    metric = mean_absolute_error(y_tests, y_preds)\n",
    "                elif metric_name == \"mse\":\n",
    "                    metric = mean_squared_error(y_tests, y_preds)\n",
    "                elif metric_name == \"corr\":\n",
    "                    metric = pearsonr(y_tests, y_preds)[0]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported metric: {metric_name}\")\n",
    "            target_metrics.append(metric)\n",
    "\n",
    "        # Mean  std across targets\n",
    "        mean_metric = np.nanmean(target_metrics)\n",
    "        std_metric = np.nanstd(target_metrics)\n",
    "\n",
    "        # Time stats\n",
    "        imp_times = np.array(res.get(\"imputation_time\", []), dtype=np.float64)\n",
    "        fit_times = np.array(res.get(\"fitting_time\", []), dtype=np.float64)\n",
    "\n",
    "        imp_time_mean = np.nanmean(imp_times)\n",
    "        imp_time_std = np.nanstd(imp_times)\n",
    "\n",
    "        fit_time_mean = np.nanmean(fit_times)\n",
    "        fit_time_std = np.nanstd(fit_times)\n",
    "\n",
    "        params = res.get(\"params\", {})\n",
    "        df_rows.append({\n",
    "            \"Ordinal Imputer\": params.get(\"ordinal_imputer\"),\n",
    "            \"Continuous Imputer\": params.get(\"continuous_imputer\"),\n",
    "            \"Model\": params.get(\"model\"),\n",
    "            **{target: m for target, m in zip(targets, target_metrics)},\n",
    "            \"Mean  Std\": f\"{mean_metric:.3f}  {std_metric:.3f}\",\n",
    "            \"Imputation Time\": f\"{imp_time_mean:.2f}  {imp_time_std:.2f}\",\n",
    "            \"Fitting Time\": f\"{fit_time_mean:.2f}  {fit_time_std:.2f}\"\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(df_rows)\n",
    "\n",
    "    # Optional: sort\n",
    "    if sort_order and targets:\n",
    "        df = df.sort_values(by=targets[0], ascending=(sort_order == \"ascending\"))\n",
    "\n",
    "    # Optional: Save to CSV\n",
    "    if csv_filename:\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # Format for LaTeX\n",
    "    latex_df = df.copy()\n",
    "    for col in targets:\n",
    "        latex_df[col] = latex_df[col].apply(lambda x: f\"{x:.3f}\" if pd.notnull(x) else \"\")\n",
    "    latex_output = latex_df.to_latex(index=False, escape=False)\n",
    "\n",
    "    return df, latex_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllll}\n",
      "\\toprule\n",
      "Ordinal Imputer & Continuous Imputer & Model & ADNI_MEM & ADNI_EF & ADNI_VS & ADNI_LAN & Mean  Std & Imputation Time & Fitting Time \\\\\n",
      "\\midrule\n",
      "SimpleImputer_most_frequent & KNNImputer & GatedAdditiveTreeEnsembleConfig_tab & 0.701 & 0.774 & 0.245 & 0.823 & 0.636  0.230 & 3.50  0.07 & 17.88  0.19 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & RandomForestRegressor & 0.686 & 0.653 & 0.297 & 0.763 & 0.600  0.180 & 3.38  0.07 & 52.01  0.67 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabNetRegressor_default & 0.637 & 0.642 & 0.682 & 0.529 & 0.622  0.057 & 3.40  0.19 & 13.19  0.90 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskLasso_tuned & 0.606 & 0.717 & 0.343 & 0.676 & 0.585  0.146 & 3.53  0.05 & 1.51  0.01 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskElasticNet_tuned & 0.597 & 0.714 & 0.346 & 0.672 & 0.582  0.143 & 3.50  0.03 & 1.50  0.01 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & LinearRegression & 0.591 & 0.705 & 0.371 & 0.663 & 0.583  0.129 & 3.42  0.11 & 0.15  0.06 \\\\\n",
      "NoImputer & NoImputer & RandomForestRegressor & 0.590 & 0.548 & -0.047 & 0.646 & 0.434  0.280 & nan  nan & 32.37  0.24 \\\\\n",
      "NoImputer & NoImputer & XGBoostRegressor & 0.543 & 0.636 & 0.246 & 0.636 & 0.515  0.160 & nan  nan & 0.78  0.02 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabNetRegressor_custom & 0.505 & 0.482 & 0.055 & 0.511 & 0.388  0.193 & 3.30  0.07 & 12.84  0.69 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & PLSRegression_4_components & 0.504 & 0.556 & 0.078 & 0.525 & 0.416  0.196 & 3.44  0.13 & 0.18  0.06 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabTransformerConfig_tab & 0.470 & 0.259 & 0.146 & 0.159 & 0.258  0.130 & 3.41  0.07 & 1.50  0.11 \\\\\n",
      "NoImputer & NoImputer & PLSRegression_4_components & 0.434 & 0.447 & -0.052 & 0.458 & 0.322  0.216 & nan  nan & 0.06  0.01 \\\\\n",
      "NoImputer & NoImputer & MultiTaskElasticNet & 0.420 & 0.176 & -0.167 & 0.446 & 0.219  0.246 & nan  nan & 0.01  0.00 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskElasticNet & 0.420 & 0.176 & -0.167 & 0.446 & 0.219  0.246 & 3.48  0.04 & 0.09  0.00 \\\\\n",
      "NoImputer & NoImputer & TabNetRegressor_default & 0.415 & 0.585 & 0.427 & 0.339 & 0.442  0.089 & nan  nan & 12.02  0.46 \\\\\n",
      "NoImputer & NoImputer & XGBoostRegressor_tuned & 0.392 & 0.600 & 0.118 & 0.535 & 0.411  0.185 & nan  nan & 1.95  0.07 \\\\\n",
      "NoImputer & NoImputer & MultiTaskLasso_tuned & 0.346 & 0.556 & 0.017 & 0.396 & 0.329  0.196 & nan  nan & 0.83  0.01 \\\\\n",
      "NoImputer & NoImputer & MultiTaskElasticNet_tuned & 0.344 & 0.558 & 0.013 & 0.397 & 0.328  0.198 & nan  nan & 0.83  0.01 \\\\\n",
      "NoImputer & NoImputer & LinearRegression & 0.338 & 0.555 & 0.021 & 0.385 & 0.325  0.193 & nan  nan & 0.06  0.03 \\\\\n",
      "NoImputer & NoImputer & TabNetRegressor_custom & 0.306 & 0.150 & 0.115 & 0.238 & 0.202  0.075 & nan  nan & 12.36  0.63 \\\\\n",
      "NoImputer & NoImputer & MultiTaskLasso & 0.288 & -0.213 & 0.463 & -0.324 & 0.054  0.330 & nan  nan & 0.01  0.00 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskLasso & 0.288 & -0.213 & 0.463 & -0.324 & 0.054  0.330 & 3.47  0.05 & 0.07  0.00 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & DANetConfig_tab & 0.163 & -0.347 & -0.405 & 0.398 & -0.048  0.339 & 3.41  0.07 & 1.75  0.10 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabNetModelConfig_tab & -0.220 & 0.151 & 0.491 & 0.170 & 0.148  0.252 & 3.43  0.08 & 2.24  0.12 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_df, latex_corr = generate_metric_table(\n",
    "    results_list=dict_results_loo_nona,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='corr',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/3_training_loonona_corr_adjusted_sorted.csv\",\n",
    "    sort_order=\"descending\"\n",
    ")\n",
    "print(latex_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllll}\n",
      "\\toprule\n",
      "Ordinal Imputer & Continuous Imputer & Model & ADNI_MEM & ADNI_EF & ADNI_VS & ADNI_LAN & Mean  Std & Imputation Time & Fitting Time \\\\\n",
      "\\midrule\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskLasso_tuned & 0.322 & 0.475 & 0.088 & 0.304 & 0.297  0.138 & 3.53  0.05 & 1.51  0.01 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & LinearRegression & 0.314 & 0.463 & 0.109 & 0.316 & 0.301  0.126 & 3.42  0.11 & 0.15  0.06 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskElasticNet_tuned & 0.312 & 0.471 & 0.099 & 0.294 & 0.294  0.132 & 3.50  0.03 & 1.50  0.01 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & RandomForestRegressor & 0.200 & 0.328 & 0.057 & 0.214 & 0.200  0.096 & 3.38  0.07 & 52.01  0.67 \\\\\n",
      "NoImputer & NoImputer & RandomForestRegressor & 0.100 & 0.169 & -0.168 & 0.113 & 0.054  0.131 & nan  nan & 32.37  0.24 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabTransformerConfig_tab & 0.057 & -0.476 & -1.514 & -0.647 & -0.645  0.565 & 3.41  0.07 & 1.50  0.11 \\\\\n",
      "NoImputer & NoImputer & XGBoostRegressor & 0.051 & 0.301 & -0.000 & 0.103 & 0.114  0.114 & nan  nan & 0.78  0.02 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & GatedAdditiveTreeEnsembleConfig_tab & 0.023 & 0.529 & -0.082 & 0.348 & 0.204  0.245 & 3.50  0.07 & 17.88  0.19 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabNetRegressor_custom & 0.014 & 0.134 & -0.266 & -0.008 & -0.031  0.146 & 3.30  0.07 & 12.84  0.69 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & PLSRegression_4_components & -0.000 & 0.228 & -0.066 & -0.064 & 0.025  0.120 & 3.44  0.13 & 0.18  0.06 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabNetRegressor_default & -0.023 & 0.194 & 0.430 & -0.440 & 0.040  0.320 & 3.40  0.19 & 13.19  0.90 \\\\\n",
      "NoImputer & NoImputer & PLSRegression_4_components & -0.086 & 0.111 & -0.186 & -0.143 & -0.076  0.114 & nan  nan & 0.06  0.01 \\\\\n",
      "NoImputer & NoImputer & MultiTaskElasticNet_tuned & -0.092 & 0.249 & -0.277 & -0.151 & -0.068  0.195 & nan  nan & 0.83  0.01 \\\\\n",
      "NoImputer & NoImputer & MultiTaskLasso_tuned & -0.093 & 0.242 & -0.277 & -0.154 & -0.070  0.192 & nan  nan & 0.83  0.01 \\\\\n",
      "NoImputer & NoImputer & LinearRegression & -0.104 & 0.243 & -0.281 & -0.171 & -0.078  0.196 & nan  nan & 0.06  0.03 \\\\\n",
      "NoImputer & NoImputer & TabNetRegressor_custom & -0.172 & -0.096 & -0.102 & -0.248 & -0.155  0.062 & nan  nan & 12.36  0.63 \\\\\n",
      "NoImputer & NoImputer & TabNetRegressor_default & -0.173 & 0.214 & 0.140 & -0.453 & -0.068  0.265 & nan  nan & 12.02  0.46 \\\\\n",
      "NoImputer & NoImputer & MultiTaskElasticNet & -0.190 & -0.079 & -0.060 & -0.249 & -0.144  0.078 & nan  nan & 0.01  0.00 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskElasticNet & -0.190 & -0.079 & -0.060 & -0.249 & -0.144  0.078 & 3.48  0.04 & 0.09  0.00 \\\\\n",
      "NoImputer & NoImputer & XGBoostRegressor_tuned & -0.231 & 0.287 & -0.112 & 0.002 & -0.014  0.192 & nan  nan & 1.95  0.07 \\\\\n",
      "NoImputer & NoImputer & MultiTaskLasso & -0.405 & -0.179 & -0.020 & -0.504 & -0.277  0.189 & nan  nan & 0.01  0.00 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskLasso & -0.405 & -0.179 & -0.020 & -0.504 & -0.277  0.189 & 3.47  0.05 & 0.07  0.00 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & DANetConfig_tab & -0.488 & -0.197 & -0.228 & -0.309 & -0.306  0.113 & 3.41  0.07 & 1.75  0.10 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabNetModelConfig_tab & -0.526 & -0.400 & 0.014 & -1.210 & -0.531  0.440 & 3.43  0.08 & 2.24  0.12 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_df, latex_r2 = generate_metric_table(\n",
    "    results_list=dict_results_loo_nona,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='r2',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/3_training_loonona_r2_adjusted_sorted.csv\",\n",
    "    sort_order=\"descending\"\n",
    ")\n",
    "print(latex_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllll}\n",
      "\\toprule\n",
      "Ordinal Imputer & Continuous Imputer & Model & ADNI_MEM & ADNI_EF & ADNI_VS & ADNI_LAN & Mean  Std & Imputation Time & Fitting Time \\\\\n",
      "\\midrule\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskLasso_tuned & 0.633 & 0.547 & 0.584 & 0.596 & 0.590  0.031 & 3.53  0.05 & 1.51  0.01 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskElasticNet_tuned & 0.639 & 0.551 & 0.587 & 0.600 & 0.594  0.032 & 3.50  0.03 & 1.50  0.01 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & LinearRegression & 0.649 & 0.546 & 0.571 & 0.603 & 0.592  0.039 & 3.42  0.11 & 0.15  0.06 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & RandomForestRegressor & 0.666 & 0.669 & 0.645 & 0.603 & 0.646  0.026 & 3.38  0.07 & 52.01  0.67 \\\\\n",
      "NoImputer & NoImputer & RandomForestRegressor & 0.709 & 0.727 & 0.699 & 0.585 & 0.680  0.056 & nan  nan & 32.37  0.24 \\\\\n",
      "NoImputer & NoImputer & MultiTaskLasso_tuned & 0.725 & 0.699 & 0.711 & 0.695 & 0.708  0.012 & nan  nan & 0.83  0.01 \\\\\n",
      "NoImputer & NoImputer & MultiTaskElasticNet_tuned & 0.728 & 0.697 & 0.712 & 0.697 & 0.709  0.013 & nan  nan & 0.83  0.01 \\\\\n",
      "NoImputer & NoImputer & LinearRegression & 0.729 & 0.696 & 0.708 & 0.701 & 0.708  0.013 & nan  nan & 0.06  0.03 \\\\\n",
      "NoImputer & NoImputer & XGBoostRegressor & 0.736 & 0.702 & 0.626 & 0.595 & 0.665  0.056 & nan  nan & 0.78  0.02 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabNetRegressor_custom & 0.758 & 0.742 & 0.625 & 0.689 & 0.703  0.052 & 3.30  0.07 & 12.84  0.69 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabTransformerConfig_tab & 0.771 & 0.924 & 0.833 & 0.934 & 0.865  0.068 & 3.41  0.07 & 1.50  0.11 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabNetRegressor_default & 0.774 & 0.653 & 0.486 & 0.781 & 0.673  0.119 & 3.40  0.19 & 13.19  0.90 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & GatedAdditiveTreeEnsembleConfig_tab & 0.790 & 0.564 & 0.643 & 0.543 & 0.635  0.097 & 3.50  0.07 & 17.88  0.19 \\\\\n",
      "NoImputer & NoImputer & PLSRegression_4_components & 0.790 & 0.788 & 0.721 & 0.731 & 0.758  0.032 & nan  nan & 0.06  0.01 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & PLSRegression_4_components & 0.801 & 0.738 & 0.689 & 0.761 & 0.747  0.040 & 3.44  0.13 & 0.18  0.06 \\\\\n",
      "NoImputer & NoImputer & XGBoostRegressor_tuned & 0.835 & 0.686 & 0.678 & 0.684 & 0.720  0.066 & nan  nan & 1.95  0.07 \\\\\n",
      "NoImputer & NoImputer & TabNetRegressor_default & 0.849 & 0.692 & 0.509 & 0.780 & 0.708  0.127 & nan  nan & 12.02  0.46 \\\\\n",
      "NoImputer & NoImputer & TabNetRegressor_custom & 0.857 & 0.875 & 0.611 & 0.810 & 0.788  0.105 & nan  nan & 12.36  0.63 \\\\\n",
      "NoImputer & NoImputer & MultiTaskElasticNet & 0.884 & 0.787 & 0.655 & 0.826 & 0.788  0.084 & nan  nan & 0.01  0.00 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskElasticNet & 0.884 & 0.787 & 0.655 & 0.826 & 0.788  0.084 & 3.48  0.04 & 0.09  0.00 \\\\\n",
      "NoImputer & NoImputer & MultiTaskLasso & 0.991 & 0.785 & 0.622 & 0.934 & 0.833  0.143 & nan  nan & 0.01  0.00 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskLasso & 0.991 & 0.785 & 0.622 & 0.934 & 0.833  0.143 & 3.47  0.05 & 0.07  0.00 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabNetModelConfig_tab & 1.023 & 0.973 & 0.572 & 1.144 & 0.928  0.215 & 3.43  0.08 & 2.24  0.12 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & DANetConfig_tab & 1.026 & 0.791 & 0.607 & 0.878 & 0.825  0.152 & 3.41  0.07 & 1.75  0.10 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_df, latex_mae = generate_metric_table(\n",
    "    results_list=dict_results_loo_nona,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='mae',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/3_training_loonona_mae_adjusted_sorted.csv\",\n",
    "    sort_order=\"ascending\"\n",
    ")\n",
    "print(latex_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllll}\n",
      "\\toprule\n",
      "Ordinal Imputer & Continuous Imputer & Model & ADNI_MEM & ADNI_EF & ADNI_VS & ADNI_LAN & Mean  Std & Imputation Time & Fitting Time \\\\\n",
      "\\midrule\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskLasso_tuned & 0.672 & 0.463 & 0.438 & 0.572 & 0.536  0.093 & 3.53  0.05 & 1.51  0.01 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & LinearRegression & 0.679 & 0.473 & 0.428 & 0.562 & 0.536  0.096 & 3.42  0.11 & 0.15  0.06 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskElasticNet_tuned & 0.682 & 0.467 & 0.432 & 0.581 & 0.541  0.098 & 3.50  0.03 & 1.50  0.01 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & RandomForestRegressor & 0.793 & 0.593 & 0.452 & 0.646 & 0.621  0.122 & 3.38  0.07 & 52.01  0.67 \\\\\n",
      "NoImputer & NoImputer & RandomForestRegressor & 0.892 & 0.733 & 0.561 & 0.729 & 0.729  0.117 & nan  nan & 32.37  0.24 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabTransformerConfig_tab & 0.935 & 1.303 & 1.207 & 1.355 & 1.200  0.162 & 3.41  0.07 & 1.50  0.11 \\\\\n",
      "NoImputer & NoImputer & XGBoostRegressor & 0.940 & 0.617 & 0.480 & 0.738 & 0.694  0.169 & nan  nan & 0.78  0.02 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & GatedAdditiveTreeEnsembleConfig_tab & 0.968 & 0.416 & 0.519 & 0.536 & 0.610  0.212 & 3.50  0.07 & 17.88  0.19 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabNetRegressor_custom & 0.977 & 0.764 & 0.608 & 0.829 & 0.794  0.132 & 3.30  0.07 & 12.84  0.69 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & PLSRegression_4_components & 0.991 & 0.681 & 0.511 & 0.875 & 0.765  0.183 & 3.44  0.13 & 0.18  0.06 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabNetRegressor_default & 1.014 & 0.711 & 0.274 & 1.184 & 0.796  0.346 & 3.40  0.19 & 13.19  0.90 \\\\\n",
      "NoImputer & NoImputer & PLSRegression_4_components & 1.076 & 0.784 & 0.569 & 0.940 & 0.843  0.189 & nan  nan & 0.06  0.01 \\\\\n",
      "NoImputer & NoImputer & MultiTaskElasticNet_tuned & 1.082 & 0.663 & 0.613 & 0.946 & 0.826  0.195 & nan  nan & 0.83  0.01 \\\\\n",
      "NoImputer & NoImputer & MultiTaskLasso_tuned & 1.084 & 0.669 & 0.613 & 0.949 & 0.829  0.195 & nan  nan & 0.83  0.01 \\\\\n",
      "NoImputer & NoImputer & LinearRegression & 1.095 & 0.668 & 0.615 & 0.963 & 0.835  0.200 & nan  nan & 0.06  0.03 \\\\\n",
      "NoImputer & NoImputer & TabNetRegressor_custom & 1.162 & 0.967 & 0.529 & 1.026 & 0.921  0.237 & nan  nan & 12.36  0.63 \\\\\n",
      "NoImputer & NoImputer & TabNetRegressor_default & 1.163 & 0.694 & 0.413 & 1.195 & 0.866  0.328 & nan  nan & 12.02  0.46 \\\\\n",
      "NoImputer & NoImputer & MultiTaskElasticNet & 1.179 & 0.952 & 0.509 & 1.027 & 0.917  0.249 & nan  nan & 0.01  0.00 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskElasticNet & 1.179 & 0.952 & 0.509 & 1.027 & 0.917  0.249 & 3.48  0.04 & 0.09  0.00 \\\\\n",
      "NoImputer & NoImputer & XGBoostRegressor_tuned & 1.220 & 0.629 & 0.534 & 0.821 & 0.801  0.263 & nan  nan & 1.95  0.07 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & MultiTaskLasso & 1.392 & 1.041 & 0.490 & 1.237 & 1.040  0.341 & 3.47  0.05 & 0.07  0.00 \\\\\n",
      "NoImputer & NoImputer & MultiTaskLasso & 1.392 & 1.041 & 0.490 & 1.237 & 1.040  0.341 & nan  nan & 0.01  0.00 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & DANetConfig_tab & 1.475 & 1.056 & 0.590 & 1.076 & 1.049  0.314 & 3.41  0.07 & 1.75  0.10 \\\\\n",
      "SimpleImputer_most_frequent & KNNImputer & TabNetModelConfig_tab & 1.512 & 1.236 & 0.473 & 1.817 & 1.260  0.498 & 3.43  0.08 & 2.24  0.12 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_df, latex_mse = generate_metric_table(\n",
    "    results_list=dict_results_loo_nona,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='mse',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/3_training_loonona_mse_adjusted_sorted.csv\",\n",
    "    sort_order=\"ascending\"\n",
    ")\n",
    "print(latex_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
