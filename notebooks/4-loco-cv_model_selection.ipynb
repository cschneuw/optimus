{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "from itertools import product\n",
    "import warnings\n",
    "\n",
    "# System path modification\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer, MissingIndicator\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, KFold, StratifiedKFold, GroupKFold, StratifiedGroupKFold, LeaveOneOut, cross_validate, cross_val_score\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Lasso, LassoCV, MultiTaskLasso, MultiTaskLassoCV,\n",
    "    ElasticNet, ElasticNetCV, MultiTaskElasticNet, MultiTaskElasticNetCV\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from pytorch_tabular.models import (\n",
    "    GatedAdditiveTreeEnsembleConfig,\n",
    "    DANetConfig,\n",
    "    TabTransformerConfig,\n",
    "    FTTransformerConfig,\n",
    "    TabNetModelConfig,\n",
    ")\n",
    "\n",
    "from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "\n",
    "# Statistic imports \n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.special import kl_div\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Specialized imputation and visualization packages\n",
    "import miceforest as mf\n",
    "import missingno as msno\n",
    "#from missforest import MissForest\n",
    "#import magic\n",
    "from src.gain import *\n",
    "\n",
    "# Custom modules\n",
    "from src.train import *\n",
    "from src.functions import *\n",
    "from src.plots import *\n",
    "from src.dataset import *\n",
    "from src.multixgboost import *\n",
    "from src.wrapper import *\n",
    "\n",
    "# Visualizatiokn \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning and machine learning specific \n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Print CUDA availability for PyTorch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_pickle_data_palettes()\n",
    "\n",
    "results_pickle_folder = \"../pickle/\"\n",
    "\n",
    "# Unpack data\n",
    "df_X, df_y, df_all, df_FinalCombination = data[\"df_X\"], data[\"df_y\"], data[\"df_all\"], data[\"df_FinalCombination\"]\n",
    "dict_select = data[\"dict_select\"]\n",
    "\n",
    "# Unpack colormaps\n",
    "full_palette, gender_palette, dx_palette = data[\"colormaps\"].values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave-One-Complete-Out (LOCO-CV)\n",
    "\n",
    "## All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "3609    2002\n",
      "5631    4167\n",
      "5662    4176\n",
      "5780    4215\n",
      "5950    4349\n",
      "6069    4292\n",
      "6077    4453\n",
      "6085    4489\n",
      "6224    4505\n",
      "6400    4576\n",
      "6429    4300\n",
      "7021    2374\n",
      "7192    4179\n",
      "Name: RID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "idx_train = list(df_X.isna().any(axis=1))\n",
    "idx_test = list(~df_X.isna().any(axis=1))\n",
    "\n",
    "set_intersect_rid = set(df_all[idx_train].RID).intersection(set(df_all[idx_test].RID))\n",
    "intersect_rid_idx = df_all.RID.isin(set_intersect_rid)\n",
    "\n",
    "for i, bool_test in enumerate(idx_test): \n",
    "    if intersect_rid_idx.iloc[i] & bool_test:\n",
    "        idx_test[i] = False\n",
    "        idx_train[i] = True\n",
    "\n",
    "print(sum(idx_test))\n",
    "\n",
    "print(df_all[idx_test].RID)\n",
    "\n",
    "df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]] = df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]].astype(\"category\")\n",
    "\n",
    "test_indices = [i for i, val in enumerate(idx_test) if val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: LinearRegression\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskElasticNet\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskElasticNet_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskLasso\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: MultiTaskLasso_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: RandomForestRegressor\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: XGBoostRegressor\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: XGBoostRegressor_tuned\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetRegressor_default\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetRegressor_custom\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: PLSRegression_4_components\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: GatedAdditiveTreeEnsembleConfig_tab\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: DANetConfig_tab\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabTransformerConfig_tab\n",
      "Continuous Imputer: KNNImputer, Ordinal Imputer: SimpleImputer_most_frequent, Model: TabNetModelConfig_tab\n",
      "Combinations of preprocessing and models to test : 15\n"
     ]
    }
   ],
   "source": [
    "random_state=42\n",
    "n_imputation_iter = 10\n",
    "\n",
    "# Define hyperparameters\n",
    "gain_parameters = {\n",
    "    'hint_rate': 0.9,\n",
    "    'alpha': 100,\n",
    "    'iterations': 1000\n",
    "}\n",
    "\n",
    "# Continuous Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "continuous_imputer_list = [\n",
    "    #(\"SimpleImputer_mean\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"KNNImputer\", KNNImputer(n_neighbors=1)),\n",
    "]\n",
    "\n",
    "# Ordinal Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "ordinal_imputer_list = [\n",
    "    (\"SimpleImputer_most_frequent\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "]\n",
    "\n",
    "# Predictive Models List (list of tuples with unique strings and corresponding instances)\n",
    "predictive_models_list = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"MultiTaskElasticNet\", MultiTaskElasticNet()),\n",
    "    (\"MultiTaskElasticNet_tuned\", MultiTaskElasticNet(**{'alpha': 0.01, 'l1_ratio': 0.01})),\n",
    "    (\"MultiTaskLasso\", MultiTaskLasso()),\n",
    "    (\"MultiTaskLasso_tuned\", MultiTaskLasso(**{'alpha': 0.001})),\n",
    "    (\"RandomForestRegressor\", RandomForestRegressor()),\n",
    "    (\"XGBoostRegressor\", XGBoostRegressor()),\n",
    "    (\"XGBoostRegressor_tuned\", XGBoostRegressor(**{'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105})),\n",
    "    (\"TabNetRegressor_default\", TabNetModelWrapper(n_a=8, n_d=8)),\n",
    "    (\"TabNetRegressor_custom\", TabNetModelWrapper(n_a=32, n_d=32)),\n",
    "    (\"PLSRegression_4_components\", PLSRegression(n_components=4))\n",
    "]\n",
    "\n",
    "ordinal_features = ['APOE_epsilon2', 'APOE_epsilon3', 'APOE_epsilon4']\n",
    "continuous_features = [col for col in df_X.columns if col not in ordinal_features]\n",
    "\n",
    "# Prepare Tabular configurations (shared for all PyTorch models)\n",
    "data_config = DataConfig(\n",
    "    target=df_y.columns.tolist(),\n",
    "    continuous_cols=continuous_features,\n",
    "    categorical_cols=continuous_features\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024, max_epochs=1, auto_lr_find=True,\n",
    "    early_stopping=\"valid_loss\", early_stopping_mode=\"min\", early_stopping_patience=5,\n",
    "    checkpoints=\"valid_loss\", load_best=True, progress_bar=\"nones\",\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "head_config = LinearHeadConfig(dropout=0.1).__dict__\n",
    "predictive_models_list += [\n",
    "    (\"GatedAdditiveTreeEnsembleConfig_tab\", \n",
    "    TabularModelWrapper(\n",
    "        GatedAdditiveTreeEnsembleConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        gflu_stages=6,\n",
    "        gflu_dropout=0.0,\n",
    "        tree_depth=5,\n",
    "        num_trees=20,\n",
    "        chain_trees=False,\n",
    "        share_head_weights=True), data_config, trainer_config, optimizer_config \n",
    "    )),\n",
    "    (\"DANetConfig_tab\",\n",
    "    TabularModelWrapper(\n",
    "        DANetConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_layers=8,\n",
    "        k=5,\n",
    "        dropout_rate=0.1), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabTransformerConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        embedding_initialization=\"kaiming_uniform\",\n",
    "        embedding_bias=False), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    # (\"FTTransformerConfig\",\n",
    "    #     TabularModelWrapper(\n",
    "    #     FTTransformerConfig(\n",
    "    #     task=\"regression\",\n",
    "    #     head=\"LinearHead\",\n",
    "    #     head_config=head_config), data_config, trainer_config, optimizer_config\n",
    "    # )),\n",
    "    (\"TabNetModelConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        n_independent=2,\n",
    "        n_shared=2), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "# Display all combinations\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    print(f\"Continuous Imputer: {continuous_imputer[0]}, Ordinal Imputer: {ordinal_imputer[0]}, Model: {model[0]}\")\n",
    "\n",
    "print(f\"Combinations of preprocessing and models to test : {len(combinations)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HDF5 file\n",
    "results_file = '../pickle/training_3_loonona_dict_results.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(results_file): \n",
    "    with open(results_file, \"rb\") as input_file:\n",
    "        all_dict_results = pickle.load(input_file)\n",
    "\n",
    "else : \n",
    "    all_dict_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:APOE_epsilon2: object, APOE_epsilon3: object, APOE_epsilon4: object\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.20485 |  0:00:00s\n",
      "epoch 1  | loss: 1.79486 |  0:00:00s\n",
      "epoch 2  | loss: 1.3621  |  0:00:00s\n",
      "epoch 3  | loss: 1.18175 |  0:00:00s\n",
      "epoch 4  | loss: 1.03655 |  0:00:00s\n",
      "epoch 5  | loss: 0.97676 |  0:00:00s\n",
      "epoch 6  | loss: 0.9337  |  0:00:00s\n",
      "epoch 7  | loss: 0.90156 |  0:00:00s\n",
      "epoch 8  | loss: 0.88084 |  0:00:00s\n",
      "epoch 9  | loss: 0.84919 |  0:00:00s\n",
      "epoch 10 | loss: 0.84774 |  0:00:00s\n",
      "epoch 11 | loss: 0.82121 |  0:00:00s\n",
      "epoch 12 | loss: 0.79801 |  0:00:01s\n",
      "epoch 13 | loss: 0.81231 |  0:00:01s\n",
      "epoch 14 | loss: 0.78897 |  0:00:01s\n",
      "epoch 15 | loss: 0.77674 |  0:00:01s\n",
      "epoch 16 | loss: 0.76255 |  0:00:01s\n",
      "epoch 17 | loss: 0.77186 |  0:00:01s\n",
      "epoch 18 | loss: 0.75407 |  0:00:01s\n",
      "epoch 19 | loss: 0.74637 |  0:00:01s\n",
      "epoch 20 | loss: 0.74949 |  0:00:01s\n",
      "epoch 21 | loss: 0.73623 |  0:00:01s\n",
      "epoch 22 | loss: 0.74338 |  0:00:01s\n",
      "epoch 23 | loss: 0.73842 |  0:00:01s\n",
      "epoch 24 | loss: 0.71976 |  0:00:01s\n",
      "epoch 25 | loss: 0.71369 |  0:00:01s\n",
      "epoch 26 | loss: 0.724   |  0:00:01s\n",
      "epoch 27 | loss: 0.71912 |  0:00:01s\n",
      "epoch 28 | loss: 0.70369 |  0:00:01s\n",
      "epoch 29 | loss: 0.70237 |  0:00:01s\n",
      "epoch 30 | loss: 0.69863 |  0:00:01s\n",
      "epoch 31 | loss: 0.70002 |  0:00:01s\n",
      "epoch 32 | loss: 0.6982  |  0:00:02s\n",
      "epoch 33 | loss: 0.68566 |  0:00:02s\n",
      "epoch 34 | loss: 0.68669 |  0:00:02s\n",
      "epoch 35 | loss: 0.67956 |  0:00:02s\n",
      "epoch 36 | loss: 0.67577 |  0:00:02s\n",
      "epoch 37 | loss: 0.66755 |  0:00:02s\n",
      "epoch 38 | loss: 0.67709 |  0:00:02s\n",
      "epoch 39 | loss: 0.66976 |  0:00:02s\n",
      "epoch 40 | loss: 0.65941 |  0:00:02s\n",
      "epoch 41 | loss: 0.66649 |  0:00:02s\n",
      "epoch 42 | loss: 0.66428 |  0:00:02s\n",
      "epoch 43 | loss: 0.65644 |  0:00:02s\n",
      "epoch 44 | loss: 0.65615 |  0:00:02s\n",
      "epoch 45 | loss: 0.65752 |  0:00:02s\n",
      "epoch 46 | loss: 0.65384 |  0:00:02s\n",
      "epoch 47 | loss: 0.64945 |  0:00:02s\n",
      "epoch 48 | loss: 0.63908 |  0:00:02s\n",
      "epoch 49 | loss: 0.63813 |  0:00:02s\n",
      "epoch 50 | loss: 0.64269 |  0:00:02s\n",
      "epoch 51 | loss: 0.64148 |  0:00:02s\n",
      "epoch 52 | loss: 0.6364  |  0:00:03s\n",
      "epoch 53 | loss: 0.62974 |  0:00:03s\n",
      "epoch 54 | loss: 0.63029 |  0:00:03s\n",
      "epoch 55 | loss: 0.63215 |  0:00:03s\n",
      "epoch 56 | loss: 0.6277  |  0:00:03s\n",
      "epoch 57 | loss: 0.62892 |  0:00:03s\n",
      "epoch 58 | loss: 0.62641 |  0:00:03s\n",
      "epoch 59 | loss: 0.62583 |  0:00:03s\n",
      "epoch 60 | loss: 0.62733 |  0:00:03s\n",
      "epoch 61 | loss: 0.61872 |  0:00:03s\n",
      "epoch 62 | loss: 0.62182 |  0:00:03s\n",
      "epoch 63 | loss: 0.60759 |  0:00:03s\n",
      "epoch 64 | loss: 0.60941 |  0:00:03s\n",
      "epoch 65 | loss: 0.61623 |  0:00:03s\n",
      "epoch 66 | loss: 0.60058 |  0:00:03s\n",
      "epoch 67 | loss: 0.60977 |  0:00:03s\n",
      "epoch 68 | loss: 0.60319 |  0:00:03s\n",
      "epoch 69 | loss: 0.59305 |  0:00:03s\n",
      "epoch 70 | loss: 0.59837 |  0:00:03s\n",
      "epoch 71 | loss: 0.59501 |  0:00:03s\n",
      "epoch 72 | loss: 0.59503 |  0:00:03s\n",
      "epoch 73 | loss: 0.58498 |  0:00:04s\n",
      "epoch 74 | loss: 0.58605 |  0:00:04s\n",
      "epoch 75 | loss: 0.58994 |  0:00:04s\n",
      "epoch 76 | loss: 0.59246 |  0:00:04s\n",
      "epoch 77 | loss: 0.59274 |  0:00:04s\n",
      "epoch 78 | loss: 0.59607 |  0:00:04s\n",
      "epoch 79 | loss: 0.58643 |  0:00:04s\n",
      "epoch 80 | loss: 0.58565 |  0:00:04s\n",
      "epoch 81 | loss: 0.58778 |  0:00:04s\n",
      "epoch 82 | loss: 0.58573 |  0:00:04s\n",
      "epoch 83 | loss: 0.5894  |  0:00:04s\n",
      "epoch 84 | loss: 0.58337 |  0:00:04s\n",
      "epoch 85 | loss: 0.57566 |  0:00:04s\n",
      "epoch 86 | loss: 0.57091 |  0:00:04s\n",
      "epoch 87 | loss: 0.5653  |  0:00:04s\n",
      "epoch 88 | loss: 0.57211 |  0:00:04s\n",
      "epoch 89 | loss: 0.57391 |  0:00:04s\n",
      "epoch 90 | loss: 0.57036 |  0:00:04s\n",
      "epoch 91 | loss: 0.56764 |  0:00:04s\n",
      "epoch 92 | loss: 0.56426 |  0:00:05s\n",
      "epoch 93 | loss: 0.5634  |  0:00:05s\n",
      "epoch 94 | loss: 0.55799 |  0:00:05s\n",
      "epoch 95 | loss: 0.55465 |  0:00:05s\n",
      "epoch 96 | loss: 0.55385 |  0:00:05s\n",
      "epoch 97 | loss: 0.55145 |  0:00:05s\n",
      "epoch 98 | loss: 0.55737 |  0:00:05s\n",
      "epoch 99 | loss: 0.55886 |  0:00:05s\n",
      "epoch 100| loss: 0.55326 |  0:00:05s\n",
      "epoch 101| loss: 0.56318 |  0:00:05s\n",
      "epoch 102| loss: 0.55361 |  0:00:05s\n",
      "epoch 103| loss: 0.5531  |  0:00:05s\n",
      "epoch 104| loss: 0.54983 |  0:00:05s\n",
      "epoch 105| loss: 0.55312 |  0:00:05s\n",
      "epoch 106| loss: 0.5485  |  0:00:05s\n",
      "epoch 107| loss: 0.54538 |  0:00:05s\n",
      "epoch 108| loss: 0.54482 |  0:00:05s\n",
      "epoch 109| loss: 0.5436  |  0:00:05s\n",
      "epoch 110| loss: 0.54114 |  0:00:05s\n",
      "epoch 111| loss: 0.54158 |  0:00:05s\n",
      "epoch 112| loss: 0.53412 |  0:00:06s\n",
      "epoch 113| loss: 0.53725 |  0:00:06s\n",
      "epoch 114| loss: 0.54334 |  0:00:06s\n",
      "epoch 115| loss: 0.53172 |  0:00:06s\n",
      "epoch 116| loss: 0.52547 |  0:00:06s\n",
      "epoch 117| loss: 0.51994 |  0:00:06s\n",
      "epoch 118| loss: 0.52334 |  0:00:06s\n",
      "epoch 119| loss: 0.52237 |  0:00:06s\n",
      "epoch 120| loss: 0.52061 |  0:00:06s\n",
      "epoch 121| loss: 0.53716 |  0:00:06s\n",
      "epoch 122| loss: 0.51931 |  0:00:06s\n",
      "epoch 123| loss: 0.51662 |  0:00:06s\n",
      "epoch 124| loss: 0.51743 |  0:00:06s\n",
      "epoch 125| loss: 0.51128 |  0:00:06s\n",
      "epoch 126| loss: 0.5098  |  0:00:06s\n",
      "epoch 127| loss: 0.51458 |  0:00:06s\n",
      "epoch 128| loss: 0.51825 |  0:00:06s\n",
      "epoch 129| loss: 0.50203 |  0:00:06s\n",
      "epoch 130| loss: 0.50717 |  0:00:06s\n",
      "epoch 131| loss: 0.49965 |  0:00:06s\n",
      "epoch 132| loss: 0.5012  |  0:00:07s\n",
      "epoch 133| loss: 0.49732 |  0:00:07s\n",
      "epoch 134| loss: 0.4983  |  0:00:07s\n",
      "epoch 135| loss: 0.49731 |  0:00:07s\n",
      "epoch 136| loss: 0.491   |  0:00:07s\n",
      "epoch 137| loss: 0.48502 |  0:00:07s\n",
      "epoch 138| loss: 0.48688 |  0:00:07s\n",
      "epoch 139| loss: 0.49034 |  0:00:07s\n",
      "epoch 140| loss: 0.47773 |  0:00:07s\n",
      "epoch 141| loss: 0.48544 |  0:00:07s\n",
      "epoch 142| loss: 0.47902 |  0:00:07s\n",
      "epoch 143| loss: 0.48263 |  0:00:07s\n",
      "epoch 144| loss: 0.48246 |  0:00:07s\n",
      "epoch 145| loss: 0.47864 |  0:00:07s\n",
      "epoch 146| loss: 0.47267 |  0:00:07s\n",
      "epoch 147| loss: 0.46979 |  0:00:07s\n",
      "epoch 148| loss: 0.46729 |  0:00:07s\n",
      "epoch 149| loss: 0.46424 |  0:00:07s\n",
      "epoch 150| loss: 0.46741 |  0:00:08s\n",
      "epoch 151| loss: 0.46543 |  0:00:08s\n",
      "epoch 152| loss: 0.46754 |  0:00:08s\n",
      "epoch 153| loss: 0.45955 |  0:00:08s\n",
      "epoch 154| loss: 0.44995 |  0:00:08s\n",
      "epoch 155| loss: 0.45268 |  0:00:08s\n",
      "epoch 156| loss: 0.45205 |  0:00:08s\n",
      "epoch 157| loss: 0.45724 |  0:00:08s\n",
      "epoch 158| loss: 0.45718 |  0:00:08s\n",
      "epoch 159| loss: 0.45637 |  0:00:08s\n",
      "epoch 160| loss: 0.46633 |  0:00:08s\n",
      "epoch 161| loss: 0.46495 |  0:00:08s\n",
      "epoch 162| loss: 0.46256 |  0:00:08s\n",
      "epoch 163| loss: 0.46205 |  0:00:08s\n",
      "epoch 164| loss: 0.44996 |  0:00:08s\n",
      "epoch 165| loss: 0.45499 |  0:00:08s\n",
      "epoch 166| loss: 0.45138 |  0:00:08s\n",
      "epoch 167| loss: 0.45481 |  0:00:08s\n",
      "epoch 168| loss: 0.44924 |  0:00:09s\n",
      "epoch 169| loss: 0.44705 |  0:00:09s\n",
      "epoch 170| loss: 0.44371 |  0:00:09s\n",
      "epoch 171| loss: 0.44445 |  0:00:09s\n",
      "epoch 172| loss: 0.44066 |  0:00:09s\n",
      "epoch 173| loss: 0.43644 |  0:00:09s\n",
      "epoch 174| loss: 0.43677 |  0:00:09s\n",
      "epoch 175| loss: 0.42717 |  0:00:09s\n",
      "epoch 176| loss: 0.43744 |  0:00:09s\n",
      "epoch 177| loss: 0.42836 |  0:00:09s\n",
      "epoch 178| loss: 0.42942 |  0:00:09s\n",
      "epoch 179| loss: 0.42669 |  0:00:09s\n",
      "epoch 180| loss: 0.42665 |  0:00:09s\n",
      "epoch 181| loss: 0.42827 |  0:00:09s\n",
      "epoch 182| loss: 0.43295 |  0:00:09s\n",
      "epoch 183| loss: 0.42034 |  0:00:09s\n",
      "epoch 184| loss: 0.42129 |  0:00:09s\n",
      "epoch 185| loss: 0.41876 |  0:00:09s\n",
      "epoch 186| loss: 0.40835 |  0:00:09s\n",
      "epoch 187| loss: 0.41435 |  0:00:09s\n",
      "epoch 188| loss: 0.41359 |  0:00:10s\n",
      "epoch 189| loss: 0.41208 |  0:00:10s\n",
      "epoch 190| loss: 0.42341 |  0:00:10s\n",
      "epoch 191| loss: 0.42813 |  0:00:10s\n",
      "epoch 192| loss: 0.42562 |  0:00:10s\n",
      "epoch 193| loss: 0.42627 |  0:00:10s\n",
      "epoch 194| loss: 0.43339 |  0:00:10s\n",
      "epoch 195| loss: 0.42485 |  0:00:10s\n",
      "epoch 196| loss: 0.42311 |  0:00:10s\n",
      "epoch 197| loss: 0.41986 |  0:00:10s\n",
      "epoch 198| loss: 0.41404 |  0:00:10s\n",
      "epoch 199| loss: 0.41468 |  0:00:10s\n",
      "epoch 200| loss: 0.41385 |  0:00:10s\n",
      "epoch 201| loss: 0.41179 |  0:00:10s\n",
      "epoch 202| loss: 0.41558 |  0:00:10s\n",
      "epoch 203| loss: 0.40601 |  0:00:10s\n",
      "epoch 204| loss: 0.40941 |  0:00:10s\n",
      "epoch 205| loss: 0.39686 |  0:00:10s\n",
      "epoch 206| loss: 0.40034 |  0:00:10s\n",
      "epoch 207| loss: 0.40181 |  0:00:10s\n",
      "epoch 208| loss: 0.40581 |  0:00:11s\n",
      "epoch 209| loss: 0.40752 |  0:00:11s\n",
      "epoch 210| loss: 0.39662 |  0:00:11s\n",
      "epoch 211| loss: 0.39564 |  0:00:11s\n",
      "epoch 212| loss: 0.39942 |  0:00:11s\n",
      "epoch 213| loss: 0.39426 |  0:00:11s\n",
      "epoch 214| loss: 0.39922 |  0:00:11s\n",
      "epoch 215| loss: 0.39291 |  0:00:11s\n",
      "epoch 216| loss: 0.3937  |  0:00:11s\n",
      "epoch 217| loss: 0.39176 |  0:00:11s\n",
      "epoch 218| loss: 0.40199 |  0:00:11s\n",
      "epoch 219| loss: 0.41043 |  0:00:11s\n",
      "epoch 220| loss: 0.39811 |  0:00:11s\n",
      "epoch 221| loss: 0.40354 |  0:00:11s\n",
      "epoch 222| loss: 0.39445 |  0:00:11s\n",
      "epoch 223| loss: 0.41438 |  0:00:11s\n",
      "epoch 224| loss: 0.39907 |  0:00:12s\n",
      "epoch 225| loss: 0.4057  |  0:00:12s\n",
      "epoch 226| loss: 0.40419 |  0:00:12s\n",
      "epoch 227| loss: 0.40711 |  0:00:12s\n",
      "epoch 228| loss: 0.40667 |  0:00:12s\n",
      "epoch 229| loss: 0.40166 |  0:00:12s\n",
      "epoch 230| loss: 0.39089 |  0:00:12s\n",
      "epoch 231| loss: 0.40246 |  0:00:12s\n",
      "epoch 232| loss: 0.40061 |  0:00:12s\n",
      "epoch 233| loss: 0.3949  |  0:00:12s\n",
      "epoch 234| loss: 0.40011 |  0:00:12s\n",
      "epoch 235| loss: 0.39558 |  0:00:12s\n",
      "epoch 236| loss: 0.38418 |  0:00:12s\n",
      "epoch 237| loss: 0.39021 |  0:00:12s\n",
      "epoch 238| loss: 0.39165 |  0:00:12s\n",
      "epoch 239| loss: 0.38762 |  0:00:12s\n",
      "epoch 240| loss: 0.38754 |  0:00:12s\n",
      "epoch 241| loss: 0.38842 |  0:00:12s\n",
      "epoch 242| loss: 0.38666 |  0:00:12s\n",
      "epoch 243| loss: 0.38092 |  0:00:12s\n",
      "epoch 244| loss: 0.37599 |  0:00:13s\n",
      "epoch 245| loss: 0.37721 |  0:00:13s\n",
      "epoch 246| loss: 0.38699 |  0:00:13s\n",
      "epoch 247| loss: 0.38319 |  0:00:13s\n",
      "epoch 248| loss: 0.38864 |  0:00:13s\n",
      "epoch 249| loss: 0.38849 |  0:00:13s\n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.21818 |  0:00:00s\n",
      "epoch 1  | loss: 1.80748 |  0:00:00s\n",
      "epoch 2  | loss: 1.34234 |  0:00:00s\n",
      "epoch 3  | loss: 1.13742 |  0:00:00s\n",
      "epoch 4  | loss: 0.99374 |  0:00:00s\n",
      "epoch 5  | loss: 0.93332 |  0:00:00s\n",
      "epoch 6  | loss: 0.90494 |  0:00:00s\n",
      "epoch 7  | loss: 0.87858 |  0:00:00s\n",
      "epoch 8  | loss: 0.83391 |  0:00:00s\n",
      "epoch 9  | loss: 0.81531 |  0:00:00s\n",
      "epoch 10 | loss: 0.79377 |  0:00:00s\n",
      "epoch 11 | loss: 0.79362 |  0:00:00s\n",
      "epoch 12 | loss: 0.76949 |  0:00:00s\n",
      "epoch 13 | loss: 0.78231 |  0:00:00s\n",
      "epoch 14 | loss: 0.76439 |  0:00:00s\n",
      "epoch 15 | loss: 0.74842 |  0:00:00s\n",
      "epoch 16 | loss: 0.73186 |  0:00:00s\n",
      "epoch 17 | loss: 0.73008 |  0:00:01s\n",
      "epoch 18 | loss: 0.72262 |  0:00:01s\n",
      "epoch 19 | loss: 0.72468 |  0:00:01s\n",
      "epoch 20 | loss: 0.72218 |  0:00:01s\n",
      "epoch 21 | loss: 0.70955 |  0:00:01s\n",
      "epoch 22 | loss: 0.70232 |  0:00:01s\n",
      "epoch 23 | loss: 0.69474 |  0:00:01s\n",
      "epoch 24 | loss: 0.69626 |  0:00:01s\n",
      "epoch 25 | loss: 0.68664 |  0:00:01s\n",
      "epoch 26 | loss: 0.68173 |  0:00:01s\n",
      "epoch 27 | loss: 0.67788 |  0:00:01s\n",
      "epoch 28 | loss: 0.66536 |  0:00:01s\n",
      "epoch 29 | loss: 0.67906 |  0:00:01s\n",
      "epoch 30 | loss: 0.67317 |  0:00:01s\n",
      "epoch 31 | loss: 0.66783 |  0:00:01s\n",
      "epoch 32 | loss: 0.66988 |  0:00:01s\n",
      "epoch 33 | loss: 0.66637 |  0:00:01s\n",
      "epoch 34 | loss: 0.67287 |  0:00:02s\n",
      "epoch 35 | loss: 0.65668 |  0:00:02s\n",
      "epoch 36 | loss: 0.65581 |  0:00:02s\n",
      "epoch 37 | loss: 0.65582 |  0:00:02s\n",
      "epoch 38 | loss: 0.64128 |  0:00:02s\n",
      "epoch 39 | loss: 0.64475 |  0:00:02s\n",
      "epoch 40 | loss: 0.63774 |  0:00:02s\n",
      "epoch 41 | loss: 0.63534 |  0:00:02s\n",
      "epoch 42 | loss: 0.63216 |  0:00:02s\n",
      "epoch 43 | loss: 0.62251 |  0:00:02s\n",
      "epoch 44 | loss: 0.6281  |  0:00:02s\n",
      "epoch 45 | loss: 0.63495 |  0:00:02s\n",
      "epoch 46 | loss: 0.63365 |  0:00:02s\n",
      "epoch 47 | loss: 0.63595 |  0:00:02s\n",
      "epoch 48 | loss: 0.62047 |  0:00:02s\n",
      "epoch 49 | loss: 0.61888 |  0:00:02s\n",
      "epoch 50 | loss: 0.6126  |  0:00:02s\n",
      "epoch 51 | loss: 0.61736 |  0:00:03s\n",
      "epoch 52 | loss: 0.63    |  0:00:03s\n",
      "epoch 53 | loss: 0.61969 |  0:00:03s\n",
      "epoch 54 | loss: 0.61655 |  0:00:03s\n",
      "epoch 55 | loss: 0.60478 |  0:00:03s\n",
      "epoch 56 | loss: 0.60153 |  0:00:03s\n",
      "epoch 57 | loss: 0.6015  |  0:00:03s\n",
      "epoch 58 | loss: 0.58578 |  0:00:03s\n",
      "epoch 59 | loss: 0.58908 |  0:00:03s\n",
      "epoch 60 | loss: 0.59158 |  0:00:03s\n",
      "epoch 61 | loss: 0.58408 |  0:00:03s\n",
      "epoch 62 | loss: 0.58927 |  0:00:03s\n",
      "epoch 63 | loss: 0.58685 |  0:00:03s\n",
      "epoch 64 | loss: 0.58538 |  0:00:03s\n",
      "epoch 65 | loss: 0.58432 |  0:00:03s\n",
      "epoch 66 | loss: 0.58315 |  0:00:04s\n",
      "epoch 67 | loss: 0.58893 |  0:00:04s\n",
      "epoch 68 | loss: 0.58176 |  0:00:04s\n",
      "epoch 69 | loss: 0.57896 |  0:00:04s\n",
      "epoch 70 | loss: 0.58714 |  0:00:04s\n",
      "epoch 71 | loss: 0.5912  |  0:00:04s\n",
      "epoch 72 | loss: 0.59443 |  0:00:04s\n",
      "epoch 73 | loss: 0.58138 |  0:00:04s\n",
      "epoch 74 | loss: 0.57847 |  0:00:04s\n",
      "epoch 75 | loss: 0.57288 |  0:00:04s\n",
      "epoch 76 | loss: 0.57639 |  0:00:04s\n",
      "epoch 77 | loss: 0.5794  |  0:00:04s\n",
      "epoch 78 | loss: 0.56949 |  0:00:04s\n",
      "epoch 79 | loss: 0.56533 |  0:00:04s\n",
      "epoch 80 | loss: 0.56427 |  0:00:04s\n",
      "epoch 81 | loss: 0.56987 |  0:00:04s\n",
      "epoch 82 | loss: 0.56618 |  0:00:04s\n",
      "epoch 83 | loss: 0.56622 |  0:00:04s\n",
      "epoch 84 | loss: 0.56638 |  0:00:04s\n",
      "epoch 85 | loss: 0.55001 |  0:00:05s\n",
      "epoch 86 | loss: 0.55703 |  0:00:05s\n",
      "epoch 87 | loss: 0.54566 |  0:00:05s\n",
      "epoch 88 | loss: 0.5465  |  0:00:05s\n",
      "epoch 89 | loss: 0.5513  |  0:00:05s\n",
      "epoch 90 | loss: 0.54771 |  0:00:05s\n",
      "epoch 91 | loss: 0.54007 |  0:00:05s\n",
      "epoch 92 | loss: 0.53688 |  0:00:05s\n",
      "epoch 93 | loss: 0.52899 |  0:00:05s\n",
      "epoch 94 | loss: 0.52964 |  0:00:05s\n",
      "epoch 95 | loss: 0.52939 |  0:00:05s\n",
      "epoch 96 | loss: 0.53044 |  0:00:05s\n",
      "epoch 97 | loss: 0.52658 |  0:00:05s\n",
      "epoch 98 | loss: 0.51912 |  0:00:05s\n",
      "epoch 99 | loss: 0.52188 |  0:00:05s\n",
      "epoch 100| loss: 0.52553 |  0:00:05s\n",
      "epoch 101| loss: 0.53914 |  0:00:05s\n",
      "epoch 102| loss: 0.53037 |  0:00:06s\n",
      "epoch 103| loss: 0.52048 |  0:00:06s\n",
      "epoch 104| loss: 0.51361 |  0:00:06s\n",
      "epoch 105| loss: 0.51424 |  0:00:06s\n",
      "epoch 106| loss: 0.51184 |  0:00:06s\n",
      "epoch 107| loss: 0.51938 |  0:00:06s\n",
      "epoch 108| loss: 0.51212 |  0:00:06s\n",
      "epoch 109| loss: 0.5125  |  0:00:06s\n",
      "epoch 110| loss: 0.51256 |  0:00:06s\n",
      "epoch 111| loss: 0.5134  |  0:00:06s\n",
      "epoch 112| loss: 0.50371 |  0:00:06s\n",
      "epoch 113| loss: 0.50231 |  0:00:06s\n",
      "epoch 114| loss: 0.50039 |  0:00:06s\n",
      "epoch 115| loss: 0.5004  |  0:00:06s\n",
      "epoch 116| loss: 0.49392 |  0:00:06s\n",
      "epoch 117| loss: 0.48686 |  0:00:06s\n",
      "epoch 118| loss: 0.49453 |  0:00:07s\n",
      "epoch 119| loss: 0.48606 |  0:00:07s\n",
      "epoch 120| loss: 0.49153 |  0:00:07s\n",
      "epoch 121| loss: 0.48384 |  0:00:07s\n",
      "epoch 122| loss: 0.47251 |  0:00:07s\n",
      "epoch 123| loss: 0.47582 |  0:00:07s\n",
      "epoch 124| loss: 0.46956 |  0:00:07s\n",
      "epoch 125| loss: 0.47742 |  0:00:07s\n",
      "epoch 126| loss: 0.46426 |  0:00:07s\n",
      "epoch 127| loss: 0.47815 |  0:00:07s\n",
      "epoch 128| loss: 0.4734  |  0:00:07s\n",
      "epoch 129| loss: 0.47567 |  0:00:07s\n",
      "epoch 130| loss: 0.46963 |  0:00:07s\n",
      "epoch 131| loss: 0.46776 |  0:00:07s\n",
      "epoch 132| loss: 0.47143 |  0:00:07s\n",
      "epoch 133| loss: 0.46389 |  0:00:07s\n",
      "epoch 134| loss: 0.46546 |  0:00:07s\n",
      "epoch 135| loss: 0.46188 |  0:00:08s\n",
      "epoch 136| loss: 0.46085 |  0:00:08s\n",
      "epoch 137| loss: 0.45999 |  0:00:08s\n",
      "epoch 138| loss: 0.45291 |  0:00:08s\n",
      "epoch 139| loss: 0.45555 |  0:00:08s\n",
      "epoch 140| loss: 0.45054 |  0:00:08s\n",
      "epoch 141| loss: 0.45293 |  0:00:08s\n",
      "epoch 142| loss: 0.44795 |  0:00:08s\n",
      "epoch 143| loss: 0.4452  |  0:00:08s\n",
      "epoch 144| loss: 0.45371 |  0:00:08s\n",
      "epoch 145| loss: 0.45238 |  0:00:08s\n",
      "epoch 146| loss: 0.45604 |  0:00:08s\n",
      "epoch 147| loss: 0.45082 |  0:00:08s\n",
      "epoch 148| loss: 0.44857 |  0:00:08s\n",
      "epoch 149| loss: 0.4486  |  0:00:08s\n",
      "epoch 150| loss: 0.44599 |  0:00:08s\n",
      "epoch 151| loss: 0.44163 |  0:00:08s\n",
      "epoch 152| loss: 0.44415 |  0:00:09s\n",
      "epoch 153| loss: 0.4407  |  0:00:09s\n",
      "epoch 154| loss: 0.45233 |  0:00:09s\n",
      "epoch 155| loss: 0.44798 |  0:00:09s\n",
      "epoch 156| loss: 0.44141 |  0:00:09s\n",
      "epoch 157| loss: 0.4447  |  0:00:09s\n",
      "epoch 158| loss: 0.43968 |  0:00:09s\n",
      "epoch 159| loss: 0.44439 |  0:00:09s\n",
      "epoch 160| loss: 0.45346 |  0:00:09s\n",
      "epoch 161| loss: 0.4481  |  0:00:09s\n",
      "epoch 162| loss: 0.44833 |  0:00:09s\n",
      "epoch 163| loss: 0.45482 |  0:00:09s\n",
      "epoch 164| loss: 0.43956 |  0:00:09s\n",
      "epoch 165| loss: 0.44926 |  0:00:09s\n",
      "epoch 166| loss: 0.4536  |  0:00:09s\n",
      "epoch 167| loss: 0.44457 |  0:00:09s\n",
      "epoch 168| loss: 0.45742 |  0:00:10s\n",
      "epoch 169| loss: 0.45824 |  0:00:10s\n",
      "epoch 170| loss: 0.45712 |  0:00:10s\n",
      "epoch 171| loss: 0.45672 |  0:00:10s\n",
      "epoch 172| loss: 0.45638 |  0:00:10s\n",
      "epoch 173| loss: 0.43691 |  0:00:10s\n",
      "epoch 174| loss: 0.43749 |  0:00:10s\n",
      "epoch 175| loss: 0.43628 |  0:00:10s\n",
      "epoch 176| loss: 0.44168 |  0:00:10s\n",
      "epoch 177| loss: 0.4387  |  0:00:10s\n",
      "epoch 178| loss: 0.43143 |  0:00:10s\n",
      "epoch 179| loss: 0.43078 |  0:00:10s\n",
      "epoch 180| loss: 0.44162 |  0:00:10s\n",
      "epoch 181| loss: 0.43561 |  0:00:10s\n",
      "epoch 182| loss: 0.42597 |  0:00:10s\n",
      "epoch 183| loss: 0.41976 |  0:00:10s\n",
      "epoch 184| loss: 0.42427 |  0:00:10s\n",
      "epoch 185| loss: 0.42534 |  0:00:11s\n",
      "epoch 186| loss: 0.4135  |  0:00:11s\n",
      "epoch 187| loss: 0.41606 |  0:00:11s\n",
      "epoch 188| loss: 0.4219  |  0:00:11s\n",
      "epoch 189| loss: 0.41998 |  0:00:11s\n",
      "epoch 190| loss: 0.41337 |  0:00:11s\n",
      "epoch 191| loss: 0.41734 |  0:00:11s\n",
      "epoch 192| loss: 0.4167  |  0:00:11s\n",
      "epoch 193| loss: 0.42384 |  0:00:11s\n",
      "epoch 194| loss: 0.43533 |  0:00:11s\n",
      "epoch 195| loss: 0.43124 |  0:00:11s\n",
      "epoch 196| loss: 0.43244 |  0:00:11s\n",
      "epoch 197| loss: 0.42344 |  0:00:11s\n",
      "epoch 198| loss: 0.4241  |  0:00:11s\n",
      "epoch 199| loss: 0.41448 |  0:00:11s\n",
      "epoch 200| loss: 0.41279 |  0:00:11s\n",
      "epoch 201| loss: 0.40516 |  0:00:11s\n",
      "epoch 202| loss: 0.40169 |  0:00:12s\n",
      "epoch 203| loss: 0.41389 |  0:00:12s\n",
      "epoch 204| loss: 0.40198 |  0:00:12s\n",
      "epoch 205| loss: 0.41592 |  0:00:12s\n",
      "epoch 206| loss: 0.4143  |  0:00:12s\n",
      "epoch 207| loss: 0.42835 |  0:00:12s\n",
      "epoch 208| loss: 0.43287 |  0:00:12s\n",
      "epoch 209| loss: 0.42364 |  0:00:12s\n",
      "epoch 210| loss: 0.42199 |  0:00:12s\n",
      "epoch 211| loss: 0.41614 |  0:00:12s\n",
      "epoch 212| loss: 0.41388 |  0:00:12s\n",
      "epoch 213| loss: 0.40773 |  0:00:12s\n",
      "epoch 214| loss: 0.41764 |  0:00:12s\n",
      "epoch 215| loss: 0.40993 |  0:00:12s\n",
      "epoch 216| loss: 0.39993 |  0:00:12s\n",
      "epoch 217| loss: 0.41415 |  0:00:12s\n",
      "epoch 218| loss: 0.39814 |  0:00:12s\n",
      "epoch 219| loss: 0.3966  |  0:00:12s\n",
      "epoch 220| loss: 0.39916 |  0:00:13s\n",
      "epoch 221| loss: 0.40041 |  0:00:13s\n",
      "epoch 222| loss: 0.38946 |  0:00:13s\n",
      "epoch 223| loss: 0.39703 |  0:00:13s\n",
      "epoch 224| loss: 0.39601 |  0:00:13s\n",
      "epoch 225| loss: 0.39347 |  0:00:13s\n",
      "epoch 226| loss: 0.39665 |  0:00:13s\n",
      "epoch 227| loss: 0.39212 |  0:00:13s\n",
      "epoch 228| loss: 0.3917  |  0:00:13s\n",
      "epoch 229| loss: 0.38537 |  0:00:13s\n",
      "epoch 230| loss: 0.39028 |  0:00:13s\n",
      "epoch 231| loss: 0.38035 |  0:00:13s\n",
      "epoch 232| loss: 0.37923 |  0:00:13s\n",
      "epoch 233| loss: 0.3796  |  0:00:13s\n",
      "epoch 234| loss: 0.37853 |  0:00:13s\n",
      "epoch 235| loss: 0.38029 |  0:00:13s\n",
      "epoch 236| loss: 0.38265 |  0:00:13s\n",
      "epoch 237| loss: 0.38124 |  0:00:14s\n",
      "epoch 238| loss: 0.38934 |  0:00:14s\n",
      "epoch 239| loss: 0.38778 |  0:00:14s\n",
      "epoch 240| loss: 0.40296 |  0:00:14s\n",
      "epoch 241| loss: 0.39686 |  0:00:14s\n",
      "epoch 242| loss: 0.39273 |  0:00:14s\n",
      "epoch 243| loss: 0.38661 |  0:00:14s\n",
      "epoch 244| loss: 0.3911  |  0:00:14s\n",
      "epoch 245| loss: 0.42204 |  0:00:14s\n",
      "epoch 246| loss: 0.42246 |  0:00:14s\n",
      "epoch 247| loss: 0.42942 |  0:00:14s\n",
      "epoch 248| loss: 0.42986 |  0:00:14s\n",
      "epoch 249| loss: 0.43916 |  0:00:14s\n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.15051 |  0:00:00s\n",
      "epoch 1  | loss: 1.86351 |  0:00:00s\n",
      "epoch 2  | loss: 1.34508 |  0:00:00s\n",
      "epoch 3  | loss: 1.10743 |  0:00:00s\n",
      "epoch 4  | loss: 1.00565 |  0:00:00s\n",
      "epoch 5  | loss: 0.95812 |  0:00:00s\n",
      "epoch 6  | loss: 0.90926 |  0:00:00s\n",
      "epoch 7  | loss: 0.9053  |  0:00:00s\n",
      "epoch 8  | loss: 0.8709  |  0:00:00s\n",
      "epoch 9  | loss: 0.83163 |  0:00:00s\n",
      "epoch 10 | loss: 0.8144  |  0:00:00s\n",
      "epoch 11 | loss: 0.80247 |  0:00:00s\n",
      "epoch 12 | loss: 0.79221 |  0:00:00s\n",
      "epoch 13 | loss: 0.77847 |  0:00:00s\n",
      "epoch 14 | loss: 0.76952 |  0:00:00s\n",
      "epoch 15 | loss: 0.75033 |  0:00:00s\n",
      "epoch 16 | loss: 0.73711 |  0:00:01s\n",
      "epoch 17 | loss: 0.74901 |  0:00:01s\n",
      "epoch 18 | loss: 0.73818 |  0:00:01s\n",
      "epoch 19 | loss: 0.74009 |  0:00:01s\n",
      "epoch 20 | loss: 0.73528 |  0:00:01s\n",
      "epoch 21 | loss: 0.73678 |  0:00:01s\n",
      "epoch 22 | loss: 0.71594 |  0:00:01s\n",
      "epoch 23 | loss: 0.71125 |  0:00:01s\n",
      "epoch 24 | loss: 0.71008 |  0:00:01s\n",
      "epoch 25 | loss: 0.69954 |  0:00:01s\n",
      "epoch 26 | loss: 0.69657 |  0:00:01s\n",
      "epoch 27 | loss: 0.70045 |  0:00:01s\n",
      "epoch 28 | loss: 0.69155 |  0:00:01s\n",
      "epoch 29 | loss: 0.6949  |  0:00:01s\n",
      "epoch 30 | loss: 0.68246 |  0:00:01s\n",
      "epoch 31 | loss: 0.69134 |  0:00:01s\n",
      "epoch 32 | loss: 0.68311 |  0:00:01s\n",
      "epoch 33 | loss: 0.68128 |  0:00:01s\n",
      "epoch 34 | loss: 0.68213 |  0:00:02s\n",
      "epoch 35 | loss: 0.66623 |  0:00:02s\n",
      "epoch 36 | loss: 0.6665  |  0:00:02s\n",
      "epoch 37 | loss: 0.66875 |  0:00:02s\n",
      "epoch 38 | loss: 0.66045 |  0:00:02s\n",
      "epoch 39 | loss: 0.67323 |  0:00:02s\n",
      "epoch 40 | loss: 0.67841 |  0:00:02s\n",
      "epoch 41 | loss: 0.67535 |  0:00:02s\n",
      "epoch 42 | loss: 0.67066 |  0:00:02s\n",
      "epoch 43 | loss: 0.66365 |  0:00:02s\n",
      "epoch 44 | loss: 0.66791 |  0:00:02s\n",
      "epoch 45 | loss: 0.66087 |  0:00:02s\n",
      "epoch 46 | loss: 0.66771 |  0:00:02s\n",
      "epoch 47 | loss: 0.67797 |  0:00:03s\n",
      "epoch 48 | loss: 0.67124 |  0:00:03s\n",
      "epoch 49 | loss: 0.66973 |  0:00:03s\n",
      "epoch 50 | loss: 0.65421 |  0:00:03s\n",
      "epoch 51 | loss: 0.65317 |  0:00:03s\n",
      "epoch 52 | loss: 0.66343 |  0:00:03s\n",
      "epoch 53 | loss: 0.65774 |  0:00:03s\n",
      "epoch 54 | loss: 0.65784 |  0:00:03s\n",
      "epoch 55 | loss: 0.66715 |  0:00:03s\n",
      "epoch 56 | loss: 0.65763 |  0:00:03s\n",
      "epoch 57 | loss: 0.66143 |  0:00:03s\n",
      "epoch 58 | loss: 0.65398 |  0:00:03s\n",
      "epoch 59 | loss: 0.65942 |  0:00:03s\n",
      "epoch 60 | loss: 0.64506 |  0:00:03s\n",
      "epoch 61 | loss: 0.64306 |  0:00:04s\n",
      "epoch 62 | loss: 0.63962 |  0:00:04s\n",
      "epoch 63 | loss: 0.63858 |  0:00:04s\n",
      "epoch 64 | loss: 0.64312 |  0:00:04s\n",
      "epoch 65 | loss: 0.62425 |  0:00:04s\n",
      "epoch 66 | loss: 0.6258  |  0:00:04s\n",
      "epoch 67 | loss: 0.62734 |  0:00:04s\n",
      "epoch 68 | loss: 0.62912 |  0:00:04s\n",
      "epoch 69 | loss: 0.63468 |  0:00:04s\n",
      "epoch 70 | loss: 0.62545 |  0:00:04s\n",
      "epoch 71 | loss: 0.62395 |  0:00:04s\n",
      "epoch 72 | loss: 0.63074 |  0:00:04s\n",
      "epoch 73 | loss: 0.61834 |  0:00:04s\n",
      "epoch 74 | loss: 0.62082 |  0:00:04s\n",
      "epoch 75 | loss: 0.62032 |  0:00:04s\n",
      "epoch 76 | loss: 0.6156  |  0:00:04s\n",
      "epoch 77 | loss: 0.61322 |  0:00:04s\n",
      "epoch 78 | loss: 0.61292 |  0:00:04s\n",
      "epoch 79 | loss: 0.60924 |  0:00:04s\n",
      "epoch 80 | loss: 0.60708 |  0:00:04s\n",
      "epoch 81 | loss: 0.60549 |  0:00:05s\n",
      "epoch 82 | loss: 0.60009 |  0:00:05s\n",
      "epoch 83 | loss: 0.60598 |  0:00:05s\n",
      "epoch 84 | loss: 0.59249 |  0:00:05s\n",
      "epoch 85 | loss: 0.59817 |  0:00:05s\n",
      "epoch 86 | loss: 0.59473 |  0:00:05s\n",
      "epoch 87 | loss: 0.58869 |  0:00:05s\n",
      "epoch 88 | loss: 0.59432 |  0:00:05s\n",
      "epoch 89 | loss: 0.60177 |  0:00:05s\n",
      "epoch 90 | loss: 0.60654 |  0:00:05s\n",
      "epoch 91 | loss: 0.59802 |  0:00:05s\n",
      "epoch 92 | loss: 0.58801 |  0:00:05s\n",
      "epoch 93 | loss: 0.59113 |  0:00:05s\n",
      "epoch 94 | loss: 0.58826 |  0:00:05s\n",
      "epoch 95 | loss: 0.57826 |  0:00:05s\n",
      "epoch 96 | loss: 0.57324 |  0:00:05s\n",
      "epoch 97 | loss: 0.60624 |  0:00:05s\n",
      "epoch 98 | loss: 0.59351 |  0:00:05s\n",
      "epoch 99 | loss: 0.59259 |  0:00:05s\n",
      "epoch 100| loss: 0.58724 |  0:00:05s\n",
      "epoch 101| loss: 0.58077 |  0:00:05s\n",
      "epoch 102| loss: 0.58065 |  0:00:05s\n",
      "epoch 103| loss: 0.57261 |  0:00:06s\n",
      "epoch 104| loss: 0.5694  |  0:00:06s\n",
      "epoch 105| loss: 0.57035 |  0:00:06s\n",
      "epoch 106| loss: 0.57229 |  0:00:06s\n",
      "epoch 107| loss: 0.56529 |  0:00:06s\n",
      "epoch 108| loss: 0.55467 |  0:00:06s\n",
      "epoch 109| loss: 0.55798 |  0:00:06s\n",
      "epoch 110| loss: 0.56504 |  0:00:06s\n",
      "epoch 111| loss: 0.55615 |  0:00:06s\n",
      "epoch 112| loss: 0.54645 |  0:00:06s\n",
      "epoch 113| loss: 0.55461 |  0:00:06s\n",
      "epoch 114| loss: 0.5452  |  0:00:06s\n",
      "epoch 115| loss: 0.56579 |  0:00:06s\n",
      "epoch 116| loss: 0.55227 |  0:00:06s\n",
      "epoch 117| loss: 0.55423 |  0:00:06s\n",
      "epoch 118| loss: 0.54877 |  0:00:06s\n",
      "epoch 119| loss: 0.54176 |  0:00:06s\n",
      "epoch 120| loss: 0.53955 |  0:00:06s\n",
      "epoch 121| loss: 0.53732 |  0:00:06s\n",
      "epoch 122| loss: 0.53207 |  0:00:06s\n",
      "epoch 123| loss: 0.53469 |  0:00:06s\n",
      "epoch 124| loss: 0.53514 |  0:00:06s\n",
      "epoch 125| loss: 0.53103 |  0:00:06s\n",
      "epoch 126| loss: 0.52426 |  0:00:07s\n",
      "epoch 127| loss: 0.52558 |  0:00:07s\n",
      "epoch 128| loss: 0.52221 |  0:00:07s\n",
      "epoch 129| loss: 0.52387 |  0:00:07s\n",
      "epoch 130| loss: 0.52182 |  0:00:07s\n",
      "epoch 131| loss: 0.5179  |  0:00:07s\n",
      "epoch 132| loss: 0.51904 |  0:00:07s\n",
      "epoch 133| loss: 0.50781 |  0:00:07s\n",
      "epoch 134| loss: 0.51151 |  0:00:07s\n",
      "epoch 135| loss: 0.51023 |  0:00:07s\n",
      "epoch 136| loss: 0.50563 |  0:00:07s\n",
      "epoch 137| loss: 0.50659 |  0:00:07s\n",
      "epoch 138| loss: 0.50215 |  0:00:07s\n",
      "epoch 139| loss: 0.50441 |  0:00:07s\n",
      "epoch 140| loss: 0.50907 |  0:00:07s\n",
      "epoch 141| loss: 0.51535 |  0:00:07s\n",
      "epoch 142| loss: 0.51048 |  0:00:07s\n",
      "epoch 143| loss: 0.50605 |  0:00:07s\n",
      "epoch 144| loss: 0.4968  |  0:00:07s\n",
      "epoch 145| loss: 0.5024  |  0:00:07s\n",
      "epoch 146| loss: 0.4944  |  0:00:08s\n",
      "epoch 147| loss: 0.4941  |  0:00:08s\n",
      "epoch 148| loss: 0.48919 |  0:00:08s\n",
      "epoch 149| loss: 0.48997 |  0:00:08s\n",
      "epoch 150| loss: 0.48825 |  0:00:08s\n",
      "epoch 151| loss: 0.49362 |  0:00:08s\n",
      "epoch 152| loss: 0.48826 |  0:00:08s\n",
      "epoch 153| loss: 0.47871 |  0:00:08s\n",
      "epoch 154| loss: 0.48342 |  0:00:08s\n",
      "epoch 155| loss: 0.47913 |  0:00:08s\n",
      "epoch 156| loss: 0.47771 |  0:00:08s\n",
      "epoch 157| loss: 0.4824  |  0:00:08s\n",
      "epoch 158| loss: 0.47315 |  0:00:08s\n",
      "epoch 159| loss: 0.4748  |  0:00:08s\n",
      "epoch 160| loss: 0.46725 |  0:00:08s\n",
      "epoch 161| loss: 0.46723 |  0:00:08s\n",
      "epoch 162| loss: 0.47148 |  0:00:08s\n",
      "epoch 163| loss: 0.4677  |  0:00:09s\n",
      "epoch 164| loss: 0.46544 |  0:00:09s\n",
      "epoch 165| loss: 0.47282 |  0:00:09s\n",
      "epoch 166| loss: 0.47266 |  0:00:09s\n",
      "epoch 167| loss: 0.45748 |  0:00:09s\n",
      "epoch 168| loss: 0.4675  |  0:00:09s\n",
      "epoch 169| loss: 0.45777 |  0:00:09s\n",
      "epoch 170| loss: 0.46282 |  0:00:09s\n",
      "epoch 171| loss: 0.45914 |  0:00:09s\n",
      "epoch 172| loss: 0.4621  |  0:00:09s\n",
      "epoch 173| loss: 0.45596 |  0:00:09s\n",
      "epoch 174| loss: 0.45742 |  0:00:09s\n",
      "epoch 175| loss: 0.45009 |  0:00:09s\n",
      "epoch 176| loss: 0.45329 |  0:00:09s\n",
      "epoch 177| loss: 0.45976 |  0:00:09s\n",
      "epoch 178| loss: 0.45691 |  0:00:09s\n",
      "epoch 179| loss: 0.46764 |  0:00:09s\n",
      "epoch 180| loss: 0.47491 |  0:00:09s\n",
      "epoch 181| loss: 0.47512 |  0:00:10s\n",
      "epoch 182| loss: 0.47458 |  0:00:10s\n",
      "epoch 183| loss: 0.46748 |  0:00:10s\n",
      "epoch 184| loss: 0.46218 |  0:00:10s\n",
      "epoch 185| loss: 0.45909 |  0:00:10s\n",
      "epoch 186| loss: 0.45007 |  0:00:10s\n",
      "epoch 187| loss: 0.44953 |  0:00:10s\n",
      "epoch 188| loss: 0.44381 |  0:00:10s\n",
      "epoch 189| loss: 0.43469 |  0:00:10s\n",
      "epoch 190| loss: 0.43863 |  0:00:10s\n",
      "epoch 191| loss: 0.44215 |  0:00:10s\n",
      "epoch 192| loss: 0.44074 |  0:00:10s\n",
      "epoch 193| loss: 0.43661 |  0:00:10s\n",
      "epoch 194| loss: 0.44409 |  0:00:10s\n",
      "epoch 195| loss: 0.44446 |  0:00:10s\n",
      "epoch 196| loss: 0.44828 |  0:00:11s\n",
      "epoch 197| loss: 0.43905 |  0:00:11s\n",
      "epoch 198| loss: 0.43568 |  0:00:11s\n",
      "epoch 199| loss: 0.43767 |  0:00:11s\n",
      "epoch 200| loss: 0.43088 |  0:00:11s\n",
      "epoch 201| loss: 0.42615 |  0:00:11s\n",
      "epoch 202| loss: 0.43017 |  0:00:11s\n",
      "epoch 203| loss: 0.42896 |  0:00:11s\n",
      "epoch 204| loss: 0.42182 |  0:00:11s\n",
      "epoch 205| loss: 0.4194  |  0:00:11s\n",
      "epoch 206| loss: 0.42018 |  0:00:11s\n",
      "epoch 207| loss: 0.42764 |  0:00:11s\n",
      "epoch 208| loss: 0.42683 |  0:00:11s\n",
      "epoch 209| loss: 0.41382 |  0:00:11s\n",
      "epoch 210| loss: 0.41853 |  0:00:11s\n",
      "epoch 211| loss: 0.40976 |  0:00:11s\n",
      "epoch 212| loss: 0.42189 |  0:00:11s\n",
      "epoch 213| loss: 0.43928 |  0:00:11s\n",
      "epoch 214| loss: 0.45131 |  0:00:12s\n",
      "epoch 215| loss: 0.43838 |  0:00:12s\n",
      "epoch 216| loss: 0.42871 |  0:00:12s\n",
      "epoch 217| loss: 0.43241 |  0:00:12s\n",
      "epoch 218| loss: 0.43236 |  0:00:12s\n",
      "epoch 219| loss: 0.42807 |  0:00:12s\n",
      "epoch 220| loss: 0.4229  |  0:00:12s\n",
      "epoch 221| loss: 0.4299  |  0:00:12s\n",
      "epoch 222| loss: 0.42039 |  0:00:12s\n",
      "epoch 223| loss: 0.42554 |  0:00:12s\n",
      "epoch 224| loss: 0.41523 |  0:00:12s\n",
      "epoch 225| loss: 0.42506 |  0:00:12s\n",
      "epoch 226| loss: 0.41809 |  0:00:12s\n",
      "epoch 227| loss: 0.40769 |  0:00:12s\n",
      "epoch 228| loss: 0.40721 |  0:00:12s\n",
      "epoch 229| loss: 0.40274 |  0:00:12s\n",
      "epoch 230| loss: 0.40621 |  0:00:12s\n",
      "epoch 231| loss: 0.40285 |  0:00:12s\n",
      "epoch 232| loss: 0.39976 |  0:00:13s\n",
      "epoch 233| loss: 0.40647 |  0:00:13s\n",
      "epoch 234| loss: 0.40447 |  0:00:13s\n",
      "epoch 235| loss: 0.39897 |  0:00:13s\n",
      "epoch 236| loss: 0.39623 |  0:00:13s\n",
      "epoch 237| loss: 0.4024  |  0:00:13s\n",
      "epoch 238| loss: 0.39969 |  0:00:13s\n",
      "epoch 239| loss: 0.39727 |  0:00:13s\n",
      "epoch 240| loss: 0.39851 |  0:00:13s\n",
      "epoch 241| loss: 0.39616 |  0:00:13s\n",
      "epoch 242| loss: 0.39079 |  0:00:13s\n",
      "epoch 243| loss: 0.38584 |  0:00:13s\n",
      "epoch 244| loss: 0.39194 |  0:00:13s\n",
      "epoch 245| loss: 0.38958 |  0:00:13s\n",
      "epoch 246| loss: 0.38204 |  0:00:13s\n",
      "epoch 247| loss: 0.38006 |  0:00:13s\n",
      "epoch 248| loss: 0.37911 |  0:00:13s\n",
      "epoch 249| loss: 0.38011 |  0:00:13s\n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.03791 |  0:00:00s\n",
      "epoch 1  | loss: 1.85241 |  0:00:00s\n",
      "epoch 2  | loss: 1.31173 |  0:00:00s\n",
      "epoch 3  | loss: 1.14973 |  0:00:00s\n",
      "epoch 4  | loss: 1.02901 |  0:00:00s\n",
      "epoch 5  | loss: 0.99191 |  0:00:00s\n",
      "epoch 6  | loss: 0.94639 |  0:00:00s\n",
      "epoch 7  | loss: 0.90505 |  0:00:00s\n",
      "epoch 8  | loss: 0.88077 |  0:00:00s\n",
      "epoch 9  | loss: 0.86348 |  0:00:00s\n",
      "epoch 10 | loss: 0.8268  |  0:00:00s\n",
      "epoch 11 | loss: 0.82074 |  0:00:00s\n",
      "epoch 12 | loss: 0.80517 |  0:00:00s\n",
      "epoch 13 | loss: 0.76496 |  0:00:00s\n",
      "epoch 14 | loss: 0.77369 |  0:00:00s\n",
      "epoch 15 | loss: 0.75442 |  0:00:00s\n",
      "epoch 16 | loss: 0.76697 |  0:00:00s\n",
      "epoch 17 | loss: 0.75029 |  0:00:00s\n",
      "epoch 18 | loss: 0.75392 |  0:00:01s\n",
      "epoch 19 | loss: 0.74161 |  0:00:01s\n",
      "epoch 20 | loss: 0.7431  |  0:00:01s\n",
      "epoch 21 | loss: 0.74189 |  0:00:01s\n",
      "epoch 22 | loss: 0.7388  |  0:00:01s\n",
      "epoch 23 | loss: 0.73827 |  0:00:01s\n",
      "epoch 24 | loss: 0.73531 |  0:00:01s\n",
      "epoch 25 | loss: 0.71676 |  0:00:01s\n",
      "epoch 26 | loss: 0.7172  |  0:00:01s\n",
      "epoch 27 | loss: 0.70597 |  0:00:01s\n",
      "epoch 28 | loss: 0.70232 |  0:00:01s\n",
      "epoch 29 | loss: 0.70922 |  0:00:01s\n",
      "epoch 30 | loss: 0.6953  |  0:00:01s\n",
      "epoch 31 | loss: 0.69619 |  0:00:01s\n",
      "epoch 32 | loss: 0.69896 |  0:00:01s\n",
      "epoch 33 | loss: 0.69945 |  0:00:01s\n",
      "epoch 34 | loss: 0.69684 |  0:00:01s\n",
      "epoch 35 | loss: 0.68867 |  0:00:01s\n",
      "epoch 36 | loss: 0.68792 |  0:00:02s\n",
      "epoch 37 | loss: 0.68348 |  0:00:02s\n",
      "epoch 38 | loss: 0.68738 |  0:00:02s\n",
      "epoch 39 | loss: 0.69564 |  0:00:02s\n",
      "epoch 40 | loss: 0.67632 |  0:00:02s\n",
      "epoch 41 | loss: 0.67712 |  0:00:02s\n",
      "epoch 42 | loss: 0.6789  |  0:00:02s\n",
      "epoch 43 | loss: 0.66507 |  0:00:02s\n",
      "epoch 44 | loss: 0.67066 |  0:00:02s\n",
      "epoch 45 | loss: 0.67653 |  0:00:02s\n",
      "epoch 46 | loss: 0.66163 |  0:00:02s\n",
      "epoch 47 | loss: 0.66183 |  0:00:02s\n",
      "epoch 48 | loss: 0.65695 |  0:00:02s\n",
      "epoch 49 | loss: 0.65882 |  0:00:02s\n",
      "epoch 50 | loss: 0.65341 |  0:00:02s\n",
      "epoch 51 | loss: 0.65478 |  0:00:02s\n",
      "epoch 52 | loss: 0.64883 |  0:00:02s\n",
      "epoch 53 | loss: 0.65205 |  0:00:02s\n",
      "epoch 54 | loss: 0.6477  |  0:00:03s\n",
      "epoch 55 | loss: 0.65097 |  0:00:03s\n",
      "epoch 56 | loss: 0.65714 |  0:00:03s\n",
      "epoch 57 | loss: 0.65948 |  0:00:03s\n",
      "epoch 58 | loss: 0.6581  |  0:00:03s\n",
      "epoch 59 | loss: 0.64985 |  0:00:03s\n",
      "epoch 60 | loss: 0.63683 |  0:00:03s\n",
      "epoch 61 | loss: 0.65057 |  0:00:03s\n",
      "epoch 62 | loss: 0.63313 |  0:00:03s\n",
      "epoch 63 | loss: 0.64155 |  0:00:03s\n",
      "epoch 64 | loss: 0.6359  |  0:00:03s\n",
      "epoch 65 | loss: 0.63455 |  0:00:03s\n",
      "epoch 66 | loss: 0.62906 |  0:00:03s\n",
      "epoch 67 | loss: 0.62834 |  0:00:03s\n",
      "epoch 68 | loss: 0.63291 |  0:00:03s\n",
      "epoch 69 | loss: 0.62875 |  0:00:03s\n",
      "epoch 70 | loss: 0.62527 |  0:00:03s\n",
      "epoch 71 | loss: 0.62797 |  0:00:03s\n",
      "epoch 72 | loss: 0.62145 |  0:00:04s\n",
      "epoch 73 | loss: 0.61515 |  0:00:04s\n",
      "epoch 74 | loss: 0.61982 |  0:00:04s\n",
      "epoch 75 | loss: 0.61472 |  0:00:04s\n",
      "epoch 76 | loss: 0.61055 |  0:00:04s\n",
      "epoch 77 | loss: 0.61742 |  0:00:04s\n",
      "epoch 78 | loss: 0.61169 |  0:00:04s\n",
      "epoch 79 | loss: 0.60791 |  0:00:04s\n",
      "epoch 80 | loss: 0.60484 |  0:00:04s\n",
      "epoch 81 | loss: 0.60778 |  0:00:04s\n",
      "epoch 82 | loss: 0.59876 |  0:00:04s\n",
      "epoch 83 | loss: 0.60467 |  0:00:04s\n",
      "epoch 84 | loss: 0.6067  |  0:00:04s\n",
      "epoch 85 | loss: 0.59921 |  0:00:04s\n",
      "epoch 86 | loss: 0.59619 |  0:00:04s\n",
      "epoch 87 | loss: 0.6023  |  0:00:04s\n",
      "epoch 88 | loss: 0.59538 |  0:00:04s\n",
      "epoch 89 | loss: 0.60875 |  0:00:04s\n",
      "epoch 90 | loss: 0.61306 |  0:00:04s\n",
      "epoch 91 | loss: 0.60746 |  0:00:04s\n",
      "epoch 92 | loss: 0.59965 |  0:00:05s\n",
      "epoch 93 | loss: 0.60201 |  0:00:05s\n",
      "epoch 94 | loss: 0.59897 |  0:00:05s\n",
      "epoch 95 | loss: 0.59502 |  0:00:05s\n",
      "epoch 96 | loss: 0.58616 |  0:00:05s\n",
      "epoch 97 | loss: 0.59923 |  0:00:05s\n",
      "epoch 98 | loss: 0.58776 |  0:00:05s\n",
      "epoch 99 | loss: 0.59052 |  0:00:05s\n",
      "epoch 100| loss: 0.57704 |  0:00:05s\n",
      "epoch 101| loss: 0.57695 |  0:00:05s\n",
      "epoch 102| loss: 0.57197 |  0:00:05s\n",
      "epoch 103| loss: 0.57305 |  0:00:05s\n",
      "epoch 104| loss: 0.5796  |  0:00:05s\n",
      "epoch 105| loss: 0.566   |  0:00:05s\n",
      "epoch 106| loss: 0.56814 |  0:00:05s\n",
      "epoch 107| loss: 0.5703  |  0:00:05s\n",
      "epoch 108| loss: 0.56251 |  0:00:05s\n",
      "epoch 109| loss: 0.56174 |  0:00:05s\n",
      "epoch 110| loss: 0.56001 |  0:00:05s\n",
      "epoch 111| loss: 0.56068 |  0:00:05s\n",
      "epoch 112| loss: 0.57955 |  0:00:05s\n",
      "epoch 113| loss: 0.56637 |  0:00:06s\n",
      "epoch 114| loss: 0.56206 |  0:00:06s\n",
      "epoch 115| loss: 0.56794 |  0:00:06s\n",
      "epoch 116| loss: 0.5685  |  0:00:06s\n",
      "epoch 117| loss: 0.56625 |  0:00:06s\n",
      "epoch 118| loss: 0.55463 |  0:00:06s\n",
      "epoch 119| loss: 0.55823 |  0:00:06s\n",
      "epoch 120| loss: 0.56635 |  0:00:06s\n",
      "epoch 121| loss: 0.56093 |  0:00:06s\n",
      "epoch 122| loss: 0.5564  |  0:00:06s\n",
      "epoch 123| loss: 0.56447 |  0:00:06s\n",
      "epoch 124| loss: 0.54416 |  0:00:06s\n",
      "epoch 125| loss: 0.55187 |  0:00:06s\n",
      "epoch 126| loss: 0.53369 |  0:00:06s\n",
      "epoch 127| loss: 0.53799 |  0:00:06s\n",
      "epoch 128| loss: 0.54588 |  0:00:06s\n",
      "epoch 129| loss: 0.54607 |  0:00:06s\n",
      "epoch 130| loss: 0.53851 |  0:00:06s\n",
      "epoch 131| loss: 0.54799 |  0:00:06s\n",
      "epoch 132| loss: 0.54209 |  0:00:06s\n",
      "epoch 133| loss: 0.54189 |  0:00:06s\n",
      "epoch 134| loss: 0.54945 |  0:00:07s\n",
      "epoch 135| loss: 0.54474 |  0:00:07s\n",
      "epoch 136| loss: 0.53878 |  0:00:07s\n",
      "epoch 137| loss: 0.54082 |  0:00:07s\n",
      "epoch 138| loss: 0.54593 |  0:00:07s\n",
      "epoch 139| loss: 0.54421 |  0:00:07s\n",
      "epoch 140| loss: 0.52921 |  0:00:07s\n",
      "epoch 141| loss: 0.54561 |  0:00:07s\n",
      "epoch 142| loss: 0.53441 |  0:00:07s\n",
      "epoch 143| loss: 0.52897 |  0:00:07s\n",
      "epoch 144| loss: 0.51672 |  0:00:07s\n",
      "epoch 145| loss: 0.52258 |  0:00:07s\n",
      "epoch 146| loss: 0.52129 |  0:00:07s\n",
      "epoch 147| loss: 0.50979 |  0:00:07s\n",
      "epoch 148| loss: 0.51491 |  0:00:07s\n",
      "epoch 149| loss: 0.50503 |  0:00:07s\n",
      "epoch 150| loss: 0.50354 |  0:00:07s\n",
      "epoch 151| loss: 0.50057 |  0:00:07s\n",
      "epoch 152| loss: 0.51239 |  0:00:07s\n",
      "epoch 153| loss: 0.50564 |  0:00:08s\n",
      "epoch 154| loss: 0.50433 |  0:00:08s\n",
      "epoch 155| loss: 0.49586 |  0:00:08s\n",
      "epoch 156| loss: 0.5058  |  0:00:08s\n",
      "epoch 157| loss: 0.505   |  0:00:08s\n",
      "epoch 158| loss: 0.50522 |  0:00:08s\n",
      "epoch 159| loss: 0.50261 |  0:00:08s\n",
      "epoch 160| loss: 0.49743 |  0:00:08s\n",
      "epoch 161| loss: 0.49228 |  0:00:08s\n",
      "epoch 162| loss: 0.49446 |  0:00:08s\n",
      "epoch 163| loss: 0.49708 |  0:00:08s\n",
      "epoch 164| loss: 0.49066 |  0:00:08s\n",
      "epoch 165| loss: 0.50091 |  0:00:08s\n",
      "epoch 166| loss: 0.5003  |  0:00:08s\n",
      "epoch 167| loss: 0.49858 |  0:00:08s\n",
      "epoch 168| loss: 0.51813 |  0:00:08s\n",
      "epoch 169| loss: 0.51245 |  0:00:08s\n",
      "epoch 170| loss: 0.51785 |  0:00:08s\n",
      "epoch 171| loss: 0.51345 |  0:00:08s\n",
      "epoch 172| loss: 0.53724 |  0:00:09s\n",
      "epoch 173| loss: 0.51823 |  0:00:09s\n",
      "epoch 174| loss: 0.52032 |  0:00:09s\n",
      "epoch 175| loss: 0.51255 |  0:00:09s\n",
      "epoch 176| loss: 0.51501 |  0:00:09s\n",
      "epoch 177| loss: 0.50299 |  0:00:09s\n",
      "epoch 178| loss: 0.50091 |  0:00:09s\n",
      "epoch 179| loss: 0.50526 |  0:00:09s\n",
      "epoch 180| loss: 0.51458 |  0:00:09s\n",
      "epoch 181| loss: 0.5007  |  0:00:09s\n",
      "epoch 182| loss: 0.49702 |  0:00:09s\n",
      "epoch 183| loss: 0.49115 |  0:00:09s\n",
      "epoch 184| loss: 0.49323 |  0:00:09s\n",
      "epoch 185| loss: 0.48715 |  0:00:09s\n",
      "epoch 186| loss: 0.4779  |  0:00:09s\n",
      "epoch 187| loss: 0.47824 |  0:00:09s\n",
      "epoch 188| loss: 0.47369 |  0:00:09s\n",
      "epoch 189| loss: 0.46854 |  0:00:09s\n",
      "epoch 190| loss: 0.46115 |  0:00:09s\n",
      "epoch 191| loss: 0.46776 |  0:00:09s\n",
      "epoch 192| loss: 0.46284 |  0:00:10s\n",
      "epoch 193| loss: 0.46441 |  0:00:10s\n",
      "epoch 194| loss: 0.47305 |  0:00:10s\n",
      "epoch 195| loss: 0.46534 |  0:00:10s\n",
      "epoch 196| loss: 0.46834 |  0:00:10s\n",
      "epoch 197| loss: 0.46976 |  0:00:10s\n",
      "epoch 198| loss: 0.45851 |  0:00:10s\n",
      "epoch 199| loss: 0.46176 |  0:00:10s\n",
      "epoch 200| loss: 0.45617 |  0:00:10s\n",
      "epoch 201| loss: 0.45439 |  0:00:10s\n",
      "epoch 202| loss: 0.4458  |  0:00:10s\n",
      "epoch 203| loss: 0.45292 |  0:00:10s\n",
      "epoch 204| loss: 0.45455 |  0:00:10s\n",
      "epoch 205| loss: 0.45756 |  0:00:10s\n",
      "epoch 206| loss: 0.44597 |  0:00:10s\n",
      "epoch 207| loss: 0.4571  |  0:00:10s\n",
      "epoch 208| loss: 0.45112 |  0:00:10s\n",
      "epoch 209| loss: 0.44246 |  0:00:10s\n",
      "epoch 210| loss: 0.44764 |  0:00:10s\n",
      "epoch 211| loss: 0.44764 |  0:00:10s\n",
      "epoch 212| loss: 0.44266 |  0:00:10s\n",
      "epoch 213| loss: 0.4418  |  0:00:11s\n",
      "epoch 214| loss: 0.44182 |  0:00:11s\n",
      "epoch 215| loss: 0.44534 |  0:00:11s\n",
      "epoch 216| loss: 0.4397  |  0:00:11s\n",
      "epoch 217| loss: 0.44097 |  0:00:11s\n",
      "epoch 218| loss: 0.43494 |  0:00:11s\n",
      "epoch 219| loss: 0.43702 |  0:00:11s\n",
      "epoch 220| loss: 0.43422 |  0:00:11s\n",
      "epoch 221| loss: 0.43264 |  0:00:11s\n",
      "epoch 222| loss: 0.44269 |  0:00:11s\n",
      "epoch 223| loss: 0.43059 |  0:00:11s\n",
      "epoch 224| loss: 0.43074 |  0:00:11s\n",
      "epoch 225| loss: 0.42998 |  0:00:11s\n",
      "epoch 226| loss: 0.43285 |  0:00:11s\n",
      "epoch 227| loss: 0.44163 |  0:00:11s\n",
      "epoch 228| loss: 0.43015 |  0:00:11s\n",
      "epoch 229| loss: 0.42504 |  0:00:12s\n",
      "epoch 230| loss: 0.43439 |  0:00:12s\n",
      "epoch 231| loss: 0.42119 |  0:00:12s\n",
      "epoch 232| loss: 0.41839 |  0:00:12s\n",
      "epoch 233| loss: 0.41645 |  0:00:12s\n",
      "epoch 234| loss: 0.4203  |  0:00:12s\n",
      "epoch 235| loss: 0.42726 |  0:00:12s\n",
      "epoch 236| loss: 0.41839 |  0:00:12s\n",
      "epoch 237| loss: 0.4206  |  0:00:12s\n",
      "epoch 238| loss: 0.41448 |  0:00:12s\n",
      "epoch 239| loss: 0.42549 |  0:00:12s\n",
      "epoch 240| loss: 0.42034 |  0:00:12s\n",
      "epoch 241| loss: 0.42208 |  0:00:12s\n",
      "epoch 242| loss: 0.42215 |  0:00:12s\n",
      "epoch 243| loss: 0.41457 |  0:00:12s\n",
      "epoch 244| loss: 0.41616 |  0:00:12s\n",
      "epoch 245| loss: 0.41119 |  0:00:12s\n",
      "epoch 246| loss: 0.40959 |  0:00:12s\n",
      "epoch 247| loss: 0.40542 |  0:00:13s\n",
      "epoch 248| loss: 0.4031  |  0:00:13s\n",
      "epoch 249| loss: 0.41023 |  0:00:13s\n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.00868 |  0:00:00s\n",
      "epoch 1  | loss: 1.74504 |  0:00:00s\n",
      "epoch 2  | loss: 1.25367 |  0:00:00s\n",
      "epoch 3  | loss: 1.05374 |  0:00:00s\n",
      "epoch 4  | loss: 0.99741 |  0:00:00s\n",
      "epoch 5  | loss: 0.95344 |  0:00:00s\n",
      "epoch 6  | loss: 0.92711 |  0:00:00s\n",
      "epoch 7  | loss: 0.87996 |  0:00:00s\n",
      "epoch 8  | loss: 0.85517 |  0:00:00s\n",
      "epoch 9  | loss: 0.8205  |  0:00:00s\n",
      "epoch 10 | loss: 0.81221 |  0:00:00s\n",
      "epoch 11 | loss: 0.79874 |  0:00:00s\n",
      "epoch 12 | loss: 0.77808 |  0:00:00s\n",
      "epoch 13 | loss: 0.77349 |  0:00:00s\n",
      "epoch 14 | loss: 0.764   |  0:00:00s\n",
      "epoch 15 | loss: 0.745   |  0:00:00s\n",
      "epoch 16 | loss: 0.75345 |  0:00:00s\n",
      "epoch 17 | loss: 0.73494 |  0:00:00s\n",
      "epoch 18 | loss: 0.74012 |  0:00:01s\n",
      "epoch 19 | loss: 0.72622 |  0:00:01s\n",
      "epoch 20 | loss: 0.72289 |  0:00:01s\n",
      "epoch 21 | loss: 0.7288  |  0:00:01s\n",
      "epoch 22 | loss: 0.72009 |  0:00:01s\n",
      "epoch 23 | loss: 0.72184 |  0:00:01s\n",
      "epoch 24 | loss: 0.717   |  0:00:01s\n",
      "epoch 25 | loss: 0.71139 |  0:00:01s\n",
      "epoch 26 | loss: 0.69871 |  0:00:01s\n",
      "epoch 27 | loss: 0.70014 |  0:00:01s\n",
      "epoch 28 | loss: 0.70992 |  0:00:01s\n",
      "epoch 29 | loss: 0.7034  |  0:00:01s\n",
      "epoch 30 | loss: 0.69585 |  0:00:01s\n",
      "epoch 31 | loss: 0.70026 |  0:00:01s\n",
      "epoch 32 | loss: 0.68355 |  0:00:01s\n",
      "epoch 33 | loss: 0.68765 |  0:00:01s\n",
      "epoch 34 | loss: 0.68948 |  0:00:01s\n",
      "epoch 35 | loss: 0.68575 |  0:00:01s\n",
      "epoch 36 | loss: 0.68939 |  0:00:02s\n",
      "epoch 37 | loss: 0.68116 |  0:00:02s\n",
      "epoch 38 | loss: 0.67484 |  0:00:02s\n",
      "epoch 39 | loss: 0.67202 |  0:00:02s\n",
      "epoch 40 | loss: 0.66592 |  0:00:02s\n",
      "epoch 41 | loss: 0.66126 |  0:00:02s\n",
      "epoch 42 | loss: 0.66747 |  0:00:02s\n",
      "epoch 43 | loss: 0.65993 |  0:00:02s\n",
      "epoch 44 | loss: 0.65773 |  0:00:02s\n",
      "epoch 45 | loss: 0.66441 |  0:00:02s\n",
      "epoch 46 | loss: 0.6458  |  0:00:02s\n",
      "epoch 47 | loss: 0.65382 |  0:00:02s\n",
      "epoch 48 | loss: 0.63868 |  0:00:02s\n",
      "epoch 49 | loss: 0.64045 |  0:00:02s\n",
      "epoch 50 | loss: 0.63507 |  0:00:02s\n",
      "epoch 51 | loss: 0.62803 |  0:00:02s\n",
      "epoch 52 | loss: 0.63424 |  0:00:02s\n",
      "epoch 53 | loss: 0.62941 |  0:00:02s\n",
      "epoch 54 | loss: 0.62231 |  0:00:03s\n",
      "epoch 55 | loss: 0.6229  |  0:00:03s\n",
      "epoch 56 | loss: 0.62281 |  0:00:03s\n",
      "epoch 57 | loss: 0.62432 |  0:00:03s\n",
      "epoch 58 | loss: 0.61902 |  0:00:03s\n",
      "epoch 59 | loss: 0.61387 |  0:00:03s\n",
      "epoch 60 | loss: 0.6151  |  0:00:03s\n",
      "epoch 61 | loss: 0.60867 |  0:00:03s\n",
      "epoch 62 | loss: 0.60681 |  0:00:03s\n",
      "epoch 63 | loss: 0.60485 |  0:00:03s\n",
      "epoch 64 | loss: 0.59443 |  0:00:03s\n",
      "epoch 65 | loss: 0.5984  |  0:00:03s\n",
      "epoch 66 | loss: 0.59927 |  0:00:03s\n",
      "epoch 67 | loss: 0.60113 |  0:00:03s\n",
      "epoch 68 | loss: 0.59485 |  0:00:03s\n",
      "epoch 69 | loss: 0.58416 |  0:00:03s\n",
      "epoch 70 | loss: 0.586   |  0:00:03s\n",
      "epoch 71 | loss: 0.57972 |  0:00:03s\n",
      "epoch 72 | loss: 0.58027 |  0:00:03s\n",
      "epoch 73 | loss: 0.57856 |  0:00:03s\n",
      "epoch 74 | loss: 0.57329 |  0:00:04s\n",
      "epoch 75 | loss: 0.57179 |  0:00:04s\n",
      "epoch 76 | loss: 0.57929 |  0:00:04s\n",
      "epoch 77 | loss: 0.57115 |  0:00:04s\n",
      "epoch 78 | loss: 0.57464 |  0:00:04s\n",
      "epoch 79 | loss: 0.56205 |  0:00:04s\n",
      "epoch 80 | loss: 0.56704 |  0:00:04s\n",
      "epoch 81 | loss: 0.57153 |  0:00:04s\n",
      "epoch 82 | loss: 0.56112 |  0:00:04s\n",
      "epoch 83 | loss: 0.58179 |  0:00:04s\n",
      "epoch 84 | loss: 0.58288 |  0:00:04s\n",
      "epoch 85 | loss: 0.57152 |  0:00:04s\n",
      "epoch 86 | loss: 0.57103 |  0:00:04s\n",
      "epoch 87 | loss: 0.56557 |  0:00:04s\n",
      "epoch 88 | loss: 0.5808  |  0:00:04s\n",
      "epoch 89 | loss: 0.57129 |  0:00:04s\n",
      "epoch 90 | loss: 0.56026 |  0:00:04s\n",
      "epoch 91 | loss: 0.56052 |  0:00:04s\n",
      "epoch 92 | loss: 0.56151 |  0:00:04s\n",
      "epoch 93 | loss: 0.56149 |  0:00:04s\n",
      "epoch 94 | loss: 0.54939 |  0:00:04s\n",
      "epoch 95 | loss: 0.541   |  0:00:04s\n",
      "epoch 96 | loss: 0.54249 |  0:00:05s\n",
      "epoch 97 | loss: 0.54602 |  0:00:05s\n",
      "epoch 98 | loss: 0.54501 |  0:00:05s\n",
      "epoch 99 | loss: 0.54044 |  0:00:05s\n",
      "epoch 100| loss: 0.53725 |  0:00:05s\n",
      "epoch 101| loss: 0.53727 |  0:00:05s\n",
      "epoch 102| loss: 0.53684 |  0:00:05s\n",
      "epoch 103| loss: 0.53034 |  0:00:05s\n",
      "epoch 104| loss: 0.53375 |  0:00:05s\n",
      "epoch 105| loss: 0.54197 |  0:00:05s\n",
      "epoch 106| loss: 0.53513 |  0:00:05s\n",
      "epoch 107| loss: 0.53932 |  0:00:05s\n",
      "epoch 108| loss: 0.5335  |  0:00:05s\n",
      "epoch 109| loss: 0.53586 |  0:00:05s\n",
      "epoch 110| loss: 0.53566 |  0:00:05s\n",
      "epoch 111| loss: 0.54753 |  0:00:05s\n",
      "epoch 112| loss: 0.54548 |  0:00:05s\n",
      "epoch 113| loss: 0.53231 |  0:00:05s\n",
      "epoch 114| loss: 0.5306  |  0:00:06s\n",
      "epoch 115| loss: 0.51951 |  0:00:06s\n",
      "epoch 116| loss: 0.51814 |  0:00:06s\n",
      "epoch 117| loss: 0.51251 |  0:00:06s\n",
      "epoch 118| loss: 0.5209  |  0:00:06s\n",
      "epoch 119| loss: 0.50784 |  0:00:06s\n",
      "epoch 120| loss: 0.51608 |  0:00:06s\n",
      "epoch 121| loss: 0.50829 |  0:00:06s\n",
      "epoch 122| loss: 0.51086 |  0:00:06s\n",
      "epoch 123| loss: 0.51257 |  0:00:06s\n",
      "epoch 124| loss: 0.51142 |  0:00:06s\n",
      "epoch 125| loss: 0.5128  |  0:00:06s\n",
      "epoch 126| loss: 0.50847 |  0:00:06s\n",
      "epoch 127| loss: 0.50816 |  0:00:06s\n",
      "epoch 128| loss: 0.50715 |  0:00:06s\n",
      "epoch 129| loss: 0.5293  |  0:00:06s\n",
      "epoch 130| loss: 0.51633 |  0:00:06s\n",
      "epoch 131| loss: 0.5199  |  0:00:06s\n",
      "epoch 132| loss: 0.53192 |  0:00:06s\n",
      "epoch 133| loss: 0.52545 |  0:00:06s\n",
      "epoch 134| loss: 0.51748 |  0:00:07s\n",
      "epoch 135| loss: 0.52055 |  0:00:07s\n",
      "epoch 136| loss: 0.52472 |  0:00:07s\n",
      "epoch 137| loss: 0.52869 |  0:00:07s\n",
      "epoch 138| loss: 0.53121 |  0:00:07s\n",
      "epoch 139| loss: 0.53082 |  0:00:07s\n",
      "epoch 140| loss: 0.52077 |  0:00:07s\n",
      "epoch 141| loss: 0.52208 |  0:00:07s\n",
      "epoch 142| loss: 0.5253  |  0:00:07s\n",
      "epoch 143| loss: 0.52796 |  0:00:07s\n",
      "epoch 144| loss: 0.52166 |  0:00:07s\n",
      "epoch 145| loss: 0.52032 |  0:00:07s\n",
      "epoch 146| loss: 0.51771 |  0:00:07s\n",
      "epoch 147| loss: 0.52361 |  0:00:07s\n",
      "epoch 148| loss: 0.51397 |  0:00:07s\n",
      "epoch 149| loss: 0.51111 |  0:00:07s\n",
      "epoch 150| loss: 0.50189 |  0:00:07s\n",
      "epoch 151| loss: 0.50263 |  0:00:07s\n",
      "epoch 152| loss: 0.49869 |  0:00:07s\n",
      "epoch 153| loss: 0.49287 |  0:00:07s\n",
      "epoch 154| loss: 0.49622 |  0:00:07s\n",
      "epoch 155| loss: 0.48698 |  0:00:08s\n",
      "epoch 156| loss: 0.49493 |  0:00:08s\n",
      "epoch 157| loss: 0.49484 |  0:00:08s\n",
      "epoch 158| loss: 0.49121 |  0:00:08s\n",
      "epoch 159| loss: 0.48355 |  0:00:08s\n",
      "epoch 160| loss: 0.48172 |  0:00:08s\n",
      "epoch 161| loss: 0.46782 |  0:00:08s\n",
      "epoch 162| loss: 0.4727  |  0:00:08s\n",
      "epoch 163| loss: 0.47569 |  0:00:08s\n",
      "epoch 164| loss: 0.46952 |  0:00:08s\n",
      "epoch 165| loss: 0.46544 |  0:00:08s\n",
      "epoch 166| loss: 0.46403 |  0:00:08s\n",
      "epoch 167| loss: 0.46689 |  0:00:08s\n",
      "epoch 168| loss: 0.46627 |  0:00:08s\n",
      "epoch 169| loss: 0.46048 |  0:00:08s\n",
      "epoch 170| loss: 0.46023 |  0:00:08s\n",
      "epoch 171| loss: 0.45674 |  0:00:08s\n",
      "epoch 172| loss: 0.46145 |  0:00:08s\n",
      "epoch 173| loss: 0.45498 |  0:00:08s\n",
      "epoch 174| loss: 0.45015 |  0:00:08s\n",
      "epoch 175| loss: 0.45088 |  0:00:08s\n",
      "epoch 176| loss: 0.45133 |  0:00:09s\n",
      "epoch 177| loss: 0.45606 |  0:00:09s\n",
      "epoch 178| loss: 0.45128 |  0:00:09s\n",
      "epoch 179| loss: 0.44844 |  0:00:09s\n",
      "epoch 180| loss: 0.45378 |  0:00:09s\n",
      "epoch 181| loss: 0.44722 |  0:00:09s\n",
      "epoch 182| loss: 0.45066 |  0:00:09s\n",
      "epoch 183| loss: 0.44134 |  0:00:09s\n",
      "epoch 184| loss: 0.44586 |  0:00:09s\n",
      "epoch 185| loss: 0.44056 |  0:00:09s\n",
      "epoch 186| loss: 0.44446 |  0:00:09s\n",
      "epoch 187| loss: 0.44541 |  0:00:09s\n",
      "epoch 188| loss: 0.44281 |  0:00:09s\n",
      "epoch 189| loss: 0.43636 |  0:00:09s\n",
      "epoch 190| loss: 0.43829 |  0:00:09s\n",
      "epoch 191| loss: 0.43887 |  0:00:09s\n",
      "epoch 192| loss: 0.444   |  0:00:09s\n",
      "epoch 193| loss: 0.43338 |  0:00:09s\n",
      "epoch 194| loss: 0.4386  |  0:00:09s\n",
      "epoch 195| loss: 0.43236 |  0:00:09s\n",
      "epoch 196| loss: 0.43655 |  0:00:09s\n",
      "epoch 197| loss: 0.44025 |  0:00:10s\n",
      "epoch 198| loss: 0.43353 |  0:00:10s\n",
      "epoch 199| loss: 0.43306 |  0:00:10s\n",
      "epoch 200| loss: 0.43378 |  0:00:10s\n",
      "epoch 201| loss: 0.4229  |  0:00:10s\n",
      "epoch 202| loss: 0.41854 |  0:00:10s\n",
      "epoch 203| loss: 0.42287 |  0:00:10s\n",
      "epoch 204| loss: 0.42194 |  0:00:10s\n",
      "epoch 205| loss: 0.41932 |  0:00:10s\n",
      "epoch 206| loss: 0.42009 |  0:00:10s\n",
      "epoch 207| loss: 0.42234 |  0:00:10s\n",
      "epoch 208| loss: 0.41989 |  0:00:10s\n",
      "epoch 209| loss: 0.4176  |  0:00:10s\n",
      "epoch 210| loss: 0.416   |  0:00:10s\n",
      "epoch 211| loss: 0.42283 |  0:00:10s\n",
      "epoch 212| loss: 0.41745 |  0:00:10s\n",
      "epoch 213| loss: 0.41743 |  0:00:10s\n",
      "epoch 214| loss: 0.42344 |  0:00:10s\n",
      "epoch 215| loss: 0.41995 |  0:00:10s\n",
      "epoch 216| loss: 0.41573 |  0:00:10s\n",
      "epoch 217| loss: 0.42231 |  0:00:11s\n",
      "epoch 218| loss: 0.41803 |  0:00:11s\n",
      "epoch 219| loss: 0.42112 |  0:00:11s\n",
      "epoch 220| loss: 0.41226 |  0:00:11s\n",
      "epoch 221| loss: 0.43307 |  0:00:11s\n",
      "epoch 222| loss: 0.42982 |  0:00:11s\n",
      "epoch 223| loss: 0.43539 |  0:00:11s\n",
      "epoch 224| loss: 0.4242  |  0:00:11s\n",
      "epoch 225| loss: 0.42515 |  0:00:11s\n",
      "epoch 226| loss: 0.42332 |  0:00:11s\n",
      "epoch 227| loss: 0.42058 |  0:00:11s\n",
      "epoch 228| loss: 0.41475 |  0:00:11s\n",
      "epoch 229| loss: 0.4154  |  0:00:11s\n",
      "epoch 230| loss: 0.41748 |  0:00:11s\n",
      "epoch 231| loss: 0.40945 |  0:00:11s\n",
      "epoch 232| loss: 0.40969 |  0:00:11s\n",
      "epoch 233| loss: 0.41125 |  0:00:11s\n",
      "epoch 234| loss: 0.40821 |  0:00:11s\n",
      "epoch 235| loss: 0.40801 |  0:00:11s\n",
      "epoch 236| loss: 0.40763 |  0:00:11s\n",
      "epoch 237| loss: 0.40876 |  0:00:11s\n",
      "epoch 238| loss: 0.40676 |  0:00:11s\n",
      "epoch 239| loss: 0.4165  |  0:00:12s\n",
      "epoch 240| loss: 0.40963 |  0:00:12s\n",
      "epoch 241| loss: 0.4063  |  0:00:12s\n",
      "epoch 242| loss: 0.39888 |  0:00:12s\n",
      "epoch 243| loss: 0.3991  |  0:00:12s\n",
      "epoch 244| loss: 0.39745 |  0:00:12s\n",
      "epoch 245| loss: 0.4016  |  0:00:12s\n",
      "epoch 246| loss: 0.39547 |  0:00:12s\n",
      "epoch 247| loss: 0.39001 |  0:00:12s\n",
      "epoch 248| loss: 0.39079 |  0:00:12s\n",
      "epoch 249| loss: 0.39128 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.00441 |  0:00:00s\n",
      "epoch 1  | loss: 1.73857 |  0:00:00s\n",
      "epoch 2  | loss: 1.29674 |  0:00:00s\n",
      "epoch 3  | loss: 1.10551 |  0:00:00s\n",
      "epoch 4  | loss: 0.97906 |  0:00:00s\n",
      "epoch 5  | loss: 0.934   |  0:00:00s\n",
      "epoch 6  | loss: 0.90618 |  0:00:00s\n",
      "epoch 7  | loss: 0.87293 |  0:00:00s\n",
      "epoch 8  | loss: 0.8498  |  0:00:00s\n",
      "epoch 9  | loss: 0.82459 |  0:00:00s\n",
      "epoch 10 | loss: 0.81747 |  0:00:00s\n",
      "epoch 11 | loss: 0.79496 |  0:00:00s\n",
      "epoch 12 | loss: 0.77068 |  0:00:00s\n",
      "epoch 13 | loss: 0.76784 |  0:00:00s\n",
      "epoch 14 | loss: 0.75909 |  0:00:00s\n",
      "epoch 15 | loss: 0.76035 |  0:00:00s\n",
      "epoch 16 | loss: 0.75024 |  0:00:00s\n",
      "epoch 17 | loss: 0.74332 |  0:00:00s\n",
      "epoch 18 | loss: 0.72432 |  0:00:00s\n",
      "epoch 19 | loss: 0.73275 |  0:00:00s\n",
      "epoch 20 | loss: 0.7133  |  0:00:01s\n",
      "epoch 21 | loss: 0.70219 |  0:00:01s\n",
      "epoch 22 | loss: 0.69581 |  0:00:01s\n",
      "epoch 23 | loss: 0.69299 |  0:00:01s\n",
      "epoch 24 | loss: 0.68733 |  0:00:01s\n",
      "epoch 25 | loss: 0.68326 |  0:00:01s\n",
      "epoch 26 | loss: 0.6846  |  0:00:01s\n",
      "epoch 27 | loss: 0.68165 |  0:00:01s\n",
      "epoch 28 | loss: 0.6756  |  0:00:01s\n",
      "epoch 29 | loss: 0.67769 |  0:00:01s\n",
      "epoch 30 | loss: 0.67058 |  0:00:01s\n",
      "epoch 31 | loss: 0.66362 |  0:00:01s\n",
      "epoch 32 | loss: 0.65947 |  0:00:01s\n",
      "epoch 33 | loss: 0.65798 |  0:00:01s\n",
      "epoch 34 | loss: 0.65276 |  0:00:01s\n",
      "epoch 35 | loss: 0.64431 |  0:00:01s\n",
      "epoch 36 | loss: 0.64687 |  0:00:01s\n",
      "epoch 37 | loss: 0.64286 |  0:00:01s\n",
      "epoch 38 | loss: 0.62993 |  0:00:01s\n",
      "epoch 39 | loss: 0.6225  |  0:00:01s\n",
      "epoch 40 | loss: 0.61952 |  0:00:01s\n",
      "epoch 41 | loss: 0.62519 |  0:00:02s\n",
      "epoch 42 | loss: 0.61991 |  0:00:02s\n",
      "epoch 43 | loss: 0.61214 |  0:00:02s\n",
      "epoch 44 | loss: 0.61027 |  0:00:02s\n",
      "epoch 45 | loss: 0.60174 |  0:00:02s\n",
      "epoch 46 | loss: 0.59376 |  0:00:02s\n",
      "epoch 47 | loss: 0.613   |  0:00:02s\n",
      "epoch 48 | loss: 0.60262 |  0:00:02s\n",
      "epoch 49 | loss: 0.6017  |  0:00:02s\n",
      "epoch 50 | loss: 0.59095 |  0:00:02s\n",
      "epoch 51 | loss: 0.59576 |  0:00:02s\n",
      "epoch 52 | loss: 0.5959  |  0:00:02s\n",
      "epoch 53 | loss: 0.58482 |  0:00:02s\n",
      "epoch 54 | loss: 0.57444 |  0:00:02s\n",
      "epoch 55 | loss: 0.56797 |  0:00:02s\n",
      "epoch 56 | loss: 0.56786 |  0:00:02s\n",
      "epoch 57 | loss: 0.56037 |  0:00:02s\n",
      "epoch 58 | loss: 0.56273 |  0:00:02s\n",
      "epoch 59 | loss: 0.56314 |  0:00:02s\n",
      "epoch 60 | loss: 0.55649 |  0:00:02s\n",
      "epoch 61 | loss: 0.54321 |  0:00:02s\n",
      "epoch 62 | loss: 0.54719 |  0:00:02s\n",
      "epoch 63 | loss: 0.54078 |  0:00:03s\n",
      "epoch 64 | loss: 0.54092 |  0:00:03s\n",
      "epoch 65 | loss: 0.53853 |  0:00:03s\n",
      "epoch 66 | loss: 0.5316  |  0:00:03s\n",
      "epoch 67 | loss: 0.53409 |  0:00:03s\n",
      "epoch 68 | loss: 0.52565 |  0:00:03s\n",
      "epoch 69 | loss: 0.5198  |  0:00:03s\n",
      "epoch 70 | loss: 0.52132 |  0:00:03s\n",
      "epoch 71 | loss: 0.52068 |  0:00:03s\n",
      "epoch 72 | loss: 0.52037 |  0:00:03s\n",
      "epoch 73 | loss: 0.50561 |  0:00:03s\n",
      "epoch 74 | loss: 0.50615 |  0:00:03s\n",
      "epoch 75 | loss: 0.51341 |  0:00:03s\n",
      "epoch 76 | loss: 0.50101 |  0:00:03s\n",
      "epoch 77 | loss: 0.50841 |  0:00:03s\n",
      "epoch 78 | loss: 0.50513 |  0:00:03s\n",
      "epoch 79 | loss: 0.50096 |  0:00:03s\n",
      "epoch 80 | loss: 0.50668 |  0:00:03s\n",
      "epoch 81 | loss: 0.52186 |  0:00:03s\n",
      "epoch 82 | loss: 0.52624 |  0:00:03s\n",
      "epoch 83 | loss: 0.52997 |  0:00:03s\n",
      "epoch 84 | loss: 0.5429  |  0:00:03s\n",
      "epoch 85 | loss: 0.53536 |  0:00:04s\n",
      "epoch 86 | loss: 0.52383 |  0:00:04s\n",
      "epoch 87 | loss: 0.52122 |  0:00:04s\n",
      "epoch 88 | loss: 0.52342 |  0:00:04s\n",
      "epoch 89 | loss: 0.51779 |  0:00:04s\n",
      "epoch 90 | loss: 0.51117 |  0:00:04s\n",
      "epoch 91 | loss: 0.50712 |  0:00:04s\n",
      "epoch 92 | loss: 0.49932 |  0:00:04s\n",
      "epoch 93 | loss: 0.49458 |  0:00:04s\n",
      "epoch 94 | loss: 0.50471 |  0:00:04s\n",
      "epoch 95 | loss: 0.49619 |  0:00:04s\n",
      "epoch 96 | loss: 0.4952  |  0:00:04s\n",
      "epoch 97 | loss: 0.48925 |  0:00:04s\n",
      "epoch 98 | loss: 0.48095 |  0:00:04s\n",
      "epoch 99 | loss: 0.47988 |  0:00:04s\n",
      "epoch 100| loss: 0.46925 |  0:00:04s\n",
      "epoch 101| loss: 0.48176 |  0:00:04s\n",
      "epoch 102| loss: 0.48241 |  0:00:04s\n",
      "epoch 103| loss: 0.47966 |  0:00:04s\n",
      "epoch 104| loss: 0.48081 |  0:00:04s\n",
      "epoch 105| loss: 0.47546 |  0:00:04s\n",
      "epoch 106| loss: 0.46685 |  0:00:05s\n",
      "epoch 107| loss: 0.46544 |  0:00:05s\n",
      "epoch 108| loss: 0.46712 |  0:00:05s\n",
      "epoch 109| loss: 0.45746 |  0:00:05s\n",
      "epoch 110| loss: 0.46673 |  0:00:05s\n",
      "epoch 111| loss: 0.45442 |  0:00:05s\n",
      "epoch 112| loss: 0.45456 |  0:00:05s\n",
      "epoch 113| loss: 0.45047 |  0:00:05s\n",
      "epoch 114| loss: 0.4477  |  0:00:05s\n",
      "epoch 115| loss: 0.45337 |  0:00:05s\n",
      "epoch 116| loss: 0.44967 |  0:00:05s\n",
      "epoch 117| loss: 0.44395 |  0:00:05s\n",
      "epoch 118| loss: 0.43885 |  0:00:05s\n",
      "epoch 119| loss: 0.43604 |  0:00:05s\n",
      "epoch 120| loss: 0.441   |  0:00:05s\n",
      "epoch 121| loss: 0.43686 |  0:00:05s\n",
      "epoch 122| loss: 0.42817 |  0:00:05s\n",
      "epoch 123| loss: 0.42837 |  0:00:05s\n",
      "epoch 124| loss: 0.42526 |  0:00:05s\n",
      "epoch 125| loss: 0.42349 |  0:00:05s\n",
      "epoch 126| loss: 0.42046 |  0:00:05s\n",
      "epoch 127| loss: 0.41709 |  0:00:06s\n",
      "epoch 128| loss: 0.41547 |  0:00:06s\n",
      "epoch 129| loss: 0.41806 |  0:00:06s\n",
      "epoch 130| loss: 0.41774 |  0:00:06s\n",
      "epoch 131| loss: 0.42014 |  0:00:06s\n",
      "epoch 132| loss: 0.42206 |  0:00:06s\n",
      "epoch 133| loss: 0.42074 |  0:00:06s\n",
      "epoch 134| loss: 0.42645 |  0:00:06s\n",
      "epoch 135| loss: 0.43195 |  0:00:06s\n",
      "epoch 136| loss: 0.41591 |  0:00:06s\n",
      "epoch 137| loss: 0.41362 |  0:00:06s\n",
      "epoch 138| loss: 0.41271 |  0:00:06s\n",
      "epoch 139| loss: 0.41251 |  0:00:06s\n",
      "epoch 140| loss: 0.41026 |  0:00:06s\n",
      "epoch 141| loss: 0.41224 |  0:00:06s\n",
      "epoch 142| loss: 0.41609 |  0:00:06s\n",
      "epoch 143| loss: 0.41685 |  0:00:06s\n",
      "epoch 144| loss: 0.40877 |  0:00:06s\n",
      "epoch 145| loss: 0.40633 |  0:00:06s\n",
      "epoch 146| loss: 0.41171 |  0:00:06s\n",
      "epoch 147| loss: 0.41103 |  0:00:06s\n",
      "epoch 148| loss: 0.41316 |  0:00:07s\n",
      "epoch 149| loss: 0.40254 |  0:00:07s\n",
      "epoch 150| loss: 0.40028 |  0:00:07s\n",
      "epoch 151| loss: 0.394   |  0:00:07s\n",
      "epoch 152| loss: 0.39816 |  0:00:07s\n",
      "epoch 153| loss: 0.39862 |  0:00:07s\n",
      "epoch 154| loss: 0.39208 |  0:00:07s\n",
      "epoch 155| loss: 0.40903 |  0:00:07s\n",
      "epoch 156| loss: 0.41303 |  0:00:07s\n",
      "epoch 157| loss: 0.42787 |  0:00:07s\n",
      "epoch 158| loss: 0.41675 |  0:00:07s\n",
      "epoch 159| loss: 0.42215 |  0:00:07s\n",
      "epoch 160| loss: 0.42684 |  0:00:07s\n",
      "epoch 161| loss: 0.41948 |  0:00:07s\n",
      "epoch 162| loss: 0.43918 |  0:00:07s\n",
      "epoch 163| loss: 0.44343 |  0:00:07s\n",
      "epoch 164| loss: 0.42997 |  0:00:07s\n",
      "epoch 165| loss: 0.43058 |  0:00:07s\n",
      "epoch 166| loss: 0.42793 |  0:00:07s\n",
      "epoch 167| loss: 0.42103 |  0:00:07s\n",
      "epoch 168| loss: 0.42665 |  0:00:08s\n",
      "epoch 169| loss: 0.42042 |  0:00:08s\n",
      "epoch 170| loss: 0.41643 |  0:00:08s\n",
      "epoch 171| loss: 0.41001 |  0:00:08s\n",
      "epoch 172| loss: 0.40562 |  0:00:08s\n",
      "epoch 173| loss: 0.39684 |  0:00:08s\n",
      "epoch 174| loss: 0.39922 |  0:00:08s\n",
      "epoch 175| loss: 0.39809 |  0:00:08s\n",
      "epoch 176| loss: 0.40608 |  0:00:08s\n",
      "epoch 177| loss: 0.40659 |  0:00:08s\n",
      "epoch 178| loss: 0.39762 |  0:00:08s\n",
      "epoch 179| loss: 0.39438 |  0:00:08s\n",
      "epoch 180| loss: 0.38865 |  0:00:08s\n",
      "epoch 181| loss: 0.38782 |  0:00:08s\n",
      "epoch 182| loss: 0.39466 |  0:00:08s\n",
      "epoch 183| loss: 0.38088 |  0:00:08s\n",
      "epoch 184| loss: 0.39659 |  0:00:08s\n",
      "epoch 185| loss: 0.39393 |  0:00:08s\n",
      "epoch 186| loss: 0.39385 |  0:00:08s\n",
      "epoch 187| loss: 0.39703 |  0:00:08s\n",
      "epoch 188| loss: 0.3889  |  0:00:08s\n",
      "epoch 189| loss: 0.38217 |  0:00:09s\n",
      "epoch 190| loss: 0.38105 |  0:00:09s\n",
      "epoch 191| loss: 0.38021 |  0:00:09s\n",
      "epoch 192| loss: 0.38708 |  0:00:09s\n",
      "epoch 193| loss: 0.38221 |  0:00:09s\n",
      "epoch 194| loss: 0.38494 |  0:00:09s\n",
      "epoch 195| loss: 0.3786  |  0:00:09s\n",
      "epoch 196| loss: 0.37267 |  0:00:09s\n",
      "epoch 197| loss: 0.37483 |  0:00:09s\n",
      "epoch 198| loss: 0.36789 |  0:00:09s\n",
      "epoch 199| loss: 0.37433 |  0:00:09s\n",
      "epoch 200| loss: 0.36834 |  0:00:09s\n",
      "epoch 201| loss: 0.36598 |  0:00:09s\n",
      "epoch 202| loss: 0.36226 |  0:00:09s\n",
      "epoch 203| loss: 0.36523 |  0:00:09s\n",
      "epoch 204| loss: 0.3623  |  0:00:09s\n",
      "epoch 205| loss: 0.35983 |  0:00:09s\n",
      "epoch 206| loss: 0.35274 |  0:00:09s\n",
      "epoch 207| loss: 0.36066 |  0:00:09s\n",
      "epoch 208| loss: 0.37105 |  0:00:09s\n",
      "epoch 209| loss: 0.37631 |  0:00:09s\n",
      "epoch 210| loss: 0.38041 |  0:00:10s\n",
      "epoch 211| loss: 0.38132 |  0:00:10s\n",
      "epoch 212| loss: 0.38359 |  0:00:10s\n",
      "epoch 213| loss: 0.38222 |  0:00:10s\n",
      "epoch 214| loss: 0.37967 |  0:00:10s\n",
      "epoch 215| loss: 0.37253 |  0:00:10s\n",
      "epoch 216| loss: 0.36411 |  0:00:10s\n",
      "epoch 217| loss: 0.36901 |  0:00:10s\n",
      "epoch 218| loss: 0.36119 |  0:00:10s\n",
      "epoch 219| loss: 0.35761 |  0:00:10s\n",
      "epoch 220| loss: 0.35985 |  0:00:10s\n",
      "epoch 221| loss: 0.36029 |  0:00:10s\n",
      "epoch 222| loss: 0.35774 |  0:00:10s\n",
      "epoch 223| loss: 0.35304 |  0:00:10s\n",
      "epoch 224| loss: 0.35453 |  0:00:10s\n",
      "epoch 225| loss: 0.35447 |  0:00:10s\n",
      "epoch 226| loss: 0.36049 |  0:00:10s\n",
      "epoch 227| loss: 0.35788 |  0:00:11s\n",
      "epoch 228| loss: 0.35481 |  0:00:11s\n",
      "epoch 229| loss: 0.34669 |  0:00:11s\n",
      "epoch 230| loss: 0.35302 |  0:00:11s\n",
      "epoch 231| loss: 0.34855 |  0:00:11s\n",
      "epoch 232| loss: 0.35546 |  0:00:11s\n",
      "epoch 233| loss: 0.35302 |  0:00:11s\n",
      "epoch 234| loss: 0.35684 |  0:00:11s\n",
      "epoch 235| loss: 0.35316 |  0:00:11s\n",
      "epoch 236| loss: 0.35088 |  0:00:11s\n",
      "epoch 237| loss: 0.34615 |  0:00:11s\n",
      "epoch 238| loss: 0.34431 |  0:00:11s\n",
      "epoch 239| loss: 0.35185 |  0:00:11s\n",
      "epoch 240| loss: 0.36018 |  0:00:11s\n",
      "epoch 241| loss: 0.35004 |  0:00:11s\n",
      "epoch 242| loss: 0.34261 |  0:00:11s\n",
      "epoch 243| loss: 0.33987 |  0:00:11s\n",
      "epoch 244| loss: 0.33888 |  0:00:11s\n",
      "epoch 245| loss: 0.33912 |  0:00:11s\n",
      "epoch 246| loss: 0.33835 |  0:00:11s\n",
      "epoch 247| loss: 0.33346 |  0:00:11s\n",
      "epoch 248| loss: 0.33521 |  0:00:12s\n",
      "epoch 249| loss: 0.33247 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.93313 |  0:00:00s\n",
      "epoch 1  | loss: 1.73025 |  0:00:00s\n",
      "epoch 2  | loss: 1.30564 |  0:00:00s\n",
      "epoch 3  | loss: 1.10883 |  0:00:00s\n",
      "epoch 4  | loss: 1.00435 |  0:00:00s\n",
      "epoch 5  | loss: 0.93529 |  0:00:00s\n",
      "epoch 6  | loss: 0.90391 |  0:00:00s\n",
      "epoch 7  | loss: 0.88444 |  0:00:00s\n",
      "epoch 8  | loss: 0.87013 |  0:00:00s\n",
      "epoch 9  | loss: 0.86802 |  0:00:00s\n",
      "epoch 10 | loss: 0.84707 |  0:00:00s\n",
      "epoch 11 | loss: 0.82472 |  0:00:00s\n",
      "epoch 12 | loss: 0.8053  |  0:00:00s\n",
      "epoch 13 | loss: 0.79055 |  0:00:00s\n",
      "epoch 14 | loss: 0.78182 |  0:00:00s\n",
      "epoch 15 | loss: 0.77154 |  0:00:00s\n",
      "epoch 16 | loss: 0.75149 |  0:00:00s\n",
      "epoch 17 | loss: 0.75048 |  0:00:01s\n",
      "epoch 18 | loss: 0.74404 |  0:00:01s\n",
      "epoch 19 | loss: 0.73941 |  0:00:01s\n",
      "epoch 20 | loss: 0.74141 |  0:00:01s\n",
      "epoch 21 | loss: 0.7335  |  0:00:01s\n",
      "epoch 22 | loss: 0.7222  |  0:00:01s\n",
      "epoch 23 | loss: 0.72452 |  0:00:01s\n",
      "epoch 24 | loss: 0.72233 |  0:00:01s\n",
      "epoch 25 | loss: 0.71719 |  0:00:01s\n",
      "epoch 26 | loss: 0.70187 |  0:00:01s\n",
      "epoch 27 | loss: 0.70917 |  0:00:01s\n",
      "epoch 28 | loss: 0.69971 |  0:00:01s\n",
      "epoch 29 | loss: 0.68609 |  0:00:01s\n",
      "epoch 30 | loss: 0.68568 |  0:00:01s\n",
      "epoch 31 | loss: 0.68072 |  0:00:01s\n",
      "epoch 32 | loss: 0.67314 |  0:00:01s\n",
      "epoch 33 | loss: 0.68009 |  0:00:01s\n",
      "epoch 34 | loss: 0.66649 |  0:00:01s\n",
      "epoch 35 | loss: 0.66803 |  0:00:01s\n",
      "epoch 36 | loss: 0.65968 |  0:00:02s\n",
      "epoch 37 | loss: 0.66275 |  0:00:02s\n",
      "epoch 38 | loss: 0.65624 |  0:00:02s\n",
      "epoch 39 | loss: 0.66009 |  0:00:02s\n",
      "epoch 40 | loss: 0.65369 |  0:00:02s\n",
      "epoch 41 | loss: 0.6523  |  0:00:02s\n",
      "epoch 42 | loss: 0.65106 |  0:00:02s\n",
      "epoch 43 | loss: 0.65006 |  0:00:02s\n",
      "epoch 44 | loss: 0.64558 |  0:00:02s\n",
      "epoch 45 | loss: 0.64755 |  0:00:02s\n",
      "epoch 46 | loss: 0.64978 |  0:00:02s\n",
      "epoch 47 | loss: 0.64048 |  0:00:02s\n",
      "epoch 48 | loss: 0.64044 |  0:00:02s\n",
      "epoch 49 | loss: 0.6419  |  0:00:02s\n",
      "epoch 50 | loss: 0.63464 |  0:00:02s\n",
      "epoch 51 | loss: 0.63751 |  0:00:02s\n",
      "epoch 52 | loss: 0.63367 |  0:00:02s\n",
      "epoch 53 | loss: 0.63627 |  0:00:03s\n",
      "epoch 54 | loss: 0.63056 |  0:00:03s\n",
      "epoch 55 | loss: 0.62438 |  0:00:03s\n",
      "epoch 56 | loss: 0.62789 |  0:00:03s\n",
      "epoch 57 | loss: 0.62644 |  0:00:03s\n",
      "epoch 58 | loss: 0.61689 |  0:00:03s\n",
      "epoch 59 | loss: 0.61933 |  0:00:03s\n",
      "epoch 60 | loss: 0.61488 |  0:00:03s\n",
      "epoch 61 | loss: 0.61283 |  0:00:03s\n",
      "epoch 62 | loss: 0.6097  |  0:00:03s\n",
      "epoch 63 | loss: 0.60817 |  0:00:03s\n",
      "epoch 64 | loss: 0.61465 |  0:00:03s\n",
      "epoch 65 | loss: 0.618   |  0:00:03s\n",
      "epoch 66 | loss: 0.62086 |  0:00:03s\n",
      "epoch 67 | loss: 0.61756 |  0:00:03s\n",
      "epoch 68 | loss: 0.61261 |  0:00:03s\n",
      "epoch 69 | loss: 0.60362 |  0:00:03s\n",
      "epoch 70 | loss: 0.60297 |  0:00:04s\n",
      "epoch 71 | loss: 0.60366 |  0:00:04s\n",
      "epoch 72 | loss: 0.59817 |  0:00:04s\n",
      "epoch 73 | loss: 0.59651 |  0:00:04s\n",
      "epoch 74 | loss: 0.59834 |  0:00:04s\n",
      "epoch 75 | loss: 0.59538 |  0:00:04s\n",
      "epoch 76 | loss: 0.60502 |  0:00:04s\n",
      "epoch 77 | loss: 0.60223 |  0:00:04s\n",
      "epoch 78 | loss: 0.59884 |  0:00:04s\n",
      "epoch 79 | loss: 0.58992 |  0:00:04s\n",
      "epoch 80 | loss: 0.58516 |  0:00:04s\n",
      "epoch 81 | loss: 0.5872  |  0:00:04s\n",
      "epoch 82 | loss: 0.58521 |  0:00:04s\n",
      "epoch 83 | loss: 0.58302 |  0:00:04s\n",
      "epoch 84 | loss: 0.57781 |  0:00:04s\n",
      "epoch 85 | loss: 0.56599 |  0:00:04s\n",
      "epoch 86 | loss: 0.56984 |  0:00:05s\n",
      "epoch 87 | loss: 0.56748 |  0:00:05s\n",
      "epoch 88 | loss: 0.58336 |  0:00:05s\n",
      "epoch 89 | loss: 0.57987 |  0:00:05s\n",
      "epoch 90 | loss: 0.57215 |  0:00:05s\n",
      "epoch 91 | loss: 0.57267 |  0:00:05s\n",
      "epoch 92 | loss: 0.56733 |  0:00:05s\n",
      "epoch 93 | loss: 0.56525 |  0:00:05s\n",
      "epoch 94 | loss: 0.56169 |  0:00:05s\n",
      "epoch 95 | loss: 0.55298 |  0:00:05s\n",
      "epoch 96 | loss: 0.55445 |  0:00:05s\n",
      "epoch 97 | loss: 0.56879 |  0:00:05s\n",
      "epoch 98 | loss: 0.56568 |  0:00:05s\n",
      "epoch 99 | loss: 0.57146 |  0:00:05s\n",
      "epoch 100| loss: 0.56459 |  0:00:05s\n",
      "epoch 101| loss: 0.5556  |  0:00:05s\n",
      "epoch 102| loss: 0.55725 |  0:00:05s\n",
      "epoch 103| loss: 0.55405 |  0:00:05s\n",
      "epoch 104| loss: 0.55233 |  0:00:06s\n",
      "epoch 105| loss: 0.54716 |  0:00:06s\n",
      "epoch 106| loss: 0.5431  |  0:00:06s\n",
      "epoch 107| loss: 0.53645 |  0:00:06s\n",
      "epoch 108| loss: 0.53044 |  0:00:06s\n",
      "epoch 109| loss: 0.53571 |  0:00:06s\n",
      "epoch 110| loss: 0.54398 |  0:00:06s\n",
      "epoch 111| loss: 0.54509 |  0:00:06s\n",
      "epoch 112| loss: 0.54687 |  0:00:06s\n",
      "epoch 113| loss: 0.53517 |  0:00:06s\n",
      "epoch 114| loss: 0.53013 |  0:00:06s\n",
      "epoch 115| loss: 0.54095 |  0:00:06s\n",
      "epoch 116| loss: 0.53023 |  0:00:06s\n",
      "epoch 117| loss: 0.52319 |  0:00:06s\n",
      "epoch 118| loss: 0.52368 |  0:00:06s\n",
      "epoch 119| loss: 0.51547 |  0:00:06s\n",
      "epoch 120| loss: 0.52248 |  0:00:06s\n",
      "epoch 121| loss: 0.51682 |  0:00:06s\n",
      "epoch 122| loss: 0.51504 |  0:00:07s\n",
      "epoch 123| loss: 0.51334 |  0:00:07s\n",
      "epoch 124| loss: 0.50789 |  0:00:07s\n",
      "epoch 125| loss: 0.50284 |  0:00:07s\n",
      "epoch 126| loss: 0.49784 |  0:00:07s\n",
      "epoch 127| loss: 0.4936  |  0:00:07s\n",
      "epoch 128| loss: 0.4994  |  0:00:07s\n",
      "epoch 129| loss: 0.50809 |  0:00:07s\n",
      "epoch 130| loss: 0.50389 |  0:00:07s\n",
      "epoch 131| loss: 0.50866 |  0:00:07s\n",
      "epoch 132| loss: 0.49458 |  0:00:07s\n",
      "epoch 133| loss: 0.49345 |  0:00:07s\n",
      "epoch 134| loss: 0.50562 |  0:00:07s\n",
      "epoch 135| loss: 0.50736 |  0:00:07s\n",
      "epoch 136| loss: 0.51673 |  0:00:07s\n",
      "epoch 137| loss: 0.50475 |  0:00:07s\n",
      "epoch 138| loss: 0.49446 |  0:00:07s\n",
      "epoch 139| loss: 0.50118 |  0:00:07s\n",
      "epoch 140| loss: 0.49177 |  0:00:08s\n",
      "epoch 141| loss: 0.48956 |  0:00:08s\n",
      "epoch 142| loss: 0.49155 |  0:00:08s\n",
      "epoch 143| loss: 0.48549 |  0:00:08s\n",
      "epoch 144| loss: 0.4761  |  0:00:08s\n",
      "epoch 145| loss: 0.48047 |  0:00:08s\n",
      "epoch 146| loss: 0.47536 |  0:00:08s\n",
      "epoch 147| loss: 0.47433 |  0:00:08s\n",
      "epoch 148| loss: 0.47671 |  0:00:08s\n",
      "epoch 149| loss: 0.46951 |  0:00:08s\n",
      "epoch 150| loss: 0.46838 |  0:00:08s\n",
      "epoch 151| loss: 0.46977 |  0:00:08s\n",
      "epoch 152| loss: 0.46538 |  0:00:08s\n",
      "epoch 153| loss: 0.45997 |  0:00:08s\n",
      "epoch 154| loss: 0.4618  |  0:00:08s\n",
      "epoch 155| loss: 0.46465 |  0:00:08s\n",
      "epoch 156| loss: 0.46574 |  0:00:08s\n",
      "epoch 157| loss: 0.46966 |  0:00:08s\n",
      "epoch 158| loss: 0.46095 |  0:00:09s\n",
      "epoch 159| loss: 0.46278 |  0:00:09s\n",
      "epoch 160| loss: 0.46174 |  0:00:09s\n",
      "epoch 161| loss: 0.46097 |  0:00:09s\n",
      "epoch 162| loss: 0.45582 |  0:00:09s\n",
      "epoch 163| loss: 0.45888 |  0:00:09s\n",
      "epoch 164| loss: 0.45026 |  0:00:09s\n",
      "epoch 165| loss: 0.45544 |  0:00:09s\n",
      "epoch 166| loss: 0.45651 |  0:00:09s\n",
      "epoch 167| loss: 0.44799 |  0:00:09s\n",
      "epoch 168| loss: 0.45687 |  0:00:09s\n",
      "epoch 169| loss: 0.44726 |  0:00:09s\n",
      "epoch 170| loss: 0.4527  |  0:00:09s\n",
      "epoch 171| loss: 0.44766 |  0:00:09s\n",
      "epoch 172| loss: 0.4386  |  0:00:09s\n",
      "epoch 173| loss: 0.43675 |  0:00:09s\n",
      "epoch 174| loss: 0.44643 |  0:00:09s\n",
      "epoch 175| loss: 0.4319  |  0:00:09s\n",
      "epoch 176| loss: 0.43532 |  0:00:09s\n",
      "epoch 177| loss: 0.43021 |  0:00:10s\n",
      "epoch 178| loss: 0.43264 |  0:00:10s\n",
      "epoch 179| loss: 0.43342 |  0:00:10s\n",
      "epoch 180| loss: 0.44977 |  0:00:10s\n",
      "epoch 181| loss: 0.44835 |  0:00:10s\n",
      "epoch 182| loss: 0.45364 |  0:00:10s\n",
      "epoch 183| loss: 0.44802 |  0:00:10s\n",
      "epoch 184| loss: 0.44587 |  0:00:10s\n",
      "epoch 185| loss: 0.44568 |  0:00:10s\n",
      "epoch 186| loss: 0.4393  |  0:00:10s\n",
      "epoch 187| loss: 0.44526 |  0:00:10s\n",
      "epoch 188| loss: 0.4391  |  0:00:10s\n",
      "epoch 189| loss: 0.42986 |  0:00:10s\n",
      "epoch 190| loss: 0.43248 |  0:00:10s\n",
      "epoch 191| loss: 0.43107 |  0:00:10s\n",
      "epoch 192| loss: 0.43052 |  0:00:10s\n",
      "epoch 193| loss: 0.42528 |  0:00:10s\n",
      "epoch 194| loss: 0.42641 |  0:00:10s\n",
      "epoch 195| loss: 0.41927 |  0:00:11s\n",
      "epoch 196| loss: 0.4224  |  0:00:11s\n",
      "epoch 197| loss: 0.42128 |  0:00:11s\n",
      "epoch 198| loss: 0.42113 |  0:00:11s\n",
      "epoch 199| loss: 0.41372 |  0:00:11s\n",
      "epoch 200| loss: 0.41352 |  0:00:11s\n",
      "epoch 201| loss: 0.40891 |  0:00:11s\n",
      "epoch 202| loss: 0.40773 |  0:00:11s\n",
      "epoch 203| loss: 0.42211 |  0:00:11s\n",
      "epoch 204| loss: 0.44323 |  0:00:11s\n",
      "epoch 205| loss: 0.45583 |  0:00:11s\n",
      "epoch 206| loss: 0.47232 |  0:00:11s\n",
      "epoch 207| loss: 0.49559 |  0:00:11s\n",
      "epoch 208| loss: 0.48213 |  0:00:11s\n",
      "epoch 209| loss: 0.45799 |  0:00:11s\n",
      "epoch 210| loss: 0.4612  |  0:00:11s\n",
      "epoch 211| loss: 0.46083 |  0:00:11s\n",
      "epoch 212| loss: 0.46127 |  0:00:11s\n",
      "epoch 213| loss: 0.44941 |  0:00:12s\n",
      "epoch 214| loss: 0.45581 |  0:00:12s\n",
      "epoch 215| loss: 0.45765 |  0:00:12s\n",
      "epoch 216| loss: 0.44963 |  0:00:12s\n",
      "epoch 217| loss: 0.44431 |  0:00:12s\n",
      "epoch 218| loss: 0.45259 |  0:00:12s\n",
      "epoch 219| loss: 0.45098 |  0:00:12s\n",
      "epoch 220| loss: 0.44208 |  0:00:12s\n",
      "epoch 221| loss: 0.44538 |  0:00:12s\n",
      "epoch 222| loss: 0.44346 |  0:00:12s\n",
      "epoch 223| loss: 0.45077 |  0:00:12s\n",
      "epoch 224| loss: 0.43513 |  0:00:12s\n",
      "epoch 225| loss: 0.4311  |  0:00:12s\n",
      "epoch 226| loss: 0.42962 |  0:00:12s\n",
      "epoch 227| loss: 0.43934 |  0:00:12s\n",
      "epoch 228| loss: 0.4355  |  0:00:12s\n",
      "epoch 229| loss: 0.44161 |  0:00:12s\n",
      "epoch 230| loss: 0.44682 |  0:00:12s\n",
      "epoch 231| loss: 0.43689 |  0:00:13s\n",
      "epoch 232| loss: 0.4358  |  0:00:13s\n",
      "epoch 233| loss: 0.43744 |  0:00:13s\n",
      "epoch 234| loss: 0.43155 |  0:00:13s\n",
      "epoch 235| loss: 0.44145 |  0:00:13s\n",
      "epoch 236| loss: 0.43157 |  0:00:13s\n",
      "epoch 237| loss: 0.44041 |  0:00:13s\n",
      "epoch 238| loss: 0.44565 |  0:00:13s\n",
      "epoch 239| loss: 0.44101 |  0:00:13s\n",
      "epoch 240| loss: 0.43969 |  0:00:13s\n",
      "epoch 241| loss: 0.43663 |  0:00:13s\n",
      "epoch 242| loss: 0.43252 |  0:00:13s\n",
      "epoch 243| loss: 0.42538 |  0:00:13s\n",
      "epoch 244| loss: 0.42561 |  0:00:13s\n",
      "epoch 245| loss: 0.42596 |  0:00:13s\n",
      "epoch 246| loss: 0.42164 |  0:00:13s\n",
      "epoch 247| loss: 0.41842 |  0:00:13s\n",
      "epoch 248| loss: 0.4171  |  0:00:13s\n",
      "epoch 249| loss: 0.4131  |  0:00:14s\n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.97306 |  0:00:00s\n",
      "epoch 1  | loss: 1.78775 |  0:00:00s\n",
      "epoch 2  | loss: 1.29574 |  0:00:00s\n",
      "epoch 3  | loss: 1.13483 |  0:00:00s\n",
      "epoch 4  | loss: 1.04295 |  0:00:00s\n",
      "epoch 5  | loss: 0.97252 |  0:00:00s\n",
      "epoch 6  | loss: 0.92803 |  0:00:00s\n",
      "epoch 7  | loss: 0.89192 |  0:00:00s\n",
      "epoch 8  | loss: 0.84472 |  0:00:00s\n",
      "epoch 9  | loss: 0.82537 |  0:00:00s\n",
      "epoch 10 | loss: 0.80103 |  0:00:00s\n",
      "epoch 11 | loss: 0.78138 |  0:00:00s\n",
      "epoch 12 | loss: 0.77522 |  0:00:00s\n",
      "epoch 13 | loss: 0.76817 |  0:00:00s\n",
      "epoch 14 | loss: 0.75095 |  0:00:00s\n",
      "epoch 15 | loss: 0.74171 |  0:00:00s\n",
      "epoch 16 | loss: 0.74537 |  0:00:00s\n",
      "epoch 17 | loss: 0.73865 |  0:00:00s\n",
      "epoch 18 | loss: 0.71801 |  0:00:00s\n",
      "epoch 19 | loss: 0.72    |  0:00:01s\n",
      "epoch 20 | loss: 0.72228 |  0:00:01s\n",
      "epoch 21 | loss: 0.72211 |  0:00:01s\n",
      "epoch 22 | loss: 0.72053 |  0:00:01s\n",
      "epoch 23 | loss: 0.70824 |  0:00:01s\n",
      "epoch 24 | loss: 0.70699 |  0:00:01s\n",
      "epoch 25 | loss: 0.69838 |  0:00:01s\n",
      "epoch 26 | loss: 0.70204 |  0:00:01s\n",
      "epoch 27 | loss: 0.68986 |  0:00:01s\n",
      "epoch 28 | loss: 0.68371 |  0:00:01s\n",
      "epoch 29 | loss: 0.69721 |  0:00:01s\n",
      "epoch 30 | loss: 0.69049 |  0:00:01s\n",
      "epoch 31 | loss: 0.68824 |  0:00:01s\n",
      "epoch 32 | loss: 0.68354 |  0:00:01s\n",
      "epoch 33 | loss: 0.6838  |  0:00:01s\n",
      "epoch 34 | loss: 0.67083 |  0:00:01s\n",
      "epoch 35 | loss: 0.67936 |  0:00:01s\n",
      "epoch 36 | loss: 0.67543 |  0:00:01s\n",
      "epoch 37 | loss: 0.66923 |  0:00:01s\n",
      "epoch 38 | loss: 0.66066 |  0:00:02s\n",
      "epoch 39 | loss: 0.65043 |  0:00:02s\n",
      "epoch 40 | loss: 0.65479 |  0:00:02s\n",
      "epoch 41 | loss: 0.64529 |  0:00:02s\n",
      "epoch 42 | loss: 0.64619 |  0:00:02s\n",
      "epoch 43 | loss: 0.65253 |  0:00:02s\n",
      "epoch 44 | loss: 0.64775 |  0:00:02s\n",
      "epoch 45 | loss: 0.65081 |  0:00:02s\n",
      "epoch 46 | loss: 0.64654 |  0:00:02s\n",
      "epoch 47 | loss: 0.64824 |  0:00:02s\n",
      "epoch 48 | loss: 0.64533 |  0:00:02s\n",
      "epoch 49 | loss: 0.63607 |  0:00:02s\n",
      "epoch 50 | loss: 0.63242 |  0:00:02s\n",
      "epoch 51 | loss: 0.63312 |  0:00:02s\n",
      "epoch 52 | loss: 0.63913 |  0:00:02s\n",
      "epoch 53 | loss: 0.62843 |  0:00:02s\n",
      "epoch 54 | loss: 0.63118 |  0:00:02s\n",
      "epoch 55 | loss: 0.62668 |  0:00:02s\n",
      "epoch 56 | loss: 0.62653 |  0:00:02s\n",
      "epoch 57 | loss: 0.62764 |  0:00:03s\n",
      "epoch 58 | loss: 0.62344 |  0:00:03s\n",
      "epoch 59 | loss: 0.62045 |  0:00:03s\n",
      "epoch 60 | loss: 0.62406 |  0:00:03s\n",
      "epoch 61 | loss: 0.6207  |  0:00:03s\n",
      "epoch 62 | loss: 0.61406 |  0:00:03s\n",
      "epoch 63 | loss: 0.61653 |  0:00:03s\n",
      "epoch 64 | loss: 0.61558 |  0:00:03s\n",
      "epoch 65 | loss: 0.61981 |  0:00:03s\n",
      "epoch 66 | loss: 0.61637 |  0:00:03s\n",
      "epoch 67 | loss: 0.6086  |  0:00:03s\n",
      "epoch 68 | loss: 0.60742 |  0:00:03s\n",
      "epoch 69 | loss: 0.59784 |  0:00:03s\n",
      "epoch 70 | loss: 0.59294 |  0:00:03s\n",
      "epoch 71 | loss: 0.59373 |  0:00:03s\n",
      "epoch 72 | loss: 0.5924  |  0:00:03s\n",
      "epoch 73 | loss: 0.59226 |  0:00:03s\n",
      "epoch 74 | loss: 0.5914  |  0:00:03s\n",
      "epoch 75 | loss: 0.58251 |  0:00:03s\n",
      "epoch 76 | loss: 0.58541 |  0:00:03s\n",
      "epoch 77 | loss: 0.59137 |  0:00:04s\n",
      "epoch 78 | loss: 0.58116 |  0:00:04s\n",
      "epoch 79 | loss: 0.57934 |  0:00:04s\n",
      "epoch 80 | loss: 0.57392 |  0:00:04s\n",
      "epoch 81 | loss: 0.57644 |  0:00:04s\n",
      "epoch 82 | loss: 0.5729  |  0:00:04s\n",
      "epoch 83 | loss: 0.57349 |  0:00:04s\n",
      "epoch 84 | loss: 0.57135 |  0:00:04s\n",
      "epoch 85 | loss: 0.56749 |  0:00:04s\n",
      "epoch 86 | loss: 0.57126 |  0:00:04s\n",
      "epoch 87 | loss: 0.56338 |  0:00:04s\n",
      "epoch 88 | loss: 0.57256 |  0:00:04s\n",
      "epoch 89 | loss: 0.58094 |  0:00:04s\n",
      "epoch 90 | loss: 0.58422 |  0:00:04s\n",
      "epoch 91 | loss: 0.58672 |  0:00:04s\n",
      "epoch 92 | loss: 0.57345 |  0:00:04s\n",
      "epoch 93 | loss: 0.57742 |  0:00:04s\n",
      "epoch 94 | loss: 0.57356 |  0:00:04s\n",
      "epoch 95 | loss: 0.57129 |  0:00:04s\n",
      "epoch 96 | loss: 0.56433 |  0:00:04s\n",
      "epoch 97 | loss: 0.57358 |  0:00:05s\n",
      "epoch 98 | loss: 0.55842 |  0:00:05s\n",
      "epoch 99 | loss: 0.55583 |  0:00:05s\n",
      "epoch 100| loss: 0.55024 |  0:00:05s\n",
      "epoch 101| loss: 0.55481 |  0:00:05s\n",
      "epoch 102| loss: 0.54728 |  0:00:05s\n",
      "epoch 103| loss: 0.54766 |  0:00:05s\n",
      "epoch 104| loss: 0.53988 |  0:00:05s\n",
      "epoch 105| loss: 0.54005 |  0:00:05s\n",
      "epoch 106| loss: 0.53478 |  0:00:05s\n",
      "epoch 107| loss: 0.53564 |  0:00:05s\n",
      "epoch 108| loss: 0.53087 |  0:00:05s\n",
      "epoch 109| loss: 0.52807 |  0:00:05s\n",
      "epoch 110| loss: 0.52939 |  0:00:05s\n",
      "epoch 111| loss: 0.52917 |  0:00:05s\n",
      "epoch 112| loss: 0.53191 |  0:00:05s\n",
      "epoch 113| loss: 0.52046 |  0:00:05s\n",
      "epoch 114| loss: 0.51876 |  0:00:05s\n",
      "epoch 115| loss: 0.5325  |  0:00:05s\n",
      "epoch 116| loss: 0.51402 |  0:00:05s\n",
      "epoch 117| loss: 0.51792 |  0:00:05s\n",
      "epoch 118| loss: 0.50965 |  0:00:05s\n",
      "epoch 119| loss: 0.51177 |  0:00:06s\n",
      "epoch 120| loss: 0.50399 |  0:00:06s\n",
      "epoch 121| loss: 0.50223 |  0:00:06s\n",
      "epoch 122| loss: 0.49922 |  0:00:06s\n",
      "epoch 123| loss: 0.50467 |  0:00:06s\n",
      "epoch 124| loss: 0.49103 |  0:00:06s\n",
      "epoch 125| loss: 0.50157 |  0:00:06s\n",
      "epoch 126| loss: 0.49521 |  0:00:06s\n",
      "epoch 127| loss: 0.50822 |  0:00:06s\n",
      "epoch 128| loss: 0.50257 |  0:00:06s\n",
      "epoch 129| loss: 0.51461 |  0:00:06s\n",
      "epoch 130| loss: 0.50228 |  0:00:06s\n",
      "epoch 131| loss: 0.49454 |  0:00:06s\n",
      "epoch 132| loss: 0.4902  |  0:00:06s\n",
      "epoch 133| loss: 0.49153 |  0:00:06s\n",
      "epoch 134| loss: 0.49624 |  0:00:06s\n",
      "epoch 135| loss: 0.49005 |  0:00:06s\n",
      "epoch 136| loss: 0.48858 |  0:00:06s\n",
      "epoch 137| loss: 0.49206 |  0:00:06s\n",
      "epoch 138| loss: 0.47849 |  0:00:06s\n",
      "epoch 139| loss: 0.48936 |  0:00:06s\n",
      "epoch 140| loss: 0.47687 |  0:00:07s\n",
      "epoch 141| loss: 0.47676 |  0:00:07s\n",
      "epoch 142| loss: 0.48438 |  0:00:07s\n",
      "epoch 143| loss: 0.48219 |  0:00:07s\n",
      "epoch 144| loss: 0.4773  |  0:00:07s\n",
      "epoch 145| loss: 0.47075 |  0:00:07s\n",
      "epoch 146| loss: 0.47626 |  0:00:07s\n",
      "epoch 147| loss: 0.47101 |  0:00:07s\n",
      "epoch 148| loss: 0.46923 |  0:00:07s\n",
      "epoch 149| loss: 0.45926 |  0:00:07s\n",
      "epoch 150| loss: 0.46302 |  0:00:07s\n",
      "epoch 151| loss: 0.46644 |  0:00:07s\n",
      "epoch 152| loss: 0.46879 |  0:00:07s\n",
      "epoch 153| loss: 0.46476 |  0:00:07s\n",
      "epoch 154| loss: 0.46335 |  0:00:07s\n",
      "epoch 155| loss: 0.45784 |  0:00:07s\n",
      "epoch 156| loss: 0.46399 |  0:00:07s\n",
      "epoch 157| loss: 0.46148 |  0:00:07s\n",
      "epoch 158| loss: 0.45695 |  0:00:07s\n",
      "epoch 159| loss: 0.4533  |  0:00:07s\n",
      "epoch 160| loss: 0.46037 |  0:00:07s\n",
      "epoch 161| loss: 0.45501 |  0:00:08s\n",
      "epoch 162| loss: 0.46866 |  0:00:08s\n",
      "epoch 163| loss: 0.46413 |  0:00:08s\n",
      "epoch 164| loss: 0.4544  |  0:00:08s\n",
      "epoch 165| loss: 0.44949 |  0:00:08s\n",
      "epoch 166| loss: 0.45707 |  0:00:08s\n",
      "epoch 167| loss: 0.45402 |  0:00:08s\n",
      "epoch 168| loss: 0.4641  |  0:00:08s\n",
      "epoch 169| loss: 0.44794 |  0:00:08s\n",
      "epoch 170| loss: 0.45236 |  0:00:08s\n",
      "epoch 171| loss: 0.44264 |  0:00:08s\n",
      "epoch 172| loss: 0.44171 |  0:00:08s\n",
      "epoch 173| loss: 0.4341  |  0:00:08s\n",
      "epoch 174| loss: 0.43938 |  0:00:08s\n",
      "epoch 175| loss: 0.43492 |  0:00:08s\n",
      "epoch 176| loss: 0.44465 |  0:00:08s\n",
      "epoch 177| loss: 0.44238 |  0:00:08s\n",
      "epoch 178| loss: 0.44768 |  0:00:08s\n",
      "epoch 179| loss: 0.43998 |  0:00:08s\n",
      "epoch 180| loss: 0.44689 |  0:00:08s\n",
      "epoch 181| loss: 0.44849 |  0:00:09s\n",
      "epoch 182| loss: 0.44206 |  0:00:09s\n",
      "epoch 183| loss: 0.43872 |  0:00:09s\n",
      "epoch 184| loss: 0.45119 |  0:00:09s\n",
      "epoch 185| loss: 0.44007 |  0:00:09s\n",
      "epoch 186| loss: 0.43411 |  0:00:09s\n",
      "epoch 187| loss: 0.42843 |  0:00:09s\n",
      "epoch 188| loss: 0.441   |  0:00:09s\n",
      "epoch 189| loss: 0.44825 |  0:00:09s\n",
      "epoch 190| loss: 0.4544  |  0:00:09s\n",
      "epoch 191| loss: 0.45887 |  0:00:09s\n",
      "epoch 192| loss: 0.46317 |  0:00:09s\n",
      "epoch 193| loss: 0.45421 |  0:00:09s\n",
      "epoch 194| loss: 0.46317 |  0:00:09s\n",
      "epoch 195| loss: 0.45826 |  0:00:09s\n",
      "epoch 196| loss: 0.45805 |  0:00:09s\n",
      "epoch 197| loss: 0.45605 |  0:00:09s\n",
      "epoch 198| loss: 0.46017 |  0:00:09s\n",
      "epoch 199| loss: 0.45079 |  0:00:09s\n",
      "epoch 200| loss: 0.44523 |  0:00:09s\n",
      "epoch 201| loss: 0.43543 |  0:00:10s\n",
      "epoch 202| loss: 0.43978 |  0:00:10s\n",
      "epoch 203| loss: 0.44135 |  0:00:10s\n",
      "epoch 204| loss: 0.43586 |  0:00:10s\n",
      "epoch 205| loss: 0.42766 |  0:00:10s\n",
      "epoch 206| loss: 0.42148 |  0:00:10s\n",
      "epoch 207| loss: 0.42362 |  0:00:10s\n",
      "epoch 208| loss: 0.431   |  0:00:10s\n",
      "epoch 209| loss: 0.41935 |  0:00:10s\n",
      "epoch 210| loss: 0.41323 |  0:00:10s\n",
      "epoch 211| loss: 0.41603 |  0:00:10s\n",
      "epoch 212| loss: 0.4155  |  0:00:10s\n",
      "epoch 213| loss: 0.40701 |  0:00:10s\n",
      "epoch 214| loss: 0.41453 |  0:00:10s\n",
      "epoch 215| loss: 0.41343 |  0:00:10s\n",
      "epoch 216| loss: 0.41174 |  0:00:10s\n",
      "epoch 217| loss: 0.42036 |  0:00:10s\n",
      "epoch 218| loss: 0.41342 |  0:00:10s\n",
      "epoch 219| loss: 0.40766 |  0:00:11s\n",
      "epoch 220| loss: 0.40979 |  0:00:11s\n",
      "epoch 221| loss: 0.40911 |  0:00:11s\n",
      "epoch 222| loss: 0.40459 |  0:00:11s\n",
      "epoch 223| loss: 0.4082  |  0:00:11s\n",
      "epoch 224| loss: 0.40311 |  0:00:11s\n",
      "epoch 225| loss: 0.39838 |  0:00:11s\n",
      "epoch 226| loss: 0.39575 |  0:00:11s\n",
      "epoch 227| loss: 0.39351 |  0:00:11s\n",
      "epoch 228| loss: 0.39385 |  0:00:11s\n",
      "epoch 229| loss: 0.3925  |  0:00:11s\n",
      "epoch 230| loss: 0.396   |  0:00:11s\n",
      "epoch 231| loss: 0.39195 |  0:00:11s\n",
      "epoch 232| loss: 0.39223 |  0:00:11s\n",
      "epoch 233| loss: 0.38737 |  0:00:11s\n",
      "epoch 234| loss: 0.38988 |  0:00:11s\n",
      "epoch 235| loss: 0.3875  |  0:00:11s\n",
      "epoch 236| loss: 0.38367 |  0:00:11s\n",
      "epoch 237| loss: 0.38884 |  0:00:11s\n",
      "epoch 238| loss: 0.38689 |  0:00:11s\n",
      "epoch 239| loss: 0.39362 |  0:00:12s\n",
      "epoch 240| loss: 0.39101 |  0:00:12s\n",
      "epoch 241| loss: 0.39318 |  0:00:12s\n",
      "epoch 242| loss: 0.38597 |  0:00:12s\n",
      "epoch 243| loss: 0.37845 |  0:00:12s\n",
      "epoch 244| loss: 0.38356 |  0:00:12s\n",
      "epoch 245| loss: 0.38508 |  0:00:12s\n",
      "epoch 246| loss: 0.38261 |  0:00:12s\n",
      "epoch 247| loss: 0.37989 |  0:00:12s\n",
      "epoch 248| loss: 0.38998 |  0:00:12s\n",
      "epoch 249| loss: 0.39138 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.97253 |  0:00:00s\n",
      "epoch 1  | loss: 1.73854 |  0:00:00s\n",
      "epoch 2  | loss: 1.2771  |  0:00:00s\n",
      "epoch 3  | loss: 1.08903 |  0:00:00s\n",
      "epoch 4  | loss: 0.99586 |  0:00:00s\n",
      "epoch 5  | loss: 0.95128 |  0:00:00s\n",
      "epoch 6  | loss: 0.91831 |  0:00:00s\n",
      "epoch 7  | loss: 0.87072 |  0:00:00s\n",
      "epoch 8  | loss: 0.83955 |  0:00:00s\n",
      "epoch 9  | loss: 0.82762 |  0:00:00s\n",
      "epoch 10 | loss: 0.81279 |  0:00:00s\n",
      "epoch 11 | loss: 0.78692 |  0:00:00s\n",
      "epoch 12 | loss: 0.76908 |  0:00:00s\n",
      "epoch 13 | loss: 0.76305 |  0:00:00s\n",
      "epoch 14 | loss: 0.76218 |  0:00:00s\n",
      "epoch 15 | loss: 0.73615 |  0:00:00s\n",
      "epoch 16 | loss: 0.72206 |  0:00:00s\n",
      "epoch 17 | loss: 0.71361 |  0:00:00s\n",
      "epoch 18 | loss: 0.71444 |  0:00:01s\n",
      "epoch 19 | loss: 0.70866 |  0:00:01s\n",
      "epoch 20 | loss: 0.69444 |  0:00:01s\n",
      "epoch 21 | loss: 0.6906  |  0:00:01s\n",
      "epoch 22 | loss: 0.68382 |  0:00:01s\n",
      "epoch 23 | loss: 0.67455 |  0:00:01s\n",
      "epoch 24 | loss: 0.67874 |  0:00:01s\n",
      "epoch 25 | loss: 0.67505 |  0:00:01s\n",
      "epoch 26 | loss: 0.66841 |  0:00:01s\n",
      "epoch 27 | loss: 0.66412 |  0:00:01s\n",
      "epoch 28 | loss: 0.66657 |  0:00:01s\n",
      "epoch 29 | loss: 0.66735 |  0:00:01s\n",
      "epoch 30 | loss: 0.65844 |  0:00:01s\n",
      "epoch 31 | loss: 0.65317 |  0:00:01s\n",
      "epoch 32 | loss: 0.65318 |  0:00:01s\n",
      "epoch 33 | loss: 0.65081 |  0:00:01s\n",
      "epoch 34 | loss: 0.64906 |  0:00:01s\n",
      "epoch 35 | loss: 0.63944 |  0:00:01s\n",
      "epoch 36 | loss: 0.64026 |  0:00:01s\n",
      "epoch 37 | loss: 0.64675 |  0:00:01s\n",
      "epoch 38 | loss: 0.63632 |  0:00:02s\n",
      "epoch 39 | loss: 0.63507 |  0:00:02s\n",
      "epoch 40 | loss: 0.62985 |  0:00:02s\n",
      "epoch 41 | loss: 0.62109 |  0:00:02s\n",
      "epoch 42 | loss: 0.6185  |  0:00:02s\n",
      "epoch 43 | loss: 0.62878 |  0:00:02s\n",
      "epoch 44 | loss: 0.61766 |  0:00:02s\n",
      "epoch 45 | loss: 0.61297 |  0:00:02s\n",
      "epoch 46 | loss: 0.61578 |  0:00:02s\n",
      "epoch 47 | loss: 0.62268 |  0:00:02s\n",
      "epoch 48 | loss: 0.61198 |  0:00:02s\n",
      "epoch 49 | loss: 0.61413 |  0:00:02s\n",
      "epoch 50 | loss: 0.61669 |  0:00:02s\n",
      "epoch 51 | loss: 0.61577 |  0:00:02s\n",
      "epoch 52 | loss: 0.61787 |  0:00:02s\n",
      "epoch 53 | loss: 0.61787 |  0:00:02s\n",
      "epoch 54 | loss: 0.61524 |  0:00:02s\n",
      "epoch 55 | loss: 0.60481 |  0:00:02s\n",
      "epoch 56 | loss: 0.60739 |  0:00:02s\n",
      "epoch 57 | loss: 0.60879 |  0:00:02s\n",
      "epoch 58 | loss: 0.60894 |  0:00:03s\n",
      "epoch 59 | loss: 0.61141 |  0:00:03s\n",
      "epoch 60 | loss: 0.60295 |  0:00:03s\n",
      "epoch 61 | loss: 0.60336 |  0:00:03s\n",
      "epoch 62 | loss: 0.59501 |  0:00:03s\n",
      "epoch 63 | loss: 0.59699 |  0:00:03s\n",
      "epoch 64 | loss: 0.59396 |  0:00:03s\n",
      "epoch 65 | loss: 0.59395 |  0:00:03s\n",
      "epoch 66 | loss: 0.59558 |  0:00:03s\n",
      "epoch 67 | loss: 0.60211 |  0:00:03s\n",
      "epoch 68 | loss: 0.58582 |  0:00:03s\n",
      "epoch 69 | loss: 0.5846  |  0:00:03s\n",
      "epoch 70 | loss: 0.59038 |  0:00:03s\n",
      "epoch 71 | loss: 0.58539 |  0:00:03s\n",
      "epoch 72 | loss: 0.58391 |  0:00:03s\n",
      "epoch 73 | loss: 0.58465 |  0:00:03s\n",
      "epoch 74 | loss: 0.58004 |  0:00:03s\n",
      "epoch 75 | loss: 0.57445 |  0:00:04s\n",
      "epoch 76 | loss: 0.57549 |  0:00:04s\n",
      "epoch 77 | loss: 0.58735 |  0:00:04s\n",
      "epoch 78 | loss: 0.57481 |  0:00:04s\n",
      "epoch 79 | loss: 0.58088 |  0:00:04s\n",
      "epoch 80 | loss: 0.56781 |  0:00:04s\n",
      "epoch 81 | loss: 0.56791 |  0:00:04s\n",
      "epoch 82 | loss: 0.5692  |  0:00:04s\n",
      "epoch 83 | loss: 0.56856 |  0:00:04s\n",
      "epoch 84 | loss: 0.57163 |  0:00:04s\n",
      "epoch 85 | loss: 0.56222 |  0:00:04s\n",
      "epoch 86 | loss: 0.5581  |  0:00:04s\n",
      "epoch 87 | loss: 0.57112 |  0:00:04s\n",
      "epoch 88 | loss: 0.56298 |  0:00:04s\n",
      "epoch 89 | loss: 0.55777 |  0:00:04s\n",
      "epoch 90 | loss: 0.55758 |  0:00:04s\n",
      "epoch 91 | loss: 0.55346 |  0:00:04s\n",
      "epoch 92 | loss: 0.55672 |  0:00:04s\n",
      "epoch 93 | loss: 0.55247 |  0:00:04s\n",
      "epoch 94 | loss: 0.55536 |  0:00:04s\n",
      "epoch 95 | loss: 0.54947 |  0:00:04s\n",
      "epoch 96 | loss: 0.54311 |  0:00:05s\n",
      "epoch 97 | loss: 0.55594 |  0:00:05s\n",
      "epoch 98 | loss: 0.54675 |  0:00:05s\n",
      "epoch 99 | loss: 0.54555 |  0:00:05s\n",
      "epoch 100| loss: 0.5512  |  0:00:05s\n",
      "epoch 101| loss: 0.55001 |  0:00:05s\n",
      "epoch 102| loss: 0.5542  |  0:00:05s\n",
      "epoch 103| loss: 0.55783 |  0:00:05s\n",
      "epoch 104| loss: 0.55587 |  0:00:05s\n",
      "epoch 105| loss: 0.56057 |  0:00:05s\n",
      "epoch 106| loss: 0.54666 |  0:00:05s\n",
      "epoch 107| loss: 0.54422 |  0:00:05s\n",
      "epoch 108| loss: 0.53864 |  0:00:05s\n",
      "epoch 109| loss: 0.52998 |  0:00:05s\n",
      "epoch 110| loss: 0.53503 |  0:00:05s\n",
      "epoch 111| loss: 0.52644 |  0:00:05s\n",
      "epoch 112| loss: 0.5343  |  0:00:05s\n",
      "epoch 113| loss: 0.5381  |  0:00:05s\n",
      "epoch 114| loss: 0.52334 |  0:00:05s\n",
      "epoch 115| loss: 0.5272  |  0:00:05s\n",
      "epoch 116| loss: 0.52306 |  0:00:05s\n",
      "epoch 117| loss: 0.50822 |  0:00:06s\n",
      "epoch 118| loss: 0.50554 |  0:00:06s\n",
      "epoch 119| loss: 0.50489 |  0:00:06s\n",
      "epoch 120| loss: 0.50435 |  0:00:06s\n",
      "epoch 121| loss: 0.49168 |  0:00:06s\n",
      "epoch 122| loss: 0.49716 |  0:00:06s\n",
      "epoch 123| loss: 0.49576 |  0:00:06s\n",
      "epoch 124| loss: 0.49736 |  0:00:06s\n",
      "epoch 125| loss: 0.50271 |  0:00:06s\n",
      "epoch 126| loss: 0.48062 |  0:00:06s\n",
      "epoch 127| loss: 0.48988 |  0:00:06s\n",
      "epoch 128| loss: 0.49243 |  0:00:06s\n",
      "epoch 129| loss: 0.4959  |  0:00:06s\n",
      "epoch 130| loss: 0.48577 |  0:00:06s\n",
      "epoch 131| loss: 0.48625 |  0:00:06s\n",
      "epoch 132| loss: 0.48992 |  0:00:06s\n",
      "epoch 133| loss: 0.48888 |  0:00:06s\n",
      "epoch 134| loss: 0.48926 |  0:00:06s\n",
      "epoch 135| loss: 0.49207 |  0:00:07s\n",
      "epoch 136| loss: 0.49957 |  0:00:07s\n",
      "epoch 137| loss: 0.498   |  0:00:07s\n",
      "epoch 138| loss: 0.49883 |  0:00:07s\n",
      "epoch 139| loss: 0.5004  |  0:00:07s\n",
      "epoch 140| loss: 0.49739 |  0:00:07s\n",
      "epoch 141| loss: 0.49072 |  0:00:07s\n",
      "epoch 142| loss: 0.49322 |  0:00:07s\n",
      "epoch 143| loss: 0.49124 |  0:00:07s\n",
      "epoch 144| loss: 0.48518 |  0:00:07s\n",
      "epoch 145| loss: 0.48048 |  0:00:07s\n",
      "epoch 146| loss: 0.48427 |  0:00:07s\n",
      "epoch 147| loss: 0.47855 |  0:00:07s\n",
      "epoch 148| loss: 0.4792  |  0:00:07s\n",
      "epoch 149| loss: 0.47244 |  0:00:07s\n",
      "epoch 150| loss: 0.46576 |  0:00:07s\n",
      "epoch 151| loss: 0.46896 |  0:00:07s\n",
      "epoch 152| loss: 0.47502 |  0:00:07s\n",
      "epoch 153| loss: 0.4621  |  0:00:08s\n",
      "epoch 154| loss: 0.4703  |  0:00:08s\n",
      "epoch 155| loss: 0.46851 |  0:00:08s\n",
      "epoch 156| loss: 0.46173 |  0:00:08s\n",
      "epoch 157| loss: 0.4619  |  0:00:08s\n",
      "epoch 158| loss: 0.45847 |  0:00:08s\n",
      "epoch 159| loss: 0.45705 |  0:00:08s\n",
      "epoch 160| loss: 0.45394 |  0:00:08s\n",
      "epoch 161| loss: 0.4559  |  0:00:08s\n",
      "epoch 162| loss: 0.45729 |  0:00:08s\n",
      "epoch 163| loss: 0.46459 |  0:00:08s\n",
      "epoch 164| loss: 0.45314 |  0:00:08s\n",
      "epoch 165| loss: 0.44695 |  0:00:08s\n",
      "epoch 166| loss: 0.45262 |  0:00:08s\n",
      "epoch 167| loss: 0.45419 |  0:00:08s\n",
      "epoch 168| loss: 0.45112 |  0:00:08s\n",
      "epoch 169| loss: 0.44943 |  0:00:08s\n",
      "epoch 170| loss: 0.4508  |  0:00:08s\n",
      "epoch 171| loss: 0.44223 |  0:00:09s\n",
      "epoch 172| loss: 0.44248 |  0:00:09s\n",
      "epoch 173| loss: 0.43583 |  0:00:09s\n",
      "epoch 174| loss: 0.43558 |  0:00:09s\n",
      "epoch 175| loss: 0.42946 |  0:00:09s\n",
      "epoch 176| loss: 0.43305 |  0:00:09s\n",
      "epoch 177| loss: 0.43259 |  0:00:09s\n",
      "epoch 178| loss: 0.43106 |  0:00:09s\n",
      "epoch 179| loss: 0.43319 |  0:00:09s\n",
      "epoch 180| loss: 0.42791 |  0:00:09s\n",
      "epoch 181| loss: 0.42644 |  0:00:09s\n",
      "epoch 182| loss: 0.43356 |  0:00:09s\n",
      "epoch 183| loss: 0.42523 |  0:00:09s\n",
      "epoch 184| loss: 0.42028 |  0:00:09s\n",
      "epoch 185| loss: 0.41739 |  0:00:09s\n",
      "epoch 186| loss: 0.41454 |  0:00:09s\n",
      "epoch 187| loss: 0.41692 |  0:00:09s\n",
      "epoch 188| loss: 0.41826 |  0:00:09s\n",
      "epoch 189| loss: 0.41365 |  0:00:10s\n",
      "epoch 190| loss: 0.41447 |  0:00:10s\n",
      "epoch 191| loss: 0.41113 |  0:00:10s\n",
      "epoch 192| loss: 0.41164 |  0:00:10s\n",
      "epoch 193| loss: 0.4026  |  0:00:10s\n",
      "epoch 194| loss: 0.41134 |  0:00:10s\n",
      "epoch 195| loss: 0.40807 |  0:00:10s\n",
      "epoch 196| loss: 0.40695 |  0:00:10s\n",
      "epoch 197| loss: 0.40541 |  0:00:10s\n",
      "epoch 198| loss: 0.40617 |  0:00:10s\n",
      "epoch 199| loss: 0.40548 |  0:00:10s\n",
      "epoch 200| loss: 0.40427 |  0:00:10s\n",
      "epoch 201| loss: 0.40235 |  0:00:10s\n",
      "epoch 202| loss: 0.39877 |  0:00:10s\n",
      "epoch 203| loss: 0.39694 |  0:00:10s\n",
      "epoch 204| loss: 0.39767 |  0:00:10s\n",
      "epoch 205| loss: 0.39852 |  0:00:10s\n",
      "epoch 206| loss: 0.40046 |  0:00:10s\n",
      "epoch 207| loss: 0.40904 |  0:00:11s\n",
      "epoch 208| loss: 0.40584 |  0:00:11s\n",
      "epoch 209| loss: 0.39898 |  0:00:11s\n",
      "epoch 210| loss: 0.4008  |  0:00:11s\n",
      "epoch 211| loss: 0.39664 |  0:00:11s\n",
      "epoch 212| loss: 0.39474 |  0:00:11s\n",
      "epoch 213| loss: 0.39108 |  0:00:11s\n",
      "epoch 214| loss: 0.39578 |  0:00:11s\n",
      "epoch 215| loss: 0.39204 |  0:00:11s\n",
      "epoch 216| loss: 0.38464 |  0:00:11s\n",
      "epoch 217| loss: 0.38668 |  0:00:11s\n",
      "epoch 218| loss: 0.38068 |  0:00:11s\n",
      "epoch 219| loss: 0.3823  |  0:00:11s\n",
      "epoch 220| loss: 0.38524 |  0:00:11s\n",
      "epoch 221| loss: 0.3835  |  0:00:11s\n",
      "epoch 222| loss: 0.3857  |  0:00:11s\n",
      "epoch 223| loss: 0.38852 |  0:00:11s\n",
      "epoch 224| loss: 0.38683 |  0:00:11s\n",
      "epoch 225| loss: 0.37794 |  0:00:11s\n",
      "epoch 226| loss: 0.38354 |  0:00:12s\n",
      "epoch 227| loss: 0.38014 |  0:00:12s\n",
      "epoch 228| loss: 0.37476 |  0:00:12s\n",
      "epoch 229| loss: 0.37444 |  0:00:12s\n",
      "epoch 230| loss: 0.37289 |  0:00:12s\n",
      "epoch 231| loss: 0.37583 |  0:00:12s\n",
      "epoch 232| loss: 0.37686 |  0:00:12s\n",
      "epoch 233| loss: 0.37518 |  0:00:12s\n",
      "epoch 234| loss: 0.37242 |  0:00:12s\n",
      "epoch 235| loss: 0.37231 |  0:00:12s\n",
      "epoch 236| loss: 0.37542 |  0:00:12s\n",
      "epoch 237| loss: 0.38071 |  0:00:12s\n",
      "epoch 238| loss: 0.37446 |  0:00:12s\n",
      "epoch 239| loss: 0.37708 |  0:00:12s\n",
      "epoch 240| loss: 0.37399 |  0:00:12s\n",
      "epoch 241| loss: 0.37696 |  0:00:12s\n",
      "epoch 242| loss: 0.37413 |  0:00:12s\n",
      "epoch 243| loss: 0.37327 |  0:00:12s\n",
      "epoch 244| loss: 0.36828 |  0:00:12s\n",
      "epoch 245| loss: 0.37603 |  0:00:13s\n",
      "epoch 246| loss: 0.37756 |  0:00:13s\n",
      "epoch 247| loss: 0.38572 |  0:00:13s\n",
      "epoch 248| loss: 0.39018 |  0:00:13s\n",
      "epoch 249| loss: 0.40177 |  0:00:13s\n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.95002 |  0:00:00s\n",
      "epoch 1  | loss: 1.69541 |  0:00:00s\n",
      "epoch 2  | loss: 1.29998 |  0:00:00s\n",
      "epoch 3  | loss: 1.13574 |  0:00:00s\n",
      "epoch 4  | loss: 1.00688 |  0:00:00s\n",
      "epoch 5  | loss: 0.93239 |  0:00:00s\n",
      "epoch 6  | loss: 0.88825 |  0:00:00s\n",
      "epoch 7  | loss: 0.86528 |  0:00:00s\n",
      "epoch 8  | loss: 0.83362 |  0:00:00s\n",
      "epoch 9  | loss: 0.80898 |  0:00:00s\n",
      "epoch 10 | loss: 0.79447 |  0:00:00s\n",
      "epoch 11 | loss: 0.77451 |  0:00:00s\n",
      "epoch 12 | loss: 0.76331 |  0:00:00s\n",
      "epoch 13 | loss: 0.75736 |  0:00:00s\n",
      "epoch 14 | loss: 0.75241 |  0:00:00s\n",
      "epoch 15 | loss: 0.73278 |  0:00:00s\n",
      "epoch 16 | loss: 0.73089 |  0:00:00s\n",
      "epoch 17 | loss: 0.72883 |  0:00:00s\n",
      "epoch 18 | loss: 0.71397 |  0:00:01s\n",
      "epoch 19 | loss: 0.70176 |  0:00:01s\n",
      "epoch 20 | loss: 0.70604 |  0:00:01s\n",
      "epoch 21 | loss: 0.70368 |  0:00:01s\n",
      "epoch 22 | loss: 0.69334 |  0:00:01s\n",
      "epoch 23 | loss: 0.6869  |  0:00:01s\n",
      "epoch 24 | loss: 0.68954 |  0:00:01s\n",
      "epoch 25 | loss: 0.68686 |  0:00:01s\n",
      "epoch 26 | loss: 0.6795  |  0:00:01s\n",
      "epoch 27 | loss: 0.68331 |  0:00:01s\n",
      "epoch 28 | loss: 0.67423 |  0:00:01s\n",
      "epoch 29 | loss: 0.66913 |  0:00:01s\n",
      "epoch 30 | loss: 0.66895 |  0:00:01s\n",
      "epoch 31 | loss: 0.66319 |  0:00:01s\n",
      "epoch 32 | loss: 0.66351 |  0:00:01s\n",
      "epoch 33 | loss: 0.66033 |  0:00:01s\n",
      "epoch 34 | loss: 0.66202 |  0:00:01s\n",
      "epoch 35 | loss: 0.64457 |  0:00:01s\n",
      "epoch 36 | loss: 0.63407 |  0:00:01s\n",
      "epoch 37 | loss: 0.63118 |  0:00:02s\n",
      "epoch 38 | loss: 0.63483 |  0:00:02s\n",
      "epoch 39 | loss: 0.63298 |  0:00:02s\n",
      "epoch 40 | loss: 0.61612 |  0:00:02s\n",
      "epoch 41 | loss: 0.61867 |  0:00:02s\n",
      "epoch 42 | loss: 0.61295 |  0:00:02s\n",
      "epoch 43 | loss: 0.61447 |  0:00:02s\n",
      "epoch 44 | loss: 0.61885 |  0:00:02s\n",
      "epoch 45 | loss: 0.61275 |  0:00:02s\n",
      "epoch 46 | loss: 0.60259 |  0:00:02s\n",
      "epoch 47 | loss: 0.60502 |  0:00:02s\n",
      "epoch 48 | loss: 0.59655 |  0:00:02s\n",
      "epoch 49 | loss: 0.59712 |  0:00:02s\n",
      "epoch 50 | loss: 0.591   |  0:00:02s\n",
      "epoch 51 | loss: 0.58366 |  0:00:02s\n",
      "epoch 52 | loss: 0.59305 |  0:00:02s\n",
      "epoch 53 | loss: 0.5904  |  0:00:02s\n",
      "epoch 54 | loss: 0.58931 |  0:00:02s\n",
      "epoch 55 | loss: 0.58938 |  0:00:02s\n",
      "epoch 56 | loss: 0.58107 |  0:00:03s\n",
      "epoch 57 | loss: 0.58059 |  0:00:03s\n",
      "epoch 58 | loss: 0.57525 |  0:00:03s\n",
      "epoch 59 | loss: 0.57746 |  0:00:03s\n",
      "epoch 60 | loss: 0.5795  |  0:00:03s\n",
      "epoch 61 | loss: 0.57562 |  0:00:03s\n",
      "epoch 62 | loss: 0.566   |  0:00:03s\n",
      "epoch 63 | loss: 0.5811  |  0:00:03s\n",
      "epoch 64 | loss: 0.57216 |  0:00:03s\n",
      "epoch 65 | loss: 0.58032 |  0:00:03s\n",
      "epoch 66 | loss: 0.5714  |  0:00:03s\n",
      "epoch 67 | loss: 0.5788  |  0:00:03s\n",
      "epoch 68 | loss: 0.56184 |  0:00:03s\n",
      "epoch 69 | loss: 0.55812 |  0:00:03s\n",
      "epoch 70 | loss: 0.55437 |  0:00:03s\n",
      "epoch 71 | loss: 0.55296 |  0:00:03s\n",
      "epoch 72 | loss: 0.54963 |  0:00:03s\n",
      "epoch 73 | loss: 0.55365 |  0:00:03s\n",
      "epoch 74 | loss: 0.54315 |  0:00:04s\n",
      "epoch 75 | loss: 0.55133 |  0:00:04s\n",
      "epoch 76 | loss: 0.55164 |  0:00:04s\n",
      "epoch 77 | loss: 0.54792 |  0:00:04s\n",
      "epoch 78 | loss: 0.5477  |  0:00:04s\n",
      "epoch 79 | loss: 0.53579 |  0:00:04s\n",
      "epoch 80 | loss: 0.53569 |  0:00:04s\n",
      "epoch 81 | loss: 0.52997 |  0:00:04s\n",
      "epoch 82 | loss: 0.51778 |  0:00:04s\n",
      "epoch 83 | loss: 0.52852 |  0:00:04s\n",
      "epoch 84 | loss: 0.52318 |  0:00:04s\n",
      "epoch 85 | loss: 0.52553 |  0:00:04s\n",
      "epoch 86 | loss: 0.51907 |  0:00:04s\n",
      "epoch 87 | loss: 0.51032 |  0:00:04s\n",
      "epoch 88 | loss: 0.51484 |  0:00:04s\n",
      "epoch 89 | loss: 0.51032 |  0:00:04s\n",
      "epoch 90 | loss: 0.50774 |  0:00:04s\n",
      "epoch 91 | loss: 0.50125 |  0:00:04s\n",
      "epoch 92 | loss: 0.49361 |  0:00:05s\n",
      "epoch 93 | loss: 0.50705 |  0:00:05s\n",
      "epoch 94 | loss: 0.49948 |  0:00:05s\n",
      "epoch 95 | loss: 0.49133 |  0:00:05s\n",
      "epoch 96 | loss: 0.49043 |  0:00:05s\n",
      "epoch 97 | loss: 0.49565 |  0:00:05s\n",
      "epoch 98 | loss: 0.49422 |  0:00:05s\n",
      "epoch 99 | loss: 0.48855 |  0:00:05s\n",
      "epoch 100| loss: 0.48668 |  0:00:05s\n",
      "epoch 101| loss: 0.49363 |  0:00:05s\n",
      "epoch 102| loss: 0.48809 |  0:00:05s\n",
      "epoch 103| loss: 0.48729 |  0:00:05s\n",
      "epoch 104| loss: 0.48095 |  0:00:05s\n",
      "epoch 105| loss: 0.48766 |  0:00:05s\n",
      "epoch 106| loss: 0.48548 |  0:00:05s\n",
      "epoch 107| loss: 0.49637 |  0:00:05s\n",
      "epoch 108| loss: 0.48481 |  0:00:05s\n",
      "epoch 109| loss: 0.48352 |  0:00:05s\n",
      "epoch 110| loss: 0.48832 |  0:00:06s\n",
      "epoch 111| loss: 0.48269 |  0:00:06s\n",
      "epoch 112| loss: 0.48835 |  0:00:06s\n",
      "epoch 113| loss: 0.48255 |  0:00:06s\n",
      "epoch 114| loss: 0.48416 |  0:00:06s\n",
      "epoch 115| loss: 0.48378 |  0:00:06s\n",
      "epoch 116| loss: 0.48205 |  0:00:06s\n",
      "epoch 117| loss: 0.47449 |  0:00:06s\n",
      "epoch 118| loss: 0.47439 |  0:00:06s\n",
      "epoch 119| loss: 0.46625 |  0:00:06s\n",
      "epoch 120| loss: 0.47036 |  0:00:06s\n",
      "epoch 121| loss: 0.47566 |  0:00:06s\n",
      "epoch 122| loss: 0.47164 |  0:00:06s\n",
      "epoch 123| loss: 0.47648 |  0:00:06s\n",
      "epoch 124| loss: 0.47638 |  0:00:06s\n",
      "epoch 125| loss: 0.47378 |  0:00:06s\n",
      "epoch 126| loss: 0.46423 |  0:00:06s\n",
      "epoch 127| loss: 0.46175 |  0:00:06s\n",
      "epoch 128| loss: 0.46094 |  0:00:06s\n",
      "epoch 129| loss: 0.46442 |  0:00:07s\n",
      "epoch 130| loss: 0.45511 |  0:00:07s\n",
      "epoch 131| loss: 0.45345 |  0:00:07s\n",
      "epoch 132| loss: 0.45507 |  0:00:07s\n",
      "epoch 133| loss: 0.45741 |  0:00:07s\n",
      "epoch 134| loss: 0.45423 |  0:00:07s\n",
      "epoch 135| loss: 0.44763 |  0:00:07s\n",
      "epoch 136| loss: 0.44582 |  0:00:07s\n",
      "epoch 137| loss: 0.44801 |  0:00:07s\n",
      "epoch 138| loss: 0.44468 |  0:00:07s\n",
      "epoch 139| loss: 0.44929 |  0:00:07s\n",
      "epoch 140| loss: 0.44081 |  0:00:07s\n",
      "epoch 141| loss: 0.44528 |  0:00:07s\n",
      "epoch 142| loss: 0.45252 |  0:00:07s\n",
      "epoch 143| loss: 0.45163 |  0:00:07s\n",
      "epoch 144| loss: 0.44182 |  0:00:07s\n",
      "epoch 145| loss: 0.4403  |  0:00:07s\n",
      "epoch 146| loss: 0.44286 |  0:00:07s\n",
      "epoch 147| loss: 0.43628 |  0:00:08s\n",
      "epoch 148| loss: 0.44183 |  0:00:08s\n",
      "epoch 149| loss: 0.43647 |  0:00:08s\n",
      "epoch 150| loss: 0.43824 |  0:00:08s\n",
      "epoch 151| loss: 0.43331 |  0:00:08s\n",
      "epoch 152| loss: 0.42894 |  0:00:08s\n",
      "epoch 153| loss: 0.42624 |  0:00:08s\n",
      "epoch 154| loss: 0.42487 |  0:00:08s\n",
      "epoch 155| loss: 0.42661 |  0:00:08s\n",
      "epoch 156| loss: 0.42603 |  0:00:08s\n",
      "epoch 157| loss: 0.4219  |  0:00:08s\n",
      "epoch 158| loss: 0.42387 |  0:00:08s\n",
      "epoch 159| loss: 0.42633 |  0:00:08s\n",
      "epoch 160| loss: 0.42811 |  0:00:08s\n",
      "epoch 161| loss: 0.42612 |  0:00:08s\n",
      "epoch 162| loss: 0.42414 |  0:00:08s\n",
      "epoch 163| loss: 0.42692 |  0:00:08s\n",
      "epoch 164| loss: 0.41969 |  0:00:08s\n",
      "epoch 165| loss: 0.42297 |  0:00:09s\n",
      "epoch 166| loss: 0.42527 |  0:00:09s\n",
      "epoch 167| loss: 0.41584 |  0:00:09s\n",
      "epoch 168| loss: 0.41581 |  0:00:09s\n",
      "epoch 169| loss: 0.41913 |  0:00:09s\n",
      "epoch 170| loss: 0.41566 |  0:00:09s\n",
      "epoch 171| loss: 0.40949 |  0:00:09s\n",
      "epoch 172| loss: 0.40661 |  0:00:09s\n",
      "epoch 173| loss: 0.39819 |  0:00:09s\n",
      "epoch 174| loss: 0.40378 |  0:00:09s\n",
      "epoch 175| loss: 0.40798 |  0:00:09s\n",
      "epoch 176| loss: 0.41582 |  0:00:09s\n",
      "epoch 177| loss: 0.40741 |  0:00:09s\n",
      "epoch 178| loss: 0.40401 |  0:00:09s\n",
      "epoch 179| loss: 0.39218 |  0:00:09s\n",
      "epoch 180| loss: 0.39747 |  0:00:09s\n",
      "epoch 181| loss: 0.39803 |  0:00:09s\n",
      "epoch 182| loss: 0.39938 |  0:00:09s\n",
      "epoch 183| loss: 0.4013  |  0:00:10s\n",
      "epoch 184| loss: 0.40001 |  0:00:10s\n",
      "epoch 185| loss: 0.39198 |  0:00:10s\n",
      "epoch 186| loss: 0.39322 |  0:00:10s\n",
      "epoch 187| loss: 0.39993 |  0:00:10s\n",
      "epoch 188| loss: 0.39676 |  0:00:10s\n",
      "epoch 189| loss: 0.39773 |  0:00:10s\n",
      "epoch 190| loss: 0.3987  |  0:00:10s\n",
      "epoch 191| loss: 0.39781 |  0:00:10s\n",
      "epoch 192| loss: 0.39666 |  0:00:10s\n",
      "epoch 193| loss: 0.39238 |  0:00:10s\n",
      "epoch 194| loss: 0.3907  |  0:00:10s\n",
      "epoch 195| loss: 0.39968 |  0:00:10s\n",
      "epoch 196| loss: 0.39589 |  0:00:10s\n",
      "epoch 197| loss: 0.3942  |  0:00:10s\n",
      "epoch 198| loss: 0.38997 |  0:00:10s\n",
      "epoch 199| loss: 0.38254 |  0:00:10s\n",
      "epoch 200| loss: 0.3895  |  0:00:10s\n",
      "epoch 201| loss: 0.39563 |  0:00:11s\n",
      "epoch 202| loss: 0.38623 |  0:00:11s\n",
      "epoch 203| loss: 0.38648 |  0:00:11s\n",
      "epoch 204| loss: 0.39479 |  0:00:11s\n",
      "epoch 205| loss: 0.39281 |  0:00:11s\n",
      "epoch 206| loss: 0.3896  |  0:00:11s\n",
      "epoch 207| loss: 0.398   |  0:00:11s\n",
      "epoch 208| loss: 0.39122 |  0:00:11s\n",
      "epoch 209| loss: 0.38999 |  0:00:11s\n",
      "epoch 210| loss: 0.39233 |  0:00:11s\n",
      "epoch 211| loss: 0.38162 |  0:00:11s\n",
      "epoch 212| loss: 0.37383 |  0:00:11s\n",
      "epoch 213| loss: 0.37628 |  0:00:11s\n",
      "epoch 214| loss: 0.38052 |  0:00:11s\n",
      "epoch 215| loss: 0.37642 |  0:00:11s\n",
      "epoch 216| loss: 0.37789 |  0:00:11s\n",
      "epoch 217| loss: 0.37478 |  0:00:11s\n",
      "epoch 218| loss: 0.37815 |  0:00:12s\n",
      "epoch 219| loss: 0.37496 |  0:00:12s\n",
      "epoch 220| loss: 0.37389 |  0:00:12s\n",
      "epoch 221| loss: 0.37576 |  0:00:12s\n",
      "epoch 222| loss: 0.38028 |  0:00:12s\n",
      "epoch 223| loss: 0.38149 |  0:00:12s\n",
      "epoch 224| loss: 0.37502 |  0:00:12s\n",
      "epoch 225| loss: 0.37901 |  0:00:12s\n",
      "epoch 226| loss: 0.37671 |  0:00:12s\n",
      "epoch 227| loss: 0.38203 |  0:00:12s\n",
      "epoch 228| loss: 0.38187 |  0:00:12s\n",
      "epoch 229| loss: 0.37582 |  0:00:12s\n",
      "epoch 230| loss: 0.37286 |  0:00:12s\n",
      "epoch 231| loss: 0.37024 |  0:00:12s\n",
      "epoch 232| loss: 0.37324 |  0:00:12s\n",
      "epoch 233| loss: 0.38169 |  0:00:12s\n",
      "epoch 234| loss: 0.37597 |  0:00:12s\n",
      "epoch 235| loss: 0.36963 |  0:00:12s\n",
      "epoch 236| loss: 0.36514 |  0:00:13s\n",
      "epoch 237| loss: 0.36748 |  0:00:13s\n",
      "epoch 238| loss: 0.3606  |  0:00:13s\n",
      "epoch 239| loss: 0.36946 |  0:00:13s\n",
      "epoch 240| loss: 0.36765 |  0:00:13s\n",
      "epoch 241| loss: 0.36452 |  0:00:13s\n",
      "epoch 242| loss: 0.35772 |  0:00:13s\n",
      "epoch 243| loss: 0.35572 |  0:00:13s\n",
      "epoch 244| loss: 0.36213 |  0:00:13s\n",
      "epoch 245| loss: 0.35158 |  0:00:13s\n",
      "epoch 246| loss: 0.35619 |  0:00:13s\n",
      "epoch 247| loss: 0.35358 |  0:00:13s\n",
      "epoch 248| loss: 0.35555 |  0:00:13s\n",
      "epoch 249| loss: 0.35338 |  0:00:13s\n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.9991  |  0:00:00s\n",
      "epoch 1  | loss: 1.76656 |  0:00:00s\n",
      "epoch 2  | loss: 1.28051 |  0:00:00s\n",
      "epoch 3  | loss: 1.11354 |  0:00:00s\n",
      "epoch 4  | loss: 1.02079 |  0:00:00s\n",
      "epoch 5  | loss: 0.97096 |  0:00:00s\n",
      "epoch 6  | loss: 0.91573 |  0:00:00s\n",
      "epoch 7  | loss: 0.8887  |  0:00:00s\n",
      "epoch 8  | loss: 0.87043 |  0:00:00s\n",
      "epoch 9  | loss: 0.85159 |  0:00:00s\n",
      "epoch 10 | loss: 0.81487 |  0:00:00s\n",
      "epoch 11 | loss: 0.80339 |  0:00:00s\n",
      "epoch 12 | loss: 0.78532 |  0:00:00s\n",
      "epoch 13 | loss: 0.7711  |  0:00:00s\n",
      "epoch 14 | loss: 0.77934 |  0:00:00s\n",
      "epoch 15 | loss: 0.75315 |  0:00:00s\n",
      "epoch 16 | loss: 0.75119 |  0:00:00s\n",
      "epoch 17 | loss: 0.74084 |  0:00:00s\n",
      "epoch 18 | loss: 0.731   |  0:00:00s\n",
      "epoch 19 | loss: 0.73828 |  0:00:01s\n",
      "epoch 20 | loss: 0.72559 |  0:00:01s\n",
      "epoch 21 | loss: 0.73243 |  0:00:01s\n",
      "epoch 22 | loss: 0.72131 |  0:00:01s\n",
      "epoch 23 | loss: 0.717   |  0:00:01s\n",
      "epoch 24 | loss: 0.71692 |  0:00:01s\n",
      "epoch 25 | loss: 0.7059  |  0:00:01s\n",
      "epoch 26 | loss: 0.71463 |  0:00:01s\n",
      "epoch 27 | loss: 0.70841 |  0:00:01s\n",
      "epoch 28 | loss: 0.70055 |  0:00:01s\n",
      "epoch 29 | loss: 0.7005  |  0:00:01s\n",
      "epoch 30 | loss: 0.70196 |  0:00:01s\n",
      "epoch 31 | loss: 0.70625 |  0:00:01s\n",
      "epoch 32 | loss: 0.69349 |  0:00:01s\n",
      "epoch 33 | loss: 0.68865 |  0:00:01s\n",
      "epoch 34 | loss: 0.68457 |  0:00:01s\n",
      "epoch 35 | loss: 0.68838 |  0:00:01s\n",
      "epoch 36 | loss: 0.68793 |  0:00:01s\n",
      "epoch 37 | loss: 0.6987  |  0:00:02s\n",
      "epoch 38 | loss: 0.69997 |  0:00:02s\n",
      "epoch 39 | loss: 0.69677 |  0:00:02s\n",
      "epoch 40 | loss: 0.70036 |  0:00:02s\n",
      "epoch 41 | loss: 0.69685 |  0:00:02s\n",
      "epoch 42 | loss: 0.68328 |  0:00:02s\n",
      "epoch 43 | loss: 0.68696 |  0:00:02s\n",
      "epoch 44 | loss: 0.68884 |  0:00:02s\n",
      "epoch 45 | loss: 0.69423 |  0:00:02s\n",
      "epoch 46 | loss: 0.68597 |  0:00:02s\n",
      "epoch 47 | loss: 0.69368 |  0:00:02s\n",
      "epoch 48 | loss: 0.68944 |  0:00:02s\n",
      "epoch 49 | loss: 0.69218 |  0:00:02s\n",
      "epoch 50 | loss: 0.68829 |  0:00:02s\n",
      "epoch 51 | loss: 0.67499 |  0:00:02s\n",
      "epoch 52 | loss: 0.67466 |  0:00:02s\n",
      "epoch 53 | loss: 0.66801 |  0:00:02s\n",
      "epoch 54 | loss: 0.67011 |  0:00:02s\n",
      "epoch 55 | loss: 0.674   |  0:00:02s\n",
      "epoch 56 | loss: 0.67068 |  0:00:03s\n",
      "epoch 57 | loss: 0.6731  |  0:00:03s\n",
      "epoch 58 | loss: 0.67156 |  0:00:03s\n",
      "epoch 59 | loss: 0.67772 |  0:00:03s\n",
      "epoch 60 | loss: 0.67378 |  0:00:03s\n",
      "epoch 61 | loss: 0.6666  |  0:00:03s\n",
      "epoch 62 | loss: 0.66121 |  0:00:03s\n",
      "epoch 63 | loss: 0.66134 |  0:00:03s\n",
      "epoch 64 | loss: 0.66958 |  0:00:03s\n",
      "epoch 65 | loss: 0.67038 |  0:00:03s\n",
      "epoch 66 | loss: 0.67402 |  0:00:03s\n",
      "epoch 67 | loss: 0.66871 |  0:00:03s\n",
      "epoch 68 | loss: 0.66926 |  0:00:03s\n",
      "epoch 69 | loss: 0.66093 |  0:00:03s\n",
      "epoch 70 | loss: 0.66675 |  0:00:03s\n",
      "epoch 71 | loss: 0.66997 |  0:00:03s\n",
      "epoch 72 | loss: 0.66555 |  0:00:04s\n",
      "epoch 73 | loss: 0.6616  |  0:00:04s\n",
      "epoch 74 | loss: 0.6614  |  0:00:04s\n",
      "epoch 75 | loss: 0.66307 |  0:00:04s\n",
      "epoch 76 | loss: 0.65524 |  0:00:04s\n",
      "epoch 77 | loss: 0.65571 |  0:00:04s\n",
      "epoch 78 | loss: 0.64846 |  0:00:04s\n",
      "epoch 79 | loss: 0.64735 |  0:00:04s\n",
      "epoch 80 | loss: 0.64805 |  0:00:04s\n",
      "epoch 81 | loss: 0.65655 |  0:00:04s\n",
      "epoch 82 | loss: 0.65264 |  0:00:04s\n",
      "epoch 83 | loss: 0.65429 |  0:00:04s\n",
      "epoch 84 | loss: 0.6427  |  0:00:04s\n",
      "epoch 85 | loss: 0.64501 |  0:00:04s\n",
      "epoch 86 | loss: 0.64857 |  0:00:04s\n",
      "epoch 87 | loss: 0.63841 |  0:00:04s\n",
      "epoch 88 | loss: 0.65296 |  0:00:04s\n",
      "epoch 89 | loss: 0.64869 |  0:00:04s\n",
      "epoch 90 | loss: 0.64092 |  0:00:04s\n",
      "epoch 91 | loss: 0.64981 |  0:00:05s\n",
      "epoch 92 | loss: 0.64075 |  0:00:05s\n",
      "epoch 93 | loss: 0.6365  |  0:00:05s\n",
      "epoch 94 | loss: 0.63905 |  0:00:05s\n",
      "epoch 95 | loss: 0.63954 |  0:00:05s\n",
      "epoch 96 | loss: 0.64265 |  0:00:05s\n",
      "epoch 97 | loss: 0.63788 |  0:00:05s\n",
      "epoch 98 | loss: 0.62918 |  0:00:05s\n",
      "epoch 99 | loss: 0.63071 |  0:00:05s\n",
      "epoch 100| loss: 0.63402 |  0:00:05s\n",
      "epoch 101| loss: 0.62143 |  0:00:05s\n",
      "epoch 102| loss: 0.62426 |  0:00:05s\n",
      "epoch 103| loss: 0.61745 |  0:00:05s\n",
      "epoch 104| loss: 0.61322 |  0:00:05s\n",
      "epoch 105| loss: 0.61723 |  0:00:05s\n",
      "epoch 106| loss: 0.6139  |  0:00:05s\n",
      "epoch 107| loss: 0.60736 |  0:00:05s\n",
      "epoch 108| loss: 0.5976  |  0:00:05s\n",
      "epoch 109| loss: 0.59567 |  0:00:06s\n",
      "epoch 110| loss: 0.59644 |  0:00:06s\n",
      "epoch 111| loss: 0.59364 |  0:00:06s\n",
      "epoch 112| loss: 0.59404 |  0:00:06s\n",
      "epoch 113| loss: 0.5977  |  0:00:06s\n",
      "epoch 114| loss: 0.58864 |  0:00:06s\n",
      "epoch 115| loss: 0.59159 |  0:00:06s\n",
      "epoch 116| loss: 0.58965 |  0:00:06s\n",
      "epoch 117| loss: 0.58704 |  0:00:06s\n",
      "epoch 118| loss: 0.57978 |  0:00:06s\n",
      "epoch 119| loss: 0.582   |  0:00:06s\n",
      "epoch 120| loss: 0.58088 |  0:00:06s\n",
      "epoch 121| loss: 0.57886 |  0:00:06s\n",
      "epoch 122| loss: 0.57696 |  0:00:06s\n",
      "epoch 123| loss: 0.5743  |  0:00:06s\n",
      "epoch 124| loss: 0.56965 |  0:00:06s\n",
      "epoch 125| loss: 0.57346 |  0:00:06s\n",
      "epoch 126| loss: 0.56524 |  0:00:06s\n",
      "epoch 127| loss: 0.57314 |  0:00:06s\n",
      "epoch 128| loss: 0.56334 |  0:00:07s\n",
      "epoch 129| loss: 0.56889 |  0:00:07s\n",
      "epoch 130| loss: 0.56649 |  0:00:07s\n",
      "epoch 131| loss: 0.5668  |  0:00:07s\n",
      "epoch 132| loss: 0.57014 |  0:00:07s\n",
      "epoch 133| loss: 0.56004 |  0:00:07s\n",
      "epoch 134| loss: 0.56254 |  0:00:07s\n",
      "epoch 135| loss: 0.56943 |  0:00:07s\n",
      "epoch 136| loss: 0.56341 |  0:00:07s\n",
      "epoch 137| loss: 0.57289 |  0:00:07s\n",
      "epoch 138| loss: 0.55889 |  0:00:07s\n",
      "epoch 139| loss: 0.5612  |  0:00:07s\n",
      "epoch 140| loss: 0.56371 |  0:00:07s\n",
      "epoch 141| loss: 0.5648  |  0:00:07s\n",
      "epoch 142| loss: 0.56375 |  0:00:07s\n",
      "epoch 143| loss: 0.55384 |  0:00:07s\n",
      "epoch 144| loss: 0.54196 |  0:00:07s\n",
      "epoch 145| loss: 0.55096 |  0:00:07s\n",
      "epoch 146| loss: 0.54902 |  0:00:07s\n",
      "epoch 147| loss: 0.54899 |  0:00:08s\n",
      "epoch 148| loss: 0.55832 |  0:00:08s\n",
      "epoch 149| loss: 0.55773 |  0:00:08s\n",
      "epoch 150| loss: 0.56164 |  0:00:08s\n",
      "epoch 151| loss: 0.55316 |  0:00:08s\n",
      "epoch 152| loss: 0.55615 |  0:00:08s\n",
      "epoch 153| loss: 0.56756 |  0:00:08s\n",
      "epoch 154| loss: 0.57371 |  0:00:08s\n",
      "epoch 155| loss: 0.58555 |  0:00:08s\n",
      "epoch 156| loss: 0.57686 |  0:00:08s\n",
      "epoch 157| loss: 0.57888 |  0:00:08s\n",
      "epoch 158| loss: 0.56149 |  0:00:08s\n",
      "epoch 159| loss: 0.56904 |  0:00:08s\n",
      "epoch 160| loss: 0.56134 |  0:00:08s\n",
      "epoch 161| loss: 0.54621 |  0:00:08s\n",
      "epoch 162| loss: 0.55912 |  0:00:08s\n",
      "epoch 163| loss: 0.55426 |  0:00:08s\n",
      "epoch 164| loss: 0.54466 |  0:00:08s\n",
      "epoch 165| loss: 0.54597 |  0:00:08s\n",
      "epoch 166| loss: 0.5482  |  0:00:09s\n",
      "epoch 167| loss: 0.53986 |  0:00:09s\n",
      "epoch 168| loss: 0.55063 |  0:00:09s\n",
      "epoch 169| loss: 0.53663 |  0:00:09s\n",
      "epoch 170| loss: 0.54041 |  0:00:09s\n",
      "epoch 171| loss: 0.52721 |  0:00:09s\n",
      "epoch 172| loss: 0.53708 |  0:00:09s\n",
      "epoch 173| loss: 0.53002 |  0:00:09s\n",
      "epoch 174| loss: 0.53583 |  0:00:09s\n",
      "epoch 175| loss: 0.53036 |  0:00:09s\n",
      "epoch 176| loss: 0.52757 |  0:00:09s\n",
      "epoch 177| loss: 0.53251 |  0:00:09s\n",
      "epoch 178| loss: 0.52895 |  0:00:09s\n",
      "epoch 179| loss: 0.52659 |  0:00:09s\n",
      "epoch 180| loss: 0.52509 |  0:00:09s\n",
      "epoch 181| loss: 0.52842 |  0:00:09s\n",
      "epoch 182| loss: 0.52589 |  0:00:09s\n",
      "epoch 183| loss: 0.53154 |  0:00:09s\n",
      "epoch 184| loss: 0.52534 |  0:00:09s\n",
      "epoch 185| loss: 0.5217  |  0:00:10s\n",
      "epoch 186| loss: 0.5178  |  0:00:10s\n",
      "epoch 187| loss: 0.517   |  0:00:10s\n",
      "epoch 188| loss: 0.51626 |  0:00:10s\n",
      "epoch 189| loss: 0.51412 |  0:00:10s\n",
      "epoch 190| loss: 0.50615 |  0:00:10s\n",
      "epoch 191| loss: 0.50695 |  0:00:10s\n",
      "epoch 192| loss: 0.51019 |  0:00:10s\n",
      "epoch 193| loss: 0.50649 |  0:00:10s\n",
      "epoch 194| loss: 0.5207  |  0:00:10s\n",
      "epoch 195| loss: 0.52842 |  0:00:10s\n",
      "epoch 196| loss: 0.51435 |  0:00:10s\n",
      "epoch 197| loss: 0.50694 |  0:00:10s\n",
      "epoch 198| loss: 0.5178  |  0:00:10s\n",
      "epoch 199| loss: 0.51188 |  0:00:10s\n",
      "epoch 200| loss: 0.50594 |  0:00:10s\n",
      "epoch 201| loss: 0.50529 |  0:00:10s\n",
      "epoch 202| loss: 0.49941 |  0:00:10s\n",
      "epoch 203| loss: 0.50396 |  0:00:10s\n",
      "epoch 204| loss: 0.50279 |  0:00:10s\n",
      "epoch 205| loss: 0.50213 |  0:00:11s\n",
      "epoch 206| loss: 0.506   |  0:00:11s\n",
      "epoch 207| loss: 0.50854 |  0:00:11s\n",
      "epoch 208| loss: 0.519   |  0:00:11s\n",
      "epoch 209| loss: 0.51968 |  0:00:11s\n",
      "epoch 210| loss: 0.52603 |  0:00:11s\n",
      "epoch 211| loss: 0.51672 |  0:00:11s\n",
      "epoch 212| loss: 0.51224 |  0:00:11s\n",
      "epoch 213| loss: 0.51929 |  0:00:11s\n",
      "epoch 214| loss: 0.51772 |  0:00:11s\n",
      "epoch 215| loss: 0.51616 |  0:00:11s\n",
      "epoch 216| loss: 0.51343 |  0:00:11s\n",
      "epoch 217| loss: 0.51608 |  0:00:11s\n",
      "epoch 218| loss: 0.51714 |  0:00:11s\n",
      "epoch 219| loss: 0.50538 |  0:00:11s\n",
      "epoch 220| loss: 0.50879 |  0:00:11s\n",
      "epoch 221| loss: 0.50883 |  0:00:11s\n",
      "epoch 222| loss: 0.50209 |  0:00:11s\n",
      "epoch 223| loss: 0.51868 |  0:00:11s\n",
      "epoch 224| loss: 0.51626 |  0:00:11s\n",
      "epoch 225| loss: 0.51096 |  0:00:11s\n",
      "epoch 226| loss: 0.50434 |  0:00:12s\n",
      "epoch 227| loss: 0.50458 |  0:00:12s\n",
      "epoch 228| loss: 0.49657 |  0:00:12s\n",
      "epoch 229| loss: 0.50069 |  0:00:12s\n",
      "epoch 230| loss: 0.49855 |  0:00:12s\n",
      "epoch 231| loss: 0.49874 |  0:00:12s\n",
      "epoch 232| loss: 0.5042  |  0:00:12s\n",
      "epoch 233| loss: 0.50164 |  0:00:12s\n",
      "epoch 234| loss: 0.4955  |  0:00:12s\n",
      "epoch 235| loss: 0.49355 |  0:00:12s\n",
      "epoch 236| loss: 0.48511 |  0:00:12s\n",
      "epoch 237| loss: 0.48715 |  0:00:12s\n",
      "epoch 238| loss: 0.48684 |  0:00:12s\n",
      "epoch 239| loss: 0.49362 |  0:00:12s\n",
      "epoch 240| loss: 0.49931 |  0:00:12s\n",
      "epoch 241| loss: 0.50821 |  0:00:12s\n",
      "epoch 242| loss: 0.49526 |  0:00:12s\n",
      "epoch 243| loss: 0.49814 |  0:00:12s\n",
      "epoch 244| loss: 0.49822 |  0:00:12s\n",
      "epoch 245| loss: 0.48569 |  0:00:12s\n",
      "epoch 246| loss: 0.49104 |  0:00:13s\n",
      "epoch 247| loss: 0.49065 |  0:00:13s\n",
      "epoch 248| loss: 0.49474 |  0:00:13s\n",
      "epoch 249| loss: 0.49161 |  0:00:13s\n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.04802 |  0:00:00s\n",
      "epoch 1  | loss: 1.96245 |  0:00:00s\n",
      "epoch 2  | loss: 1.43745 |  0:00:00s\n",
      "epoch 3  | loss: 1.17835 |  0:00:00s\n",
      "epoch 4  | loss: 1.02311 |  0:00:00s\n",
      "epoch 5  | loss: 0.98004 |  0:00:00s\n",
      "epoch 6  | loss: 0.91193 |  0:00:00s\n",
      "epoch 7  | loss: 0.90017 |  0:00:00s\n",
      "epoch 8  | loss: 0.89031 |  0:00:00s\n",
      "epoch 9  | loss: 0.85922 |  0:00:00s\n",
      "epoch 10 | loss: 0.82505 |  0:00:00s\n",
      "epoch 11 | loss: 0.81488 |  0:00:00s\n",
      "epoch 12 | loss: 0.79611 |  0:00:00s\n",
      "epoch 13 | loss: 0.78147 |  0:00:00s\n",
      "epoch 14 | loss: 0.77791 |  0:00:00s\n",
      "epoch 15 | loss: 0.76061 |  0:00:00s\n",
      "epoch 16 | loss: 0.76052 |  0:00:00s\n",
      "epoch 17 | loss: 0.7757  |  0:00:00s\n",
      "epoch 18 | loss: 0.76111 |  0:00:00s\n",
      "epoch 19 | loss: 0.74905 |  0:00:00s\n",
      "epoch 20 | loss: 0.7292  |  0:00:01s\n",
      "epoch 21 | loss: 0.72786 |  0:00:01s\n",
      "epoch 22 | loss: 0.73284 |  0:00:01s\n",
      "epoch 23 | loss: 0.73237 |  0:00:01s\n",
      "epoch 24 | loss: 0.73744 |  0:00:01s\n",
      "epoch 25 | loss: 0.71826 |  0:00:01s\n",
      "epoch 26 | loss: 0.69557 |  0:00:01s\n",
      "epoch 27 | loss: 0.69243 |  0:00:01s\n",
      "epoch 28 | loss: 0.70933 |  0:00:01s\n",
      "epoch 29 | loss: 0.70575 |  0:00:01s\n",
      "epoch 30 | loss: 0.7081  |  0:00:01s\n",
      "epoch 31 | loss: 0.70099 |  0:00:01s\n",
      "epoch 32 | loss: 0.69602 |  0:00:01s\n",
      "epoch 33 | loss: 0.69338 |  0:00:01s\n",
      "epoch 34 | loss: 0.69054 |  0:00:01s\n",
      "epoch 35 | loss: 0.67286 |  0:00:01s\n",
      "epoch 36 | loss: 0.67335 |  0:00:01s\n",
      "epoch 37 | loss: 0.6737  |  0:00:01s\n",
      "epoch 38 | loss: 0.67841 |  0:00:01s\n",
      "epoch 39 | loss: 0.67449 |  0:00:01s\n",
      "epoch 40 | loss: 0.67892 |  0:00:01s\n",
      "epoch 41 | loss: 0.66392 |  0:00:01s\n",
      "epoch 42 | loss: 0.6597  |  0:00:02s\n",
      "epoch 43 | loss: 0.65919 |  0:00:02s\n",
      "epoch 44 | loss: 0.66249 |  0:00:02s\n",
      "epoch 45 | loss: 0.66313 |  0:00:02s\n",
      "epoch 46 | loss: 0.66155 |  0:00:02s\n",
      "epoch 47 | loss: 0.66503 |  0:00:02s\n",
      "epoch 48 | loss: 0.66243 |  0:00:02s\n",
      "epoch 49 | loss: 0.64706 |  0:00:02s\n",
      "epoch 50 | loss: 0.64601 |  0:00:02s\n",
      "epoch 51 | loss: 0.64263 |  0:00:02s\n",
      "epoch 52 | loss: 0.65934 |  0:00:02s\n",
      "epoch 53 | loss: 0.6465  |  0:00:02s\n",
      "epoch 54 | loss: 0.65125 |  0:00:02s\n",
      "epoch 55 | loss: 0.64053 |  0:00:02s\n",
      "epoch 56 | loss: 0.64072 |  0:00:02s\n",
      "epoch 57 | loss: 0.63489 |  0:00:02s\n",
      "epoch 58 | loss: 0.63995 |  0:00:02s\n",
      "epoch 59 | loss: 0.63208 |  0:00:02s\n",
      "epoch 60 | loss: 0.63236 |  0:00:02s\n",
      "epoch 61 | loss: 0.62941 |  0:00:02s\n",
      "epoch 62 | loss: 0.61973 |  0:00:02s\n",
      "epoch 63 | loss: 0.62819 |  0:00:02s\n",
      "epoch 64 | loss: 0.6171  |  0:00:03s\n",
      "epoch 65 | loss: 0.61593 |  0:00:03s\n",
      "epoch 66 | loss: 0.62598 |  0:00:03s\n",
      "epoch 67 | loss: 0.62865 |  0:00:03s\n",
      "epoch 68 | loss: 0.62887 |  0:00:03s\n",
      "epoch 69 | loss: 0.62169 |  0:00:03s\n",
      "epoch 70 | loss: 0.62027 |  0:00:03s\n",
      "epoch 71 | loss: 0.63004 |  0:00:03s\n",
      "epoch 72 | loss: 0.62623 |  0:00:03s\n",
      "epoch 73 | loss: 0.63331 |  0:00:03s\n",
      "epoch 74 | loss: 0.62278 |  0:00:03s\n",
      "epoch 75 | loss: 0.63054 |  0:00:03s\n",
      "epoch 76 | loss: 0.63339 |  0:00:03s\n",
      "epoch 77 | loss: 0.62981 |  0:00:03s\n",
      "epoch 78 | loss: 0.61828 |  0:00:03s\n",
      "epoch 79 | loss: 0.61285 |  0:00:03s\n",
      "epoch 80 | loss: 0.60686 |  0:00:03s\n",
      "epoch 81 | loss: 0.59884 |  0:00:03s\n",
      "epoch 82 | loss: 0.59988 |  0:00:03s\n",
      "epoch 83 | loss: 0.61095 |  0:00:03s\n",
      "epoch 84 | loss: 0.60729 |  0:00:03s\n",
      "epoch 85 | loss: 0.60249 |  0:00:03s\n",
      "epoch 86 | loss: 0.59445 |  0:00:04s\n",
      "epoch 87 | loss: 0.5997  |  0:00:04s\n",
      "epoch 88 | loss: 0.5909  |  0:00:04s\n",
      "epoch 89 | loss: 0.58729 |  0:00:04s\n",
      "epoch 90 | loss: 0.60534 |  0:00:04s\n",
      "epoch 91 | loss: 0.59922 |  0:00:04s\n",
      "epoch 92 | loss: 0.59116 |  0:00:04s\n",
      "epoch 93 | loss: 0.59976 |  0:00:04s\n",
      "epoch 94 | loss: 0.59251 |  0:00:04s\n",
      "epoch 95 | loss: 0.58133 |  0:00:04s\n",
      "epoch 96 | loss: 0.58135 |  0:00:04s\n",
      "epoch 97 | loss: 0.57944 |  0:00:04s\n",
      "epoch 98 | loss: 0.57511 |  0:00:04s\n",
      "epoch 99 | loss: 0.57659 |  0:00:04s\n",
      "epoch 100| loss: 0.57375 |  0:00:04s\n",
      "epoch 101| loss: 0.56981 |  0:00:04s\n",
      "epoch 102| loss: 0.57213 |  0:00:04s\n",
      "epoch 103| loss: 0.56389 |  0:00:04s\n",
      "epoch 104| loss: 0.56057 |  0:00:04s\n",
      "epoch 105| loss: 0.56363 |  0:00:04s\n",
      "epoch 106| loss: 0.56025 |  0:00:04s\n",
      "epoch 107| loss: 0.55473 |  0:00:04s\n",
      "epoch 108| loss: 0.55522 |  0:00:04s\n",
      "epoch 109| loss: 0.54814 |  0:00:05s\n",
      "epoch 110| loss: 0.5449  |  0:00:05s\n",
      "epoch 111| loss: 0.54795 |  0:00:05s\n",
      "epoch 112| loss: 0.55078 |  0:00:05s\n",
      "epoch 113| loss: 0.54533 |  0:00:05s\n",
      "epoch 114| loss: 0.54399 |  0:00:05s\n",
      "epoch 115| loss: 0.54052 |  0:00:05s\n",
      "epoch 116| loss: 0.53646 |  0:00:05s\n",
      "epoch 117| loss: 0.53463 |  0:00:05s\n",
      "epoch 118| loss: 0.54233 |  0:00:05s\n",
      "epoch 119| loss: 0.53271 |  0:00:05s\n",
      "epoch 120| loss: 0.53609 |  0:00:05s\n",
      "epoch 121| loss: 0.52416 |  0:00:05s\n",
      "epoch 122| loss: 0.52818 |  0:00:05s\n",
      "epoch 123| loss: 0.52321 |  0:00:05s\n",
      "epoch 124| loss: 0.52159 |  0:00:05s\n",
      "epoch 125| loss: 0.52221 |  0:00:05s\n",
      "epoch 126| loss: 0.52664 |  0:00:05s\n",
      "epoch 127| loss: 0.52137 |  0:00:05s\n",
      "epoch 128| loss: 0.52227 |  0:00:05s\n",
      "epoch 129| loss: 0.52187 |  0:00:05s\n",
      "epoch 130| loss: 0.52282 |  0:00:05s\n",
      "epoch 131| loss: 0.52107 |  0:00:06s\n",
      "epoch 132| loss: 0.51523 |  0:00:06s\n",
      "epoch 133| loss: 0.53029 |  0:00:06s\n",
      "epoch 134| loss: 0.53021 |  0:00:06s\n",
      "epoch 135| loss: 0.52827 |  0:00:06s\n",
      "epoch 136| loss: 0.52504 |  0:00:06s\n",
      "epoch 137| loss: 0.5195  |  0:00:06s\n",
      "epoch 138| loss: 0.51679 |  0:00:06s\n",
      "epoch 139| loss: 0.51731 |  0:00:06s\n",
      "epoch 140| loss: 0.50611 |  0:00:06s\n",
      "epoch 141| loss: 0.51558 |  0:00:06s\n",
      "epoch 142| loss: 0.50294 |  0:00:06s\n",
      "epoch 143| loss: 0.5078  |  0:00:06s\n",
      "epoch 144| loss: 0.49747 |  0:00:06s\n",
      "epoch 145| loss: 0.50747 |  0:00:06s\n",
      "epoch 146| loss: 0.50741 |  0:00:06s\n",
      "epoch 147| loss: 0.50787 |  0:00:06s\n",
      "epoch 148| loss: 0.50851 |  0:00:06s\n",
      "epoch 149| loss: 0.50086 |  0:00:06s\n",
      "epoch 150| loss: 0.50291 |  0:00:06s\n",
      "epoch 151| loss: 0.49833 |  0:00:06s\n",
      "epoch 152| loss: 0.5058  |  0:00:06s\n",
      "epoch 153| loss: 0.50524 |  0:00:07s\n",
      "epoch 154| loss: 0.50051 |  0:00:07s\n",
      "epoch 155| loss: 0.49623 |  0:00:07s\n",
      "epoch 156| loss: 0.50037 |  0:00:07s\n",
      "epoch 157| loss: 0.49428 |  0:00:07s\n",
      "epoch 158| loss: 0.49138 |  0:00:07s\n",
      "epoch 159| loss: 0.48619 |  0:00:07s\n",
      "epoch 160| loss: 0.4918  |  0:00:07s\n",
      "epoch 161| loss: 0.48973 |  0:00:07s\n",
      "epoch 162| loss: 0.49952 |  0:00:07s\n",
      "epoch 163| loss: 0.50414 |  0:00:07s\n",
      "epoch 164| loss: 0.49925 |  0:00:07s\n",
      "epoch 165| loss: 0.50164 |  0:00:07s\n",
      "epoch 166| loss: 0.49672 |  0:00:07s\n",
      "epoch 167| loss: 0.4923  |  0:00:07s\n",
      "epoch 168| loss: 0.48924 |  0:00:07s\n",
      "epoch 169| loss: 0.47552 |  0:00:07s\n",
      "epoch 170| loss: 0.48178 |  0:00:07s\n",
      "epoch 171| loss: 0.48007 |  0:00:07s\n",
      "epoch 172| loss: 0.48446 |  0:00:07s\n",
      "epoch 173| loss: 0.47945 |  0:00:08s\n",
      "epoch 174| loss: 0.4819  |  0:00:08s\n",
      "epoch 175| loss: 0.47339 |  0:00:08s\n",
      "epoch 176| loss: 0.47075 |  0:00:08s\n",
      "epoch 177| loss: 0.46627 |  0:00:08s\n",
      "epoch 178| loss: 0.46287 |  0:00:08s\n",
      "epoch 179| loss: 0.45642 |  0:00:08s\n",
      "epoch 180| loss: 0.46846 |  0:00:08s\n",
      "epoch 181| loss: 0.48667 |  0:00:08s\n",
      "epoch 182| loss: 0.48763 |  0:00:08s\n",
      "epoch 183| loss: 0.4736  |  0:00:08s\n",
      "epoch 184| loss: 0.47371 |  0:00:08s\n",
      "epoch 185| loss: 0.47659 |  0:00:08s\n",
      "epoch 186| loss: 0.47311 |  0:00:08s\n",
      "epoch 187| loss: 0.47301 |  0:00:08s\n",
      "epoch 188| loss: 0.47489 |  0:00:08s\n",
      "epoch 189| loss: 0.47794 |  0:00:08s\n",
      "epoch 190| loss: 0.46907 |  0:00:08s\n",
      "epoch 191| loss: 0.46703 |  0:00:08s\n",
      "epoch 192| loss: 0.4644  |  0:00:08s\n",
      "epoch 193| loss: 0.46602 |  0:00:09s\n",
      "epoch 194| loss: 0.46378 |  0:00:09s\n",
      "epoch 195| loss: 0.46571 |  0:00:09s\n",
      "epoch 196| loss: 0.46598 |  0:00:09s\n",
      "epoch 197| loss: 0.46383 |  0:00:09s\n",
      "epoch 198| loss: 0.46765 |  0:00:09s\n",
      "epoch 199| loss: 0.4675  |  0:00:09s\n",
      "epoch 200| loss: 0.46081 |  0:00:09s\n",
      "epoch 201| loss: 0.45621 |  0:00:09s\n",
      "epoch 202| loss: 0.45715 |  0:00:09s\n",
      "epoch 203| loss: 0.46097 |  0:00:09s\n",
      "epoch 204| loss: 0.46078 |  0:00:09s\n",
      "epoch 205| loss: 0.44551 |  0:00:09s\n",
      "epoch 206| loss: 0.44429 |  0:00:09s\n",
      "epoch 207| loss: 0.45009 |  0:00:09s\n",
      "epoch 208| loss: 0.44409 |  0:00:09s\n",
      "epoch 209| loss: 0.44445 |  0:00:09s\n",
      "epoch 210| loss: 0.45138 |  0:00:09s\n",
      "epoch 211| loss: 0.44076 |  0:00:10s\n",
      "epoch 212| loss: 0.4354  |  0:00:10s\n",
      "epoch 213| loss: 0.44112 |  0:00:10s\n",
      "epoch 214| loss: 0.44724 |  0:00:10s\n",
      "epoch 215| loss: 0.44079 |  0:00:10s\n",
      "epoch 216| loss: 0.43296 |  0:00:10s\n",
      "epoch 217| loss: 0.42967 |  0:00:10s\n",
      "epoch 218| loss: 0.43509 |  0:00:10s\n",
      "epoch 219| loss: 0.42863 |  0:00:10s\n",
      "epoch 220| loss: 0.42731 |  0:00:10s\n",
      "epoch 221| loss: 0.42794 |  0:00:10s\n",
      "epoch 222| loss: 0.42435 |  0:00:10s\n",
      "epoch 223| loss: 0.42026 |  0:00:10s\n",
      "epoch 224| loss: 0.41734 |  0:00:10s\n",
      "epoch 225| loss: 0.42436 |  0:00:10s\n",
      "epoch 226| loss: 0.43193 |  0:00:10s\n",
      "epoch 227| loss: 0.42449 |  0:00:10s\n",
      "epoch 228| loss: 0.42837 |  0:00:10s\n",
      "epoch 229| loss: 0.42764 |  0:00:10s\n",
      "epoch 230| loss: 0.42521 |  0:00:10s\n",
      "epoch 231| loss: 0.42121 |  0:00:10s\n",
      "epoch 232| loss: 0.42779 |  0:00:11s\n",
      "epoch 233| loss: 0.4299  |  0:00:11s\n",
      "epoch 234| loss: 0.42367 |  0:00:11s\n",
      "epoch 235| loss: 0.41375 |  0:00:11s\n",
      "epoch 236| loss: 0.40936 |  0:00:11s\n",
      "epoch 237| loss: 0.41427 |  0:00:11s\n",
      "epoch 238| loss: 0.41916 |  0:00:11s\n",
      "epoch 239| loss: 0.41623 |  0:00:11s\n",
      "epoch 240| loss: 0.40991 |  0:00:11s\n",
      "epoch 241| loss: 0.42138 |  0:00:11s\n",
      "epoch 242| loss: 0.42104 |  0:00:11s\n",
      "epoch 243| loss: 0.41291 |  0:00:11s\n",
      "epoch 244| loss: 0.41524 |  0:00:11s\n",
      "epoch 245| loss: 0.40805 |  0:00:11s\n",
      "epoch 246| loss: 0.40957 |  0:00:11s\n",
      "epoch 247| loss: 0.4122  |  0:00:11s\n",
      "epoch 248| loss: 0.41664 |  0:00:11s\n",
      "epoch 249| loss: 0.41529 |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.0438  |  0:00:00s\n",
      "epoch 1  | loss: 1.68682 |  0:00:00s\n",
      "epoch 2  | loss: 1.26963 |  0:00:00s\n",
      "epoch 3  | loss: 1.0811  |  0:00:00s\n",
      "epoch 4  | loss: 1.00416 |  0:00:00s\n",
      "epoch 5  | loss: 0.94922 |  0:00:00s\n",
      "epoch 6  | loss: 0.9273  |  0:00:00s\n",
      "epoch 7  | loss: 0.89328 |  0:00:00s\n",
      "epoch 8  | loss: 0.87824 |  0:00:00s\n",
      "epoch 9  | loss: 0.85858 |  0:00:00s\n",
      "epoch 10 | loss: 0.82651 |  0:00:00s\n",
      "epoch 11 | loss: 0.81114 |  0:00:00s\n",
      "epoch 12 | loss: 0.79213 |  0:00:00s\n",
      "epoch 13 | loss: 0.77066 |  0:00:00s\n",
      "epoch 14 | loss: 0.75722 |  0:00:00s\n",
      "epoch 15 | loss: 0.72664 |  0:00:00s\n",
      "epoch 16 | loss: 0.73224 |  0:00:00s\n",
      "epoch 17 | loss: 0.71235 |  0:00:00s\n",
      "epoch 18 | loss: 0.70642 |  0:00:00s\n",
      "epoch 19 | loss: 0.7084  |  0:00:00s\n",
      "epoch 20 | loss: 0.71803 |  0:00:01s\n",
      "epoch 21 | loss: 0.69845 |  0:00:01s\n",
      "epoch 22 | loss: 0.6967  |  0:00:01s\n",
      "epoch 23 | loss: 0.69244 |  0:00:01s\n",
      "epoch 24 | loss: 0.69052 |  0:00:01s\n",
      "epoch 25 | loss: 0.69076 |  0:00:01s\n",
      "epoch 26 | loss: 0.68611 |  0:00:01s\n",
      "epoch 27 | loss: 0.67573 |  0:00:01s\n",
      "epoch 28 | loss: 0.68081 |  0:00:01s\n",
      "epoch 29 | loss: 0.67264 |  0:00:01s\n",
      "epoch 30 | loss: 0.67089 |  0:00:01s\n",
      "epoch 31 | loss: 0.6604  |  0:00:01s\n",
      "epoch 32 | loss: 0.6607  |  0:00:01s\n",
      "epoch 33 | loss: 0.66675 |  0:00:01s\n",
      "epoch 34 | loss: 0.66168 |  0:00:01s\n",
      "epoch 35 | loss: 0.65379 |  0:00:01s\n",
      "epoch 36 | loss: 0.64863 |  0:00:01s\n",
      "epoch 37 | loss: 0.64227 |  0:00:01s\n",
      "epoch 38 | loss: 0.63337 |  0:00:01s\n",
      "epoch 39 | loss: 0.63908 |  0:00:01s\n",
      "epoch 40 | loss: 0.63375 |  0:00:01s\n",
      "epoch 41 | loss: 0.63076 |  0:00:02s\n",
      "epoch 42 | loss: 0.62693 |  0:00:02s\n",
      "epoch 43 | loss: 0.6173  |  0:00:02s\n",
      "epoch 44 | loss: 0.62921 |  0:00:02s\n",
      "epoch 45 | loss: 0.61283 |  0:00:02s\n",
      "epoch 46 | loss: 0.61837 |  0:00:02s\n",
      "epoch 47 | loss: 0.61716 |  0:00:02s\n",
      "epoch 48 | loss: 0.61143 |  0:00:02s\n",
      "epoch 49 | loss: 0.61021 |  0:00:02s\n",
      "epoch 50 | loss: 0.60256 |  0:00:02s\n",
      "epoch 51 | loss: 0.59988 |  0:00:02s\n",
      "epoch 52 | loss: 0.59456 |  0:00:02s\n",
      "epoch 53 | loss: 0.59286 |  0:00:02s\n",
      "epoch 54 | loss: 0.58768 |  0:00:02s\n",
      "epoch 55 | loss: 0.58213 |  0:00:02s\n",
      "epoch 56 | loss: 0.58519 |  0:00:02s\n",
      "epoch 57 | loss: 0.58962 |  0:00:02s\n",
      "epoch 58 | loss: 0.5703  |  0:00:02s\n",
      "epoch 59 | loss: 0.57571 |  0:00:03s\n",
      "epoch 60 | loss: 0.57897 |  0:00:03s\n",
      "epoch 61 | loss: 0.57518 |  0:00:03s\n",
      "epoch 62 | loss: 0.57422 |  0:00:03s\n",
      "epoch 63 | loss: 0.5688  |  0:00:03s\n",
      "epoch 64 | loss: 0.56551 |  0:00:03s\n",
      "epoch 65 | loss: 0.56282 |  0:00:03s\n",
      "epoch 66 | loss: 0.56855 |  0:00:03s\n",
      "epoch 67 | loss: 0.55892 |  0:00:03s\n",
      "epoch 68 | loss: 0.55467 |  0:00:03s\n",
      "epoch 69 | loss: 0.55181 |  0:00:03s\n",
      "epoch 70 | loss: 0.5564  |  0:00:03s\n",
      "epoch 71 | loss: 0.55296 |  0:00:03s\n",
      "epoch 72 | loss: 0.55082 |  0:00:03s\n",
      "epoch 73 | loss: 0.53959 |  0:00:03s\n",
      "epoch 74 | loss: 0.54297 |  0:00:03s\n",
      "epoch 75 | loss: 0.53963 |  0:00:03s\n",
      "epoch 76 | loss: 0.52896 |  0:00:03s\n",
      "epoch 77 | loss: 0.5347  |  0:00:03s\n",
      "epoch 78 | loss: 0.52673 |  0:00:03s\n",
      "epoch 79 | loss: 0.52306 |  0:00:03s\n",
      "epoch 80 | loss: 0.52607 |  0:00:03s\n",
      "epoch 81 | loss: 0.51677 |  0:00:04s\n",
      "epoch 82 | loss: 0.51697 |  0:00:04s\n",
      "epoch 83 | loss: 0.52148 |  0:00:04s\n",
      "epoch 84 | loss: 0.51737 |  0:00:04s\n",
      "epoch 85 | loss: 0.51168 |  0:00:04s\n",
      "epoch 86 | loss: 0.50454 |  0:00:04s\n",
      "epoch 87 | loss: 0.50508 |  0:00:04s\n",
      "epoch 88 | loss: 0.5052  |  0:00:04s\n",
      "epoch 89 | loss: 0.51144 |  0:00:04s\n",
      "epoch 90 | loss: 0.5049  |  0:00:04s\n",
      "epoch 91 | loss: 0.49907 |  0:00:04s\n",
      "epoch 92 | loss: 0.49031 |  0:00:04s\n",
      "epoch 93 | loss: 0.49822 |  0:00:04s\n",
      "epoch 94 | loss: 0.49233 |  0:00:04s\n",
      "epoch 95 | loss: 0.47966 |  0:00:04s\n",
      "epoch 96 | loss: 0.48141 |  0:00:04s\n",
      "epoch 97 | loss: 0.48648 |  0:00:04s\n",
      "epoch 98 | loss: 0.47822 |  0:00:04s\n",
      "epoch 99 | loss: 0.47527 |  0:00:04s\n",
      "epoch 100| loss: 0.47973 |  0:00:04s\n",
      "epoch 101| loss: 0.47864 |  0:00:04s\n",
      "epoch 102| loss: 0.47502 |  0:00:04s\n",
      "epoch 103| loss: 0.46723 |  0:00:05s\n",
      "epoch 104| loss: 0.47263 |  0:00:05s\n",
      "epoch 105| loss: 0.47067 |  0:00:05s\n",
      "epoch 106| loss: 0.46614 |  0:00:05s\n",
      "epoch 107| loss: 0.46901 |  0:00:05s\n",
      "epoch 108| loss: 0.46923 |  0:00:05s\n",
      "epoch 109| loss: 0.46504 |  0:00:05s\n",
      "epoch 110| loss: 0.46797 |  0:00:05s\n",
      "epoch 111| loss: 0.47386 |  0:00:05s\n",
      "epoch 112| loss: 0.476   |  0:00:05s\n",
      "epoch 113| loss: 0.47114 |  0:00:05s\n",
      "epoch 114| loss: 0.45736 |  0:00:05s\n",
      "epoch 115| loss: 0.46662 |  0:00:05s\n",
      "epoch 116| loss: 0.46305 |  0:00:05s\n",
      "epoch 117| loss: 0.46131 |  0:00:05s\n",
      "epoch 118| loss: 0.45447 |  0:00:05s\n",
      "epoch 119| loss: 0.45761 |  0:00:05s\n",
      "epoch 120| loss: 0.45525 |  0:00:05s\n",
      "epoch 121| loss: 0.45149 |  0:00:05s\n",
      "epoch 122| loss: 0.44733 |  0:00:05s\n",
      "epoch 123| loss: 0.45134 |  0:00:05s\n",
      "epoch 124| loss: 0.44608 |  0:00:06s\n",
      "epoch 125| loss: 0.44955 |  0:00:06s\n",
      "epoch 126| loss: 0.43727 |  0:00:06s\n",
      "epoch 127| loss: 0.44018 |  0:00:06s\n",
      "epoch 128| loss: 0.44062 |  0:00:06s\n",
      "epoch 129| loss: 0.4443  |  0:00:06s\n",
      "epoch 130| loss: 0.44592 |  0:00:06s\n",
      "epoch 131| loss: 0.44305 |  0:00:06s\n",
      "epoch 132| loss: 0.43232 |  0:00:06s\n",
      "epoch 133| loss: 0.43222 |  0:00:06s\n",
      "epoch 134| loss: 0.43709 |  0:00:06s\n",
      "epoch 135| loss: 0.43798 |  0:00:06s\n",
      "epoch 136| loss: 0.43429 |  0:00:06s\n",
      "epoch 137| loss: 0.43036 |  0:00:06s\n",
      "epoch 138| loss: 0.42457 |  0:00:06s\n",
      "epoch 139| loss: 0.43432 |  0:00:06s\n",
      "epoch 140| loss: 0.42341 |  0:00:06s\n",
      "epoch 141| loss: 0.44992 |  0:00:06s\n",
      "epoch 142| loss: 0.43868 |  0:00:06s\n",
      "epoch 143| loss: 0.44453 |  0:00:06s\n",
      "epoch 144| loss: 0.43641 |  0:00:06s\n",
      "epoch 145| loss: 0.42758 |  0:00:06s\n",
      "epoch 146| loss: 0.42195 |  0:00:06s\n",
      "epoch 147| loss: 0.41954 |  0:00:07s\n",
      "epoch 148| loss: 0.42274 |  0:00:07s\n",
      "epoch 149| loss: 0.41666 |  0:00:07s\n",
      "epoch 150| loss: 0.41513 |  0:00:07s\n",
      "epoch 151| loss: 0.41577 |  0:00:07s\n",
      "epoch 152| loss: 0.41127 |  0:00:07s\n",
      "epoch 153| loss: 0.40822 |  0:00:07s\n",
      "epoch 154| loss: 0.4077  |  0:00:07s\n",
      "epoch 155| loss: 0.40032 |  0:00:07s\n",
      "epoch 156| loss: 0.39974 |  0:00:07s\n",
      "epoch 157| loss: 0.40574 |  0:00:07s\n",
      "epoch 158| loss: 0.40301 |  0:00:07s\n",
      "epoch 159| loss: 0.40123 |  0:00:07s\n",
      "epoch 160| loss: 0.40033 |  0:00:07s\n",
      "epoch 161| loss: 0.40409 |  0:00:07s\n",
      "epoch 162| loss: 0.4171  |  0:00:07s\n",
      "epoch 163| loss: 0.40301 |  0:00:07s\n",
      "epoch 164| loss: 0.39817 |  0:00:07s\n",
      "epoch 165| loss: 0.40705 |  0:00:07s\n",
      "epoch 166| loss: 0.40883 |  0:00:07s\n",
      "epoch 167| loss: 0.39497 |  0:00:07s\n",
      "epoch 168| loss: 0.41308 |  0:00:07s\n",
      "epoch 169| loss: 0.3888  |  0:00:08s\n",
      "epoch 170| loss: 0.39084 |  0:00:08s\n",
      "epoch 171| loss: 0.39093 |  0:00:08s\n",
      "epoch 172| loss: 0.38804 |  0:00:08s\n",
      "epoch 173| loss: 0.38451 |  0:00:08s\n",
      "epoch 174| loss: 0.38961 |  0:00:08s\n",
      "epoch 175| loss: 0.38948 |  0:00:08s\n",
      "epoch 176| loss: 0.38809 |  0:00:08s\n",
      "epoch 177| loss: 0.38267 |  0:00:08s\n",
      "epoch 178| loss: 0.38184 |  0:00:08s\n",
      "epoch 179| loss: 0.3775  |  0:00:08s\n",
      "epoch 180| loss: 0.37503 |  0:00:08s\n",
      "epoch 181| loss: 0.38574 |  0:00:08s\n",
      "epoch 182| loss: 0.38101 |  0:00:08s\n",
      "epoch 183| loss: 0.3736  |  0:00:08s\n",
      "epoch 184| loss: 0.37761 |  0:00:08s\n",
      "epoch 185| loss: 0.3791  |  0:00:08s\n",
      "epoch 186| loss: 0.37885 |  0:00:08s\n",
      "epoch 187| loss: 0.38023 |  0:00:08s\n",
      "epoch 188| loss: 0.37819 |  0:00:08s\n",
      "epoch 189| loss: 0.38206 |  0:00:08s\n",
      "epoch 190| loss: 0.38035 |  0:00:08s\n",
      "epoch 191| loss: 0.37566 |  0:00:08s\n",
      "epoch 192| loss: 0.37336 |  0:00:09s\n",
      "epoch 193| loss: 0.37455 |  0:00:09s\n",
      "epoch 194| loss: 0.37554 |  0:00:09s\n",
      "epoch 195| loss: 0.37034 |  0:00:09s\n",
      "epoch 196| loss: 0.37866 |  0:00:09s\n",
      "epoch 197| loss: 0.38039 |  0:00:09s\n",
      "epoch 198| loss: 0.36247 |  0:00:09s\n",
      "epoch 199| loss: 0.36816 |  0:00:09s\n",
      "epoch 200| loss: 0.36774 |  0:00:09s\n",
      "epoch 201| loss: 0.364   |  0:00:09s\n",
      "epoch 202| loss: 0.35853 |  0:00:09s\n",
      "epoch 203| loss: 0.36319 |  0:00:09s\n",
      "epoch 204| loss: 0.36435 |  0:00:09s\n",
      "epoch 205| loss: 0.35621 |  0:00:09s\n",
      "epoch 206| loss: 0.35702 |  0:00:09s\n",
      "epoch 207| loss: 0.36911 |  0:00:09s\n",
      "epoch 208| loss: 0.36327 |  0:00:09s\n",
      "epoch 209| loss: 0.35865 |  0:00:09s\n",
      "epoch 210| loss: 0.36232 |  0:00:09s\n",
      "epoch 211| loss: 0.35324 |  0:00:09s\n",
      "epoch 212| loss: 0.35109 |  0:00:09s\n",
      "epoch 213| loss: 0.34788 |  0:00:09s\n",
      "epoch 214| loss: 0.35722 |  0:00:10s\n",
      "epoch 215| loss: 0.34915 |  0:00:10s\n",
      "epoch 216| loss: 0.35394 |  0:00:10s\n",
      "epoch 217| loss: 0.35171 |  0:00:10s\n",
      "epoch 218| loss: 0.35282 |  0:00:10s\n",
      "epoch 219| loss: 0.35151 |  0:00:10s\n",
      "epoch 220| loss: 0.35188 |  0:00:10s\n",
      "epoch 221| loss: 0.35426 |  0:00:10s\n",
      "epoch 222| loss: 0.34743 |  0:00:10s\n",
      "epoch 223| loss: 0.35709 |  0:00:10s\n",
      "epoch 224| loss: 0.34916 |  0:00:10s\n",
      "epoch 225| loss: 0.35739 |  0:00:10s\n",
      "epoch 226| loss: 0.34937 |  0:00:10s\n",
      "epoch 227| loss: 0.35477 |  0:00:10s\n",
      "epoch 228| loss: 0.34908 |  0:00:10s\n",
      "epoch 229| loss: 0.34549 |  0:00:10s\n",
      "epoch 230| loss: 0.35024 |  0:00:10s\n",
      "epoch 231| loss: 0.34476 |  0:00:10s\n",
      "epoch 232| loss: 0.34545 |  0:00:10s\n",
      "epoch 233| loss: 0.34635 |  0:00:11s\n",
      "epoch 234| loss: 0.35035 |  0:00:11s\n",
      "epoch 235| loss: 0.34174 |  0:00:11s\n",
      "epoch 236| loss: 0.35039 |  0:00:11s\n",
      "epoch 237| loss: 0.34737 |  0:00:11s\n",
      "epoch 238| loss: 0.35712 |  0:00:11s\n",
      "epoch 239| loss: 0.35542 |  0:00:11s\n",
      "epoch 240| loss: 0.34612 |  0:00:11s\n",
      "epoch 241| loss: 0.34555 |  0:00:11s\n",
      "epoch 242| loss: 0.34735 |  0:00:11s\n",
      "epoch 243| loss: 0.33605 |  0:00:11s\n",
      "epoch 244| loss: 0.33896 |  0:00:11s\n",
      "epoch 245| loss: 0.33301 |  0:00:11s\n",
      "epoch 246| loss: 0.33641 |  0:00:11s\n",
      "epoch 247| loss: 0.3383  |  0:00:11s\n",
      "epoch 248| loss: 0.33389 |  0:00:11s\n",
      "epoch 249| loss: 0.33308 |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.87916 |  0:00:00s\n",
      "epoch 1  | loss: 1.89941 |  0:00:00s\n",
      "epoch 2  | loss: 1.32885 |  0:00:00s\n",
      "epoch 3  | loss: 1.13831 |  0:00:00s\n",
      "epoch 4  | loss: 1.00451 |  0:00:00s\n",
      "epoch 5  | loss: 0.97478 |  0:00:00s\n",
      "epoch 6  | loss: 0.88894 |  0:00:00s\n",
      "epoch 7  | loss: 0.84133 |  0:00:00s\n",
      "epoch 8  | loss: 0.80054 |  0:00:00s\n",
      "epoch 9  | loss: 0.78698 |  0:00:00s\n",
      "epoch 10 | loss: 0.77378 |  0:00:00s\n",
      "epoch 11 | loss: 0.74662 |  0:00:00s\n",
      "epoch 12 | loss: 0.73209 |  0:00:00s\n",
      "epoch 13 | loss: 0.71208 |  0:00:00s\n",
      "epoch 14 | loss: 0.70227 |  0:00:00s\n",
      "epoch 15 | loss: 0.69499 |  0:00:00s\n",
      "epoch 16 | loss: 0.70427 |  0:00:00s\n",
      "epoch 17 | loss: 0.70013 |  0:00:00s\n",
      "epoch 18 | loss: 0.6782  |  0:00:00s\n",
      "epoch 19 | loss: 0.66775 |  0:00:00s\n",
      "epoch 20 | loss: 0.67181 |  0:00:00s\n",
      "epoch 21 | loss: 0.64883 |  0:00:01s\n",
      "epoch 22 | loss: 0.64862 |  0:00:01s\n",
      "epoch 23 | loss: 0.62953 |  0:00:01s\n",
      "epoch 24 | loss: 0.63343 |  0:00:01s\n",
      "epoch 25 | loss: 0.62587 |  0:00:01s\n",
      "epoch 26 | loss: 0.63434 |  0:00:01s\n",
      "epoch 27 | loss: 0.63364 |  0:00:01s\n",
      "epoch 28 | loss: 0.63293 |  0:00:01s\n",
      "epoch 29 | loss: 0.61758 |  0:00:01s\n",
      "epoch 30 | loss: 0.61742 |  0:00:01s\n",
      "epoch 31 | loss: 0.60265 |  0:00:01s\n",
      "epoch 32 | loss: 0.59847 |  0:00:01s\n",
      "epoch 33 | loss: 0.58972 |  0:00:01s\n",
      "epoch 34 | loss: 0.60173 |  0:00:01s\n",
      "epoch 35 | loss: 0.59251 |  0:00:01s\n",
      "epoch 36 | loss: 0.58939 |  0:00:01s\n",
      "epoch 37 | loss: 0.59188 |  0:00:01s\n",
      "epoch 38 | loss: 0.58731 |  0:00:01s\n",
      "epoch 39 | loss: 0.57413 |  0:00:01s\n",
      "epoch 40 | loss: 0.58545 |  0:00:01s\n",
      "epoch 41 | loss: 0.57873 |  0:00:01s\n",
      "epoch 42 | loss: 0.58935 |  0:00:01s\n",
      "epoch 43 | loss: 0.5929  |  0:00:02s\n",
      "epoch 44 | loss: 0.58178 |  0:00:02s\n",
      "epoch 45 | loss: 0.57749 |  0:00:02s\n",
      "epoch 46 | loss: 0.56727 |  0:00:02s\n",
      "epoch 47 | loss: 0.5677  |  0:00:02s\n",
      "epoch 48 | loss: 0.56725 |  0:00:02s\n",
      "epoch 49 | loss: 0.56814 |  0:00:02s\n",
      "epoch 50 | loss: 0.57489 |  0:00:02s\n",
      "epoch 51 | loss: 0.56162 |  0:00:02s\n",
      "epoch 52 | loss: 0.54996 |  0:00:02s\n",
      "epoch 53 | loss: 0.54599 |  0:00:02s\n",
      "epoch 54 | loss: 0.54193 |  0:00:02s\n",
      "epoch 55 | loss: 0.54651 |  0:00:02s\n",
      "epoch 56 | loss: 0.54013 |  0:00:02s\n",
      "epoch 57 | loss: 0.53363 |  0:00:02s\n",
      "epoch 58 | loss: 0.53193 |  0:00:02s\n",
      "epoch 59 | loss: 0.52998 |  0:00:02s\n",
      "epoch 60 | loss: 0.53463 |  0:00:02s\n",
      "epoch 61 | loss: 0.52162 |  0:00:02s\n",
      "epoch 62 | loss: 0.51348 |  0:00:02s\n",
      "epoch 63 | loss: 0.52097 |  0:00:02s\n",
      "epoch 64 | loss: 0.50827 |  0:00:02s\n",
      "epoch 65 | loss: 0.50956 |  0:00:03s\n",
      "epoch 66 | loss: 0.50229 |  0:00:03s\n",
      "epoch 67 | loss: 0.49198 |  0:00:03s\n",
      "epoch 68 | loss: 0.49479 |  0:00:03s\n",
      "epoch 69 | loss: 0.48414 |  0:00:03s\n",
      "epoch 70 | loss: 0.48613 |  0:00:03s\n",
      "epoch 71 | loss: 0.48555 |  0:00:03s\n",
      "epoch 72 | loss: 0.47623 |  0:00:03s\n",
      "epoch 73 | loss: 0.47752 |  0:00:03s\n",
      "epoch 74 | loss: 0.4767  |  0:00:03s\n",
      "epoch 75 | loss: 0.46461 |  0:00:03s\n",
      "epoch 76 | loss: 0.46568 |  0:00:03s\n",
      "epoch 77 | loss: 0.4641  |  0:00:03s\n",
      "epoch 78 | loss: 0.45686 |  0:00:03s\n",
      "epoch 79 | loss: 0.44874 |  0:00:03s\n",
      "epoch 80 | loss: 0.44682 |  0:00:03s\n",
      "epoch 81 | loss: 0.44711 |  0:00:03s\n",
      "epoch 82 | loss: 0.44391 |  0:00:03s\n",
      "epoch 83 | loss: 0.43913 |  0:00:03s\n",
      "epoch 84 | loss: 0.43868 |  0:00:03s\n",
      "epoch 85 | loss: 0.44357 |  0:00:03s\n",
      "epoch 86 | loss: 0.44098 |  0:00:03s\n",
      "epoch 87 | loss: 0.44193 |  0:00:04s\n",
      "epoch 88 | loss: 0.44911 |  0:00:04s\n",
      "epoch 89 | loss: 0.44011 |  0:00:04s\n",
      "epoch 90 | loss: 0.42777 |  0:00:04s\n",
      "epoch 91 | loss: 0.42954 |  0:00:04s\n",
      "epoch 92 | loss: 0.43668 |  0:00:04s\n",
      "epoch 93 | loss: 0.42445 |  0:00:04s\n",
      "epoch 94 | loss: 0.41821 |  0:00:04s\n",
      "epoch 95 | loss: 0.42296 |  0:00:04s\n",
      "epoch 96 | loss: 0.42848 |  0:00:04s\n",
      "epoch 97 | loss: 0.4219  |  0:00:04s\n",
      "epoch 98 | loss: 0.43061 |  0:00:04s\n",
      "epoch 99 | loss: 0.42341 |  0:00:04s\n",
      "epoch 100| loss: 0.41751 |  0:00:04s\n",
      "epoch 101| loss: 0.41966 |  0:00:04s\n",
      "epoch 102| loss: 0.42346 |  0:00:04s\n",
      "epoch 103| loss: 0.43112 |  0:00:04s\n",
      "epoch 104| loss: 0.43976 |  0:00:05s\n",
      "epoch 105| loss: 0.43918 |  0:00:05s\n",
      "epoch 106| loss: 0.44175 |  0:00:05s\n",
      "epoch 107| loss: 0.43097 |  0:00:05s\n",
      "epoch 108| loss: 0.43372 |  0:00:05s\n",
      "epoch 109| loss: 0.44226 |  0:00:05s\n",
      "epoch 110| loss: 0.44578 |  0:00:05s\n",
      "epoch 111| loss: 0.43038 |  0:00:05s\n",
      "epoch 112| loss: 0.42173 |  0:00:05s\n",
      "epoch 113| loss: 0.41666 |  0:00:05s\n",
      "epoch 114| loss: 0.41325 |  0:00:05s\n",
      "epoch 115| loss: 0.40993 |  0:00:05s\n",
      "epoch 116| loss: 0.40621 |  0:00:05s\n",
      "epoch 117| loss: 0.40363 |  0:00:05s\n",
      "epoch 118| loss: 0.39287 |  0:00:05s\n",
      "epoch 119| loss: 0.39282 |  0:00:05s\n",
      "epoch 120| loss: 0.392   |  0:00:05s\n",
      "epoch 121| loss: 0.38559 |  0:00:05s\n",
      "epoch 122| loss: 0.38165 |  0:00:05s\n",
      "epoch 123| loss: 0.39271 |  0:00:06s\n",
      "epoch 124| loss: 0.39995 |  0:00:06s\n",
      "epoch 125| loss: 0.40595 |  0:00:06s\n",
      "epoch 126| loss: 0.41374 |  0:00:06s\n",
      "epoch 127| loss: 0.42206 |  0:00:06s\n",
      "epoch 128| loss: 0.41449 |  0:00:06s\n",
      "epoch 129| loss: 0.40382 |  0:00:06s\n",
      "epoch 130| loss: 0.40149 |  0:00:06s\n",
      "epoch 131| loss: 0.40738 |  0:00:06s\n",
      "epoch 132| loss: 0.40449 |  0:00:06s\n",
      "epoch 133| loss: 0.40399 |  0:00:06s\n",
      "epoch 134| loss: 0.39999 |  0:00:06s\n",
      "epoch 135| loss: 0.40743 |  0:00:06s\n",
      "epoch 136| loss: 0.43571 |  0:00:06s\n",
      "epoch 137| loss: 0.44717 |  0:00:06s\n",
      "epoch 138| loss: 0.44225 |  0:00:06s\n",
      "epoch 139| loss: 0.43785 |  0:00:06s\n",
      "epoch 140| loss: 0.4396  |  0:00:06s\n",
      "epoch 141| loss: 0.43325 |  0:00:06s\n",
      "epoch 142| loss: 0.42682 |  0:00:07s\n",
      "epoch 143| loss: 0.42898 |  0:00:07s\n",
      "epoch 144| loss: 0.41269 |  0:00:07s\n",
      "epoch 145| loss: 0.40734 |  0:00:07s\n",
      "epoch 146| loss: 0.40504 |  0:00:07s\n",
      "epoch 147| loss: 0.40631 |  0:00:07s\n",
      "epoch 148| loss: 0.40168 |  0:00:07s\n",
      "epoch 149| loss: 0.38947 |  0:00:07s\n",
      "epoch 150| loss: 0.39018 |  0:00:07s\n",
      "epoch 151| loss: 0.37906 |  0:00:07s\n",
      "epoch 152| loss: 0.37206 |  0:00:07s\n",
      "epoch 153| loss: 0.36718 |  0:00:07s\n",
      "epoch 154| loss: 0.36881 |  0:00:07s\n",
      "epoch 155| loss: 0.36786 |  0:00:07s\n",
      "epoch 156| loss: 0.37394 |  0:00:07s\n",
      "epoch 157| loss: 0.36149 |  0:00:07s\n",
      "epoch 158| loss: 0.35803 |  0:00:07s\n",
      "epoch 159| loss: 0.35343 |  0:00:07s\n",
      "epoch 160| loss: 0.36118 |  0:00:07s\n",
      "epoch 161| loss: 0.35571 |  0:00:07s\n",
      "epoch 162| loss: 0.34654 |  0:00:08s\n",
      "epoch 163| loss: 0.34724 |  0:00:08s\n",
      "epoch 164| loss: 0.34753 |  0:00:08s\n",
      "epoch 165| loss: 0.34962 |  0:00:08s\n",
      "epoch 166| loss: 0.35102 |  0:00:08s\n",
      "epoch 167| loss: 0.34283 |  0:00:08s\n",
      "epoch 168| loss: 0.34206 |  0:00:08s\n",
      "epoch 169| loss: 0.33662 |  0:00:08s\n",
      "epoch 170| loss: 0.34955 |  0:00:08s\n",
      "epoch 171| loss: 0.34377 |  0:00:08s\n",
      "epoch 172| loss: 0.35233 |  0:00:08s\n",
      "epoch 173| loss: 0.34796 |  0:00:08s\n",
      "epoch 174| loss: 0.35087 |  0:00:08s\n",
      "epoch 175| loss: 0.34773 |  0:00:08s\n",
      "epoch 176| loss: 0.34474 |  0:00:08s\n",
      "epoch 177| loss: 0.34554 |  0:00:08s\n",
      "epoch 178| loss: 0.33473 |  0:00:08s\n",
      "epoch 179| loss: 0.33023 |  0:00:08s\n",
      "epoch 180| loss: 0.32931 |  0:00:08s\n",
      "epoch 181| loss: 0.32928 |  0:00:09s\n",
      "epoch 182| loss: 0.3318  |  0:00:09s\n",
      "epoch 183| loss: 0.32906 |  0:00:09s\n",
      "epoch 184| loss: 0.31999 |  0:00:09s\n",
      "epoch 185| loss: 0.32121 |  0:00:09s\n",
      "epoch 186| loss: 0.32782 |  0:00:09s\n",
      "epoch 187| loss: 0.31687 |  0:00:09s\n",
      "epoch 188| loss: 0.32361 |  0:00:09s\n",
      "epoch 189| loss: 0.32639 |  0:00:09s\n",
      "epoch 190| loss: 0.31452 |  0:00:09s\n",
      "epoch 191| loss: 0.30867 |  0:00:09s\n",
      "epoch 192| loss: 0.31188 |  0:00:09s\n",
      "epoch 193| loss: 0.31355 |  0:00:09s\n",
      "epoch 194| loss: 0.31417 |  0:00:09s\n",
      "epoch 195| loss: 0.30716 |  0:00:09s\n",
      "epoch 196| loss: 0.30562 |  0:00:09s\n",
      "epoch 197| loss: 0.30706 |  0:00:09s\n",
      "epoch 198| loss: 0.31377 |  0:00:09s\n",
      "epoch 199| loss: 0.31208 |  0:00:09s\n",
      "epoch 200| loss: 0.30921 |  0:00:10s\n",
      "epoch 201| loss: 0.30128 |  0:00:10s\n",
      "epoch 202| loss: 0.29575 |  0:00:10s\n",
      "epoch 203| loss: 0.30544 |  0:00:10s\n",
      "epoch 204| loss: 0.30547 |  0:00:10s\n",
      "epoch 205| loss: 0.31532 |  0:00:10s\n",
      "epoch 206| loss: 0.32082 |  0:00:10s\n",
      "epoch 207| loss: 0.32262 |  0:00:10s\n",
      "epoch 208| loss: 0.31813 |  0:00:10s\n",
      "epoch 209| loss: 0.32608 |  0:00:10s\n",
      "epoch 210| loss: 0.32406 |  0:00:10s\n",
      "epoch 211| loss: 0.3168  |  0:00:10s\n",
      "epoch 212| loss: 0.31141 |  0:00:10s\n",
      "epoch 213| loss: 0.30708 |  0:00:10s\n",
      "epoch 214| loss: 0.31008 |  0:00:10s\n",
      "epoch 215| loss: 0.30483 |  0:00:10s\n",
      "epoch 216| loss: 0.30594 |  0:00:10s\n",
      "epoch 217| loss: 0.30657 |  0:00:10s\n",
      "epoch 218| loss: 0.29853 |  0:00:10s\n",
      "epoch 219| loss: 0.29348 |  0:00:10s\n",
      "epoch 220| loss: 0.29246 |  0:00:10s\n",
      "epoch 221| loss: 0.29638 |  0:00:10s\n",
      "epoch 222| loss: 0.28942 |  0:00:11s\n",
      "epoch 223| loss: 0.29011 |  0:00:11s\n",
      "epoch 224| loss: 0.28974 |  0:00:11s\n",
      "epoch 225| loss: 0.28769 |  0:00:11s\n",
      "epoch 226| loss: 0.28832 |  0:00:11s\n",
      "epoch 227| loss: 0.28645 |  0:00:11s\n",
      "epoch 228| loss: 0.28772 |  0:00:11s\n",
      "epoch 229| loss: 0.28182 |  0:00:11s\n",
      "epoch 230| loss: 0.27838 |  0:00:11s\n",
      "epoch 231| loss: 0.27588 |  0:00:11s\n",
      "epoch 232| loss: 0.28001 |  0:00:11s\n",
      "epoch 233| loss: 0.28421 |  0:00:11s\n",
      "epoch 234| loss: 0.27876 |  0:00:11s\n",
      "epoch 235| loss: 0.27735 |  0:00:11s\n",
      "epoch 236| loss: 0.27723 |  0:00:11s\n",
      "epoch 237| loss: 0.2894  |  0:00:11s\n",
      "epoch 238| loss: 0.28445 |  0:00:11s\n",
      "epoch 239| loss: 0.28587 |  0:00:11s\n",
      "epoch 240| loss: 0.29026 |  0:00:11s\n",
      "epoch 241| loss: 0.28774 |  0:00:11s\n",
      "epoch 242| loss: 0.28563 |  0:00:11s\n",
      "epoch 243| loss: 0.28197 |  0:00:12s\n",
      "epoch 244| loss: 0.28208 |  0:00:12s\n",
      "epoch 245| loss: 0.27635 |  0:00:12s\n",
      "epoch 246| loss: 0.27924 |  0:00:12s\n",
      "epoch 247| loss: 0.27441 |  0:00:12s\n",
      "epoch 248| loss: 0.27015 |  0:00:12s\n",
      "epoch 249| loss: 0.27071 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.97504 |  0:00:00s\n",
      "epoch 1  | loss: 1.91472 |  0:00:00s\n",
      "epoch 2  | loss: 1.43262 |  0:00:00s\n",
      "epoch 3  | loss: 1.16215 |  0:00:00s\n",
      "epoch 4  | loss: 1.04656 |  0:00:00s\n",
      "epoch 5  | loss: 0.97405 |  0:00:00s\n",
      "epoch 6  | loss: 0.91684 |  0:00:00s\n",
      "epoch 7  | loss: 0.8561  |  0:00:00s\n",
      "epoch 8  | loss: 0.84577 |  0:00:00s\n",
      "epoch 9  | loss: 0.80775 |  0:00:00s\n",
      "epoch 10 | loss: 0.77632 |  0:00:00s\n",
      "epoch 11 | loss: 0.76775 |  0:00:00s\n",
      "epoch 12 | loss: 0.74183 |  0:00:00s\n",
      "epoch 13 | loss: 0.7218  |  0:00:00s\n",
      "epoch 14 | loss: 0.71867 |  0:00:00s\n",
      "epoch 15 | loss: 0.69737 |  0:00:00s\n",
      "epoch 16 | loss: 0.70213 |  0:00:00s\n",
      "epoch 17 | loss: 0.6935  |  0:00:00s\n",
      "epoch 18 | loss: 0.69146 |  0:00:01s\n",
      "epoch 19 | loss: 0.67443 |  0:00:01s\n",
      "epoch 20 | loss: 0.68916 |  0:00:01s\n",
      "epoch 21 | loss: 0.66722 |  0:00:01s\n",
      "epoch 22 | loss: 0.66014 |  0:00:01s\n",
      "epoch 23 | loss: 0.65406 |  0:00:01s\n",
      "epoch 24 | loss: 0.64889 |  0:00:01s\n",
      "epoch 25 | loss: 0.65221 |  0:00:01s\n",
      "epoch 26 | loss: 0.64025 |  0:00:01s\n",
      "epoch 27 | loss: 0.6508  |  0:00:01s\n",
      "epoch 28 | loss: 0.63196 |  0:00:01s\n",
      "epoch 29 | loss: 0.6267  |  0:00:01s\n",
      "epoch 30 | loss: 0.63228 |  0:00:01s\n",
      "epoch 31 | loss: 0.63016 |  0:00:01s\n",
      "epoch 32 | loss: 0.62881 |  0:00:01s\n",
      "epoch 33 | loss: 0.61415 |  0:00:01s\n",
      "epoch 34 | loss: 0.61939 |  0:00:01s\n",
      "epoch 35 | loss: 0.60519 |  0:00:01s\n",
      "epoch 36 | loss: 0.60108 |  0:00:01s\n",
      "epoch 37 | loss: 0.60748 |  0:00:02s\n",
      "epoch 38 | loss: 0.59085 |  0:00:02s\n",
      "epoch 39 | loss: 0.58769 |  0:00:02s\n",
      "epoch 40 | loss: 0.59299 |  0:00:02s\n",
      "epoch 41 | loss: 0.59067 |  0:00:02s\n",
      "epoch 42 | loss: 0.58274 |  0:00:02s\n",
      "epoch 43 | loss: 0.57784 |  0:00:02s\n",
      "epoch 44 | loss: 0.57985 |  0:00:02s\n",
      "epoch 45 | loss: 0.57992 |  0:00:02s\n",
      "epoch 46 | loss: 0.58154 |  0:00:02s\n",
      "epoch 47 | loss: 0.56017 |  0:00:02s\n",
      "epoch 48 | loss: 0.56661 |  0:00:02s\n",
      "epoch 49 | loss: 0.57967 |  0:00:02s\n",
      "epoch 50 | loss: 0.57768 |  0:00:02s\n",
      "epoch 51 | loss: 0.5716  |  0:00:02s\n",
      "epoch 52 | loss: 0.56716 |  0:00:02s\n",
      "epoch 53 | loss: 0.5634  |  0:00:02s\n",
      "epoch 54 | loss: 0.56613 |  0:00:02s\n",
      "epoch 55 | loss: 0.55885 |  0:00:02s\n",
      "epoch 56 | loss: 0.55228 |  0:00:03s\n",
      "epoch 57 | loss: 0.5504  |  0:00:03s\n",
      "epoch 58 | loss: 0.54735 |  0:00:03s\n",
      "epoch 59 | loss: 0.54287 |  0:00:03s\n",
      "epoch 60 | loss: 0.53523 |  0:00:03s\n",
      "epoch 61 | loss: 0.52265 |  0:00:03s\n",
      "epoch 62 | loss: 0.52452 |  0:00:03s\n",
      "epoch 63 | loss: 0.51275 |  0:00:03s\n",
      "epoch 64 | loss: 0.5181  |  0:00:03s\n",
      "epoch 65 | loss: 0.51472 |  0:00:03s\n",
      "epoch 66 | loss: 0.51446 |  0:00:03s\n",
      "epoch 67 | loss: 0.51292 |  0:00:03s\n",
      "epoch 68 | loss: 0.50492 |  0:00:03s\n",
      "epoch 69 | loss: 0.49543 |  0:00:03s\n",
      "epoch 70 | loss: 0.50151 |  0:00:03s\n",
      "epoch 71 | loss: 0.49772 |  0:00:03s\n",
      "epoch 72 | loss: 0.49417 |  0:00:03s\n",
      "epoch 73 | loss: 0.48495 |  0:00:03s\n",
      "epoch 74 | loss: 0.48879 |  0:00:03s\n",
      "epoch 75 | loss: 0.4825  |  0:00:04s\n",
      "epoch 76 | loss: 0.48539 |  0:00:04s\n",
      "epoch 77 | loss: 0.47869 |  0:00:04s\n",
      "epoch 78 | loss: 0.47961 |  0:00:04s\n",
      "epoch 79 | loss: 0.46667 |  0:00:04s\n",
      "epoch 80 | loss: 0.46007 |  0:00:04s\n",
      "epoch 81 | loss: 0.46643 |  0:00:04s\n",
      "epoch 82 | loss: 0.46322 |  0:00:04s\n",
      "epoch 83 | loss: 0.45089 |  0:00:04s\n",
      "epoch 84 | loss: 0.45494 |  0:00:04s\n",
      "epoch 85 | loss: 0.44797 |  0:00:04s\n",
      "epoch 86 | loss: 0.44121 |  0:00:04s\n",
      "epoch 87 | loss: 0.44404 |  0:00:04s\n",
      "epoch 88 | loss: 0.44947 |  0:00:04s\n",
      "epoch 89 | loss: 0.44503 |  0:00:04s\n",
      "epoch 90 | loss: 0.43999 |  0:00:04s\n",
      "epoch 91 | loss: 0.43443 |  0:00:04s\n",
      "epoch 92 | loss: 0.44198 |  0:00:04s\n",
      "epoch 93 | loss: 0.4391  |  0:00:05s\n",
      "epoch 94 | loss: 0.43313 |  0:00:05s\n",
      "epoch 95 | loss: 0.42307 |  0:00:05s\n",
      "epoch 96 | loss: 0.42151 |  0:00:05s\n",
      "epoch 97 | loss: 0.42592 |  0:00:05s\n",
      "epoch 98 | loss: 0.41425 |  0:00:05s\n",
      "epoch 99 | loss: 0.42009 |  0:00:05s\n",
      "epoch 100| loss: 0.41377 |  0:00:05s\n",
      "epoch 101| loss: 0.43165 |  0:00:05s\n",
      "epoch 102| loss: 0.41961 |  0:00:05s\n",
      "epoch 103| loss: 0.42697 |  0:00:05s\n",
      "epoch 104| loss: 0.42144 |  0:00:05s\n",
      "epoch 105| loss: 0.42188 |  0:00:05s\n",
      "epoch 106| loss: 0.4146  |  0:00:05s\n",
      "epoch 107| loss: 0.40448 |  0:00:05s\n",
      "epoch 108| loss: 0.40946 |  0:00:05s\n",
      "epoch 109| loss: 0.41138 |  0:00:05s\n",
      "epoch 110| loss: 0.39767 |  0:00:05s\n",
      "epoch 111| loss: 0.40469 |  0:00:05s\n",
      "epoch 112| loss: 0.40095 |  0:00:06s\n",
      "epoch 113| loss: 0.40214 |  0:00:06s\n",
      "epoch 114| loss: 0.40019 |  0:00:06s\n",
      "epoch 115| loss: 0.40258 |  0:00:06s\n",
      "epoch 116| loss: 0.39711 |  0:00:06s\n",
      "epoch 117| loss: 0.39785 |  0:00:06s\n",
      "epoch 118| loss: 0.39501 |  0:00:06s\n",
      "epoch 119| loss: 0.3852  |  0:00:06s\n",
      "epoch 120| loss: 0.38284 |  0:00:06s\n",
      "epoch 121| loss: 0.37815 |  0:00:06s\n",
      "epoch 122| loss: 0.37374 |  0:00:06s\n",
      "epoch 123| loss: 0.37885 |  0:00:06s\n",
      "epoch 124| loss: 0.38268 |  0:00:06s\n",
      "epoch 125| loss: 0.37433 |  0:00:06s\n",
      "epoch 126| loss: 0.37305 |  0:00:06s\n",
      "epoch 127| loss: 0.37586 |  0:00:06s\n",
      "epoch 128| loss: 0.37125 |  0:00:06s\n",
      "epoch 129| loss: 0.3727  |  0:00:06s\n",
      "epoch 130| loss: 0.36995 |  0:00:06s\n",
      "epoch 131| loss: 0.378   |  0:00:07s\n",
      "epoch 132| loss: 0.37476 |  0:00:07s\n",
      "epoch 133| loss: 0.37425 |  0:00:07s\n",
      "epoch 134| loss: 0.36881 |  0:00:07s\n",
      "epoch 135| loss: 0.36183 |  0:00:07s\n",
      "epoch 136| loss: 0.37306 |  0:00:07s\n",
      "epoch 137| loss: 0.36506 |  0:00:07s\n",
      "epoch 138| loss: 0.36273 |  0:00:07s\n",
      "epoch 139| loss: 0.35715 |  0:00:07s\n",
      "epoch 140| loss: 0.36053 |  0:00:07s\n",
      "epoch 141| loss: 0.35523 |  0:00:07s\n",
      "epoch 142| loss: 0.3532  |  0:00:07s\n",
      "epoch 143| loss: 0.35436 |  0:00:07s\n",
      "epoch 144| loss: 0.34666 |  0:00:07s\n",
      "epoch 145| loss: 0.34501 |  0:00:07s\n",
      "epoch 146| loss: 0.34807 |  0:00:07s\n",
      "epoch 147| loss: 0.35091 |  0:00:07s\n",
      "epoch 148| loss: 0.34876 |  0:00:07s\n",
      "epoch 149| loss: 0.34333 |  0:00:07s\n",
      "epoch 150| loss: 0.3417  |  0:00:08s\n",
      "epoch 151| loss: 0.33786 |  0:00:08s\n",
      "epoch 152| loss: 0.34119 |  0:00:08s\n",
      "epoch 153| loss: 0.3406  |  0:00:08s\n",
      "epoch 154| loss: 0.34159 |  0:00:08s\n",
      "epoch 155| loss: 0.33446 |  0:00:08s\n",
      "epoch 156| loss: 0.33956 |  0:00:08s\n",
      "epoch 157| loss: 0.33118 |  0:00:08s\n",
      "epoch 158| loss: 0.33151 |  0:00:08s\n",
      "epoch 159| loss: 0.33207 |  0:00:08s\n",
      "epoch 160| loss: 0.33214 |  0:00:08s\n",
      "epoch 161| loss: 0.33355 |  0:00:08s\n",
      "epoch 162| loss: 0.32134 |  0:00:08s\n",
      "epoch 163| loss: 0.32373 |  0:00:08s\n",
      "epoch 164| loss: 0.33104 |  0:00:08s\n",
      "epoch 165| loss: 0.3256  |  0:00:08s\n",
      "epoch 166| loss: 0.31929 |  0:00:08s\n",
      "epoch 167| loss: 0.32624 |  0:00:08s\n",
      "epoch 168| loss: 0.32691 |  0:00:08s\n",
      "epoch 169| loss: 0.32969 |  0:00:09s\n",
      "epoch 170| loss: 0.34371 |  0:00:09s\n",
      "epoch 171| loss: 0.34058 |  0:00:09s\n",
      "epoch 172| loss: 0.34304 |  0:00:09s\n",
      "epoch 173| loss: 0.33627 |  0:00:09s\n",
      "epoch 174| loss: 0.33605 |  0:00:09s\n",
      "epoch 175| loss: 0.33428 |  0:00:09s\n",
      "epoch 176| loss: 0.32998 |  0:00:09s\n",
      "epoch 177| loss: 0.33817 |  0:00:09s\n",
      "epoch 178| loss: 0.33922 |  0:00:09s\n",
      "epoch 179| loss: 0.34619 |  0:00:09s\n",
      "epoch 180| loss: 0.36066 |  0:00:09s\n",
      "epoch 181| loss: 0.35605 |  0:00:09s\n",
      "epoch 182| loss: 0.35218 |  0:00:09s\n",
      "epoch 183| loss: 0.35309 |  0:00:09s\n",
      "epoch 184| loss: 0.34628 |  0:00:09s\n",
      "epoch 185| loss: 0.36034 |  0:00:09s\n",
      "epoch 186| loss: 0.3566  |  0:00:10s\n",
      "epoch 187| loss: 0.34036 |  0:00:10s\n",
      "epoch 188| loss: 0.33737 |  0:00:10s\n",
      "epoch 189| loss: 0.3481  |  0:00:10s\n",
      "epoch 190| loss: 0.33367 |  0:00:10s\n",
      "epoch 191| loss: 0.33518 |  0:00:10s\n",
      "epoch 192| loss: 0.336   |  0:00:10s\n",
      "epoch 193| loss: 0.34486 |  0:00:10s\n",
      "epoch 194| loss: 0.35177 |  0:00:10s\n",
      "epoch 195| loss: 0.34294 |  0:00:10s\n",
      "epoch 196| loss: 0.34306 |  0:00:10s\n",
      "epoch 197| loss: 0.35445 |  0:00:10s\n",
      "epoch 198| loss: 0.3444  |  0:00:10s\n",
      "epoch 199| loss: 0.33894 |  0:00:10s\n",
      "epoch 200| loss: 0.33274 |  0:00:10s\n",
      "epoch 201| loss: 0.3337  |  0:00:10s\n",
      "epoch 202| loss: 0.33268 |  0:00:10s\n",
      "epoch 203| loss: 0.32567 |  0:00:10s\n",
      "epoch 204| loss: 0.32035 |  0:00:10s\n",
      "epoch 205| loss: 0.3206  |  0:00:10s\n",
      "epoch 206| loss: 0.31752 |  0:00:10s\n",
      "epoch 207| loss: 0.3178  |  0:00:10s\n",
      "epoch 208| loss: 0.30982 |  0:00:11s\n",
      "epoch 209| loss: 0.30408 |  0:00:11s\n",
      "epoch 210| loss: 0.3053  |  0:00:11s\n",
      "epoch 211| loss: 0.3056  |  0:00:11s\n",
      "epoch 212| loss: 0.29893 |  0:00:11s\n",
      "epoch 213| loss: 0.29405 |  0:00:11s\n",
      "epoch 214| loss: 0.29492 |  0:00:11s\n",
      "epoch 215| loss: 0.29298 |  0:00:11s\n",
      "epoch 216| loss: 0.2903  |  0:00:11s\n",
      "epoch 217| loss: 0.28751 |  0:00:11s\n",
      "epoch 218| loss: 0.2923  |  0:00:11s\n",
      "epoch 219| loss: 0.28887 |  0:00:11s\n",
      "epoch 220| loss: 0.29566 |  0:00:11s\n",
      "epoch 221| loss: 0.30414 |  0:00:11s\n",
      "epoch 222| loss: 0.31492 |  0:00:11s\n",
      "epoch 223| loss: 0.32068 |  0:00:11s\n",
      "epoch 224| loss: 0.31406 |  0:00:11s\n",
      "epoch 225| loss: 0.31355 |  0:00:11s\n",
      "epoch 226| loss: 0.30725 |  0:00:11s\n",
      "epoch 227| loss: 0.30552 |  0:00:11s\n",
      "epoch 228| loss: 0.30874 |  0:00:11s\n",
      "epoch 229| loss: 0.30286 |  0:00:11s\n",
      "epoch 230| loss: 0.29826 |  0:00:12s\n",
      "epoch 231| loss: 0.29911 |  0:00:12s\n",
      "epoch 232| loss: 0.30015 |  0:00:12s\n",
      "epoch 233| loss: 0.3154  |  0:00:12s\n",
      "epoch 234| loss: 0.31855 |  0:00:12s\n",
      "epoch 235| loss: 0.31062 |  0:00:12s\n",
      "epoch 236| loss: 0.31153 |  0:00:12s\n",
      "epoch 237| loss: 0.31325 |  0:00:12s\n",
      "epoch 238| loss: 0.30534 |  0:00:12s\n",
      "epoch 239| loss: 0.30008 |  0:00:12s\n",
      "epoch 240| loss: 0.28682 |  0:00:12s\n",
      "epoch 241| loss: 0.29009 |  0:00:12s\n",
      "epoch 242| loss: 0.28463 |  0:00:12s\n",
      "epoch 243| loss: 0.28208 |  0:00:12s\n",
      "epoch 244| loss: 0.28737 |  0:00:12s\n",
      "epoch 245| loss: 0.27967 |  0:00:12s\n",
      "epoch 246| loss: 0.27394 |  0:00:12s\n",
      "epoch 247| loss: 0.27319 |  0:00:12s\n",
      "epoch 248| loss: 0.26836 |  0:00:12s\n",
      "epoch 249| loss: 0.27007 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.88698 |  0:00:00s\n",
      "epoch 1  | loss: 1.80587 |  0:00:00s\n",
      "epoch 2  | loss: 1.34842 |  0:00:00s\n",
      "epoch 3  | loss: 1.1311  |  0:00:00s\n",
      "epoch 4  | loss: 0.98234 |  0:00:00s\n",
      "epoch 5  | loss: 0.91849 |  0:00:00s\n",
      "epoch 6  | loss: 0.8719  |  0:00:00s\n",
      "epoch 7  | loss: 0.82499 |  0:00:00s\n",
      "epoch 8  | loss: 0.79122 |  0:00:00s\n",
      "epoch 9  | loss: 0.75763 |  0:00:00s\n",
      "epoch 10 | loss: 0.75407 |  0:00:00s\n",
      "epoch 11 | loss: 0.71327 |  0:00:00s\n",
      "epoch 12 | loss: 0.7026  |  0:00:00s\n",
      "epoch 13 | loss: 0.70234 |  0:00:00s\n",
      "epoch 14 | loss: 0.69554 |  0:00:00s\n",
      "epoch 15 | loss: 0.6871  |  0:00:00s\n",
      "epoch 16 | loss: 0.68353 |  0:00:00s\n",
      "epoch 17 | loss: 0.68474 |  0:00:00s\n",
      "epoch 18 | loss: 0.6755  |  0:00:01s\n",
      "epoch 19 | loss: 0.66738 |  0:00:01s\n",
      "epoch 20 | loss: 0.65346 |  0:00:01s\n",
      "epoch 21 | loss: 0.6433  |  0:00:01s\n",
      "epoch 22 | loss: 0.64943 |  0:00:01s\n",
      "epoch 23 | loss: 0.64214 |  0:00:01s\n",
      "epoch 24 | loss: 0.62253 |  0:00:01s\n",
      "epoch 25 | loss: 0.63715 |  0:00:01s\n",
      "epoch 26 | loss: 0.63373 |  0:00:01s\n",
      "epoch 27 | loss: 0.61571 |  0:00:01s\n",
      "epoch 28 | loss: 0.60774 |  0:00:01s\n",
      "epoch 29 | loss: 0.60345 |  0:00:01s\n",
      "epoch 30 | loss: 0.60284 |  0:00:01s\n",
      "epoch 31 | loss: 0.59971 |  0:00:01s\n",
      "epoch 32 | loss: 0.58834 |  0:00:01s\n",
      "epoch 33 | loss: 0.58862 |  0:00:01s\n",
      "epoch 34 | loss: 0.57842 |  0:00:01s\n",
      "epoch 35 | loss: 0.58291 |  0:00:01s\n",
      "epoch 36 | loss: 0.55869 |  0:00:02s\n",
      "epoch 37 | loss: 0.56564 |  0:00:02s\n",
      "epoch 38 | loss: 0.55085 |  0:00:02s\n",
      "epoch 39 | loss: 0.54582 |  0:00:02s\n",
      "epoch 40 | loss: 0.55454 |  0:00:02s\n",
      "epoch 41 | loss: 0.53842 |  0:00:02s\n",
      "epoch 42 | loss: 0.53469 |  0:00:02s\n",
      "epoch 43 | loss: 0.53599 |  0:00:02s\n",
      "epoch 44 | loss: 0.53165 |  0:00:02s\n",
      "epoch 45 | loss: 0.53008 |  0:00:02s\n",
      "epoch 46 | loss: 0.52603 |  0:00:02s\n",
      "epoch 47 | loss: 0.52701 |  0:00:02s\n",
      "epoch 48 | loss: 0.51333 |  0:00:02s\n",
      "epoch 49 | loss: 0.51365 |  0:00:02s\n",
      "epoch 50 | loss: 0.51478 |  0:00:02s\n",
      "epoch 51 | loss: 0.50257 |  0:00:02s\n",
      "epoch 52 | loss: 0.50901 |  0:00:02s\n",
      "epoch 53 | loss: 0.52056 |  0:00:03s\n",
      "epoch 54 | loss: 0.50913 |  0:00:03s\n",
      "epoch 55 | loss: 0.49953 |  0:00:03s\n",
      "epoch 56 | loss: 0.50017 |  0:00:03s\n",
      "epoch 57 | loss: 0.50258 |  0:00:03s\n",
      "epoch 58 | loss: 0.49916 |  0:00:03s\n",
      "epoch 59 | loss: 0.49099 |  0:00:03s\n",
      "epoch 60 | loss: 0.49406 |  0:00:03s\n",
      "epoch 61 | loss: 0.493   |  0:00:03s\n",
      "epoch 62 | loss: 0.50004 |  0:00:03s\n",
      "epoch 63 | loss: 0.49055 |  0:00:03s\n",
      "epoch 64 | loss: 0.49078 |  0:00:03s\n",
      "epoch 65 | loss: 0.48534 |  0:00:03s\n",
      "epoch 66 | loss: 0.47236 |  0:00:03s\n",
      "epoch 67 | loss: 0.4752  |  0:00:03s\n",
      "epoch 68 | loss: 0.48037 |  0:00:03s\n",
      "epoch 69 | loss: 0.46586 |  0:00:03s\n",
      "epoch 70 | loss: 0.46709 |  0:00:03s\n",
      "epoch 71 | loss: 0.465   |  0:00:03s\n",
      "epoch 72 | loss: 0.45218 |  0:00:03s\n",
      "epoch 73 | loss: 0.4442  |  0:00:03s\n",
      "epoch 74 | loss: 0.45653 |  0:00:03s\n",
      "epoch 75 | loss: 0.45334 |  0:00:04s\n",
      "epoch 76 | loss: 0.44279 |  0:00:04s\n",
      "epoch 77 | loss: 0.46085 |  0:00:04s\n",
      "epoch 78 | loss: 0.45122 |  0:00:04s\n",
      "epoch 79 | loss: 0.46866 |  0:00:04s\n",
      "epoch 80 | loss: 0.46068 |  0:00:04s\n",
      "epoch 81 | loss: 0.46238 |  0:00:04s\n",
      "epoch 82 | loss: 0.44463 |  0:00:04s\n",
      "epoch 83 | loss: 0.43829 |  0:00:04s\n",
      "epoch 84 | loss: 0.43065 |  0:00:04s\n",
      "epoch 85 | loss: 0.4281  |  0:00:04s\n",
      "epoch 86 | loss: 0.43271 |  0:00:04s\n",
      "epoch 87 | loss: 0.42787 |  0:00:04s\n",
      "epoch 88 | loss: 0.42572 |  0:00:04s\n",
      "epoch 89 | loss: 0.42265 |  0:00:04s\n",
      "epoch 90 | loss: 0.4228  |  0:00:04s\n",
      "epoch 91 | loss: 0.41067 |  0:00:04s\n",
      "epoch 92 | loss: 0.4166  |  0:00:04s\n",
      "epoch 93 | loss: 0.40673 |  0:00:04s\n",
      "epoch 94 | loss: 0.40743 |  0:00:04s\n",
      "epoch 95 | loss: 0.41341 |  0:00:04s\n",
      "epoch 96 | loss: 0.41574 |  0:00:05s\n",
      "epoch 97 | loss: 0.41178 |  0:00:05s\n",
      "epoch 98 | loss: 0.40053 |  0:00:05s\n",
      "epoch 99 | loss: 0.404   |  0:00:05s\n",
      "epoch 100| loss: 0.4031  |  0:00:05s\n",
      "epoch 101| loss: 0.40276 |  0:00:05s\n",
      "epoch 102| loss: 0.40405 |  0:00:05s\n",
      "epoch 103| loss: 0.39864 |  0:00:05s\n",
      "epoch 104| loss: 0.39829 |  0:00:05s\n",
      "epoch 105| loss: 0.38982 |  0:00:05s\n",
      "epoch 106| loss: 0.39104 |  0:00:05s\n",
      "epoch 107| loss: 0.38831 |  0:00:05s\n",
      "epoch 108| loss: 0.38749 |  0:00:05s\n",
      "epoch 109| loss: 0.39447 |  0:00:05s\n",
      "epoch 110| loss: 0.40276 |  0:00:05s\n",
      "epoch 111| loss: 0.40438 |  0:00:05s\n",
      "epoch 112| loss: 0.39562 |  0:00:05s\n",
      "epoch 113| loss: 0.38836 |  0:00:05s\n",
      "epoch 114| loss: 0.38103 |  0:00:05s\n",
      "epoch 115| loss: 0.38188 |  0:00:05s\n",
      "epoch 116| loss: 0.38401 |  0:00:05s\n",
      "epoch 117| loss: 0.38949 |  0:00:05s\n",
      "epoch 118| loss: 0.37439 |  0:00:06s\n",
      "epoch 119| loss: 0.36005 |  0:00:06s\n",
      "epoch 120| loss: 0.3607  |  0:00:06s\n",
      "epoch 121| loss: 0.36379 |  0:00:06s\n",
      "epoch 122| loss: 0.36512 |  0:00:06s\n",
      "epoch 123| loss: 0.36845 |  0:00:06s\n",
      "epoch 124| loss: 0.36681 |  0:00:06s\n",
      "epoch 125| loss: 0.35682 |  0:00:06s\n",
      "epoch 126| loss: 0.37035 |  0:00:06s\n",
      "epoch 127| loss: 0.367   |  0:00:06s\n",
      "epoch 128| loss: 0.36003 |  0:00:06s\n",
      "epoch 129| loss: 0.36328 |  0:00:06s\n",
      "epoch 130| loss: 0.35426 |  0:00:06s\n",
      "epoch 131| loss: 0.35299 |  0:00:06s\n",
      "epoch 132| loss: 0.35681 |  0:00:06s\n",
      "epoch 133| loss: 0.34713 |  0:00:06s\n",
      "epoch 134| loss: 0.34708 |  0:00:06s\n",
      "epoch 135| loss: 0.34814 |  0:00:06s\n",
      "epoch 136| loss: 0.36363 |  0:00:06s\n",
      "epoch 137| loss: 0.36937 |  0:00:06s\n",
      "epoch 138| loss: 0.37104 |  0:00:06s\n",
      "epoch 139| loss: 0.36402 |  0:00:06s\n",
      "epoch 140| loss: 0.36948 |  0:00:07s\n",
      "epoch 141| loss: 0.36037 |  0:00:07s\n",
      "epoch 142| loss: 0.37032 |  0:00:07s\n",
      "epoch 143| loss: 0.36136 |  0:00:07s\n",
      "epoch 144| loss: 0.35564 |  0:00:07s\n",
      "epoch 145| loss: 0.35119 |  0:00:07s\n",
      "epoch 146| loss: 0.34412 |  0:00:07s\n",
      "epoch 147| loss: 0.34269 |  0:00:07s\n",
      "epoch 148| loss: 0.35473 |  0:00:07s\n",
      "epoch 149| loss: 0.35051 |  0:00:07s\n",
      "epoch 150| loss: 0.3488  |  0:00:07s\n",
      "epoch 151| loss: 0.33739 |  0:00:07s\n",
      "epoch 152| loss: 0.3481  |  0:00:07s\n",
      "epoch 153| loss: 0.34993 |  0:00:07s\n",
      "epoch 154| loss: 0.34899 |  0:00:07s\n",
      "epoch 155| loss: 0.34298 |  0:00:07s\n",
      "epoch 156| loss: 0.3421  |  0:00:07s\n",
      "epoch 157| loss: 0.33427 |  0:00:07s\n",
      "epoch 158| loss: 0.33371 |  0:00:07s\n",
      "epoch 159| loss: 0.32573 |  0:00:07s\n",
      "epoch 160| loss: 0.33729 |  0:00:07s\n",
      "epoch 161| loss: 0.33067 |  0:00:07s\n",
      "epoch 162| loss: 0.33388 |  0:00:08s\n",
      "epoch 163| loss: 0.33755 |  0:00:08s\n",
      "epoch 164| loss: 0.34821 |  0:00:08s\n",
      "epoch 165| loss: 0.34546 |  0:00:08s\n",
      "epoch 166| loss: 0.35168 |  0:00:08s\n",
      "epoch 167| loss: 0.35516 |  0:00:08s\n",
      "epoch 168| loss: 0.34689 |  0:00:08s\n",
      "epoch 169| loss: 0.34015 |  0:00:08s\n",
      "epoch 170| loss: 0.3388  |  0:00:08s\n",
      "epoch 171| loss: 0.33101 |  0:00:08s\n",
      "epoch 172| loss: 0.33441 |  0:00:08s\n",
      "epoch 173| loss: 0.32046 |  0:00:08s\n",
      "epoch 174| loss: 0.31996 |  0:00:08s\n",
      "epoch 175| loss: 0.31473 |  0:00:08s\n",
      "epoch 176| loss: 0.31044 |  0:00:08s\n",
      "epoch 177| loss: 0.32102 |  0:00:08s\n",
      "epoch 178| loss: 0.33397 |  0:00:08s\n",
      "epoch 179| loss: 0.3342  |  0:00:08s\n",
      "epoch 180| loss: 0.33445 |  0:00:08s\n",
      "epoch 181| loss: 0.33046 |  0:00:08s\n",
      "epoch 182| loss: 0.32832 |  0:00:08s\n",
      "epoch 183| loss: 0.32031 |  0:00:08s\n",
      "epoch 184| loss: 0.31581 |  0:00:09s\n",
      "epoch 185| loss: 0.31577 |  0:00:09s\n",
      "epoch 186| loss: 0.31445 |  0:00:09s\n",
      "epoch 187| loss: 0.3105  |  0:00:09s\n",
      "epoch 188| loss: 0.31407 |  0:00:09s\n",
      "epoch 189| loss: 0.30942 |  0:00:09s\n",
      "epoch 190| loss: 0.30322 |  0:00:09s\n",
      "epoch 191| loss: 0.30439 |  0:00:09s\n",
      "epoch 192| loss: 0.30888 |  0:00:09s\n",
      "epoch 193| loss: 0.31958 |  0:00:09s\n",
      "epoch 194| loss: 0.31322 |  0:00:09s\n",
      "epoch 195| loss: 0.3084  |  0:00:09s\n",
      "epoch 196| loss: 0.31013 |  0:00:09s\n",
      "epoch 197| loss: 0.3034  |  0:00:09s\n",
      "epoch 198| loss: 0.30033 |  0:00:09s\n",
      "epoch 199| loss: 0.29205 |  0:00:09s\n",
      "epoch 200| loss: 0.28929 |  0:00:09s\n",
      "epoch 201| loss: 0.29045 |  0:00:09s\n",
      "epoch 202| loss: 0.28573 |  0:00:09s\n",
      "epoch 203| loss: 0.28626 |  0:00:09s\n",
      "epoch 204| loss: 0.28028 |  0:00:09s\n",
      "epoch 205| loss: 0.27817 |  0:00:09s\n",
      "epoch 206| loss: 0.28299 |  0:00:10s\n",
      "epoch 207| loss: 0.27944 |  0:00:10s\n",
      "epoch 208| loss: 0.27519 |  0:00:10s\n",
      "epoch 209| loss: 0.2839  |  0:00:10s\n",
      "epoch 210| loss: 0.27204 |  0:00:10s\n",
      "epoch 211| loss: 0.27355 |  0:00:10s\n",
      "epoch 212| loss: 0.27218 |  0:00:10s\n",
      "epoch 213| loss: 0.27004 |  0:00:10s\n",
      "epoch 214| loss: 0.27652 |  0:00:10s\n",
      "epoch 215| loss: 0.29353 |  0:00:10s\n",
      "epoch 216| loss: 0.29548 |  0:00:10s\n",
      "epoch 217| loss: 0.2976  |  0:00:10s\n",
      "epoch 218| loss: 0.29079 |  0:00:10s\n",
      "epoch 219| loss: 0.31267 |  0:00:10s\n",
      "epoch 220| loss: 0.31455 |  0:00:10s\n",
      "epoch 221| loss: 0.30853 |  0:00:10s\n",
      "epoch 222| loss: 0.30413 |  0:00:10s\n",
      "epoch 223| loss: 0.30378 |  0:00:10s\n",
      "epoch 224| loss: 0.29908 |  0:00:10s\n",
      "epoch 225| loss: 0.29162 |  0:00:10s\n",
      "epoch 226| loss: 0.28236 |  0:00:10s\n",
      "epoch 227| loss: 0.28434 |  0:00:10s\n",
      "epoch 228| loss: 0.27959 |  0:00:11s\n",
      "epoch 229| loss: 0.27045 |  0:00:11s\n",
      "epoch 230| loss: 0.27709 |  0:00:11s\n",
      "epoch 231| loss: 0.27126 |  0:00:11s\n",
      "epoch 232| loss: 0.26835 |  0:00:11s\n",
      "epoch 233| loss: 0.27262 |  0:00:11s\n",
      "epoch 234| loss: 0.27709 |  0:00:11s\n",
      "epoch 235| loss: 0.2737  |  0:00:11s\n",
      "epoch 236| loss: 0.27414 |  0:00:11s\n",
      "epoch 237| loss: 0.27351 |  0:00:11s\n",
      "epoch 238| loss: 0.2705  |  0:00:11s\n",
      "epoch 239| loss: 0.27388 |  0:00:11s\n",
      "epoch 240| loss: 0.27249 |  0:00:11s\n",
      "epoch 241| loss: 0.27189 |  0:00:11s\n",
      "epoch 242| loss: 0.26292 |  0:00:11s\n",
      "epoch 243| loss: 0.26936 |  0:00:11s\n",
      "epoch 244| loss: 0.2663  |  0:00:11s\n",
      "epoch 245| loss: 0.26093 |  0:00:11s\n",
      "epoch 246| loss: 0.26031 |  0:00:11s\n",
      "epoch 247| loss: 0.25979 |  0:00:11s\n",
      "epoch 248| loss: 0.25496 |  0:00:11s\n",
      "epoch 249| loss: 0.25723 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.95687 |  0:00:00s\n",
      "epoch 1  | loss: 1.71416 |  0:00:00s\n",
      "epoch 2  | loss: 1.33523 |  0:00:00s\n",
      "epoch 3  | loss: 1.08201 |  0:00:00s\n",
      "epoch 4  | loss: 0.97986 |  0:00:00s\n",
      "epoch 5  | loss: 0.89789 |  0:00:00s\n",
      "epoch 6  | loss: 0.85887 |  0:00:00s\n",
      "epoch 7  | loss: 0.81564 |  0:00:00s\n",
      "epoch 8  | loss: 0.78357 |  0:00:00s\n",
      "epoch 9  | loss: 0.77405 |  0:00:00s\n",
      "epoch 10 | loss: 0.74512 |  0:00:00s\n",
      "epoch 11 | loss: 0.71725 |  0:00:00s\n",
      "epoch 12 | loss: 0.70726 |  0:00:00s\n",
      "epoch 13 | loss: 0.69569 |  0:00:00s\n",
      "epoch 14 | loss: 0.67305 |  0:00:00s\n",
      "epoch 15 | loss: 0.66497 |  0:00:00s\n",
      "epoch 16 | loss: 0.6686  |  0:00:00s\n",
      "epoch 17 | loss: 0.65786 |  0:00:00s\n",
      "epoch 18 | loss: 0.64928 |  0:00:00s\n",
      "epoch 19 | loss: 0.65495 |  0:00:00s\n",
      "epoch 20 | loss: 0.65109 |  0:00:00s\n",
      "epoch 21 | loss: 0.64299 |  0:00:01s\n",
      "epoch 22 | loss: 0.63619 |  0:00:01s\n",
      "epoch 23 | loss: 0.62852 |  0:00:01s\n",
      "epoch 24 | loss: 0.61412 |  0:00:01s\n",
      "epoch 25 | loss: 0.60918 |  0:00:01s\n",
      "epoch 26 | loss: 0.61212 |  0:00:01s\n",
      "epoch 27 | loss: 0.60835 |  0:00:01s\n",
      "epoch 28 | loss: 0.61917 |  0:00:01s\n",
      "epoch 29 | loss: 0.59442 |  0:00:01s\n",
      "epoch 30 | loss: 0.60661 |  0:00:01s\n",
      "epoch 31 | loss: 0.5843  |  0:00:01s\n",
      "epoch 32 | loss: 0.57961 |  0:00:01s\n",
      "epoch 33 | loss: 0.58045 |  0:00:01s\n",
      "epoch 34 | loss: 0.57577 |  0:00:01s\n",
      "epoch 35 | loss: 0.57604 |  0:00:01s\n",
      "epoch 36 | loss: 0.56634 |  0:00:01s\n",
      "epoch 37 | loss: 0.56636 |  0:00:01s\n",
      "epoch 38 | loss: 0.56521 |  0:00:01s\n",
      "epoch 39 | loss: 0.55849 |  0:00:01s\n",
      "epoch 40 | loss: 0.55054 |  0:00:01s\n",
      "epoch 41 | loss: 0.55157 |  0:00:01s\n",
      "epoch 42 | loss: 0.5574  |  0:00:01s\n",
      "epoch 43 | loss: 0.55177 |  0:00:02s\n",
      "epoch 44 | loss: 0.555   |  0:00:02s\n",
      "epoch 45 | loss: 0.56864 |  0:00:02s\n",
      "epoch 46 | loss: 0.55574 |  0:00:02s\n",
      "epoch 47 | loss: 0.53759 |  0:00:02s\n",
      "epoch 48 | loss: 0.53528 |  0:00:02s\n",
      "epoch 49 | loss: 0.52796 |  0:00:02s\n",
      "epoch 50 | loss: 0.52592 |  0:00:02s\n",
      "epoch 51 | loss: 0.5238  |  0:00:02s\n",
      "epoch 52 | loss: 0.52146 |  0:00:02s\n",
      "epoch 53 | loss: 0.52187 |  0:00:02s\n",
      "epoch 54 | loss: 0.52343 |  0:00:02s\n",
      "epoch 55 | loss: 0.51184 |  0:00:02s\n",
      "epoch 56 | loss: 0.50816 |  0:00:02s\n",
      "epoch 57 | loss: 0.5033  |  0:00:02s\n",
      "epoch 58 | loss: 0.49756 |  0:00:02s\n",
      "epoch 59 | loss: 0.49289 |  0:00:02s\n",
      "epoch 60 | loss: 0.50001 |  0:00:02s\n",
      "epoch 61 | loss: 0.48961 |  0:00:02s\n",
      "epoch 62 | loss: 0.49244 |  0:00:02s\n",
      "epoch 63 | loss: 0.47828 |  0:00:02s\n",
      "epoch 64 | loss: 0.47437 |  0:00:02s\n",
      "epoch 65 | loss: 0.47324 |  0:00:03s\n",
      "epoch 66 | loss: 0.47904 |  0:00:03s\n",
      "epoch 67 | loss: 0.47392 |  0:00:03s\n",
      "epoch 68 | loss: 0.48105 |  0:00:03s\n",
      "epoch 69 | loss: 0.47747 |  0:00:03s\n",
      "epoch 70 | loss: 0.47985 |  0:00:03s\n",
      "epoch 71 | loss: 0.48005 |  0:00:03s\n",
      "epoch 72 | loss: 0.47107 |  0:00:03s\n",
      "epoch 73 | loss: 0.46571 |  0:00:03s\n",
      "epoch 74 | loss: 0.47253 |  0:00:03s\n",
      "epoch 75 | loss: 0.4615  |  0:00:03s\n",
      "epoch 76 | loss: 0.45525 |  0:00:03s\n",
      "epoch 77 | loss: 0.46296 |  0:00:03s\n",
      "epoch 78 | loss: 0.4493  |  0:00:03s\n",
      "epoch 79 | loss: 0.44342 |  0:00:03s\n",
      "epoch 80 | loss: 0.44856 |  0:00:03s\n",
      "epoch 81 | loss: 0.45503 |  0:00:03s\n",
      "epoch 82 | loss: 0.44686 |  0:00:03s\n",
      "epoch 83 | loss: 0.44273 |  0:00:03s\n",
      "epoch 84 | loss: 0.43586 |  0:00:03s\n",
      "epoch 85 | loss: 0.4319  |  0:00:03s\n",
      "epoch 86 | loss: 0.42924 |  0:00:04s\n",
      "epoch 87 | loss: 0.42636 |  0:00:04s\n",
      "epoch 88 | loss: 0.4192  |  0:00:04s\n",
      "epoch 89 | loss: 0.43128 |  0:00:04s\n",
      "epoch 90 | loss: 0.42563 |  0:00:04s\n",
      "epoch 91 | loss: 0.42001 |  0:00:04s\n",
      "epoch 92 | loss: 0.42333 |  0:00:04s\n",
      "epoch 93 | loss: 0.41812 |  0:00:04s\n",
      "epoch 94 | loss: 0.41654 |  0:00:04s\n",
      "epoch 95 | loss: 0.40692 |  0:00:04s\n",
      "epoch 96 | loss: 0.4149  |  0:00:04s\n",
      "epoch 97 | loss: 0.40848 |  0:00:04s\n",
      "epoch 98 | loss: 0.40442 |  0:00:04s\n",
      "epoch 99 | loss: 0.42191 |  0:00:04s\n",
      "epoch 100| loss: 0.41119 |  0:00:04s\n",
      "epoch 101| loss: 0.41834 |  0:00:04s\n",
      "epoch 102| loss: 0.40747 |  0:00:04s\n",
      "epoch 103| loss: 0.40798 |  0:00:04s\n",
      "epoch 104| loss: 0.4028  |  0:00:04s\n",
      "epoch 105| loss: 0.40025 |  0:00:04s\n",
      "epoch 106| loss: 0.4098  |  0:00:04s\n",
      "epoch 107| loss: 0.39643 |  0:00:04s\n",
      "epoch 108| loss: 0.39594 |  0:00:05s\n",
      "epoch 109| loss: 0.39798 |  0:00:05s\n",
      "epoch 110| loss: 0.39111 |  0:00:05s\n",
      "epoch 111| loss: 0.39214 |  0:00:05s\n",
      "epoch 112| loss: 0.39462 |  0:00:05s\n",
      "epoch 113| loss: 0.38007 |  0:00:05s\n",
      "epoch 114| loss: 0.37642 |  0:00:05s\n",
      "epoch 115| loss: 0.37329 |  0:00:05s\n",
      "epoch 116| loss: 0.37025 |  0:00:05s\n",
      "epoch 117| loss: 0.37967 |  0:00:05s\n",
      "epoch 118| loss: 0.37113 |  0:00:05s\n",
      "epoch 119| loss: 0.36776 |  0:00:05s\n",
      "epoch 120| loss: 0.37252 |  0:00:05s\n",
      "epoch 121| loss: 0.36326 |  0:00:05s\n",
      "epoch 122| loss: 0.35998 |  0:00:05s\n",
      "epoch 123| loss: 0.36636 |  0:00:05s\n",
      "epoch 124| loss: 0.35905 |  0:00:05s\n",
      "epoch 125| loss: 0.3554  |  0:00:05s\n",
      "epoch 126| loss: 0.36031 |  0:00:05s\n",
      "epoch 127| loss: 0.35882 |  0:00:05s\n",
      "epoch 128| loss: 0.36944 |  0:00:05s\n",
      "epoch 129| loss: 0.37737 |  0:00:05s\n",
      "epoch 130| loss: 0.37077 |  0:00:06s\n",
      "epoch 131| loss: 0.36151 |  0:00:06s\n",
      "epoch 132| loss: 0.37039 |  0:00:06s\n",
      "epoch 133| loss: 0.36501 |  0:00:06s\n",
      "epoch 134| loss: 0.36083 |  0:00:06s\n",
      "epoch 135| loss: 0.35881 |  0:00:06s\n",
      "epoch 136| loss: 0.36722 |  0:00:06s\n",
      "epoch 137| loss: 0.36503 |  0:00:06s\n",
      "epoch 138| loss: 0.36404 |  0:00:06s\n",
      "epoch 139| loss: 0.37051 |  0:00:06s\n",
      "epoch 140| loss: 0.36479 |  0:00:06s\n",
      "epoch 141| loss: 0.3565  |  0:00:06s\n",
      "epoch 142| loss: 0.36312 |  0:00:06s\n",
      "epoch 143| loss: 0.36054 |  0:00:06s\n",
      "epoch 144| loss: 0.36835 |  0:00:06s\n",
      "epoch 145| loss: 0.36896 |  0:00:06s\n",
      "epoch 146| loss: 0.36502 |  0:00:06s\n",
      "epoch 147| loss: 0.35925 |  0:00:06s\n",
      "epoch 148| loss: 0.35894 |  0:00:06s\n",
      "epoch 149| loss: 0.35237 |  0:00:06s\n",
      "epoch 150| loss: 0.35185 |  0:00:06s\n",
      "epoch 151| loss: 0.34669 |  0:00:06s\n",
      "epoch 152| loss: 0.34534 |  0:00:07s\n",
      "epoch 153| loss: 0.34185 |  0:00:07s\n",
      "epoch 154| loss: 0.33937 |  0:00:07s\n",
      "epoch 155| loss: 0.34747 |  0:00:07s\n",
      "epoch 156| loss: 0.35539 |  0:00:07s\n",
      "epoch 157| loss: 0.34559 |  0:00:07s\n",
      "epoch 158| loss: 0.34043 |  0:00:07s\n",
      "epoch 159| loss: 0.3348  |  0:00:07s\n",
      "epoch 160| loss: 0.34149 |  0:00:07s\n",
      "epoch 161| loss: 0.3322  |  0:00:07s\n",
      "epoch 162| loss: 0.32962 |  0:00:07s\n",
      "epoch 163| loss: 0.32468 |  0:00:07s\n",
      "epoch 164| loss: 0.32125 |  0:00:07s\n",
      "epoch 165| loss: 0.31948 |  0:00:07s\n",
      "epoch 166| loss: 0.32236 |  0:00:07s\n",
      "epoch 167| loss: 0.31868 |  0:00:07s\n",
      "epoch 168| loss: 0.31624 |  0:00:07s\n",
      "epoch 169| loss: 0.3139  |  0:00:07s\n",
      "epoch 170| loss: 0.31559 |  0:00:07s\n",
      "epoch 171| loss: 0.30677 |  0:00:07s\n",
      "epoch 172| loss: 0.30918 |  0:00:07s\n",
      "epoch 173| loss: 0.29922 |  0:00:07s\n",
      "epoch 174| loss: 0.30665 |  0:00:08s\n",
      "epoch 175| loss: 0.30371 |  0:00:08s\n",
      "epoch 176| loss: 0.29628 |  0:00:08s\n",
      "epoch 177| loss: 0.30441 |  0:00:08s\n",
      "epoch 178| loss: 0.30085 |  0:00:08s\n",
      "epoch 179| loss: 0.30235 |  0:00:08s\n",
      "epoch 180| loss: 0.30387 |  0:00:08s\n",
      "epoch 181| loss: 0.31741 |  0:00:08s\n",
      "epoch 182| loss: 0.31894 |  0:00:08s\n",
      "epoch 183| loss: 0.31383 |  0:00:08s\n",
      "epoch 184| loss: 0.31014 |  0:00:08s\n",
      "epoch 185| loss: 0.31558 |  0:00:08s\n",
      "epoch 186| loss: 0.31985 |  0:00:08s\n",
      "epoch 187| loss: 0.30406 |  0:00:08s\n",
      "epoch 188| loss: 0.30733 |  0:00:08s\n",
      "epoch 189| loss: 0.30305 |  0:00:08s\n",
      "epoch 190| loss: 0.29447 |  0:00:08s\n",
      "epoch 191| loss: 0.29651 |  0:00:08s\n",
      "epoch 192| loss: 0.29187 |  0:00:09s\n",
      "epoch 193| loss: 0.2993  |  0:00:09s\n",
      "epoch 194| loss: 0.28972 |  0:00:09s\n",
      "epoch 195| loss: 0.29058 |  0:00:09s\n",
      "epoch 196| loss: 0.28876 |  0:00:09s\n",
      "epoch 197| loss: 0.28636 |  0:00:09s\n",
      "epoch 198| loss: 0.29159 |  0:00:09s\n",
      "epoch 199| loss: 0.30451 |  0:00:09s\n",
      "epoch 200| loss: 0.30922 |  0:00:09s\n",
      "epoch 201| loss: 0.29962 |  0:00:09s\n",
      "epoch 202| loss: 0.3021  |  0:00:09s\n",
      "epoch 203| loss: 0.29378 |  0:00:09s\n",
      "epoch 204| loss: 0.29335 |  0:00:09s\n",
      "epoch 205| loss: 0.29162 |  0:00:09s\n",
      "epoch 206| loss: 0.28853 |  0:00:09s\n",
      "epoch 207| loss: 0.28869 |  0:00:09s\n",
      "epoch 208| loss: 0.28941 |  0:00:09s\n",
      "epoch 209| loss: 0.28883 |  0:00:09s\n",
      "epoch 210| loss: 0.28391 |  0:00:09s\n",
      "epoch 211| loss: 0.29046 |  0:00:09s\n",
      "epoch 212| loss: 0.2796  |  0:00:09s\n",
      "epoch 213| loss: 0.2755  |  0:00:09s\n",
      "epoch 214| loss: 0.27915 |  0:00:10s\n",
      "epoch 215| loss: 0.292   |  0:00:10s\n",
      "epoch 216| loss: 0.28499 |  0:00:10s\n",
      "epoch 217| loss: 0.27679 |  0:00:10s\n",
      "epoch 218| loss: 0.27263 |  0:00:10s\n",
      "epoch 219| loss: 0.27555 |  0:00:10s\n",
      "epoch 220| loss: 0.27353 |  0:00:10s\n",
      "epoch 221| loss: 0.27219 |  0:00:10s\n",
      "epoch 222| loss: 0.27075 |  0:00:10s\n",
      "epoch 223| loss: 0.27916 |  0:00:10s\n",
      "epoch 224| loss: 0.2764  |  0:00:10s\n",
      "epoch 225| loss: 0.27332 |  0:00:10s\n",
      "epoch 226| loss: 0.26751 |  0:00:10s\n",
      "epoch 227| loss: 0.26312 |  0:00:10s\n",
      "epoch 228| loss: 0.26886 |  0:00:10s\n",
      "epoch 229| loss: 0.27166 |  0:00:10s\n",
      "epoch 230| loss: 0.27063 |  0:00:10s\n",
      "epoch 231| loss: 0.26526 |  0:00:10s\n",
      "epoch 232| loss: 0.26587 |  0:00:10s\n",
      "epoch 233| loss: 0.26638 |  0:00:10s\n",
      "epoch 234| loss: 0.26468 |  0:00:10s\n",
      "epoch 235| loss: 0.26742 |  0:00:10s\n",
      "epoch 236| loss: 0.26416 |  0:00:11s\n",
      "epoch 237| loss: 0.27199 |  0:00:11s\n",
      "epoch 238| loss: 0.2683  |  0:00:11s\n",
      "epoch 239| loss: 0.26649 |  0:00:11s\n",
      "epoch 240| loss: 0.25952 |  0:00:11s\n",
      "epoch 241| loss: 0.26006 |  0:00:11s\n",
      "epoch 242| loss: 0.2582  |  0:00:11s\n",
      "epoch 243| loss: 0.24807 |  0:00:11s\n",
      "epoch 244| loss: 0.25054 |  0:00:11s\n",
      "epoch 245| loss: 0.24916 |  0:00:11s\n",
      "epoch 246| loss: 0.24414 |  0:00:11s\n",
      "epoch 247| loss: 0.2544  |  0:00:11s\n",
      "epoch 248| loss: 0.25387 |  0:00:11s\n",
      "epoch 249| loss: 0.25928 |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.90312 |  0:00:00s\n",
      "epoch 1  | loss: 1.82937 |  0:00:00s\n",
      "epoch 2  | loss: 1.35273 |  0:00:00s\n",
      "epoch 3  | loss: 1.11246 |  0:00:00s\n",
      "epoch 4  | loss: 0.99581 |  0:00:00s\n",
      "epoch 5  | loss: 0.89168 |  0:00:00s\n",
      "epoch 6  | loss: 0.87257 |  0:00:00s\n",
      "epoch 7  | loss: 0.81479 |  0:00:00s\n",
      "epoch 8  | loss: 0.79641 |  0:00:00s\n",
      "epoch 9  | loss: 0.75948 |  0:00:00s\n",
      "epoch 10 | loss: 0.7513  |  0:00:00s\n",
      "epoch 11 | loss: 0.74164 |  0:00:00s\n",
      "epoch 12 | loss: 0.71427 |  0:00:00s\n",
      "epoch 13 | loss: 0.70343 |  0:00:00s\n",
      "epoch 14 | loss: 0.69776 |  0:00:00s\n",
      "epoch 15 | loss: 0.68709 |  0:00:00s\n",
      "epoch 16 | loss: 0.68268 |  0:00:00s\n",
      "epoch 17 | loss: 0.67571 |  0:00:00s\n",
      "epoch 18 | loss: 0.68922 |  0:00:00s\n",
      "epoch 19 | loss: 0.66193 |  0:00:00s\n",
      "epoch 20 | loss: 0.67518 |  0:00:00s\n",
      "epoch 21 | loss: 0.66043 |  0:00:01s\n",
      "epoch 22 | loss: 0.65276 |  0:00:01s\n",
      "epoch 23 | loss: 0.6513  |  0:00:01s\n",
      "epoch 24 | loss: 0.64047 |  0:00:01s\n",
      "epoch 25 | loss: 0.64432 |  0:00:01s\n",
      "epoch 26 | loss: 0.6324  |  0:00:01s\n",
      "epoch 27 | loss: 0.62338 |  0:00:01s\n",
      "epoch 28 | loss: 0.63318 |  0:00:01s\n",
      "epoch 29 | loss: 0.628   |  0:00:01s\n",
      "epoch 30 | loss: 0.62948 |  0:00:01s\n",
      "epoch 31 | loss: 0.62247 |  0:00:01s\n",
      "epoch 32 | loss: 0.6168  |  0:00:01s\n",
      "epoch 33 | loss: 0.61233 |  0:00:01s\n",
      "epoch 34 | loss: 0.6038  |  0:00:01s\n",
      "epoch 35 | loss: 0.60473 |  0:00:01s\n",
      "epoch 36 | loss: 0.59764 |  0:00:01s\n",
      "epoch 37 | loss: 0.59466 |  0:00:01s\n",
      "epoch 38 | loss: 0.593   |  0:00:01s\n",
      "epoch 39 | loss: 0.58532 |  0:00:01s\n",
      "epoch 40 | loss: 0.58306 |  0:00:02s\n",
      "epoch 41 | loss: 0.57409 |  0:00:02s\n",
      "epoch 42 | loss: 0.56402 |  0:00:02s\n",
      "epoch 43 | loss: 0.56704 |  0:00:02s\n",
      "epoch 44 | loss: 0.56506 |  0:00:02s\n",
      "epoch 45 | loss: 0.55633 |  0:00:02s\n",
      "epoch 46 | loss: 0.56282 |  0:00:02s\n",
      "epoch 47 | loss: 0.55757 |  0:00:02s\n",
      "epoch 48 | loss: 0.55865 |  0:00:02s\n",
      "epoch 49 | loss: 0.55381 |  0:00:02s\n",
      "epoch 50 | loss: 0.56132 |  0:00:02s\n",
      "epoch 51 | loss: 0.53872 |  0:00:02s\n",
      "epoch 52 | loss: 0.54455 |  0:00:02s\n",
      "epoch 53 | loss: 0.54053 |  0:00:02s\n",
      "epoch 54 | loss: 0.54113 |  0:00:02s\n",
      "epoch 55 | loss: 0.52765 |  0:00:02s\n",
      "epoch 56 | loss: 0.52324 |  0:00:03s\n",
      "epoch 57 | loss: 0.53015 |  0:00:03s\n",
      "epoch 58 | loss: 0.5283  |  0:00:03s\n",
      "epoch 59 | loss: 0.52088 |  0:00:03s\n",
      "epoch 60 | loss: 0.52699 |  0:00:03s\n",
      "epoch 61 | loss: 0.5178  |  0:00:03s\n",
      "epoch 62 | loss: 0.51678 |  0:00:03s\n",
      "epoch 63 | loss: 0.50198 |  0:00:03s\n",
      "epoch 64 | loss: 0.49951 |  0:00:03s\n",
      "epoch 65 | loss: 0.49044 |  0:00:03s\n",
      "epoch 66 | loss: 0.48578 |  0:00:03s\n",
      "epoch 67 | loss: 0.4935  |  0:00:03s\n",
      "epoch 68 | loss: 0.49869 |  0:00:03s\n",
      "epoch 69 | loss: 0.49294 |  0:00:03s\n",
      "epoch 70 | loss: 0.4919  |  0:00:03s\n",
      "epoch 71 | loss: 0.49356 |  0:00:03s\n",
      "epoch 72 | loss: 0.4785  |  0:00:03s\n",
      "epoch 73 | loss: 0.47429 |  0:00:03s\n",
      "epoch 74 | loss: 0.47795 |  0:00:03s\n",
      "epoch 75 | loss: 0.46894 |  0:00:03s\n",
      "epoch 76 | loss: 0.47325 |  0:00:03s\n",
      "epoch 77 | loss: 0.46942 |  0:00:03s\n",
      "epoch 78 | loss: 0.46409 |  0:00:04s\n",
      "epoch 79 | loss: 0.47192 |  0:00:04s\n",
      "epoch 80 | loss: 0.46918 |  0:00:04s\n",
      "epoch 81 | loss: 0.46737 |  0:00:04s\n",
      "epoch 82 | loss: 0.46356 |  0:00:04s\n",
      "epoch 83 | loss: 0.45702 |  0:00:04s\n",
      "epoch 84 | loss: 0.4573  |  0:00:04s\n",
      "epoch 85 | loss: 0.4605  |  0:00:04s\n",
      "epoch 86 | loss: 0.45844 |  0:00:04s\n",
      "epoch 87 | loss: 0.44729 |  0:00:04s\n",
      "epoch 88 | loss: 0.45121 |  0:00:04s\n",
      "epoch 89 | loss: 0.45634 |  0:00:04s\n",
      "epoch 90 | loss: 0.47098 |  0:00:04s\n",
      "epoch 91 | loss: 0.46526 |  0:00:04s\n",
      "epoch 92 | loss: 0.47121 |  0:00:04s\n",
      "epoch 93 | loss: 0.44995 |  0:00:04s\n",
      "epoch 94 | loss: 0.45153 |  0:00:04s\n",
      "epoch 95 | loss: 0.44933 |  0:00:04s\n",
      "epoch 96 | loss: 0.44599 |  0:00:04s\n",
      "epoch 97 | loss: 0.44425 |  0:00:04s\n",
      "epoch 98 | loss: 0.44205 |  0:00:04s\n",
      "epoch 99 | loss: 0.43812 |  0:00:04s\n",
      "epoch 100| loss: 0.43229 |  0:00:05s\n",
      "epoch 101| loss: 0.43397 |  0:00:05s\n",
      "epoch 102| loss: 0.42589 |  0:00:05s\n",
      "epoch 103| loss: 0.43172 |  0:00:05s\n",
      "epoch 104| loss: 0.42724 |  0:00:05s\n",
      "epoch 105| loss: 0.43282 |  0:00:05s\n",
      "epoch 106| loss: 0.43176 |  0:00:05s\n",
      "epoch 107| loss: 0.43432 |  0:00:05s\n",
      "epoch 108| loss: 0.42829 |  0:00:05s\n",
      "epoch 109| loss: 0.42463 |  0:00:05s\n",
      "epoch 110| loss: 0.42935 |  0:00:05s\n",
      "epoch 111| loss: 0.42126 |  0:00:05s\n",
      "epoch 112| loss: 0.43566 |  0:00:05s\n",
      "epoch 113| loss: 0.42844 |  0:00:05s\n",
      "epoch 114| loss: 0.42226 |  0:00:05s\n",
      "epoch 115| loss: 0.44214 |  0:00:05s\n",
      "epoch 116| loss: 0.4193  |  0:00:05s\n",
      "epoch 117| loss: 0.41777 |  0:00:05s\n",
      "epoch 118| loss: 0.41112 |  0:00:05s\n",
      "epoch 119| loss: 0.40773 |  0:00:05s\n",
      "epoch 120| loss: 0.40868 |  0:00:05s\n",
      "epoch 121| loss: 0.40103 |  0:00:06s\n",
      "epoch 122| loss: 0.39531 |  0:00:06s\n",
      "epoch 123| loss: 0.40287 |  0:00:06s\n",
      "epoch 124| loss: 0.39586 |  0:00:06s\n",
      "epoch 125| loss: 0.39226 |  0:00:06s\n",
      "epoch 126| loss: 0.39106 |  0:00:06s\n",
      "epoch 127| loss: 0.39855 |  0:00:06s\n",
      "epoch 128| loss: 0.3924  |  0:00:06s\n",
      "epoch 129| loss: 0.39886 |  0:00:06s\n",
      "epoch 130| loss: 0.40081 |  0:00:06s\n",
      "epoch 131| loss: 0.40233 |  0:00:06s\n",
      "epoch 132| loss: 0.40454 |  0:00:06s\n",
      "epoch 133| loss: 0.39113 |  0:00:06s\n",
      "epoch 134| loss: 0.39491 |  0:00:06s\n",
      "epoch 135| loss: 0.38479 |  0:00:06s\n",
      "epoch 136| loss: 0.38189 |  0:00:06s\n",
      "epoch 137| loss: 0.37413 |  0:00:06s\n",
      "epoch 138| loss: 0.36831 |  0:00:06s\n",
      "epoch 139| loss: 0.36477 |  0:00:06s\n",
      "epoch 140| loss: 0.37357 |  0:00:06s\n",
      "epoch 141| loss: 0.36753 |  0:00:06s\n",
      "epoch 142| loss: 0.37024 |  0:00:06s\n",
      "epoch 143| loss: 0.37096 |  0:00:07s\n",
      "epoch 144| loss: 0.36952 |  0:00:07s\n",
      "epoch 145| loss: 0.36918 |  0:00:07s\n",
      "epoch 146| loss: 0.36059 |  0:00:07s\n",
      "epoch 147| loss: 0.36246 |  0:00:07s\n",
      "epoch 148| loss: 0.36016 |  0:00:07s\n",
      "epoch 149| loss: 0.35392 |  0:00:07s\n",
      "epoch 150| loss: 0.35121 |  0:00:07s\n",
      "epoch 151| loss: 0.35602 |  0:00:07s\n",
      "epoch 152| loss: 0.35196 |  0:00:07s\n",
      "epoch 153| loss: 0.35541 |  0:00:07s\n",
      "epoch 154| loss: 0.35019 |  0:00:07s\n",
      "epoch 155| loss: 0.35088 |  0:00:07s\n",
      "epoch 156| loss: 0.3554  |  0:00:07s\n",
      "epoch 157| loss: 0.36096 |  0:00:07s\n",
      "epoch 158| loss: 0.3501  |  0:00:07s\n",
      "epoch 159| loss: 0.35142 |  0:00:07s\n",
      "epoch 160| loss: 0.35333 |  0:00:07s\n",
      "epoch 161| loss: 0.35783 |  0:00:07s\n",
      "epoch 162| loss: 0.35036 |  0:00:07s\n",
      "epoch 163| loss: 0.35928 |  0:00:07s\n",
      "epoch 164| loss: 0.37586 |  0:00:07s\n",
      "epoch 165| loss: 0.38581 |  0:00:08s\n",
      "epoch 166| loss: 0.37324 |  0:00:08s\n",
      "epoch 167| loss: 0.36412 |  0:00:08s\n",
      "epoch 168| loss: 0.36222 |  0:00:08s\n",
      "epoch 169| loss: 0.35598 |  0:00:08s\n",
      "epoch 170| loss: 0.35646 |  0:00:08s\n",
      "epoch 171| loss: 0.34224 |  0:00:08s\n",
      "epoch 172| loss: 0.35105 |  0:00:08s\n",
      "epoch 173| loss: 0.33797 |  0:00:08s\n",
      "epoch 174| loss: 0.33202 |  0:00:08s\n",
      "epoch 175| loss: 0.33049 |  0:00:08s\n",
      "epoch 176| loss: 0.32751 |  0:00:08s\n",
      "epoch 177| loss: 0.32482 |  0:00:08s\n",
      "epoch 178| loss: 0.31635 |  0:00:08s\n",
      "epoch 179| loss: 0.3228  |  0:00:08s\n",
      "epoch 180| loss: 0.31343 |  0:00:08s\n",
      "epoch 181| loss: 0.31327 |  0:00:08s\n",
      "epoch 182| loss: 0.31326 |  0:00:08s\n",
      "epoch 183| loss: 0.30319 |  0:00:08s\n",
      "epoch 184| loss: 0.30537 |  0:00:08s\n",
      "epoch 185| loss: 0.31249 |  0:00:08s\n",
      "epoch 186| loss: 0.308   |  0:00:09s\n",
      "epoch 187| loss: 0.30463 |  0:00:09s\n",
      "epoch 188| loss: 0.30453 |  0:00:09s\n",
      "epoch 189| loss: 0.30831 |  0:00:09s\n",
      "epoch 190| loss: 0.31286 |  0:00:09s\n",
      "epoch 191| loss: 0.30536 |  0:00:09s\n",
      "epoch 192| loss: 0.30202 |  0:00:09s\n",
      "epoch 193| loss: 0.30184 |  0:00:09s\n",
      "epoch 194| loss: 0.29919 |  0:00:09s\n",
      "epoch 195| loss: 0.30124 |  0:00:09s\n",
      "epoch 196| loss: 0.29077 |  0:00:09s\n",
      "epoch 197| loss: 0.29523 |  0:00:09s\n",
      "epoch 198| loss: 0.29587 |  0:00:09s\n",
      "epoch 199| loss: 0.29272 |  0:00:09s\n",
      "epoch 200| loss: 0.29893 |  0:00:09s\n",
      "epoch 201| loss: 0.31505 |  0:00:09s\n",
      "epoch 202| loss: 0.35067 |  0:00:09s\n",
      "epoch 203| loss: 0.36358 |  0:00:09s\n",
      "epoch 204| loss: 0.35136 |  0:00:09s\n",
      "epoch 205| loss: 0.34552 |  0:00:09s\n",
      "epoch 206| loss: 0.35299 |  0:00:09s\n",
      "epoch 207| loss: 0.33592 |  0:00:09s\n",
      "epoch 208| loss: 0.33353 |  0:00:10s\n",
      "epoch 209| loss: 0.3324  |  0:00:10s\n",
      "epoch 210| loss: 0.3256  |  0:00:10s\n",
      "epoch 211| loss: 0.33327 |  0:00:10s\n",
      "epoch 212| loss: 0.32592 |  0:00:10s\n",
      "epoch 213| loss: 0.31942 |  0:00:10s\n",
      "epoch 214| loss: 0.32323 |  0:00:10s\n",
      "epoch 215| loss: 0.31663 |  0:00:10s\n",
      "epoch 216| loss: 0.31751 |  0:00:10s\n",
      "epoch 217| loss: 0.30629 |  0:00:10s\n",
      "epoch 218| loss: 0.31168 |  0:00:10s\n",
      "epoch 219| loss: 0.32736 |  0:00:10s\n",
      "epoch 220| loss: 0.32782 |  0:00:10s\n",
      "epoch 221| loss: 0.33099 |  0:00:10s\n",
      "epoch 222| loss: 0.32916 |  0:00:10s\n",
      "epoch 223| loss: 0.32363 |  0:00:10s\n",
      "epoch 224| loss: 0.33125 |  0:00:10s\n",
      "epoch 225| loss: 0.32365 |  0:00:10s\n",
      "epoch 226| loss: 0.31769 |  0:00:10s\n",
      "epoch 227| loss: 0.32453 |  0:00:10s\n",
      "epoch 228| loss: 0.33096 |  0:00:10s\n",
      "epoch 229| loss: 0.32529 |  0:00:10s\n",
      "epoch 230| loss: 0.31471 |  0:00:11s\n",
      "epoch 231| loss: 0.31343 |  0:00:11s\n",
      "epoch 232| loss: 0.31018 |  0:00:11s\n",
      "epoch 233| loss: 0.3094  |  0:00:11s\n",
      "epoch 234| loss: 0.31372 |  0:00:11s\n",
      "epoch 235| loss: 0.30388 |  0:00:11s\n",
      "epoch 236| loss: 0.29645 |  0:00:11s\n",
      "epoch 237| loss: 0.29853 |  0:00:11s\n",
      "epoch 238| loss: 0.29435 |  0:00:11s\n",
      "epoch 239| loss: 0.29751 |  0:00:11s\n",
      "epoch 240| loss: 0.28702 |  0:00:11s\n",
      "epoch 241| loss: 0.28509 |  0:00:11s\n",
      "epoch 242| loss: 0.28524 |  0:00:11s\n",
      "epoch 243| loss: 0.27764 |  0:00:11s\n",
      "epoch 244| loss: 0.28016 |  0:00:11s\n",
      "epoch 245| loss: 0.27707 |  0:00:11s\n",
      "epoch 246| loss: 0.27205 |  0:00:11s\n",
      "epoch 247| loss: 0.27508 |  0:00:11s\n",
      "epoch 248| loss: 0.27337 |  0:00:11s\n",
      "epoch 249| loss: 0.27239 |  0:00:11s\n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.83318 |  0:00:00s\n",
      "epoch 1  | loss: 1.87989 |  0:00:00s\n",
      "epoch 2  | loss: 1.35903 |  0:00:00s\n",
      "epoch 3  | loss: 1.20324 |  0:00:00s\n",
      "epoch 4  | loss: 1.06462 |  0:00:00s\n",
      "epoch 5  | loss: 0.98164 |  0:00:00s\n",
      "epoch 6  | loss: 0.94371 |  0:00:00s\n",
      "epoch 7  | loss: 0.90733 |  0:00:00s\n",
      "epoch 8  | loss: 0.85661 |  0:00:00s\n",
      "epoch 9  | loss: 0.81064 |  0:00:00s\n",
      "epoch 10 | loss: 0.76046 |  0:00:00s\n",
      "epoch 11 | loss: 0.76147 |  0:00:00s\n",
      "epoch 12 | loss: 0.74868 |  0:00:00s\n",
      "epoch 13 | loss: 0.73283 |  0:00:00s\n",
      "epoch 14 | loss: 0.71462 |  0:00:00s\n",
      "epoch 15 | loss: 0.69352 |  0:00:00s\n",
      "epoch 16 | loss: 0.69213 |  0:00:00s\n",
      "epoch 17 | loss: 0.69754 |  0:00:00s\n",
      "epoch 18 | loss: 0.683   |  0:00:00s\n",
      "epoch 19 | loss: 0.67194 |  0:00:00s\n",
      "epoch 20 | loss: 0.666   |  0:00:00s\n",
      "epoch 21 | loss: 0.65653 |  0:00:01s\n",
      "epoch 22 | loss: 0.6639  |  0:00:01s\n",
      "epoch 23 | loss: 0.65591 |  0:00:01s\n",
      "epoch 24 | loss: 0.64568 |  0:00:01s\n",
      "epoch 25 | loss: 0.65045 |  0:00:01s\n",
      "epoch 26 | loss: 0.63904 |  0:00:01s\n",
      "epoch 27 | loss: 0.64806 |  0:00:01s\n",
      "epoch 28 | loss: 0.63989 |  0:00:01s\n",
      "epoch 29 | loss: 0.63983 |  0:00:01s\n",
      "epoch 30 | loss: 0.64104 |  0:00:01s\n",
      "epoch 31 | loss: 0.63268 |  0:00:01s\n",
      "epoch 32 | loss: 0.6283  |  0:00:01s\n",
      "epoch 33 | loss: 0.63785 |  0:00:01s\n",
      "epoch 34 | loss: 0.63018 |  0:00:01s\n",
      "epoch 35 | loss: 0.61098 |  0:00:01s\n",
      "epoch 36 | loss: 0.61691 |  0:00:01s\n",
      "epoch 37 | loss: 0.61495 |  0:00:01s\n",
      "epoch 38 | loss: 0.61213 |  0:00:01s\n",
      "epoch 39 | loss: 0.59245 |  0:00:01s\n",
      "epoch 40 | loss: 0.60594 |  0:00:01s\n",
      "epoch 41 | loss: 0.60183 |  0:00:01s\n",
      "epoch 42 | loss: 0.60132 |  0:00:01s\n",
      "epoch 43 | loss: 0.59025 |  0:00:02s\n",
      "epoch 44 | loss: 0.58188 |  0:00:02s\n",
      "epoch 45 | loss: 0.57922 |  0:00:02s\n",
      "epoch 46 | loss: 0.57877 |  0:00:02s\n",
      "epoch 47 | loss: 0.56609 |  0:00:02s\n",
      "epoch 48 | loss: 0.5662  |  0:00:02s\n",
      "epoch 49 | loss: 0.56239 |  0:00:02s\n",
      "epoch 50 | loss: 0.55476 |  0:00:02s\n",
      "epoch 51 | loss: 0.54662 |  0:00:02s\n",
      "epoch 52 | loss: 0.55317 |  0:00:02s\n",
      "epoch 53 | loss: 0.54105 |  0:00:02s\n",
      "epoch 54 | loss: 0.54231 |  0:00:02s\n",
      "epoch 55 | loss: 0.53217 |  0:00:02s\n",
      "epoch 56 | loss: 0.52899 |  0:00:02s\n",
      "epoch 57 | loss: 0.52897 |  0:00:02s\n",
      "epoch 58 | loss: 0.53954 |  0:00:02s\n",
      "epoch 59 | loss: 0.52379 |  0:00:02s\n",
      "epoch 60 | loss: 0.53089 |  0:00:02s\n",
      "epoch 61 | loss: 0.50843 |  0:00:02s\n",
      "epoch 62 | loss: 0.51762 |  0:00:02s\n",
      "epoch 63 | loss: 0.50044 |  0:00:02s\n",
      "epoch 64 | loss: 0.50289 |  0:00:02s\n",
      "epoch 65 | loss: 0.49875 |  0:00:03s\n",
      "epoch 66 | loss: 0.497   |  0:00:03s\n",
      "epoch 67 | loss: 0.50219 |  0:00:03s\n",
      "epoch 68 | loss: 0.50148 |  0:00:03s\n",
      "epoch 69 | loss: 0.50195 |  0:00:03s\n",
      "epoch 70 | loss: 0.49428 |  0:00:03s\n",
      "epoch 71 | loss: 0.5002  |  0:00:03s\n",
      "epoch 72 | loss: 0.50037 |  0:00:03s\n",
      "epoch 73 | loss: 0.49395 |  0:00:03s\n",
      "epoch 74 | loss: 0.49488 |  0:00:03s\n",
      "epoch 75 | loss: 0.49641 |  0:00:03s\n",
      "epoch 76 | loss: 0.48633 |  0:00:03s\n",
      "epoch 77 | loss: 0.4766  |  0:00:03s\n",
      "epoch 78 | loss: 0.48884 |  0:00:03s\n",
      "epoch 79 | loss: 0.49118 |  0:00:03s\n",
      "epoch 80 | loss: 0.47736 |  0:00:03s\n",
      "epoch 81 | loss: 0.46935 |  0:00:03s\n",
      "epoch 82 | loss: 0.47119 |  0:00:03s\n",
      "epoch 83 | loss: 0.45978 |  0:00:03s\n",
      "epoch 84 | loss: 0.4648  |  0:00:04s\n",
      "epoch 85 | loss: 0.45647 |  0:00:04s\n",
      "epoch 86 | loss: 0.45053 |  0:00:04s\n",
      "epoch 87 | loss: 0.45464 |  0:00:04s\n",
      "epoch 88 | loss: 0.45497 |  0:00:04s\n",
      "epoch 89 | loss: 0.45883 |  0:00:04s\n",
      "epoch 90 | loss: 0.44787 |  0:00:04s\n",
      "epoch 91 | loss: 0.44183 |  0:00:04s\n",
      "epoch 92 | loss: 0.44408 |  0:00:04s\n",
      "epoch 93 | loss: 0.44139 |  0:00:04s\n",
      "epoch 94 | loss: 0.43892 |  0:00:04s\n",
      "epoch 95 | loss: 0.43083 |  0:00:04s\n",
      "epoch 96 | loss: 0.43096 |  0:00:04s\n",
      "epoch 97 | loss: 0.42401 |  0:00:04s\n",
      "epoch 98 | loss: 0.41794 |  0:00:04s\n",
      "epoch 99 | loss: 0.41916 |  0:00:04s\n",
      "epoch 100| loss: 0.41807 |  0:00:04s\n",
      "epoch 101| loss: 0.41339 |  0:00:04s\n",
      "epoch 102| loss: 0.42371 |  0:00:04s\n",
      "epoch 103| loss: 0.41691 |  0:00:04s\n",
      "epoch 104| loss: 0.42159 |  0:00:05s\n",
      "epoch 105| loss: 0.41334 |  0:00:05s\n",
      "epoch 106| loss: 0.41357 |  0:00:05s\n",
      "epoch 107| loss: 0.40634 |  0:00:05s\n",
      "epoch 108| loss: 0.39408 |  0:00:05s\n",
      "epoch 109| loss: 0.40169 |  0:00:05s\n",
      "epoch 110| loss: 0.39688 |  0:00:05s\n",
      "epoch 111| loss: 0.39262 |  0:00:05s\n",
      "epoch 112| loss: 0.39052 |  0:00:05s\n",
      "epoch 113| loss: 0.39365 |  0:00:05s\n",
      "epoch 114| loss: 0.39206 |  0:00:05s\n",
      "epoch 115| loss: 0.39104 |  0:00:05s\n",
      "epoch 116| loss: 0.39238 |  0:00:05s\n",
      "epoch 117| loss: 0.38096 |  0:00:05s\n",
      "epoch 118| loss: 0.38106 |  0:00:05s\n",
      "epoch 119| loss: 0.379   |  0:00:05s\n",
      "epoch 120| loss: 0.374   |  0:00:05s\n",
      "epoch 121| loss: 0.37204 |  0:00:05s\n",
      "epoch 122| loss: 0.37976 |  0:00:05s\n",
      "epoch 123| loss: 0.37621 |  0:00:05s\n",
      "epoch 124| loss: 0.37491 |  0:00:05s\n",
      "epoch 125| loss: 0.38481 |  0:00:06s\n",
      "epoch 126| loss: 0.38129 |  0:00:06s\n",
      "epoch 127| loss: 0.37565 |  0:00:06s\n",
      "epoch 128| loss: 0.37624 |  0:00:06s\n",
      "epoch 129| loss: 0.37744 |  0:00:06s\n",
      "epoch 130| loss: 0.38493 |  0:00:06s\n",
      "epoch 131| loss: 0.38648 |  0:00:06s\n",
      "epoch 132| loss: 0.38892 |  0:00:06s\n",
      "epoch 133| loss: 0.38181 |  0:00:06s\n",
      "epoch 134| loss: 0.3831  |  0:00:06s\n",
      "epoch 135| loss: 0.38688 |  0:00:06s\n",
      "epoch 136| loss: 0.39193 |  0:00:06s\n",
      "epoch 137| loss: 0.38774 |  0:00:06s\n",
      "epoch 138| loss: 0.38748 |  0:00:06s\n",
      "epoch 139| loss: 0.3884  |  0:00:06s\n",
      "epoch 140| loss: 0.38722 |  0:00:06s\n",
      "epoch 141| loss: 0.38398 |  0:00:06s\n",
      "epoch 142| loss: 0.37373 |  0:00:06s\n",
      "epoch 143| loss: 0.37248 |  0:00:06s\n",
      "epoch 144| loss: 0.37727 |  0:00:06s\n",
      "epoch 145| loss: 0.3705  |  0:00:06s\n",
      "epoch 146| loss: 0.36813 |  0:00:06s\n",
      "epoch 147| loss: 0.38058 |  0:00:07s\n",
      "epoch 148| loss: 0.37684 |  0:00:07s\n",
      "epoch 149| loss: 0.37308 |  0:00:07s\n",
      "epoch 150| loss: 0.37096 |  0:00:07s\n",
      "epoch 151| loss: 0.37376 |  0:00:07s\n",
      "epoch 152| loss: 0.36273 |  0:00:07s\n",
      "epoch 153| loss: 0.36454 |  0:00:07s\n",
      "epoch 154| loss: 0.35759 |  0:00:07s\n",
      "epoch 155| loss: 0.36032 |  0:00:07s\n",
      "epoch 156| loss: 0.36283 |  0:00:07s\n",
      "epoch 157| loss: 0.36111 |  0:00:07s\n",
      "epoch 158| loss: 0.35295 |  0:00:07s\n",
      "epoch 159| loss: 0.3483  |  0:00:07s\n",
      "epoch 160| loss: 0.3459  |  0:00:07s\n",
      "epoch 161| loss: 0.34166 |  0:00:07s\n",
      "epoch 162| loss: 0.33389 |  0:00:07s\n",
      "epoch 163| loss: 0.33493 |  0:00:07s\n",
      "epoch 164| loss: 0.34001 |  0:00:07s\n",
      "epoch 165| loss: 0.33367 |  0:00:07s\n",
      "epoch 166| loss: 0.33851 |  0:00:07s\n",
      "epoch 167| loss: 0.33557 |  0:00:07s\n",
      "epoch 168| loss: 0.3318  |  0:00:07s\n",
      "epoch 169| loss: 0.33509 |  0:00:08s\n",
      "epoch 170| loss: 0.335   |  0:00:08s\n",
      "epoch 171| loss: 0.32287 |  0:00:08s\n",
      "epoch 172| loss: 0.33198 |  0:00:08s\n",
      "epoch 173| loss: 0.32061 |  0:00:08s\n",
      "epoch 174| loss: 0.31829 |  0:00:08s\n",
      "epoch 175| loss: 0.315   |  0:00:08s\n",
      "epoch 176| loss: 0.31291 |  0:00:08s\n",
      "epoch 177| loss: 0.31825 |  0:00:08s\n",
      "epoch 178| loss: 0.31635 |  0:00:08s\n",
      "epoch 179| loss: 0.31313 |  0:00:08s\n",
      "epoch 180| loss: 0.30824 |  0:00:08s\n",
      "epoch 181| loss: 0.31033 |  0:00:08s\n",
      "epoch 182| loss: 0.32257 |  0:00:08s\n",
      "epoch 183| loss: 0.31793 |  0:00:08s\n",
      "epoch 184| loss: 0.31316 |  0:00:08s\n",
      "epoch 185| loss: 0.3153  |  0:00:09s\n",
      "epoch 186| loss: 0.32072 |  0:00:09s\n",
      "epoch 187| loss: 0.31007 |  0:00:09s\n",
      "epoch 188| loss: 0.30966 |  0:00:09s\n",
      "epoch 189| loss: 0.31555 |  0:00:09s\n",
      "epoch 190| loss: 0.30626 |  0:00:09s\n",
      "epoch 191| loss: 0.31099 |  0:00:09s\n",
      "epoch 192| loss: 0.30798 |  0:00:09s\n",
      "epoch 193| loss: 0.31485 |  0:00:09s\n",
      "epoch 194| loss: 0.31188 |  0:00:09s\n",
      "epoch 195| loss: 0.30812 |  0:00:09s\n",
      "epoch 196| loss: 0.31156 |  0:00:09s\n",
      "epoch 197| loss: 0.31483 |  0:00:09s\n",
      "epoch 198| loss: 0.31019 |  0:00:09s\n",
      "epoch 199| loss: 0.29486 |  0:00:09s\n",
      "epoch 200| loss: 0.30142 |  0:00:09s\n",
      "epoch 201| loss: 0.29852 |  0:00:09s\n",
      "epoch 202| loss: 0.29793 |  0:00:09s\n",
      "epoch 203| loss: 0.29802 |  0:00:09s\n",
      "epoch 204| loss: 0.29767 |  0:00:10s\n",
      "epoch 205| loss: 0.30333 |  0:00:10s\n",
      "epoch 206| loss: 0.30335 |  0:00:10s\n",
      "epoch 207| loss: 0.30488 |  0:00:10s\n",
      "epoch 208| loss: 0.30234 |  0:00:10s\n",
      "epoch 209| loss: 0.31117 |  0:00:10s\n",
      "epoch 210| loss: 0.30791 |  0:00:10s\n",
      "epoch 211| loss: 0.29704 |  0:00:10s\n",
      "epoch 212| loss: 0.29274 |  0:00:10s\n",
      "epoch 213| loss: 0.287   |  0:00:10s\n",
      "epoch 214| loss: 0.28903 |  0:00:10s\n",
      "epoch 215| loss: 0.28312 |  0:00:10s\n",
      "epoch 216| loss: 0.28441 |  0:00:10s\n",
      "epoch 217| loss: 0.28131 |  0:00:10s\n",
      "epoch 218| loss: 0.2903  |  0:00:10s\n",
      "epoch 219| loss: 0.2963  |  0:00:10s\n",
      "epoch 220| loss: 0.29134 |  0:00:10s\n",
      "epoch 221| loss: 0.28801 |  0:00:10s\n",
      "epoch 222| loss: 0.28973 |  0:00:10s\n",
      "epoch 223| loss: 0.29247 |  0:00:11s\n",
      "epoch 224| loss: 0.29657 |  0:00:11s\n",
      "epoch 225| loss: 0.29297 |  0:00:11s\n",
      "epoch 226| loss: 0.30032 |  0:00:11s\n",
      "epoch 227| loss: 0.28432 |  0:00:11s\n",
      "epoch 228| loss: 0.2852  |  0:00:11s\n",
      "epoch 229| loss: 0.28928 |  0:00:11s\n",
      "epoch 230| loss: 0.28502 |  0:00:11s\n",
      "epoch 231| loss: 0.27528 |  0:00:11s\n",
      "epoch 232| loss: 0.27797 |  0:00:11s\n",
      "epoch 233| loss: 0.27605 |  0:00:11s\n",
      "epoch 234| loss: 0.26616 |  0:00:11s\n",
      "epoch 235| loss: 0.26911 |  0:00:11s\n",
      "epoch 236| loss: 0.26454 |  0:00:11s\n",
      "epoch 237| loss: 0.26944 |  0:00:11s\n",
      "epoch 238| loss: 0.25831 |  0:00:11s\n",
      "epoch 239| loss: 0.26071 |  0:00:11s\n",
      "epoch 240| loss: 0.25657 |  0:00:11s\n",
      "epoch 241| loss: 0.25328 |  0:00:12s\n",
      "epoch 242| loss: 0.257   |  0:00:12s\n",
      "epoch 243| loss: 0.2548  |  0:00:12s\n",
      "epoch 244| loss: 0.25995 |  0:00:12s\n",
      "epoch 245| loss: 0.25795 |  0:00:12s\n",
      "epoch 246| loss: 0.25918 |  0:00:12s\n",
      "epoch 247| loss: 0.26527 |  0:00:12s\n",
      "epoch 248| loss: 0.26768 |  0:00:12s\n",
      "epoch 249| loss: 0.26514 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.8392  |  0:00:00s\n",
      "epoch 1  | loss: 1.87204 |  0:00:00s\n",
      "epoch 2  | loss: 1.37119 |  0:00:00s\n",
      "epoch 3  | loss: 1.14361 |  0:00:00s\n",
      "epoch 4  | loss: 1.02377 |  0:00:00s\n",
      "epoch 5  | loss: 0.91225 |  0:00:00s\n",
      "epoch 6  | loss: 0.87782 |  0:00:00s\n",
      "epoch 7  | loss: 0.82844 |  0:00:00s\n",
      "epoch 8  | loss: 0.80648 |  0:00:00s\n",
      "epoch 9  | loss: 0.77351 |  0:00:00s\n",
      "epoch 10 | loss: 0.75752 |  0:00:00s\n",
      "epoch 11 | loss: 0.73479 |  0:00:00s\n",
      "epoch 12 | loss: 0.72154 |  0:00:00s\n",
      "epoch 13 | loss: 0.71497 |  0:00:00s\n",
      "epoch 14 | loss: 0.70189 |  0:00:00s\n",
      "epoch 15 | loss: 0.69253 |  0:00:00s\n",
      "epoch 16 | loss: 0.67615 |  0:00:00s\n",
      "epoch 17 | loss: 0.67004 |  0:00:00s\n",
      "epoch 18 | loss: 0.67004 |  0:00:00s\n",
      "epoch 19 | loss: 0.67086 |  0:00:00s\n",
      "epoch 20 | loss: 0.64955 |  0:00:00s\n",
      "epoch 21 | loss: 0.6588  |  0:00:01s\n",
      "epoch 22 | loss: 0.64595 |  0:00:01s\n",
      "epoch 23 | loss: 0.62727 |  0:00:01s\n",
      "epoch 24 | loss: 0.62511 |  0:00:01s\n",
      "epoch 25 | loss: 0.63194 |  0:00:01s\n",
      "epoch 26 | loss: 0.61989 |  0:00:01s\n",
      "epoch 27 | loss: 0.61482 |  0:00:01s\n",
      "epoch 28 | loss: 0.5995  |  0:00:01s\n",
      "epoch 29 | loss: 0.59176 |  0:00:01s\n",
      "epoch 30 | loss: 0.59006 |  0:00:01s\n",
      "epoch 31 | loss: 0.58971 |  0:00:01s\n",
      "epoch 32 | loss: 0.57208 |  0:00:01s\n",
      "epoch 33 | loss: 0.57913 |  0:00:01s\n",
      "epoch 34 | loss: 0.57329 |  0:00:01s\n",
      "epoch 35 | loss: 0.57096 |  0:00:01s\n",
      "epoch 36 | loss: 0.56359 |  0:00:01s\n",
      "epoch 37 | loss: 0.55988 |  0:00:01s\n",
      "epoch 38 | loss: 0.55326 |  0:00:01s\n",
      "epoch 39 | loss: 0.55808 |  0:00:02s\n",
      "epoch 40 | loss: 0.56181 |  0:00:02s\n",
      "epoch 41 | loss: 0.56003 |  0:00:02s\n",
      "epoch 42 | loss: 0.55614 |  0:00:02s\n",
      "epoch 43 | loss: 0.55263 |  0:00:02s\n",
      "epoch 44 | loss: 0.54282 |  0:00:02s\n",
      "epoch 45 | loss: 0.54339 |  0:00:02s\n",
      "epoch 46 | loss: 0.54582 |  0:00:02s\n",
      "epoch 47 | loss: 0.53516 |  0:00:02s\n",
      "epoch 48 | loss: 0.53064 |  0:00:02s\n",
      "epoch 49 | loss: 0.53485 |  0:00:02s\n",
      "epoch 50 | loss: 0.53422 |  0:00:02s\n",
      "epoch 51 | loss: 0.53252 |  0:00:02s\n",
      "epoch 52 | loss: 0.53486 |  0:00:02s\n",
      "epoch 53 | loss: 0.5237  |  0:00:02s\n",
      "epoch 54 | loss: 0.52955 |  0:00:02s\n",
      "epoch 55 | loss: 0.51513 |  0:00:02s\n",
      "epoch 56 | loss: 0.51495 |  0:00:02s\n",
      "epoch 57 | loss: 0.50384 |  0:00:02s\n",
      "epoch 58 | loss: 0.50779 |  0:00:03s\n",
      "epoch 59 | loss: 0.50462 |  0:00:03s\n",
      "epoch 60 | loss: 0.50794 |  0:00:03s\n",
      "epoch 61 | loss: 0.49576 |  0:00:03s\n",
      "epoch 62 | loss: 0.48557 |  0:00:03s\n",
      "epoch 63 | loss: 0.49702 |  0:00:03s\n",
      "epoch 64 | loss: 0.49205 |  0:00:03s\n",
      "epoch 65 | loss: 0.48632 |  0:00:03s\n",
      "epoch 66 | loss: 0.47118 |  0:00:03s\n",
      "epoch 67 | loss: 0.47007 |  0:00:03s\n",
      "epoch 68 | loss: 0.47051 |  0:00:03s\n",
      "epoch 69 | loss: 0.46144 |  0:00:03s\n",
      "epoch 70 | loss: 0.46643 |  0:00:03s\n",
      "epoch 71 | loss: 0.45761 |  0:00:03s\n",
      "epoch 72 | loss: 0.45064 |  0:00:03s\n",
      "epoch 73 | loss: 0.44351 |  0:00:03s\n",
      "epoch 74 | loss: 0.45319 |  0:00:03s\n",
      "epoch 75 | loss: 0.44153 |  0:00:03s\n",
      "epoch 76 | loss: 0.44498 |  0:00:03s\n",
      "epoch 77 | loss: 0.44868 |  0:00:03s\n",
      "epoch 78 | loss: 0.43384 |  0:00:03s\n",
      "epoch 79 | loss: 0.43621 |  0:00:04s\n",
      "epoch 80 | loss: 0.43857 |  0:00:04s\n",
      "epoch 81 | loss: 0.4404  |  0:00:04s\n",
      "epoch 82 | loss: 0.43747 |  0:00:04s\n",
      "epoch 83 | loss: 0.42891 |  0:00:04s\n",
      "epoch 84 | loss: 0.42919 |  0:00:04s\n",
      "epoch 85 | loss: 0.42215 |  0:00:04s\n",
      "epoch 86 | loss: 0.42121 |  0:00:04s\n",
      "epoch 87 | loss: 0.4132  |  0:00:04s\n",
      "epoch 88 | loss: 0.4124  |  0:00:04s\n",
      "epoch 89 | loss: 0.41795 |  0:00:04s\n",
      "epoch 90 | loss: 0.41425 |  0:00:04s\n",
      "epoch 91 | loss: 0.40692 |  0:00:04s\n",
      "epoch 92 | loss: 0.41593 |  0:00:04s\n",
      "epoch 93 | loss: 0.40421 |  0:00:04s\n",
      "epoch 94 | loss: 0.39895 |  0:00:04s\n",
      "epoch 95 | loss: 0.40875 |  0:00:04s\n",
      "epoch 96 | loss: 0.40871 |  0:00:04s\n",
      "epoch 97 | loss: 0.40878 |  0:00:04s\n",
      "epoch 98 | loss: 0.40226 |  0:00:04s\n",
      "epoch 99 | loss: 0.41282 |  0:00:04s\n",
      "epoch 100| loss: 0.39449 |  0:00:05s\n",
      "epoch 101| loss: 0.4135  |  0:00:05s\n",
      "epoch 102| loss: 0.40156 |  0:00:05s\n",
      "epoch 103| loss: 0.39222 |  0:00:05s\n",
      "epoch 104| loss: 0.38739 |  0:00:05s\n",
      "epoch 105| loss: 0.39834 |  0:00:05s\n",
      "epoch 106| loss: 0.3832  |  0:00:05s\n",
      "epoch 107| loss: 0.38222 |  0:00:05s\n",
      "epoch 108| loss: 0.37995 |  0:00:05s\n",
      "epoch 109| loss: 0.37643 |  0:00:05s\n",
      "epoch 110| loss: 0.37944 |  0:00:05s\n",
      "epoch 111| loss: 0.37853 |  0:00:05s\n",
      "epoch 112| loss: 0.37039 |  0:00:05s\n",
      "epoch 113| loss: 0.36424 |  0:00:05s\n",
      "epoch 114| loss: 0.3639  |  0:00:05s\n",
      "epoch 115| loss: 0.36658 |  0:00:05s\n",
      "epoch 116| loss: 0.35688 |  0:00:05s\n",
      "epoch 117| loss: 0.35978 |  0:00:05s\n",
      "epoch 118| loss: 0.35243 |  0:00:05s\n",
      "epoch 119| loss: 0.35093 |  0:00:05s\n",
      "epoch 120| loss: 0.35281 |  0:00:06s\n",
      "epoch 121| loss: 0.34977 |  0:00:06s\n",
      "epoch 122| loss: 0.34135 |  0:00:06s\n",
      "epoch 123| loss: 0.34211 |  0:00:06s\n",
      "epoch 124| loss: 0.33346 |  0:00:06s\n",
      "epoch 125| loss: 0.33283 |  0:00:06s\n",
      "epoch 126| loss: 0.33625 |  0:00:06s\n",
      "epoch 127| loss: 0.33595 |  0:00:06s\n",
      "epoch 128| loss: 0.33134 |  0:00:06s\n",
      "epoch 129| loss: 0.3364  |  0:00:06s\n",
      "epoch 130| loss: 0.3297  |  0:00:06s\n",
      "epoch 131| loss: 0.33951 |  0:00:06s\n",
      "epoch 132| loss: 0.34734 |  0:00:06s\n",
      "epoch 133| loss: 0.34253 |  0:00:06s\n",
      "epoch 134| loss: 0.33043 |  0:00:06s\n",
      "epoch 135| loss: 0.32916 |  0:00:06s\n",
      "epoch 136| loss: 0.3346  |  0:00:06s\n",
      "epoch 137| loss: 0.33366 |  0:00:06s\n",
      "epoch 138| loss: 0.33976 |  0:00:06s\n",
      "epoch 139| loss: 0.34372 |  0:00:06s\n",
      "epoch 140| loss: 0.34182 |  0:00:06s\n",
      "epoch 141| loss: 0.35112 |  0:00:07s\n",
      "epoch 142| loss: 0.37224 |  0:00:07s\n",
      "epoch 143| loss: 0.37324 |  0:00:07s\n",
      "epoch 144| loss: 0.37139 |  0:00:07s\n",
      "epoch 145| loss: 0.36446 |  0:00:07s\n",
      "epoch 146| loss: 0.36524 |  0:00:07s\n",
      "epoch 147| loss: 0.36016 |  0:00:07s\n",
      "epoch 148| loss: 0.35469 |  0:00:07s\n",
      "epoch 149| loss: 0.3529  |  0:00:07s\n",
      "epoch 150| loss: 0.35796 |  0:00:07s\n",
      "epoch 151| loss: 0.34314 |  0:00:07s\n",
      "epoch 152| loss: 0.35298 |  0:00:07s\n",
      "epoch 153| loss: 0.34381 |  0:00:07s\n",
      "epoch 154| loss: 0.33303 |  0:00:07s\n",
      "epoch 155| loss: 0.33134 |  0:00:07s\n",
      "epoch 156| loss: 0.33557 |  0:00:07s\n",
      "epoch 157| loss: 0.33146 |  0:00:07s\n",
      "epoch 158| loss: 0.32498 |  0:00:07s\n",
      "epoch 159| loss: 0.3244  |  0:00:07s\n",
      "epoch 160| loss: 0.32081 |  0:00:07s\n",
      "epoch 161| loss: 0.31642 |  0:00:07s\n",
      "epoch 162| loss: 0.31173 |  0:00:07s\n",
      "epoch 163| loss: 0.31561 |  0:00:08s\n",
      "epoch 164| loss: 0.33116 |  0:00:08s\n",
      "epoch 165| loss: 0.3291  |  0:00:08s\n",
      "epoch 166| loss: 0.32991 |  0:00:08s\n",
      "epoch 167| loss: 0.33451 |  0:00:08s\n",
      "epoch 168| loss: 0.32866 |  0:00:08s\n",
      "epoch 169| loss: 0.3343  |  0:00:08s\n",
      "epoch 170| loss: 0.33862 |  0:00:08s\n",
      "epoch 171| loss: 0.33517 |  0:00:08s\n",
      "epoch 172| loss: 0.33609 |  0:00:08s\n",
      "epoch 173| loss: 0.3299  |  0:00:08s\n",
      "epoch 174| loss: 0.31981 |  0:00:08s\n",
      "epoch 175| loss: 0.32603 |  0:00:08s\n",
      "epoch 176| loss: 0.32801 |  0:00:08s\n",
      "epoch 177| loss: 0.34152 |  0:00:08s\n",
      "epoch 178| loss: 0.35336 |  0:00:08s\n",
      "epoch 179| loss: 0.35321 |  0:00:08s\n",
      "epoch 180| loss: 0.35127 |  0:00:08s\n",
      "epoch 181| loss: 0.33121 |  0:00:08s\n",
      "epoch 182| loss: 0.32923 |  0:00:08s\n",
      "epoch 183| loss: 0.31544 |  0:00:08s\n",
      "epoch 184| loss: 0.30819 |  0:00:08s\n",
      "epoch 185| loss: 0.31969 |  0:00:09s\n",
      "epoch 186| loss: 0.32366 |  0:00:09s\n",
      "epoch 187| loss: 0.31429 |  0:00:09s\n",
      "epoch 188| loss: 0.31128 |  0:00:09s\n",
      "epoch 189| loss: 0.31072 |  0:00:09s\n",
      "epoch 190| loss: 0.30636 |  0:00:09s\n",
      "epoch 191| loss: 0.31181 |  0:00:09s\n",
      "epoch 192| loss: 0.29836 |  0:00:09s\n",
      "epoch 193| loss: 0.29732 |  0:00:09s\n",
      "epoch 194| loss: 0.2877  |  0:00:09s\n",
      "epoch 195| loss: 0.28553 |  0:00:09s\n",
      "epoch 196| loss: 0.29004 |  0:00:09s\n",
      "epoch 197| loss: 0.28499 |  0:00:09s\n",
      "epoch 198| loss: 0.28291 |  0:00:09s\n",
      "epoch 199| loss: 0.27826 |  0:00:09s\n",
      "epoch 200| loss: 0.28135 |  0:00:09s\n",
      "epoch 201| loss: 0.275   |  0:00:10s\n",
      "epoch 202| loss: 0.27675 |  0:00:10s\n",
      "epoch 203| loss: 0.26788 |  0:00:10s\n",
      "epoch 204| loss: 0.27643 |  0:00:10s\n",
      "epoch 205| loss: 0.27193 |  0:00:10s\n",
      "epoch 206| loss: 0.26782 |  0:00:10s\n",
      "epoch 207| loss: 0.2671  |  0:00:10s\n",
      "epoch 208| loss: 0.27544 |  0:00:10s\n",
      "epoch 209| loss: 0.27468 |  0:00:10s\n",
      "epoch 210| loss: 0.27755 |  0:00:10s\n",
      "epoch 211| loss: 0.27736 |  0:00:10s\n",
      "epoch 212| loss: 0.27282 |  0:00:10s\n",
      "epoch 213| loss: 0.27275 |  0:00:10s\n",
      "epoch 214| loss: 0.27264 |  0:00:10s\n",
      "epoch 215| loss: 0.2683  |  0:00:10s\n",
      "epoch 216| loss: 0.26463 |  0:00:10s\n",
      "epoch 217| loss: 0.26134 |  0:00:10s\n",
      "epoch 218| loss: 0.26519 |  0:00:11s\n",
      "epoch 219| loss: 0.27557 |  0:00:11s\n",
      "epoch 220| loss: 0.27337 |  0:00:11s\n",
      "epoch 221| loss: 0.28464 |  0:00:11s\n",
      "epoch 222| loss: 0.27367 |  0:00:11s\n",
      "epoch 223| loss: 0.27295 |  0:00:11s\n",
      "epoch 224| loss: 0.30071 |  0:00:11s\n",
      "epoch 225| loss: 0.29603 |  0:00:11s\n",
      "epoch 226| loss: 0.29584 |  0:00:11s\n",
      "epoch 227| loss: 0.29318 |  0:00:11s\n",
      "epoch 228| loss: 0.29759 |  0:00:11s\n",
      "epoch 229| loss: 0.28947 |  0:00:11s\n",
      "epoch 230| loss: 0.29836 |  0:00:11s\n",
      "epoch 231| loss: 0.31014 |  0:00:11s\n",
      "epoch 232| loss: 0.29909 |  0:00:11s\n",
      "epoch 233| loss: 0.33271 |  0:00:11s\n",
      "epoch 234| loss: 0.33817 |  0:00:11s\n",
      "epoch 235| loss: 0.32654 |  0:00:11s\n",
      "epoch 236| loss: 0.31845 |  0:00:11s\n",
      "epoch 237| loss: 0.3122  |  0:00:11s\n",
      "epoch 238| loss: 0.30689 |  0:00:12s\n",
      "epoch 239| loss: 0.3041  |  0:00:12s\n",
      "epoch 240| loss: 0.28797 |  0:00:12s\n",
      "epoch 241| loss: 0.29069 |  0:00:12s\n",
      "epoch 242| loss: 0.27387 |  0:00:12s\n",
      "epoch 243| loss: 0.2712  |  0:00:12s\n",
      "epoch 244| loss: 0.27763 |  0:00:12s\n",
      "epoch 245| loss: 0.27854 |  0:00:12s\n",
      "epoch 246| loss: 0.26689 |  0:00:12s\n",
      "epoch 247| loss: 0.26215 |  0:00:12s\n",
      "epoch 248| loss: 0.26051 |  0:00:12s\n",
      "epoch 249| loss: 0.26422 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.92699 |  0:00:00s\n",
      "epoch 1  | loss: 1.88845 |  0:00:00s\n",
      "epoch 2  | loss: 1.30501 |  0:00:00s\n",
      "epoch 3  | loss: 1.11136 |  0:00:00s\n",
      "epoch 4  | loss: 1.0123  |  0:00:00s\n",
      "epoch 5  | loss: 0.94243 |  0:00:00s\n",
      "epoch 6  | loss: 0.87622 |  0:00:00s\n",
      "epoch 7  | loss: 0.81563 |  0:00:00s\n",
      "epoch 8  | loss: 0.77288 |  0:00:00s\n",
      "epoch 9  | loss: 0.75282 |  0:00:00s\n",
      "epoch 10 | loss: 0.75927 |  0:00:00s\n",
      "epoch 11 | loss: 0.74394 |  0:00:00s\n",
      "epoch 12 | loss: 0.73213 |  0:00:00s\n",
      "epoch 13 | loss: 0.71449 |  0:00:00s\n",
      "epoch 14 | loss: 0.7186  |  0:00:00s\n",
      "epoch 15 | loss: 0.70794 |  0:00:00s\n",
      "epoch 16 | loss: 0.69576 |  0:00:00s\n",
      "epoch 17 | loss: 0.69043 |  0:00:00s\n",
      "epoch 18 | loss: 0.68379 |  0:00:01s\n",
      "epoch 19 | loss: 0.67023 |  0:00:01s\n",
      "epoch 20 | loss: 0.6616  |  0:00:01s\n",
      "epoch 21 | loss: 0.66032 |  0:00:01s\n",
      "epoch 22 | loss: 0.65663 |  0:00:01s\n",
      "epoch 23 | loss: 0.64813 |  0:00:01s\n",
      "epoch 24 | loss: 0.64729 |  0:00:01s\n",
      "epoch 25 | loss: 0.64988 |  0:00:01s\n",
      "epoch 26 | loss: 0.63995 |  0:00:01s\n",
      "epoch 27 | loss: 0.64747 |  0:00:01s\n",
      "epoch 28 | loss: 0.63576 |  0:00:01s\n",
      "epoch 29 | loss: 0.63574 |  0:00:01s\n",
      "epoch 30 | loss: 0.64317 |  0:00:01s\n",
      "epoch 31 | loss: 0.62084 |  0:00:01s\n",
      "epoch 32 | loss: 0.62655 |  0:00:01s\n",
      "epoch 33 | loss: 0.62801 |  0:00:01s\n",
      "epoch 34 | loss: 0.61529 |  0:00:01s\n",
      "epoch 35 | loss: 0.61676 |  0:00:01s\n",
      "epoch 36 | loss: 0.60469 |  0:00:01s\n",
      "epoch 37 | loss: 0.60475 |  0:00:01s\n",
      "epoch 38 | loss: 0.59163 |  0:00:01s\n",
      "epoch 39 | loss: 0.60411 |  0:00:02s\n",
      "epoch 40 | loss: 0.59894 |  0:00:02s\n",
      "epoch 41 | loss: 0.59489 |  0:00:02s\n",
      "epoch 42 | loss: 0.59245 |  0:00:02s\n",
      "epoch 43 | loss: 0.57939 |  0:00:02s\n",
      "epoch 44 | loss: 0.57952 |  0:00:02s\n",
      "epoch 45 | loss: 0.57349 |  0:00:02s\n",
      "epoch 46 | loss: 0.57103 |  0:00:02s\n",
      "epoch 47 | loss: 0.56974 |  0:00:02s\n",
      "epoch 48 | loss: 0.55906 |  0:00:02s\n",
      "epoch 49 | loss: 0.56732 |  0:00:02s\n",
      "epoch 50 | loss: 0.55946 |  0:00:02s\n",
      "epoch 51 | loss: 0.55446 |  0:00:02s\n",
      "epoch 52 | loss: 0.53781 |  0:00:02s\n",
      "epoch 53 | loss: 0.53241 |  0:00:02s\n",
      "epoch 54 | loss: 0.5265  |  0:00:02s\n",
      "epoch 55 | loss: 0.51908 |  0:00:02s\n",
      "epoch 56 | loss: 0.51861 |  0:00:02s\n",
      "epoch 57 | loss: 0.51617 |  0:00:02s\n",
      "epoch 58 | loss: 0.51587 |  0:00:02s\n",
      "epoch 59 | loss: 0.51781 |  0:00:02s\n",
      "epoch 60 | loss: 0.51346 |  0:00:02s\n",
      "epoch 61 | loss: 0.50997 |  0:00:03s\n",
      "epoch 62 | loss: 0.51214 |  0:00:03s\n",
      "epoch 63 | loss: 0.51399 |  0:00:03s\n",
      "epoch 64 | loss: 0.49372 |  0:00:03s\n",
      "epoch 65 | loss: 0.49641 |  0:00:03s\n",
      "epoch 66 | loss: 0.48572 |  0:00:03s\n",
      "epoch 67 | loss: 0.4813  |  0:00:03s\n",
      "epoch 68 | loss: 0.48601 |  0:00:03s\n",
      "epoch 69 | loss: 0.47933 |  0:00:03s\n",
      "epoch 70 | loss: 0.48182 |  0:00:03s\n",
      "epoch 71 | loss: 0.48221 |  0:00:03s\n",
      "epoch 72 | loss: 0.46965 |  0:00:03s\n",
      "epoch 73 | loss: 0.45765 |  0:00:03s\n",
      "epoch 74 | loss: 0.46422 |  0:00:03s\n",
      "epoch 75 | loss: 0.45614 |  0:00:03s\n",
      "epoch 76 | loss: 0.45804 |  0:00:03s\n",
      "epoch 77 | loss: 0.45974 |  0:00:03s\n",
      "epoch 78 | loss: 0.45193 |  0:00:03s\n",
      "epoch 79 | loss: 0.45599 |  0:00:03s\n",
      "epoch 80 | loss: 0.45303 |  0:00:04s\n",
      "epoch 81 | loss: 0.44391 |  0:00:04s\n",
      "epoch 82 | loss: 0.44365 |  0:00:04s\n",
      "epoch 83 | loss: 0.43968 |  0:00:04s\n",
      "epoch 84 | loss: 0.43485 |  0:00:04s\n",
      "epoch 85 | loss: 0.4422  |  0:00:04s\n",
      "epoch 86 | loss: 0.43901 |  0:00:04s\n",
      "epoch 87 | loss: 0.4374  |  0:00:04s\n",
      "epoch 88 | loss: 0.43078 |  0:00:04s\n",
      "epoch 89 | loss: 0.4335  |  0:00:04s\n",
      "epoch 90 | loss: 0.42681 |  0:00:04s\n",
      "epoch 91 | loss: 0.42255 |  0:00:04s\n",
      "epoch 92 | loss: 0.436   |  0:00:04s\n",
      "epoch 93 | loss: 0.41791 |  0:00:04s\n",
      "epoch 94 | loss: 0.41773 |  0:00:04s\n",
      "epoch 95 | loss: 0.42027 |  0:00:04s\n",
      "epoch 96 | loss: 0.41108 |  0:00:04s\n",
      "epoch 97 | loss: 0.41298 |  0:00:04s\n",
      "epoch 98 | loss: 0.40883 |  0:00:04s\n",
      "epoch 99 | loss: 0.40619 |  0:00:04s\n",
      "epoch 100| loss: 0.40556 |  0:00:05s\n",
      "epoch 101| loss: 0.40381 |  0:00:05s\n",
      "epoch 102| loss: 0.40373 |  0:00:05s\n",
      "epoch 103| loss: 0.40219 |  0:00:05s\n",
      "epoch 104| loss: 0.40659 |  0:00:05s\n",
      "epoch 105| loss: 0.40721 |  0:00:05s\n",
      "epoch 106| loss: 0.40529 |  0:00:05s\n",
      "epoch 107| loss: 0.3962  |  0:00:05s\n",
      "epoch 108| loss: 0.39759 |  0:00:05s\n",
      "epoch 109| loss: 0.3961  |  0:00:05s\n",
      "epoch 110| loss: 0.39804 |  0:00:05s\n",
      "epoch 111| loss: 0.39493 |  0:00:05s\n",
      "epoch 112| loss: 0.39087 |  0:00:05s\n",
      "epoch 113| loss: 0.38988 |  0:00:05s\n",
      "epoch 114| loss: 0.3871  |  0:00:05s\n",
      "epoch 115| loss: 0.39219 |  0:00:05s\n",
      "epoch 116| loss: 0.39794 |  0:00:05s\n",
      "epoch 117| loss: 0.3897  |  0:00:05s\n",
      "epoch 118| loss: 0.39053 |  0:00:05s\n",
      "epoch 119| loss: 0.38014 |  0:00:05s\n",
      "epoch 120| loss: 0.37986 |  0:00:06s\n",
      "epoch 121| loss: 0.37936 |  0:00:06s\n",
      "epoch 122| loss: 0.38207 |  0:00:06s\n",
      "epoch 123| loss: 0.37706 |  0:00:06s\n",
      "epoch 124| loss: 0.36965 |  0:00:06s\n",
      "epoch 125| loss: 0.36762 |  0:00:06s\n",
      "epoch 126| loss: 0.36437 |  0:00:06s\n",
      "epoch 127| loss: 0.3642  |  0:00:06s\n",
      "epoch 128| loss: 0.35786 |  0:00:06s\n",
      "epoch 129| loss: 0.36175 |  0:00:06s\n",
      "epoch 130| loss: 0.35625 |  0:00:06s\n",
      "epoch 131| loss: 0.35132 |  0:00:06s\n",
      "epoch 132| loss: 0.3706  |  0:00:06s\n",
      "epoch 133| loss: 0.35611 |  0:00:06s\n",
      "epoch 134| loss: 0.35594 |  0:00:06s\n",
      "epoch 135| loss: 0.35428 |  0:00:06s\n",
      "epoch 136| loss: 0.35899 |  0:00:06s\n",
      "epoch 137| loss: 0.35626 |  0:00:06s\n",
      "epoch 138| loss: 0.35141 |  0:00:06s\n",
      "epoch 139| loss: 0.35131 |  0:00:06s\n",
      "epoch 140| loss: 0.34836 |  0:00:06s\n",
      "epoch 141| loss: 0.34312 |  0:00:07s\n",
      "epoch 142| loss: 0.35423 |  0:00:07s\n",
      "epoch 143| loss: 0.36027 |  0:00:07s\n",
      "epoch 144| loss: 0.36466 |  0:00:07s\n",
      "epoch 145| loss: 0.36822 |  0:00:07s\n",
      "epoch 146| loss: 0.36528 |  0:00:07s\n",
      "epoch 147| loss: 0.36344 |  0:00:07s\n",
      "epoch 148| loss: 0.36288 |  0:00:07s\n",
      "epoch 149| loss: 0.36148 |  0:00:07s\n",
      "epoch 150| loss: 0.36114 |  0:00:07s\n",
      "epoch 151| loss: 0.35049 |  0:00:07s\n",
      "epoch 152| loss: 0.35447 |  0:00:07s\n",
      "epoch 153| loss: 0.34722 |  0:00:07s\n",
      "epoch 154| loss: 0.3487  |  0:00:07s\n",
      "epoch 155| loss: 0.344   |  0:00:07s\n",
      "epoch 156| loss: 0.34899 |  0:00:07s\n",
      "epoch 157| loss: 0.34894 |  0:00:07s\n",
      "epoch 158| loss: 0.35109 |  0:00:07s\n",
      "epoch 159| loss: 0.34664 |  0:00:07s\n",
      "epoch 160| loss: 0.35    |  0:00:07s\n",
      "epoch 161| loss: 0.33942 |  0:00:07s\n",
      "epoch 162| loss: 0.33708 |  0:00:08s\n",
      "epoch 163| loss: 0.34204 |  0:00:08s\n",
      "epoch 164| loss: 0.33494 |  0:00:08s\n",
      "epoch 165| loss: 0.338   |  0:00:08s\n",
      "epoch 166| loss: 0.33173 |  0:00:08s\n",
      "epoch 167| loss: 0.33464 |  0:00:08s\n",
      "epoch 168| loss: 0.33771 |  0:00:08s\n",
      "epoch 169| loss: 0.33118 |  0:00:08s\n",
      "epoch 170| loss: 0.33086 |  0:00:08s\n",
      "epoch 171| loss: 0.32004 |  0:00:08s\n",
      "epoch 172| loss: 0.33319 |  0:00:08s\n",
      "epoch 173| loss: 0.32249 |  0:00:08s\n",
      "epoch 174| loss: 0.32107 |  0:00:08s\n",
      "epoch 175| loss: 0.32375 |  0:00:08s\n",
      "epoch 176| loss: 0.30826 |  0:00:08s\n",
      "epoch 177| loss: 0.31779 |  0:00:08s\n",
      "epoch 178| loss: 0.31882 |  0:00:08s\n",
      "epoch 179| loss: 0.3185  |  0:00:09s\n",
      "epoch 180| loss: 0.31354 |  0:00:09s\n",
      "epoch 181| loss: 0.31205 |  0:00:09s\n",
      "epoch 182| loss: 0.30401 |  0:00:09s\n",
      "epoch 183| loss: 0.30368 |  0:00:09s\n",
      "epoch 184| loss: 0.29921 |  0:00:09s\n",
      "epoch 185| loss: 0.30551 |  0:00:09s\n",
      "epoch 186| loss: 0.30669 |  0:00:09s\n",
      "epoch 187| loss: 0.29721 |  0:00:09s\n",
      "epoch 188| loss: 0.30385 |  0:00:09s\n",
      "epoch 189| loss: 0.29853 |  0:00:09s\n",
      "epoch 190| loss: 0.29978 |  0:00:09s\n",
      "epoch 191| loss: 0.29005 |  0:00:09s\n",
      "epoch 192| loss: 0.28584 |  0:00:09s\n",
      "epoch 193| loss: 0.29365 |  0:00:09s\n",
      "epoch 194| loss: 0.28471 |  0:00:09s\n",
      "epoch 195| loss: 0.28919 |  0:00:09s\n",
      "epoch 196| loss: 0.28895 |  0:00:09s\n",
      "epoch 197| loss: 0.29287 |  0:00:09s\n",
      "epoch 198| loss: 0.28488 |  0:00:10s\n",
      "epoch 199| loss: 0.28232 |  0:00:10s\n",
      "epoch 200| loss: 0.2858  |  0:00:10s\n",
      "epoch 201| loss: 0.28077 |  0:00:10s\n",
      "epoch 202| loss: 0.28073 |  0:00:10s\n",
      "epoch 203| loss: 0.28214 |  0:00:10s\n",
      "epoch 204| loss: 0.2793  |  0:00:10s\n",
      "epoch 205| loss: 0.29224 |  0:00:10s\n",
      "epoch 206| loss: 0.28753 |  0:00:10s\n",
      "epoch 207| loss: 0.28766 |  0:00:10s\n",
      "epoch 208| loss: 0.30121 |  0:00:10s\n",
      "epoch 209| loss: 0.31002 |  0:00:10s\n",
      "epoch 210| loss: 0.30281 |  0:00:10s\n",
      "epoch 211| loss: 0.30032 |  0:00:10s\n",
      "epoch 212| loss: 0.30799 |  0:00:10s\n",
      "epoch 213| loss: 0.29979 |  0:00:10s\n",
      "epoch 214| loss: 0.29255 |  0:00:10s\n",
      "epoch 215| loss: 0.29836 |  0:00:10s\n",
      "epoch 216| loss: 0.29344 |  0:00:10s\n",
      "epoch 217| loss: 0.29414 |  0:00:10s\n",
      "epoch 218| loss: 0.29267 |  0:00:11s\n",
      "epoch 219| loss: 0.29121 |  0:00:11s\n",
      "epoch 220| loss: 0.29343 |  0:00:11s\n",
      "epoch 221| loss: 0.29349 |  0:00:11s\n",
      "epoch 222| loss: 0.2928  |  0:00:11s\n",
      "epoch 223| loss: 0.29616 |  0:00:11s\n",
      "epoch 224| loss: 0.29562 |  0:00:11s\n",
      "epoch 225| loss: 0.29717 |  0:00:11s\n",
      "epoch 226| loss: 0.28868 |  0:00:11s\n",
      "epoch 227| loss: 0.28052 |  0:00:11s\n",
      "epoch 228| loss: 0.27817 |  0:00:11s\n",
      "epoch 229| loss: 0.28433 |  0:00:11s\n",
      "epoch 230| loss: 0.28538 |  0:00:11s\n",
      "epoch 231| loss: 0.27525 |  0:00:11s\n",
      "epoch 232| loss: 0.27692 |  0:00:11s\n",
      "epoch 233| loss: 0.28285 |  0:00:11s\n",
      "epoch 234| loss: 0.27138 |  0:00:11s\n",
      "epoch 235| loss: 0.27635 |  0:00:11s\n",
      "epoch 236| loss: 0.26272 |  0:00:11s\n",
      "epoch 237| loss: 0.26772 |  0:00:12s\n",
      "epoch 238| loss: 0.26186 |  0:00:12s\n",
      "epoch 239| loss: 0.26526 |  0:00:12s\n",
      "epoch 240| loss: 0.26943 |  0:00:12s\n",
      "epoch 241| loss: 0.2688  |  0:00:12s\n",
      "epoch 242| loss: 0.26525 |  0:00:12s\n",
      "epoch 243| loss: 0.26993 |  0:00:12s\n",
      "epoch 244| loss: 0.27536 |  0:00:12s\n",
      "epoch 245| loss: 0.27587 |  0:00:12s\n",
      "epoch 246| loss: 0.27991 |  0:00:12s\n",
      "epoch 247| loss: 0.28293 |  0:00:12s\n",
      "epoch 248| loss: 0.29151 |  0:00:12s\n",
      "epoch 249| loss: 0.29657 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.78194 |  0:00:00s\n",
      "epoch 1  | loss: 1.8027  |  0:00:00s\n",
      "epoch 2  | loss: 1.30767 |  0:00:00s\n",
      "epoch 3  | loss: 1.12961 |  0:00:00s\n",
      "epoch 4  | loss: 1.03667 |  0:00:00s\n",
      "epoch 5  | loss: 0.9399  |  0:00:00s\n",
      "epoch 6  | loss: 0.8695  |  0:00:00s\n",
      "epoch 7  | loss: 0.8232  |  0:00:00s\n",
      "epoch 8  | loss: 0.79526 |  0:00:00s\n",
      "epoch 9  | loss: 0.78636 |  0:00:00s\n",
      "epoch 10 | loss: 0.76884 |  0:00:00s\n",
      "epoch 11 | loss: 0.74632 |  0:00:00s\n",
      "epoch 12 | loss: 0.71987 |  0:00:00s\n",
      "epoch 13 | loss: 0.71882 |  0:00:00s\n",
      "epoch 14 | loss: 0.68577 |  0:00:00s\n",
      "epoch 15 | loss: 0.69184 |  0:00:00s\n",
      "epoch 16 | loss: 0.67545 |  0:00:00s\n",
      "epoch 17 | loss: 0.65638 |  0:00:00s\n",
      "epoch 18 | loss: 0.65488 |  0:00:01s\n",
      "epoch 19 | loss: 0.64253 |  0:00:01s\n",
      "epoch 20 | loss: 0.63399 |  0:00:01s\n",
      "epoch 21 | loss: 0.61893 |  0:00:01s\n",
      "epoch 22 | loss: 0.62054 |  0:00:01s\n",
      "epoch 23 | loss: 0.60853 |  0:00:01s\n",
      "epoch 24 | loss: 0.60316 |  0:00:01s\n",
      "epoch 25 | loss: 0.62847 |  0:00:01s\n",
      "epoch 26 | loss: 0.60943 |  0:00:01s\n",
      "epoch 27 | loss: 0.60519 |  0:00:01s\n",
      "epoch 28 | loss: 0.6156  |  0:00:01s\n",
      "epoch 29 | loss: 0.62647 |  0:00:01s\n",
      "epoch 30 | loss: 0.60966 |  0:00:01s\n",
      "epoch 31 | loss: 0.61166 |  0:00:01s\n",
      "epoch 32 | loss: 0.60593 |  0:00:01s\n",
      "epoch 33 | loss: 0.59412 |  0:00:01s\n",
      "epoch 34 | loss: 0.58051 |  0:00:01s\n",
      "epoch 35 | loss: 0.57802 |  0:00:01s\n",
      "epoch 36 | loss: 0.59021 |  0:00:01s\n",
      "epoch 37 | loss: 0.58089 |  0:00:02s\n",
      "epoch 38 | loss: 0.56509 |  0:00:02s\n",
      "epoch 39 | loss: 0.55764 |  0:00:02s\n",
      "epoch 40 | loss: 0.55023 |  0:00:02s\n",
      "epoch 41 | loss: 0.53718 |  0:00:02s\n",
      "epoch 42 | loss: 0.53173 |  0:00:02s\n",
      "epoch 43 | loss: 0.51848 |  0:00:02s\n",
      "epoch 44 | loss: 0.52756 |  0:00:02s\n",
      "epoch 45 | loss: 0.51629 |  0:00:02s\n",
      "epoch 46 | loss: 0.5017  |  0:00:02s\n",
      "epoch 47 | loss: 0.50691 |  0:00:02s\n",
      "epoch 48 | loss: 0.50163 |  0:00:02s\n",
      "epoch 49 | loss: 0.488   |  0:00:02s\n",
      "epoch 50 | loss: 0.48895 |  0:00:02s\n",
      "epoch 51 | loss: 0.47893 |  0:00:02s\n",
      "epoch 52 | loss: 0.47558 |  0:00:02s\n",
      "epoch 53 | loss: 0.47126 |  0:00:02s\n",
      "epoch 54 | loss: 0.47552 |  0:00:02s\n",
      "epoch 55 | loss: 0.46406 |  0:00:02s\n",
      "epoch 56 | loss: 0.47903 |  0:00:03s\n",
      "epoch 57 | loss: 0.47877 |  0:00:03s\n",
      "epoch 58 | loss: 0.47312 |  0:00:03s\n",
      "epoch 59 | loss: 0.46825 |  0:00:03s\n",
      "epoch 60 | loss: 0.46424 |  0:00:03s\n",
      "epoch 61 | loss: 0.45791 |  0:00:03s\n",
      "epoch 62 | loss: 0.45423 |  0:00:03s\n",
      "epoch 63 | loss: 0.4483  |  0:00:03s\n",
      "epoch 64 | loss: 0.44638 |  0:00:03s\n",
      "epoch 65 | loss: 0.44513 |  0:00:03s\n",
      "epoch 66 | loss: 0.45213 |  0:00:03s\n",
      "epoch 67 | loss: 0.46601 |  0:00:03s\n",
      "epoch 68 | loss: 0.46661 |  0:00:03s\n",
      "epoch 69 | loss: 0.45769 |  0:00:03s\n",
      "epoch 70 | loss: 0.46527 |  0:00:03s\n",
      "epoch 71 | loss: 0.45655 |  0:00:03s\n",
      "epoch 72 | loss: 0.45286 |  0:00:03s\n",
      "epoch 73 | loss: 0.45363 |  0:00:03s\n",
      "epoch 74 | loss: 0.47065 |  0:00:03s\n",
      "epoch 75 | loss: 0.45126 |  0:00:04s\n",
      "epoch 76 | loss: 0.44737 |  0:00:04s\n",
      "epoch 77 | loss: 0.4508  |  0:00:04s\n",
      "epoch 78 | loss: 0.43581 |  0:00:04s\n",
      "epoch 79 | loss: 0.44551 |  0:00:04s\n",
      "epoch 80 | loss: 0.43864 |  0:00:04s\n",
      "epoch 81 | loss: 0.43697 |  0:00:04s\n",
      "epoch 82 | loss: 0.42841 |  0:00:04s\n",
      "epoch 83 | loss: 0.43737 |  0:00:04s\n",
      "epoch 84 | loss: 0.42757 |  0:00:04s\n",
      "epoch 85 | loss: 0.4247  |  0:00:04s\n",
      "epoch 86 | loss: 0.42964 |  0:00:04s\n",
      "epoch 87 | loss: 0.42687 |  0:00:04s\n",
      "epoch 88 | loss: 0.42566 |  0:00:04s\n",
      "epoch 89 | loss: 0.4232  |  0:00:04s\n",
      "epoch 90 | loss: 0.42471 |  0:00:04s\n",
      "epoch 91 | loss: 0.41761 |  0:00:04s\n",
      "epoch 92 | loss: 0.41646 |  0:00:04s\n",
      "epoch 93 | loss: 0.39964 |  0:00:05s\n",
      "epoch 94 | loss: 0.39561 |  0:00:05s\n",
      "epoch 95 | loss: 0.40685 |  0:00:05s\n",
      "epoch 96 | loss: 0.3926  |  0:00:05s\n",
      "epoch 97 | loss: 0.40085 |  0:00:05s\n",
      "epoch 98 | loss: 0.39078 |  0:00:05s\n",
      "epoch 99 | loss: 0.39112 |  0:00:05s\n",
      "epoch 100| loss: 0.388   |  0:00:05s\n",
      "epoch 101| loss: 0.38338 |  0:00:05s\n",
      "epoch 102| loss: 0.37478 |  0:00:05s\n",
      "epoch 103| loss: 0.37183 |  0:00:05s\n",
      "epoch 104| loss: 0.36734 |  0:00:05s\n",
      "epoch 105| loss: 0.37151 |  0:00:05s\n",
      "epoch 106| loss: 0.37209 |  0:00:05s\n",
      "epoch 107| loss: 0.37205 |  0:00:05s\n",
      "epoch 108| loss: 0.36228 |  0:00:05s\n",
      "epoch 109| loss: 0.36556 |  0:00:05s\n",
      "epoch 110| loss: 0.36592 |  0:00:06s\n",
      "epoch 111| loss: 0.3649  |  0:00:06s\n",
      "epoch 112| loss: 0.36901 |  0:00:06s\n",
      "epoch 113| loss: 0.3655  |  0:00:06s\n",
      "epoch 114| loss: 0.36162 |  0:00:06s\n",
      "epoch 115| loss: 0.36401 |  0:00:06s\n",
      "epoch 116| loss: 0.36585 |  0:00:06s\n",
      "epoch 117| loss: 0.35832 |  0:00:06s\n",
      "epoch 118| loss: 0.35864 |  0:00:06s\n",
      "epoch 119| loss: 0.35763 |  0:00:06s\n",
      "epoch 120| loss: 0.35235 |  0:00:06s\n",
      "epoch 121| loss: 0.35159 |  0:00:06s\n",
      "epoch 122| loss: 0.34931 |  0:00:06s\n",
      "epoch 123| loss: 0.34179 |  0:00:06s\n",
      "epoch 124| loss: 0.349   |  0:00:06s\n",
      "epoch 125| loss: 0.34279 |  0:00:06s\n",
      "epoch 126| loss: 0.33561 |  0:00:06s\n",
      "epoch 127| loss: 0.33923 |  0:00:07s\n",
      "epoch 128| loss: 0.32915 |  0:00:07s\n",
      "epoch 129| loss: 0.34108 |  0:00:07s\n",
      "epoch 130| loss: 0.34333 |  0:00:07s\n",
      "epoch 131| loss: 0.34434 |  0:00:07s\n",
      "epoch 132| loss: 0.35017 |  0:00:07s\n",
      "epoch 133| loss: 0.34479 |  0:00:07s\n",
      "epoch 134| loss: 0.34074 |  0:00:07s\n",
      "epoch 135| loss: 0.34244 |  0:00:07s\n",
      "epoch 136| loss: 0.35224 |  0:00:07s\n",
      "epoch 137| loss: 0.3511  |  0:00:07s\n",
      "epoch 138| loss: 0.36614 |  0:00:07s\n",
      "epoch 139| loss: 0.37498 |  0:00:07s\n",
      "epoch 140| loss: 0.38592 |  0:00:07s\n",
      "epoch 141| loss: 0.37141 |  0:00:07s\n",
      "epoch 142| loss: 0.35796 |  0:00:07s\n",
      "epoch 143| loss: 0.3637  |  0:00:07s\n",
      "epoch 144| loss: 0.35125 |  0:00:07s\n",
      "epoch 145| loss: 0.35079 |  0:00:08s\n",
      "epoch 146| loss: 0.34555 |  0:00:08s\n",
      "epoch 147| loss: 0.34321 |  0:00:08s\n",
      "epoch 148| loss: 0.34878 |  0:00:08s\n",
      "epoch 149| loss: 0.34253 |  0:00:08s\n",
      "epoch 150| loss: 0.33236 |  0:00:08s\n",
      "epoch 151| loss: 0.32988 |  0:00:08s\n",
      "epoch 152| loss: 0.33491 |  0:00:08s\n",
      "epoch 153| loss: 0.32453 |  0:00:08s\n",
      "epoch 154| loss: 0.32197 |  0:00:08s\n",
      "epoch 155| loss: 0.33088 |  0:00:08s\n",
      "epoch 156| loss: 0.32648 |  0:00:08s\n",
      "epoch 157| loss: 0.31968 |  0:00:08s\n",
      "epoch 158| loss: 0.32304 |  0:00:08s\n",
      "epoch 159| loss: 0.31511 |  0:00:08s\n",
      "epoch 160| loss: 0.32183 |  0:00:08s\n",
      "epoch 161| loss: 0.31787 |  0:00:08s\n",
      "epoch 162| loss: 0.31394 |  0:00:08s\n",
      "epoch 163| loss: 0.30837 |  0:00:08s\n",
      "epoch 164| loss: 0.30599 |  0:00:09s\n",
      "epoch 165| loss: 0.30234 |  0:00:09s\n",
      "epoch 166| loss: 0.30302 |  0:00:09s\n",
      "epoch 167| loss: 0.30052 |  0:00:09s\n",
      "epoch 168| loss: 0.30409 |  0:00:09s\n",
      "epoch 169| loss: 0.2966  |  0:00:09s\n",
      "epoch 170| loss: 0.30243 |  0:00:09s\n",
      "epoch 171| loss: 0.28623 |  0:00:09s\n",
      "epoch 172| loss: 0.29656 |  0:00:09s\n",
      "epoch 173| loss: 0.29264 |  0:00:09s\n",
      "epoch 174| loss: 0.29018 |  0:00:09s\n",
      "epoch 175| loss: 0.29534 |  0:00:09s\n",
      "epoch 176| loss: 0.29208 |  0:00:09s\n",
      "epoch 177| loss: 0.29924 |  0:00:09s\n",
      "epoch 178| loss: 0.28802 |  0:00:09s\n",
      "epoch 179| loss: 0.30394 |  0:00:09s\n",
      "epoch 180| loss: 0.29649 |  0:00:09s\n",
      "epoch 181| loss: 0.29312 |  0:00:09s\n",
      "epoch 182| loss: 0.29375 |  0:00:09s\n",
      "epoch 183| loss: 0.28593 |  0:00:10s\n",
      "epoch 184| loss: 0.27625 |  0:00:10s\n",
      "epoch 185| loss: 0.28191 |  0:00:10s\n",
      "epoch 186| loss: 0.27888 |  0:00:10s\n",
      "epoch 187| loss: 0.27397 |  0:00:10s\n",
      "epoch 188| loss: 0.27827 |  0:00:10s\n",
      "epoch 189| loss: 0.28395 |  0:00:10s\n",
      "epoch 190| loss: 0.2755  |  0:00:10s\n",
      "epoch 191| loss: 0.27581 |  0:00:10s\n",
      "epoch 192| loss: 0.26785 |  0:00:10s\n",
      "epoch 193| loss: 0.27896 |  0:00:10s\n",
      "epoch 194| loss: 0.26918 |  0:00:10s\n",
      "epoch 195| loss: 0.2667  |  0:00:10s\n",
      "epoch 196| loss: 0.27441 |  0:00:10s\n",
      "epoch 197| loss: 0.27317 |  0:00:10s\n",
      "epoch 198| loss: 0.26817 |  0:00:10s\n",
      "epoch 199| loss: 0.27148 |  0:00:10s\n",
      "epoch 200| loss: 0.27494 |  0:00:10s\n",
      "epoch 201| loss: 0.27372 |  0:00:10s\n",
      "epoch 202| loss: 0.26971 |  0:00:11s\n",
      "epoch 203| loss: 0.27606 |  0:00:11s\n",
      "epoch 204| loss: 0.27563 |  0:00:11s\n",
      "epoch 205| loss: 0.26835 |  0:00:11s\n",
      "epoch 206| loss: 0.27486 |  0:00:11s\n",
      "epoch 207| loss: 0.26261 |  0:00:11s\n",
      "epoch 208| loss: 0.26412 |  0:00:11s\n",
      "epoch 209| loss: 0.26013 |  0:00:11s\n",
      "epoch 210| loss: 0.26483 |  0:00:11s\n",
      "epoch 211| loss: 0.26037 |  0:00:11s\n",
      "epoch 212| loss: 0.27198 |  0:00:11s\n",
      "epoch 213| loss: 0.26144 |  0:00:11s\n",
      "epoch 214| loss: 0.26993 |  0:00:11s\n",
      "epoch 215| loss: 0.29136 |  0:00:11s\n",
      "epoch 216| loss: 0.27701 |  0:00:11s\n",
      "epoch 217| loss: 0.27386 |  0:00:12s\n",
      "epoch 218| loss: 0.26555 |  0:00:12s\n",
      "epoch 219| loss: 0.26585 |  0:00:12s\n",
      "epoch 220| loss: 0.27033 |  0:00:12s\n",
      "epoch 221| loss: 0.25912 |  0:00:12s\n",
      "epoch 222| loss: 0.25687 |  0:00:12s\n",
      "epoch 223| loss: 0.26154 |  0:00:12s\n",
      "epoch 224| loss: 0.26339 |  0:00:12s\n",
      "epoch 225| loss: 0.2578  |  0:00:12s\n",
      "epoch 226| loss: 0.25141 |  0:00:12s\n",
      "epoch 227| loss: 0.26501 |  0:00:12s\n",
      "epoch 228| loss: 0.28067 |  0:00:12s\n",
      "epoch 229| loss: 0.27582 |  0:00:12s\n",
      "epoch 230| loss: 0.27382 |  0:00:12s\n",
      "epoch 231| loss: 0.27029 |  0:00:12s\n",
      "epoch 232| loss: 0.27703 |  0:00:12s\n",
      "epoch 233| loss: 0.27791 |  0:00:12s\n",
      "epoch 234| loss: 0.27568 |  0:00:12s\n",
      "epoch 235| loss: 0.27503 |  0:00:13s\n",
      "epoch 236| loss: 0.28591 |  0:00:13s\n",
      "epoch 237| loss: 0.29131 |  0:00:13s\n",
      "epoch 238| loss: 0.28282 |  0:00:13s\n",
      "epoch 239| loss: 0.28531 |  0:00:13s\n",
      "epoch 240| loss: 0.27765 |  0:00:13s\n",
      "epoch 241| loss: 0.27342 |  0:00:13s\n",
      "epoch 242| loss: 0.2667  |  0:00:13s\n",
      "epoch 243| loss: 0.26292 |  0:00:13s\n",
      "epoch 244| loss: 0.26185 |  0:00:13s\n",
      "epoch 245| loss: 0.25204 |  0:00:13s\n",
      "epoch 246| loss: 0.25327 |  0:00:13s\n",
      "epoch 247| loss: 0.24737 |  0:00:13s\n",
      "epoch 248| loss: 0.24642 |  0:00:13s\n",
      "epoch 249| loss: 0.25207 |  0:00:13s\n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.87869 |  0:00:00s\n",
      "epoch 1  | loss: 1.85137 |  0:00:00s\n",
      "epoch 2  | loss: 1.29484 |  0:00:00s\n",
      "epoch 3  | loss: 1.13026 |  0:00:00s\n",
      "epoch 4  | loss: 0.99699 |  0:00:00s\n",
      "epoch 5  | loss: 0.93976 |  0:00:00s\n",
      "epoch 6  | loss: 0.87437 |  0:00:00s\n",
      "epoch 7  | loss: 0.83912 |  0:00:00s\n",
      "epoch 8  | loss: 0.79022 |  0:00:00s\n",
      "epoch 9  | loss: 0.75861 |  0:00:00s\n",
      "epoch 10 | loss: 0.75615 |  0:00:00s\n",
      "epoch 11 | loss: 0.75222 |  0:00:00s\n",
      "epoch 12 | loss: 0.73363 |  0:00:00s\n",
      "epoch 13 | loss: 0.71456 |  0:00:00s\n",
      "epoch 14 | loss: 0.71802 |  0:00:00s\n",
      "epoch 15 | loss: 0.69654 |  0:00:00s\n",
      "epoch 16 | loss: 0.68879 |  0:00:00s\n",
      "epoch 17 | loss: 0.66605 |  0:00:00s\n",
      "epoch 18 | loss: 0.66777 |  0:00:01s\n",
      "epoch 19 | loss: 0.65901 |  0:00:01s\n",
      "epoch 20 | loss: 0.64993 |  0:00:01s\n",
      "epoch 21 | loss: 0.64372 |  0:00:01s\n",
      "epoch 22 | loss: 0.64346 |  0:00:01s\n",
      "epoch 23 | loss: 0.63695 |  0:00:01s\n",
      "epoch 24 | loss: 0.61856 |  0:00:01s\n",
      "epoch 25 | loss: 0.62292 |  0:00:01s\n",
      "epoch 26 | loss: 0.62278 |  0:00:01s\n",
      "epoch 27 | loss: 0.61306 |  0:00:01s\n",
      "epoch 28 | loss: 0.6055  |  0:00:01s\n",
      "epoch 29 | loss: 0.60478 |  0:00:01s\n",
      "epoch 30 | loss: 0.60367 |  0:00:01s\n",
      "epoch 31 | loss: 0.59515 |  0:00:01s\n",
      "epoch 32 | loss: 0.59198 |  0:00:01s\n",
      "epoch 33 | loss: 0.60624 |  0:00:01s\n",
      "epoch 34 | loss: 0.59116 |  0:00:01s\n",
      "epoch 35 | loss: 0.58567 |  0:00:01s\n",
      "epoch 36 | loss: 0.59039 |  0:00:02s\n",
      "epoch 37 | loss: 0.59285 |  0:00:02s\n",
      "epoch 38 | loss: 0.58426 |  0:00:02s\n",
      "epoch 39 | loss: 0.5824  |  0:00:02s\n",
      "epoch 40 | loss: 0.58701 |  0:00:02s\n",
      "epoch 41 | loss: 0.56974 |  0:00:02s\n",
      "epoch 42 | loss: 0.5829  |  0:00:02s\n",
      "epoch 43 | loss: 0.5651  |  0:00:02s\n",
      "epoch 44 | loss: 0.5626  |  0:00:02s\n",
      "epoch 45 | loss: 0.56599 |  0:00:02s\n",
      "epoch 46 | loss: 0.5623  |  0:00:02s\n",
      "epoch 47 | loss: 0.55709 |  0:00:02s\n",
      "epoch 48 | loss: 0.55565 |  0:00:02s\n",
      "epoch 49 | loss: 0.55615 |  0:00:02s\n",
      "epoch 50 | loss: 0.55202 |  0:00:02s\n",
      "epoch 51 | loss: 0.54588 |  0:00:02s\n",
      "epoch 52 | loss: 0.53105 |  0:00:02s\n",
      "epoch 53 | loss: 0.54017 |  0:00:02s\n",
      "epoch 54 | loss: 0.53883 |  0:00:02s\n",
      "epoch 55 | loss: 0.52947 |  0:00:02s\n",
      "epoch 56 | loss: 0.53554 |  0:00:03s\n",
      "epoch 57 | loss: 0.53831 |  0:00:03s\n",
      "epoch 58 | loss: 0.53843 |  0:00:03s\n",
      "epoch 59 | loss: 0.54412 |  0:00:03s\n",
      "epoch 60 | loss: 0.52344 |  0:00:03s\n",
      "epoch 61 | loss: 0.52016 |  0:00:03s\n",
      "epoch 62 | loss: 0.52755 |  0:00:03s\n",
      "epoch 63 | loss: 0.50833 |  0:00:03s\n",
      "epoch 64 | loss: 0.50857 |  0:00:03s\n",
      "epoch 65 | loss: 0.50677 |  0:00:03s\n",
      "epoch 66 | loss: 0.50478 |  0:00:03s\n",
      "epoch 67 | loss: 0.49834 |  0:00:03s\n",
      "epoch 68 | loss: 0.50135 |  0:00:03s\n",
      "epoch 69 | loss: 0.49252 |  0:00:03s\n",
      "epoch 70 | loss: 0.48954 |  0:00:03s\n",
      "epoch 71 | loss: 0.49757 |  0:00:03s\n",
      "epoch 72 | loss: 0.48328 |  0:00:03s\n",
      "epoch 73 | loss: 0.47831 |  0:00:03s\n",
      "epoch 74 | loss: 0.47308 |  0:00:03s\n",
      "epoch 75 | loss: 0.45788 |  0:00:04s\n",
      "epoch 76 | loss: 0.45859 |  0:00:04s\n",
      "epoch 77 | loss: 0.45381 |  0:00:04s\n",
      "epoch 78 | loss: 0.44934 |  0:00:04s\n",
      "epoch 79 | loss: 0.45184 |  0:00:04s\n",
      "epoch 80 | loss: 0.44514 |  0:00:04s\n",
      "epoch 81 | loss: 0.45027 |  0:00:04s\n",
      "epoch 82 | loss: 0.44612 |  0:00:04s\n",
      "epoch 83 | loss: 0.42995 |  0:00:04s\n",
      "epoch 84 | loss: 0.43108 |  0:00:04s\n",
      "epoch 85 | loss: 0.43845 |  0:00:04s\n",
      "epoch 86 | loss: 0.43972 |  0:00:04s\n",
      "epoch 87 | loss: 0.4375  |  0:00:04s\n",
      "epoch 88 | loss: 0.44549 |  0:00:04s\n",
      "epoch 89 | loss: 0.43949 |  0:00:05s\n",
      "epoch 90 | loss: 0.43642 |  0:00:05s\n",
      "epoch 91 | loss: 0.43953 |  0:00:05s\n",
      "epoch 92 | loss: 0.44669 |  0:00:05s\n",
      "epoch 93 | loss: 0.43558 |  0:00:05s\n",
      "epoch 94 | loss: 0.42501 |  0:00:05s\n",
      "epoch 95 | loss: 0.41871 |  0:00:05s\n",
      "epoch 96 | loss: 0.41834 |  0:00:05s\n",
      "epoch 97 | loss: 0.41477 |  0:00:05s\n",
      "epoch 98 | loss: 0.40577 |  0:00:05s\n",
      "epoch 99 | loss: 0.41343 |  0:00:05s\n",
      "epoch 100| loss: 0.40599 |  0:00:05s\n",
      "epoch 101| loss: 0.41155 |  0:00:05s\n",
      "epoch 102| loss: 0.39397 |  0:00:05s\n",
      "epoch 103| loss: 0.40656 |  0:00:05s\n",
      "epoch 104| loss: 0.39362 |  0:00:05s\n",
      "epoch 105| loss: 0.40083 |  0:00:05s\n",
      "epoch 106| loss: 0.40346 |  0:00:05s\n",
      "epoch 107| loss: 0.39323 |  0:00:06s\n",
      "epoch 108| loss: 0.3932  |  0:00:06s\n",
      "epoch 109| loss: 0.39722 |  0:00:06s\n",
      "epoch 110| loss: 0.39728 |  0:00:06s\n",
      "epoch 111| loss: 0.39896 |  0:00:06s\n",
      "epoch 112| loss: 0.41926 |  0:00:06s\n",
      "epoch 113| loss: 0.45433 |  0:00:06s\n",
      "epoch 114| loss: 0.4498  |  0:00:06s\n",
      "epoch 115| loss: 0.45319 |  0:00:06s\n",
      "epoch 116| loss: 0.45038 |  0:00:06s\n",
      "epoch 117| loss: 0.44621 |  0:00:06s\n",
      "epoch 118| loss: 0.42759 |  0:00:06s\n",
      "epoch 119| loss: 0.42988 |  0:00:06s\n",
      "epoch 120| loss: 0.41733 |  0:00:06s\n",
      "epoch 121| loss: 0.41528 |  0:00:06s\n",
      "epoch 122| loss: 0.40285 |  0:00:06s\n",
      "epoch 123| loss: 0.4037  |  0:00:06s\n",
      "epoch 124| loss: 0.40203 |  0:00:06s\n",
      "epoch 125| loss: 0.39203 |  0:00:06s\n",
      "epoch 126| loss: 0.38501 |  0:00:07s\n",
      "epoch 127| loss: 0.38663 |  0:00:07s\n",
      "epoch 128| loss: 0.3812  |  0:00:07s\n",
      "epoch 129| loss: 0.38412 |  0:00:07s\n",
      "epoch 130| loss: 0.37196 |  0:00:07s\n",
      "epoch 131| loss: 0.37514 |  0:00:07s\n",
      "epoch 132| loss: 0.37784 |  0:00:07s\n",
      "epoch 133| loss: 0.36879 |  0:00:07s\n",
      "epoch 134| loss: 0.37812 |  0:00:07s\n",
      "epoch 135| loss: 0.38062 |  0:00:07s\n",
      "epoch 136| loss: 0.38456 |  0:00:07s\n",
      "epoch 137| loss: 0.37936 |  0:00:07s\n",
      "epoch 138| loss: 0.37777 |  0:00:07s\n",
      "epoch 139| loss: 0.36841 |  0:00:07s\n",
      "epoch 140| loss: 0.36671 |  0:00:07s\n",
      "epoch 141| loss: 0.35987 |  0:00:07s\n",
      "epoch 142| loss: 0.35827 |  0:00:07s\n",
      "epoch 143| loss: 0.35664 |  0:00:08s\n",
      "epoch 144| loss: 0.34753 |  0:00:08s\n",
      "epoch 145| loss: 0.3499  |  0:00:08s\n",
      "epoch 146| loss: 0.34962 |  0:00:08s\n",
      "epoch 147| loss: 0.35095 |  0:00:08s\n",
      "epoch 148| loss: 0.34753 |  0:00:08s\n",
      "epoch 149| loss: 0.3448  |  0:00:08s\n",
      "epoch 150| loss: 0.35447 |  0:00:08s\n",
      "epoch 151| loss: 0.34861 |  0:00:08s\n",
      "epoch 152| loss: 0.35573 |  0:00:08s\n",
      "epoch 153| loss: 0.35182 |  0:00:08s\n",
      "epoch 154| loss: 0.34497 |  0:00:08s\n",
      "epoch 155| loss: 0.33289 |  0:00:08s\n",
      "epoch 156| loss: 0.34598 |  0:00:08s\n",
      "epoch 157| loss: 0.34077 |  0:00:08s\n",
      "epoch 158| loss: 0.33454 |  0:00:08s\n",
      "epoch 159| loss: 0.33347 |  0:00:08s\n",
      "epoch 160| loss: 0.32999 |  0:00:08s\n",
      "epoch 161| loss: 0.3281  |  0:00:08s\n",
      "epoch 162| loss: 0.31858 |  0:00:09s\n",
      "epoch 163| loss: 0.32241 |  0:00:09s\n",
      "epoch 164| loss: 0.3182  |  0:00:09s\n",
      "epoch 165| loss: 0.31434 |  0:00:09s\n",
      "epoch 166| loss: 0.3159  |  0:00:09s\n",
      "epoch 167| loss: 0.31661 |  0:00:09s\n",
      "epoch 168| loss: 0.3171  |  0:00:09s\n",
      "epoch 169| loss: 0.32003 |  0:00:09s\n",
      "epoch 170| loss: 0.32193 |  0:00:09s\n",
      "epoch 171| loss: 0.31555 |  0:00:09s\n",
      "epoch 172| loss: 0.32484 |  0:00:09s\n",
      "epoch 173| loss: 0.32642 |  0:00:09s\n",
      "epoch 174| loss: 0.32223 |  0:00:09s\n",
      "epoch 175| loss: 0.31255 |  0:00:09s\n",
      "epoch 176| loss: 0.31709 |  0:00:09s\n",
      "epoch 177| loss: 0.31723 |  0:00:09s\n",
      "epoch 178| loss: 0.31603 |  0:00:09s\n",
      "epoch 179| loss: 0.31897 |  0:00:09s\n",
      "epoch 180| loss: 0.32117 |  0:00:09s\n",
      "epoch 181| loss: 0.32442 |  0:00:09s\n",
      "epoch 182| loss: 0.33095 |  0:00:10s\n",
      "epoch 183| loss: 0.33288 |  0:00:10s\n",
      "epoch 184| loss: 0.3299  |  0:00:10s\n",
      "epoch 185| loss: 0.33397 |  0:00:10s\n",
      "epoch 186| loss: 0.3323  |  0:00:10s\n",
      "epoch 187| loss: 0.33396 |  0:00:10s\n",
      "epoch 188| loss: 0.33735 |  0:00:10s\n",
      "epoch 189| loss: 0.33246 |  0:00:10s\n",
      "epoch 190| loss: 0.32092 |  0:00:10s\n",
      "epoch 191| loss: 0.31798 |  0:00:10s\n",
      "epoch 192| loss: 0.31371 |  0:00:10s\n",
      "epoch 193| loss: 0.32528 |  0:00:10s\n",
      "epoch 194| loss: 0.31101 |  0:00:10s\n",
      "epoch 195| loss: 0.31062 |  0:00:10s\n",
      "epoch 196| loss: 0.32313 |  0:00:10s\n",
      "epoch 197| loss: 0.33684 |  0:00:10s\n",
      "epoch 198| loss: 0.3354  |  0:00:10s\n",
      "epoch 199| loss: 0.32985 |  0:00:10s\n",
      "epoch 200| loss: 0.32572 |  0:00:10s\n",
      "epoch 201| loss: 0.32716 |  0:00:10s\n",
      "epoch 202| loss: 0.32227 |  0:00:10s\n",
      "epoch 203| loss: 0.32857 |  0:00:11s\n",
      "epoch 204| loss: 0.31706 |  0:00:11s\n",
      "epoch 205| loss: 0.31699 |  0:00:11s\n",
      "epoch 206| loss: 0.31511 |  0:00:11s\n",
      "epoch 207| loss: 0.31603 |  0:00:11s\n",
      "epoch 208| loss: 0.30807 |  0:00:11s\n",
      "epoch 209| loss: 0.31431 |  0:00:11s\n",
      "epoch 210| loss: 0.30721 |  0:00:11s\n",
      "epoch 211| loss: 0.30284 |  0:00:11s\n",
      "epoch 212| loss: 0.30473 |  0:00:11s\n",
      "epoch 213| loss: 0.30198 |  0:00:11s\n",
      "epoch 214| loss: 0.30057 |  0:00:11s\n",
      "epoch 215| loss: 0.29847 |  0:00:11s\n",
      "epoch 216| loss: 0.30522 |  0:00:11s\n",
      "epoch 217| loss: 0.301   |  0:00:11s\n",
      "epoch 218| loss: 0.30157 |  0:00:11s\n",
      "epoch 219| loss: 0.30327 |  0:00:11s\n",
      "epoch 220| loss: 0.29734 |  0:00:11s\n",
      "epoch 221| loss: 0.30699 |  0:00:11s\n",
      "epoch 222| loss: 0.30113 |  0:00:11s\n",
      "epoch 223| loss: 0.30023 |  0:00:11s\n",
      "epoch 224| loss: 0.30549 |  0:00:12s\n",
      "epoch 225| loss: 0.29837 |  0:00:12s\n",
      "epoch 226| loss: 0.3027  |  0:00:12s\n",
      "epoch 227| loss: 0.28965 |  0:00:12s\n",
      "epoch 228| loss: 0.29163 |  0:00:12s\n",
      "epoch 229| loss: 0.28757 |  0:00:12s\n",
      "epoch 230| loss: 0.28387 |  0:00:12s\n",
      "epoch 231| loss: 0.28333 |  0:00:12s\n",
      "epoch 232| loss: 0.2757  |  0:00:12s\n",
      "epoch 233| loss: 0.27961 |  0:00:12s\n",
      "epoch 234| loss: 0.28187 |  0:00:12s\n",
      "epoch 235| loss: 0.2853  |  0:00:12s\n",
      "epoch 236| loss: 0.28403 |  0:00:12s\n",
      "epoch 237| loss: 0.2816  |  0:00:12s\n",
      "epoch 238| loss: 0.28701 |  0:00:12s\n",
      "epoch 239| loss: 0.31054 |  0:00:12s\n",
      "epoch 240| loss: 0.31714 |  0:00:12s\n",
      "epoch 241| loss: 0.32364 |  0:00:12s\n",
      "epoch 242| loss: 0.31792 |  0:00:13s\n",
      "epoch 243| loss: 0.31505 |  0:00:13s\n",
      "epoch 244| loss: 0.32424 |  0:00:13s\n",
      "epoch 245| loss: 0.31805 |  0:00:13s\n",
      "epoch 246| loss: 0.30821 |  0:00:13s\n",
      "epoch 247| loss: 0.31333 |  0:00:13s\n",
      "epoch 248| loss: 0.32323 |  0:00:13s\n",
      "epoch 249| loss: 0.31859 |  0:00:13s\n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.81739 |  0:00:00s\n",
      "epoch 1  | loss: 1.9502  |  0:00:00s\n",
      "epoch 2  | loss: 1.39759 |  0:00:00s\n",
      "epoch 3  | loss: 1.1451  |  0:00:00s\n",
      "epoch 4  | loss: 0.99285 |  0:00:00s\n",
      "epoch 5  | loss: 0.93746 |  0:00:00s\n",
      "epoch 6  | loss: 0.87896 |  0:00:00s\n",
      "epoch 7  | loss: 0.85118 |  0:00:00s\n",
      "epoch 8  | loss: 0.81473 |  0:00:00s\n",
      "epoch 9  | loss: 0.78216 |  0:00:00s\n",
      "epoch 10 | loss: 0.78285 |  0:00:00s\n",
      "epoch 11 | loss: 0.75086 |  0:00:00s\n",
      "epoch 12 | loss: 0.75307 |  0:00:00s\n",
      "epoch 13 | loss: 0.73741 |  0:00:00s\n",
      "epoch 14 | loss: 0.72797 |  0:00:00s\n",
      "epoch 15 | loss: 0.70407 |  0:00:00s\n",
      "epoch 16 | loss: 0.6945  |  0:00:00s\n",
      "epoch 17 | loss: 0.68304 |  0:00:00s\n",
      "epoch 18 | loss: 0.67824 |  0:00:00s\n",
      "epoch 19 | loss: 0.66198 |  0:00:01s\n",
      "epoch 20 | loss: 0.65994 |  0:00:01s\n",
      "epoch 21 | loss: 0.65044 |  0:00:01s\n",
      "epoch 22 | loss: 0.64562 |  0:00:01s\n",
      "epoch 23 | loss: 0.65486 |  0:00:01s\n",
      "epoch 24 | loss: 0.63528 |  0:00:01s\n",
      "epoch 25 | loss: 0.6476  |  0:00:01s\n",
      "epoch 26 | loss: 0.62024 |  0:00:01s\n",
      "epoch 27 | loss: 0.627   |  0:00:01s\n",
      "epoch 28 | loss: 0.62254 |  0:00:01s\n",
      "epoch 29 | loss: 0.60399 |  0:00:01s\n",
      "epoch 30 | loss: 0.60332 |  0:00:01s\n",
      "epoch 31 | loss: 0.59401 |  0:00:01s\n",
      "epoch 32 | loss: 0.588   |  0:00:01s\n",
      "epoch 33 | loss: 0.59782 |  0:00:01s\n",
      "epoch 34 | loss: 0.58326 |  0:00:01s\n",
      "epoch 35 | loss: 0.57951 |  0:00:01s\n",
      "epoch 36 | loss: 0.58226 |  0:00:01s\n",
      "epoch 37 | loss: 0.56855 |  0:00:01s\n",
      "epoch 38 | loss: 0.56042 |  0:00:01s\n",
      "epoch 39 | loss: 0.56397 |  0:00:01s\n",
      "epoch 40 | loss: 0.5612  |  0:00:01s\n",
      "epoch 41 | loss: 0.54961 |  0:00:02s\n",
      "epoch 42 | loss: 0.55425 |  0:00:02s\n",
      "epoch 43 | loss: 0.55024 |  0:00:02s\n",
      "epoch 44 | loss: 0.55509 |  0:00:02s\n",
      "epoch 45 | loss: 0.55475 |  0:00:02s\n",
      "epoch 46 | loss: 0.55628 |  0:00:02s\n",
      "epoch 47 | loss: 0.54921 |  0:00:02s\n",
      "epoch 48 | loss: 0.55385 |  0:00:02s\n",
      "epoch 49 | loss: 0.54548 |  0:00:02s\n",
      "epoch 50 | loss: 0.53593 |  0:00:02s\n",
      "epoch 51 | loss: 0.52401 |  0:00:02s\n",
      "epoch 52 | loss: 0.5258  |  0:00:02s\n",
      "epoch 53 | loss: 0.51468 |  0:00:02s\n",
      "epoch 54 | loss: 0.5109  |  0:00:02s\n",
      "epoch 55 | loss: 0.50642 |  0:00:02s\n",
      "epoch 56 | loss: 0.51369 |  0:00:02s\n",
      "epoch 57 | loss: 0.49849 |  0:00:02s\n",
      "epoch 58 | loss: 0.48867 |  0:00:02s\n",
      "epoch 59 | loss: 0.48671 |  0:00:02s\n",
      "epoch 60 | loss: 0.49011 |  0:00:02s\n",
      "epoch 61 | loss: 0.48289 |  0:00:02s\n",
      "epoch 62 | loss: 0.48628 |  0:00:03s\n",
      "epoch 63 | loss: 0.47758 |  0:00:03s\n",
      "epoch 64 | loss: 0.48434 |  0:00:03s\n",
      "epoch 65 | loss: 0.48192 |  0:00:03s\n",
      "epoch 66 | loss: 0.46368 |  0:00:03s\n",
      "epoch 67 | loss: 0.47331 |  0:00:03s\n",
      "epoch 68 | loss: 0.46903 |  0:00:03s\n",
      "epoch 69 | loss: 0.46892 |  0:00:03s\n",
      "epoch 70 | loss: 0.46235 |  0:00:03s\n",
      "epoch 71 | loss: 0.45835 |  0:00:03s\n",
      "epoch 72 | loss: 0.44797 |  0:00:03s\n",
      "epoch 73 | loss: 0.44721 |  0:00:03s\n",
      "epoch 74 | loss: 0.4642  |  0:00:03s\n",
      "epoch 75 | loss: 0.46103 |  0:00:03s\n",
      "epoch 76 | loss: 0.45269 |  0:00:03s\n",
      "epoch 77 | loss: 0.45454 |  0:00:03s\n",
      "epoch 78 | loss: 0.44592 |  0:00:03s\n",
      "epoch 79 | loss: 0.43851 |  0:00:04s\n",
      "epoch 80 | loss: 0.42861 |  0:00:04s\n",
      "epoch 81 | loss: 0.43711 |  0:00:04s\n",
      "epoch 82 | loss: 0.43447 |  0:00:04s\n",
      "epoch 83 | loss: 0.41889 |  0:00:04s\n",
      "epoch 84 | loss: 0.42493 |  0:00:04s\n",
      "epoch 85 | loss: 0.42591 |  0:00:04s\n",
      "epoch 86 | loss: 0.42037 |  0:00:04s\n",
      "epoch 87 | loss: 0.41794 |  0:00:04s\n",
      "epoch 88 | loss: 0.41799 |  0:00:04s\n",
      "epoch 89 | loss: 0.42064 |  0:00:04s\n",
      "epoch 90 | loss: 0.41709 |  0:00:04s\n",
      "epoch 91 | loss: 0.41156 |  0:00:04s\n",
      "epoch 92 | loss: 0.40922 |  0:00:04s\n",
      "epoch 93 | loss: 0.39814 |  0:00:04s\n",
      "epoch 94 | loss: 0.39911 |  0:00:04s\n",
      "epoch 95 | loss: 0.39958 |  0:00:04s\n",
      "epoch 96 | loss: 0.39779 |  0:00:04s\n",
      "epoch 97 | loss: 0.39769 |  0:00:04s\n",
      "epoch 98 | loss: 0.39814 |  0:00:04s\n",
      "epoch 99 | loss: 0.40741 |  0:00:04s\n",
      "epoch 100| loss: 0.40547 |  0:00:05s\n",
      "epoch 101| loss: 0.38912 |  0:00:05s\n",
      "epoch 102| loss: 0.3917  |  0:00:05s\n",
      "epoch 103| loss: 0.39625 |  0:00:05s\n",
      "epoch 104| loss: 0.38915 |  0:00:05s\n",
      "epoch 105| loss: 0.39305 |  0:00:05s\n",
      "epoch 106| loss: 0.38948 |  0:00:05s\n",
      "epoch 107| loss: 0.39781 |  0:00:05s\n",
      "epoch 108| loss: 0.3974  |  0:00:05s\n",
      "epoch 109| loss: 0.39292 |  0:00:05s\n",
      "epoch 110| loss: 0.38539 |  0:00:05s\n",
      "epoch 111| loss: 0.39258 |  0:00:05s\n",
      "epoch 112| loss: 0.40572 |  0:00:05s\n",
      "epoch 113| loss: 0.39067 |  0:00:05s\n",
      "epoch 114| loss: 0.38124 |  0:00:05s\n",
      "epoch 115| loss: 0.38499 |  0:00:05s\n",
      "epoch 116| loss: 0.3763  |  0:00:05s\n",
      "epoch 117| loss: 0.3696  |  0:00:05s\n",
      "epoch 118| loss: 0.36547 |  0:00:05s\n",
      "epoch 119| loss: 0.36128 |  0:00:06s\n",
      "epoch 120| loss: 0.36178 |  0:00:06s\n",
      "epoch 121| loss: 0.36525 |  0:00:06s\n",
      "epoch 122| loss: 0.38258 |  0:00:06s\n",
      "epoch 123| loss: 0.36693 |  0:00:06s\n",
      "epoch 124| loss: 0.35909 |  0:00:06s\n",
      "epoch 125| loss: 0.36328 |  0:00:06s\n",
      "epoch 126| loss: 0.36101 |  0:00:06s\n",
      "epoch 127| loss: 0.36357 |  0:00:06s\n",
      "epoch 128| loss: 0.36817 |  0:00:06s\n",
      "epoch 129| loss: 0.36506 |  0:00:06s\n",
      "epoch 130| loss: 0.35729 |  0:00:06s\n",
      "epoch 131| loss: 0.36759 |  0:00:06s\n",
      "epoch 132| loss: 0.36561 |  0:00:06s\n",
      "epoch 133| loss: 0.3593  |  0:00:06s\n",
      "epoch 134| loss: 0.35766 |  0:00:06s\n",
      "epoch 135| loss: 0.35332 |  0:00:06s\n",
      "epoch 136| loss: 0.34852 |  0:00:07s\n",
      "epoch 137| loss: 0.3422  |  0:00:07s\n",
      "epoch 138| loss: 0.33834 |  0:00:07s\n",
      "epoch 139| loss: 0.34381 |  0:00:07s\n",
      "epoch 140| loss: 0.33318 |  0:00:07s\n",
      "epoch 141| loss: 0.32812 |  0:00:07s\n",
      "epoch 142| loss: 0.32533 |  0:00:07s\n",
      "epoch 143| loss: 0.32318 |  0:00:07s\n",
      "epoch 144| loss: 0.3218  |  0:00:07s\n",
      "epoch 145| loss: 0.32016 |  0:00:07s\n",
      "epoch 146| loss: 0.32501 |  0:00:07s\n",
      "epoch 147| loss: 0.32567 |  0:00:07s\n",
      "epoch 148| loss: 0.32625 |  0:00:07s\n",
      "epoch 149| loss: 0.32397 |  0:00:07s\n",
      "epoch 150| loss: 0.32146 |  0:00:07s\n",
      "epoch 151| loss: 0.32393 |  0:00:07s\n",
      "epoch 152| loss: 0.3296  |  0:00:07s\n",
      "epoch 153| loss: 0.32425 |  0:00:07s\n",
      "epoch 154| loss: 0.32544 |  0:00:08s\n",
      "epoch 155| loss: 0.32463 |  0:00:08s\n",
      "epoch 156| loss: 0.32939 |  0:00:08s\n",
      "epoch 157| loss: 0.3269  |  0:00:08s\n",
      "epoch 158| loss: 0.33082 |  0:00:08s\n",
      "epoch 159| loss: 0.32647 |  0:00:08s\n",
      "epoch 160| loss: 0.33093 |  0:00:08s\n",
      "epoch 161| loss: 0.31836 |  0:00:08s\n",
      "epoch 162| loss: 0.31646 |  0:00:08s\n",
      "epoch 163| loss: 0.32356 |  0:00:08s\n",
      "epoch 164| loss: 0.32169 |  0:00:08s\n",
      "epoch 165| loss: 0.32117 |  0:00:08s\n",
      "epoch 166| loss: 0.32391 |  0:00:08s\n",
      "epoch 167| loss: 0.32863 |  0:00:08s\n",
      "epoch 168| loss: 0.33089 |  0:00:08s\n",
      "epoch 169| loss: 0.32629 |  0:00:08s\n",
      "epoch 170| loss: 0.32166 |  0:00:08s\n",
      "epoch 171| loss: 0.31296 |  0:00:08s\n",
      "epoch 172| loss: 0.32556 |  0:00:08s\n",
      "epoch 173| loss: 0.34476 |  0:00:09s\n",
      "epoch 174| loss: 0.34096 |  0:00:09s\n",
      "epoch 175| loss: 0.34308 |  0:00:09s\n",
      "epoch 176| loss: 0.33302 |  0:00:09s\n",
      "epoch 177| loss: 0.33554 |  0:00:09s\n",
      "epoch 178| loss: 0.32832 |  0:00:09s\n",
      "epoch 179| loss: 0.32965 |  0:00:09s\n",
      "epoch 180| loss: 0.32127 |  0:00:09s\n",
      "epoch 181| loss: 0.32186 |  0:00:09s\n",
      "epoch 182| loss: 0.32371 |  0:00:09s\n",
      "epoch 183| loss: 0.32398 |  0:00:09s\n",
      "epoch 184| loss: 0.31193 |  0:00:09s\n",
      "epoch 185| loss: 0.31631 |  0:00:09s\n",
      "epoch 186| loss: 0.30889 |  0:00:09s\n",
      "epoch 187| loss: 0.30987 |  0:00:09s\n",
      "epoch 188| loss: 0.31729 |  0:00:09s\n",
      "epoch 189| loss: 0.3111  |  0:00:09s\n",
      "epoch 190| loss: 0.30806 |  0:00:09s\n",
      "epoch 191| loss: 0.30538 |  0:00:09s\n",
      "epoch 192| loss: 0.29798 |  0:00:09s\n",
      "epoch 193| loss: 0.3133  |  0:00:09s\n",
      "epoch 194| loss: 0.30674 |  0:00:10s\n",
      "epoch 195| loss: 0.30269 |  0:00:10s\n",
      "epoch 196| loss: 0.30861 |  0:00:10s\n",
      "epoch 197| loss: 0.30354 |  0:00:10s\n",
      "epoch 198| loss: 0.30307 |  0:00:10s\n",
      "epoch 199| loss: 0.29348 |  0:00:10s\n",
      "epoch 200| loss: 0.29981 |  0:00:10s\n",
      "epoch 201| loss: 0.29273 |  0:00:10s\n",
      "epoch 202| loss: 0.28253 |  0:00:10s\n",
      "epoch 203| loss: 0.29239 |  0:00:10s\n",
      "epoch 204| loss: 0.28294 |  0:00:10s\n",
      "epoch 205| loss: 0.27896 |  0:00:10s\n",
      "epoch 206| loss: 0.28236 |  0:00:10s\n",
      "epoch 207| loss: 0.27578 |  0:00:10s\n",
      "epoch 208| loss: 0.27945 |  0:00:10s\n",
      "epoch 209| loss: 0.27586 |  0:00:10s\n",
      "epoch 210| loss: 0.27378 |  0:00:10s\n",
      "epoch 211| loss: 0.27275 |  0:00:10s\n",
      "epoch 212| loss: 0.27101 |  0:00:11s\n",
      "epoch 213| loss: 0.27141 |  0:00:11s\n",
      "epoch 214| loss: 0.27076 |  0:00:11s\n",
      "epoch 215| loss: 0.26903 |  0:00:11s\n",
      "epoch 216| loss: 0.26148 |  0:00:11s\n",
      "epoch 217| loss: 0.26321 |  0:00:11s\n",
      "epoch 218| loss: 0.26128 |  0:00:11s\n",
      "epoch 219| loss: 0.26569 |  0:00:11s\n",
      "epoch 220| loss: 0.26364 |  0:00:11s\n",
      "epoch 221| loss: 0.26229 |  0:00:11s\n",
      "epoch 222| loss: 0.2691  |  0:00:11s\n",
      "epoch 223| loss: 0.2685  |  0:00:11s\n",
      "epoch 224| loss: 0.27729 |  0:00:11s\n",
      "epoch 225| loss: 0.2777  |  0:00:11s\n",
      "epoch 226| loss: 0.27114 |  0:00:11s\n",
      "epoch 227| loss: 0.2659  |  0:00:11s\n",
      "epoch 228| loss: 0.26694 |  0:00:11s\n",
      "epoch 229| loss: 0.26051 |  0:00:11s\n",
      "epoch 230| loss: 0.25551 |  0:00:11s\n",
      "epoch 231| loss: 0.2513  |  0:00:12s\n",
      "epoch 232| loss: 0.25369 |  0:00:12s\n",
      "epoch 233| loss: 0.24873 |  0:00:12s\n",
      "epoch 234| loss: 0.24694 |  0:00:12s\n",
      "epoch 235| loss: 0.24535 |  0:00:12s\n",
      "epoch 236| loss: 0.24759 |  0:00:12s\n",
      "epoch 237| loss: 0.26342 |  0:00:12s\n",
      "epoch 238| loss: 0.28979 |  0:00:12s\n",
      "epoch 239| loss: 0.28517 |  0:00:12s\n",
      "epoch 240| loss: 0.28296 |  0:00:12s\n",
      "epoch 241| loss: 0.27361 |  0:00:12s\n",
      "epoch 242| loss: 0.26383 |  0:00:12s\n",
      "epoch 243| loss: 0.26785 |  0:00:12s\n",
      "epoch 244| loss: 0.26855 |  0:00:12s\n",
      "epoch 245| loss: 0.26375 |  0:00:12s\n",
      "epoch 246| loss: 0.26661 |  0:00:12s\n",
      "epoch 247| loss: 0.26556 |  0:00:12s\n",
      "epoch 248| loss: 0.27136 |  0:00:12s\n",
      "epoch 249| loss: 0.26316 |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.92061 |  0:00:00s\n",
      "epoch 1  | loss: 1.87847 |  0:00:00s\n",
      "epoch 2  | loss: 1.41256 |  0:00:00s\n",
      "epoch 3  | loss: 1.17852 |  0:00:00s\n",
      "epoch 4  | loss: 1.03624 |  0:00:00s\n",
      "epoch 5  | loss: 0.95412 |  0:00:00s\n",
      "epoch 6  | loss: 0.86999 |  0:00:00s\n",
      "epoch 7  | loss: 0.86063 |  0:00:00s\n",
      "epoch 8  | loss: 0.83695 |  0:00:00s\n",
      "epoch 9  | loss: 0.79986 |  0:00:00s\n",
      "epoch 10 | loss: 0.77152 |  0:00:00s\n",
      "epoch 11 | loss: 0.74728 |  0:00:00s\n",
      "epoch 12 | loss: 0.71824 |  0:00:00s\n",
      "epoch 13 | loss: 0.70374 |  0:00:00s\n",
      "epoch 14 | loss: 0.68623 |  0:00:00s\n",
      "epoch 15 | loss: 0.67642 |  0:00:00s\n",
      "epoch 16 | loss: 0.6512  |  0:00:00s\n",
      "epoch 17 | loss: 0.64273 |  0:00:00s\n",
      "epoch 18 | loss: 0.64668 |  0:00:00s\n",
      "epoch 19 | loss: 0.64027 |  0:00:00s\n",
      "epoch 20 | loss: 0.63935 |  0:00:00s\n",
      "epoch 21 | loss: 0.61931 |  0:00:01s\n",
      "epoch 22 | loss: 0.62988 |  0:00:01s\n",
      "epoch 23 | loss: 0.61092 |  0:00:01s\n",
      "epoch 24 | loss: 0.61015 |  0:00:01s\n",
      "epoch 25 | loss: 0.61206 |  0:00:01s\n",
      "epoch 26 | loss: 0.60394 |  0:00:01s\n",
      "epoch 27 | loss: 0.61174 |  0:00:01s\n",
      "epoch 28 | loss: 0.61525 |  0:00:01s\n",
      "epoch 29 | loss: 0.60968 |  0:00:01s\n",
      "epoch 30 | loss: 0.5923  |  0:00:01s\n",
      "epoch 31 | loss: 0.58875 |  0:00:01s\n",
      "epoch 32 | loss: 0.589   |  0:00:01s\n",
      "epoch 33 | loss: 0.58487 |  0:00:01s\n",
      "epoch 34 | loss: 0.57808 |  0:00:01s\n",
      "epoch 35 | loss: 0.5794  |  0:00:01s\n",
      "epoch 36 | loss: 0.57571 |  0:00:01s\n",
      "epoch 37 | loss: 0.57622 |  0:00:01s\n",
      "epoch 38 | loss: 0.56976 |  0:00:01s\n",
      "epoch 39 | loss: 0.56114 |  0:00:01s\n",
      "epoch 40 | loss: 0.55748 |  0:00:01s\n",
      "epoch 41 | loss: 0.54703 |  0:00:01s\n",
      "epoch 42 | loss: 0.54465 |  0:00:01s\n",
      "epoch 43 | loss: 0.53562 |  0:00:02s\n",
      "epoch 44 | loss: 0.53077 |  0:00:02s\n",
      "epoch 45 | loss: 0.53776 |  0:00:02s\n",
      "epoch 46 | loss: 0.53512 |  0:00:02s\n",
      "epoch 47 | loss: 0.5283  |  0:00:02s\n",
      "epoch 48 | loss: 0.51578 |  0:00:02s\n",
      "epoch 49 | loss: 0.50864 |  0:00:02s\n",
      "epoch 50 | loss: 0.5027  |  0:00:02s\n",
      "epoch 51 | loss: 0.4931  |  0:00:02s\n",
      "epoch 52 | loss: 0.49581 |  0:00:02s\n",
      "epoch 53 | loss: 0.49959 |  0:00:02s\n",
      "epoch 54 | loss: 0.49512 |  0:00:02s\n",
      "epoch 55 | loss: 0.48773 |  0:00:02s\n",
      "epoch 56 | loss: 0.49378 |  0:00:02s\n",
      "epoch 57 | loss: 0.4916  |  0:00:02s\n",
      "epoch 58 | loss: 0.48863 |  0:00:02s\n",
      "epoch 59 | loss: 0.47559 |  0:00:02s\n",
      "epoch 60 | loss: 0.48989 |  0:00:02s\n",
      "epoch 61 | loss: 0.48405 |  0:00:03s\n",
      "epoch 62 | loss: 0.48544 |  0:00:03s\n",
      "epoch 63 | loss: 0.47703 |  0:00:03s\n",
      "epoch 64 | loss: 0.47081 |  0:00:03s\n",
      "epoch 65 | loss: 0.4676  |  0:00:03s\n",
      "epoch 66 | loss: 0.46737 |  0:00:03s\n",
      "epoch 67 | loss: 0.45612 |  0:00:03s\n",
      "epoch 68 | loss: 0.45896 |  0:00:03s\n",
      "epoch 69 | loss: 0.45147 |  0:00:03s\n",
      "epoch 70 | loss: 0.45948 |  0:00:03s\n",
      "epoch 71 | loss: 0.45966 |  0:00:03s\n",
      "epoch 72 | loss: 0.4638  |  0:00:03s\n",
      "epoch 73 | loss: 0.45529 |  0:00:03s\n",
      "epoch 74 | loss: 0.46422 |  0:00:03s\n",
      "epoch 75 | loss: 0.45718 |  0:00:03s\n",
      "epoch 76 | loss: 0.44412 |  0:00:03s\n",
      "epoch 77 | loss: 0.4433  |  0:00:03s\n",
      "epoch 78 | loss: 0.44328 |  0:00:03s\n",
      "epoch 79 | loss: 0.43999 |  0:00:03s\n",
      "epoch 80 | loss: 0.42619 |  0:00:03s\n",
      "epoch 81 | loss: 0.42441 |  0:00:04s\n",
      "epoch 82 | loss: 0.43064 |  0:00:04s\n",
      "epoch 83 | loss: 0.42585 |  0:00:04s\n",
      "epoch 84 | loss: 0.42397 |  0:00:04s\n",
      "epoch 85 | loss: 0.42185 |  0:00:04s\n",
      "epoch 86 | loss: 0.41914 |  0:00:04s\n",
      "epoch 87 | loss: 0.41338 |  0:00:04s\n",
      "epoch 88 | loss: 0.40707 |  0:00:04s\n",
      "epoch 89 | loss: 0.40738 |  0:00:04s\n",
      "epoch 90 | loss: 0.40729 |  0:00:04s\n",
      "epoch 91 | loss: 0.40473 |  0:00:04s\n",
      "epoch 92 | loss: 0.41565 |  0:00:04s\n",
      "epoch 93 | loss: 0.40653 |  0:00:04s\n",
      "epoch 94 | loss: 0.40461 |  0:00:04s\n",
      "epoch 95 | loss: 0.40138 |  0:00:04s\n",
      "epoch 96 | loss: 0.39598 |  0:00:04s\n",
      "epoch 97 | loss: 0.3881  |  0:00:04s\n",
      "epoch 98 | loss: 0.39336 |  0:00:04s\n",
      "epoch 99 | loss: 0.38427 |  0:00:04s\n",
      "epoch 100| loss: 0.38327 |  0:00:04s\n",
      "epoch 101| loss: 0.38265 |  0:00:05s\n",
      "epoch 102| loss: 0.38904 |  0:00:05s\n",
      "epoch 103| loss: 0.39169 |  0:00:05s\n",
      "epoch 104| loss: 0.39432 |  0:00:05s\n",
      "epoch 105| loss: 0.39914 |  0:00:05s\n",
      "epoch 106| loss: 0.39256 |  0:00:05s\n",
      "epoch 107| loss: 0.39408 |  0:00:05s\n",
      "epoch 108| loss: 0.38315 |  0:00:05s\n",
      "epoch 109| loss: 0.37782 |  0:00:05s\n",
      "epoch 110| loss: 0.38174 |  0:00:05s\n",
      "epoch 111| loss: 0.37992 |  0:00:05s\n",
      "epoch 112| loss: 0.37714 |  0:00:05s\n",
      "epoch 113| loss: 0.38169 |  0:00:05s\n",
      "epoch 114| loss: 0.37595 |  0:00:05s\n",
      "epoch 115| loss: 0.3794  |  0:00:05s\n",
      "epoch 116| loss: 0.36826 |  0:00:05s\n",
      "epoch 117| loss: 0.36364 |  0:00:05s\n",
      "epoch 118| loss: 0.36253 |  0:00:05s\n",
      "epoch 119| loss: 0.3661  |  0:00:05s\n",
      "epoch 120| loss: 0.35261 |  0:00:06s\n",
      "epoch 121| loss: 0.35599 |  0:00:06s\n",
      "epoch 122| loss: 0.36499 |  0:00:06s\n",
      "epoch 123| loss: 0.35628 |  0:00:06s\n",
      "epoch 124| loss: 0.35854 |  0:00:06s\n",
      "epoch 125| loss: 0.34927 |  0:00:06s\n",
      "epoch 126| loss: 0.34923 |  0:00:06s\n",
      "epoch 127| loss: 0.34911 |  0:00:06s\n",
      "epoch 128| loss: 0.34362 |  0:00:06s\n",
      "epoch 129| loss: 0.34241 |  0:00:06s\n",
      "epoch 130| loss: 0.34172 |  0:00:06s\n",
      "epoch 131| loss: 0.34485 |  0:00:06s\n",
      "epoch 132| loss: 0.35217 |  0:00:06s\n",
      "epoch 133| loss: 0.33541 |  0:00:06s\n",
      "epoch 134| loss: 0.34013 |  0:00:06s\n",
      "epoch 135| loss: 0.32697 |  0:00:06s\n",
      "epoch 136| loss: 0.32963 |  0:00:06s\n",
      "epoch 137| loss: 0.32596 |  0:00:06s\n",
      "epoch 138| loss: 0.33267 |  0:00:06s\n",
      "epoch 139| loss: 0.32686 |  0:00:07s\n",
      "epoch 140| loss: 0.3253  |  0:00:07s\n",
      "epoch 141| loss: 0.31551 |  0:00:07s\n",
      "epoch 142| loss: 0.32115 |  0:00:07s\n",
      "epoch 143| loss: 0.31713 |  0:00:07s\n",
      "epoch 144| loss: 0.31108 |  0:00:07s\n",
      "epoch 145| loss: 0.30902 |  0:00:07s\n",
      "epoch 146| loss: 0.31307 |  0:00:07s\n",
      "epoch 147| loss: 0.3205  |  0:00:07s\n",
      "epoch 148| loss: 0.31474 |  0:00:07s\n",
      "epoch 149| loss: 0.31901 |  0:00:07s\n",
      "epoch 150| loss: 0.31065 |  0:00:07s\n",
      "epoch 151| loss: 0.30682 |  0:00:07s\n",
      "epoch 152| loss: 0.30767 |  0:00:07s\n",
      "epoch 153| loss: 0.30484 |  0:00:07s\n",
      "epoch 154| loss: 0.30073 |  0:00:07s\n",
      "epoch 155| loss: 0.30194 |  0:00:07s\n",
      "epoch 156| loss: 0.29914 |  0:00:07s\n",
      "epoch 157| loss: 0.29664 |  0:00:07s\n",
      "epoch 158| loss: 0.30027 |  0:00:08s\n",
      "epoch 159| loss: 0.29295 |  0:00:08s\n",
      "epoch 160| loss: 0.29742 |  0:00:08s\n",
      "epoch 161| loss: 0.28922 |  0:00:08s\n",
      "epoch 162| loss: 0.29158 |  0:00:08s\n",
      "epoch 163| loss: 0.28982 |  0:00:08s\n",
      "epoch 164| loss: 0.29332 |  0:00:08s\n",
      "epoch 165| loss: 0.2947  |  0:00:08s\n",
      "epoch 166| loss: 0.28818 |  0:00:08s\n",
      "epoch 167| loss: 0.29549 |  0:00:08s\n",
      "epoch 168| loss: 0.29291 |  0:00:08s\n",
      "epoch 169| loss: 0.29534 |  0:00:08s\n",
      "epoch 170| loss: 0.30736 |  0:00:08s\n",
      "epoch 171| loss: 0.30031 |  0:00:08s\n",
      "epoch 172| loss: 0.3047  |  0:00:08s\n",
      "epoch 173| loss: 0.2982  |  0:00:08s\n",
      "epoch 174| loss: 0.30319 |  0:00:08s\n",
      "epoch 175| loss: 0.2977  |  0:00:08s\n",
      "epoch 176| loss: 0.28903 |  0:00:08s\n",
      "epoch 177| loss: 0.29158 |  0:00:09s\n",
      "epoch 178| loss: 0.28925 |  0:00:09s\n",
      "epoch 179| loss: 0.28195 |  0:00:09s\n",
      "epoch 180| loss: 0.2838  |  0:00:09s\n",
      "epoch 181| loss: 0.28365 |  0:00:09s\n",
      "epoch 182| loss: 0.28442 |  0:00:09s\n",
      "epoch 183| loss: 0.28839 |  0:00:09s\n",
      "epoch 184| loss: 0.29876 |  0:00:09s\n",
      "epoch 185| loss: 0.30576 |  0:00:09s\n",
      "epoch 186| loss: 0.31078 |  0:00:09s\n",
      "epoch 187| loss: 0.30679 |  0:00:09s\n",
      "epoch 188| loss: 0.3147  |  0:00:09s\n",
      "epoch 189| loss: 0.32033 |  0:00:09s\n",
      "epoch 190| loss: 0.30814 |  0:00:09s\n",
      "epoch 191| loss: 0.31316 |  0:00:09s\n",
      "epoch 192| loss: 0.30808 |  0:00:09s\n",
      "epoch 193| loss: 0.3225  |  0:00:09s\n",
      "epoch 194| loss: 0.31263 |  0:00:09s\n",
      "epoch 195| loss: 0.31406 |  0:00:10s\n",
      "epoch 196| loss: 0.30142 |  0:00:10s\n",
      "epoch 197| loss: 0.30886 |  0:00:10s\n",
      "epoch 198| loss: 0.30161 |  0:00:10s\n",
      "epoch 199| loss: 0.29984 |  0:00:10s\n",
      "epoch 200| loss: 0.30336 |  0:00:10s\n",
      "epoch 201| loss: 0.30182 |  0:00:10s\n",
      "epoch 202| loss: 0.30363 |  0:00:10s\n",
      "epoch 203| loss: 0.30991 |  0:00:10s\n",
      "epoch 204| loss: 0.30545 |  0:00:10s\n",
      "epoch 205| loss: 0.30558 |  0:00:10s\n",
      "epoch 206| loss: 0.30735 |  0:00:10s\n",
      "epoch 207| loss: 0.30407 |  0:00:10s\n",
      "epoch 208| loss: 0.29865 |  0:00:10s\n",
      "epoch 209| loss: 0.28903 |  0:00:10s\n",
      "epoch 210| loss: 0.28957 |  0:00:10s\n",
      "epoch 211| loss: 0.29284 |  0:00:10s\n",
      "epoch 212| loss: 0.29011 |  0:00:10s\n",
      "epoch 213| loss: 0.2842  |  0:00:10s\n",
      "epoch 214| loss: 0.28352 |  0:00:11s\n",
      "epoch 215| loss: 0.2783  |  0:00:11s\n",
      "epoch 216| loss: 0.26793 |  0:00:11s\n",
      "epoch 217| loss: 0.27414 |  0:00:11s\n",
      "epoch 218| loss: 0.2707  |  0:00:11s\n",
      "epoch 219| loss: 0.26441 |  0:00:11s\n",
      "epoch 220| loss: 0.26047 |  0:00:11s\n",
      "epoch 221| loss: 0.26662 |  0:00:11s\n",
      "epoch 222| loss: 0.26277 |  0:00:11s\n",
      "epoch 223| loss: 0.27048 |  0:00:11s\n",
      "epoch 224| loss: 0.26583 |  0:00:11s\n",
      "epoch 225| loss: 0.26699 |  0:00:11s\n",
      "epoch 226| loss: 0.26129 |  0:00:11s\n",
      "epoch 227| loss: 0.25873 |  0:00:11s\n",
      "epoch 228| loss: 0.25624 |  0:00:11s\n",
      "epoch 229| loss: 0.25602 |  0:00:11s\n",
      "epoch 230| loss: 0.2507  |  0:00:11s\n",
      "epoch 231| loss: 0.24937 |  0:00:11s\n",
      "epoch 232| loss: 0.25449 |  0:00:11s\n",
      "epoch 233| loss: 0.25657 |  0:00:12s\n",
      "epoch 234| loss: 0.25747 |  0:00:12s\n",
      "epoch 235| loss: 0.25374 |  0:00:12s\n",
      "epoch 236| loss: 0.24713 |  0:00:12s\n",
      "epoch 237| loss: 0.25336 |  0:00:12s\n",
      "epoch 238| loss: 0.24963 |  0:00:12s\n",
      "epoch 239| loss: 0.2536  |  0:00:12s\n",
      "epoch 240| loss: 0.25202 |  0:00:12s\n",
      "epoch 241| loss: 0.2612  |  0:00:12s\n",
      "epoch 242| loss: 0.26289 |  0:00:12s\n",
      "epoch 243| loss: 0.25685 |  0:00:12s\n",
      "epoch 244| loss: 0.25493 |  0:00:12s\n",
      "epoch 245| loss: 0.25257 |  0:00:12s\n",
      "epoch 246| loss: 0.246   |  0:00:12s\n",
      "epoch 247| loss: 0.24539 |  0:00:12s\n",
      "epoch 248| loss: 0.24528 |  0:00:12s\n",
      "epoch 249| loss: 0.2446  |  0:00:12s\n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 2.86535 |  0:00:00s\n",
      "epoch 1  | loss: 1.79278 |  0:00:00s\n",
      "epoch 2  | loss: 1.36643 |  0:00:00s\n",
      "epoch 3  | loss: 1.15652 |  0:00:00s\n",
      "epoch 4  | loss: 1.04668 |  0:00:00s\n",
      "epoch 5  | loss: 0.95066 |  0:00:00s\n",
      "epoch 6  | loss: 0.89283 |  0:00:00s\n",
      "epoch 7  | loss: 0.84859 |  0:00:00s\n",
      "epoch 8  | loss: 0.799   |  0:00:00s\n",
      "epoch 9  | loss: 0.77999 |  0:00:00s\n",
      "epoch 10 | loss: 0.76338 |  0:00:00s\n",
      "epoch 11 | loss: 0.75929 |  0:00:00s\n",
      "epoch 12 | loss: 0.72831 |  0:00:00s\n",
      "epoch 13 | loss: 0.72493 |  0:00:00s\n",
      "epoch 14 | loss: 0.71434 |  0:00:00s\n",
      "epoch 15 | loss: 0.70479 |  0:00:00s\n",
      "epoch 16 | loss: 0.69858 |  0:00:00s\n",
      "epoch 17 | loss: 0.68732 |  0:00:00s\n",
      "epoch 18 | loss: 0.69304 |  0:00:01s\n",
      "epoch 19 | loss: 0.68028 |  0:00:01s\n",
      "epoch 20 | loss: 0.67489 |  0:00:01s\n",
      "epoch 21 | loss: 0.66942 |  0:00:01s\n",
      "epoch 22 | loss: 0.65448 |  0:00:01s\n",
      "epoch 23 | loss: 0.63501 |  0:00:01s\n",
      "epoch 24 | loss: 0.63557 |  0:00:01s\n",
      "epoch 25 | loss: 0.62069 |  0:00:01s\n",
      "epoch 26 | loss: 0.61781 |  0:00:01s\n",
      "epoch 27 | loss: 0.61937 |  0:00:01s\n",
      "epoch 28 | loss: 0.62455 |  0:00:01s\n",
      "epoch 29 | loss: 0.61538 |  0:00:01s\n",
      "epoch 30 | loss: 0.60563 |  0:00:01s\n",
      "epoch 31 | loss: 0.60589 |  0:00:01s\n",
      "epoch 32 | loss: 0.60755 |  0:00:01s\n",
      "epoch 33 | loss: 0.5949  |  0:00:01s\n",
      "epoch 34 | loss: 0.59719 |  0:00:01s\n",
      "epoch 35 | loss: 0.58846 |  0:00:01s\n",
      "epoch 36 | loss: 0.57878 |  0:00:01s\n",
      "epoch 37 | loss: 0.57298 |  0:00:01s\n",
      "epoch 38 | loss: 0.57492 |  0:00:02s\n",
      "epoch 39 | loss: 0.57128 |  0:00:02s\n",
      "epoch 40 | loss: 0.57054 |  0:00:02s\n",
      "epoch 41 | loss: 0.56587 |  0:00:02s\n",
      "epoch 42 | loss: 0.55603 |  0:00:02s\n",
      "epoch 43 | loss: 0.55236 |  0:00:02s\n",
      "epoch 44 | loss: 0.57422 |  0:00:02s\n",
      "epoch 45 | loss: 0.56108 |  0:00:02s\n",
      "epoch 46 | loss: 0.54846 |  0:00:02s\n",
      "epoch 47 | loss: 0.54651 |  0:00:02s\n",
      "epoch 48 | loss: 0.54348 |  0:00:02s\n",
      "epoch 49 | loss: 0.53432 |  0:00:02s\n",
      "epoch 50 | loss: 0.52887 |  0:00:02s\n",
      "epoch 51 | loss: 0.52384 |  0:00:02s\n",
      "epoch 52 | loss: 0.52417 |  0:00:02s\n",
      "epoch 53 | loss: 0.51926 |  0:00:02s\n",
      "epoch 54 | loss: 0.5192  |  0:00:02s\n",
      "epoch 55 | loss: 0.5114  |  0:00:02s\n",
      "epoch 56 | loss: 0.51508 |  0:00:03s\n",
      "epoch 57 | loss: 0.50459 |  0:00:03s\n",
      "epoch 58 | loss: 0.50174 |  0:00:03s\n",
      "epoch 59 | loss: 0.51116 |  0:00:03s\n",
      "epoch 60 | loss: 0.51069 |  0:00:03s\n",
      "epoch 61 | loss: 0.50231 |  0:00:03s\n",
      "epoch 62 | loss: 0.51176 |  0:00:03s\n",
      "epoch 63 | loss: 0.49514 |  0:00:03s\n",
      "epoch 64 | loss: 0.49212 |  0:00:03s\n",
      "epoch 65 | loss: 0.48502 |  0:00:03s\n",
      "epoch 66 | loss: 0.48207 |  0:00:03s\n",
      "epoch 67 | loss: 0.47839 |  0:00:03s\n",
      "epoch 68 | loss: 0.47369 |  0:00:03s\n",
      "epoch 69 | loss: 0.46852 |  0:00:03s\n",
      "epoch 70 | loss: 0.46874 |  0:00:03s\n",
      "epoch 71 | loss: 0.47322 |  0:00:03s\n",
      "epoch 72 | loss: 0.46123 |  0:00:03s\n",
      "epoch 73 | loss: 0.45424 |  0:00:03s\n",
      "epoch 74 | loss: 0.46746 |  0:00:03s\n",
      "epoch 75 | loss: 0.45507 |  0:00:04s\n",
      "epoch 76 | loss: 0.45967 |  0:00:04s\n",
      "epoch 77 | loss: 0.4558  |  0:00:04s\n",
      "epoch 78 | loss: 0.44825 |  0:00:04s\n",
      "epoch 79 | loss: 0.43867 |  0:00:04s\n",
      "epoch 80 | loss: 0.43777 |  0:00:04s\n",
      "epoch 81 | loss: 0.43753 |  0:00:04s\n",
      "epoch 82 | loss: 0.43594 |  0:00:04s\n",
      "epoch 83 | loss: 0.43911 |  0:00:04s\n",
      "epoch 84 | loss: 0.43341 |  0:00:04s\n",
      "epoch 85 | loss: 0.43056 |  0:00:04s\n",
      "epoch 86 | loss: 0.44562 |  0:00:04s\n",
      "epoch 87 | loss: 0.43193 |  0:00:04s\n",
      "epoch 88 | loss: 0.43046 |  0:00:04s\n",
      "epoch 89 | loss: 0.42211 |  0:00:04s\n",
      "epoch 90 | loss: 0.43399 |  0:00:04s\n",
      "epoch 91 | loss: 0.42676 |  0:00:04s\n",
      "epoch 92 | loss: 0.43372 |  0:00:04s\n",
      "epoch 93 | loss: 0.42737 |  0:00:04s\n",
      "epoch 94 | loss: 0.4289  |  0:00:05s\n",
      "epoch 95 | loss: 0.42956 |  0:00:05s\n",
      "epoch 96 | loss: 0.42867 |  0:00:05s\n",
      "epoch 97 | loss: 0.42353 |  0:00:05s\n",
      "epoch 98 | loss: 0.42088 |  0:00:05s\n",
      "epoch 99 | loss: 0.42898 |  0:00:05s\n",
      "epoch 100| loss: 0.44161 |  0:00:05s\n",
      "epoch 101| loss: 0.42965 |  0:00:05s\n",
      "epoch 102| loss: 0.42729 |  0:00:05s\n",
      "epoch 103| loss: 0.42968 |  0:00:05s\n",
      "epoch 104| loss: 0.42888 |  0:00:05s\n",
      "epoch 105| loss: 0.4287  |  0:00:05s\n",
      "epoch 106| loss: 0.4292  |  0:00:05s\n",
      "epoch 107| loss: 0.42887 |  0:00:05s\n",
      "epoch 108| loss: 0.42368 |  0:00:05s\n",
      "epoch 109| loss: 0.42037 |  0:00:05s\n",
      "epoch 110| loss: 0.41392 |  0:00:05s\n",
      "epoch 111| loss: 0.41832 |  0:00:05s\n",
      "epoch 112| loss: 0.40945 |  0:00:05s\n",
      "epoch 113| loss: 0.41892 |  0:00:06s\n",
      "epoch 114| loss: 0.40095 |  0:00:06s\n",
      "epoch 115| loss: 0.41501 |  0:00:06s\n",
      "epoch 116| loss: 0.40962 |  0:00:06s\n",
      "epoch 117| loss: 0.40265 |  0:00:06s\n",
      "epoch 118| loss: 0.40243 |  0:00:06s\n",
      "epoch 119| loss: 0.40359 |  0:00:06s\n",
      "epoch 120| loss: 0.40078 |  0:00:06s\n",
      "epoch 121| loss: 0.40296 |  0:00:06s\n",
      "epoch 122| loss: 0.41285 |  0:00:06s\n",
      "epoch 123| loss: 0.39986 |  0:00:06s\n",
      "epoch 124| loss: 0.39958 |  0:00:06s\n",
      "epoch 125| loss: 0.39705 |  0:00:06s\n",
      "epoch 126| loss: 0.40172 |  0:00:06s\n",
      "epoch 127| loss: 0.39152 |  0:00:06s\n",
      "epoch 128| loss: 0.3886  |  0:00:06s\n",
      "epoch 129| loss: 0.39143 |  0:00:06s\n",
      "epoch 130| loss: 0.38899 |  0:00:06s\n",
      "epoch 131| loss: 0.38964 |  0:00:06s\n",
      "epoch 132| loss: 0.39169 |  0:00:07s\n",
      "epoch 133| loss: 0.37979 |  0:00:07s\n",
      "epoch 134| loss: 0.37645 |  0:00:07s\n",
      "epoch 135| loss: 0.37209 |  0:00:07s\n",
      "epoch 136| loss: 0.37882 |  0:00:07s\n",
      "epoch 137| loss: 0.38235 |  0:00:07s\n",
      "epoch 138| loss: 0.37778 |  0:00:07s\n",
      "epoch 139| loss: 0.38001 |  0:00:07s\n",
      "epoch 140| loss: 0.37775 |  0:00:07s\n",
      "epoch 141| loss: 0.37774 |  0:00:07s\n",
      "epoch 142| loss: 0.37448 |  0:00:07s\n",
      "epoch 143| loss: 0.3655  |  0:00:07s\n",
      "epoch 144| loss: 0.36578 |  0:00:07s\n",
      "epoch 145| loss: 0.36479 |  0:00:07s\n",
      "epoch 146| loss: 0.36406 |  0:00:07s\n",
      "epoch 147| loss: 0.36118 |  0:00:07s\n",
      "epoch 148| loss: 0.35981 |  0:00:07s\n",
      "epoch 149| loss: 0.36161 |  0:00:07s\n",
      "epoch 150| loss: 0.3535  |  0:00:07s\n",
      "epoch 151| loss: 0.35188 |  0:00:08s\n",
      "epoch 152| loss: 0.34756 |  0:00:08s\n",
      "epoch 153| loss: 0.34308 |  0:00:08s\n",
      "epoch 154| loss: 0.3477  |  0:00:08s\n",
      "epoch 155| loss: 0.35979 |  0:00:08s\n",
      "epoch 156| loss: 0.36346 |  0:00:08s\n",
      "epoch 157| loss: 0.34918 |  0:00:08s\n",
      "epoch 158| loss: 0.3495  |  0:00:08s\n",
      "epoch 159| loss: 0.3506  |  0:00:08s\n",
      "epoch 160| loss: 0.36124 |  0:00:08s\n",
      "epoch 161| loss: 0.36037 |  0:00:08s\n",
      "epoch 162| loss: 0.36213 |  0:00:08s\n",
      "epoch 163| loss: 0.37447 |  0:00:08s\n",
      "epoch 164| loss: 0.36025 |  0:00:08s\n",
      "epoch 165| loss: 0.3546  |  0:00:08s\n",
      "epoch 166| loss: 0.34945 |  0:00:08s\n",
      "epoch 167| loss: 0.35194 |  0:00:08s\n",
      "epoch 168| loss: 0.34111 |  0:00:08s\n",
      "epoch 169| loss: 0.33952 |  0:00:08s\n",
      "epoch 170| loss: 0.35194 |  0:00:09s\n",
      "epoch 171| loss: 0.34056 |  0:00:09s\n",
      "epoch 172| loss: 0.34812 |  0:00:09s\n",
      "epoch 173| loss: 0.33551 |  0:00:09s\n",
      "epoch 174| loss: 0.33969 |  0:00:09s\n",
      "epoch 175| loss: 0.33223 |  0:00:09s\n",
      "epoch 176| loss: 0.32182 |  0:00:09s\n",
      "epoch 177| loss: 0.32586 |  0:00:09s\n",
      "epoch 178| loss: 0.32769 |  0:00:09s\n",
      "epoch 179| loss: 0.32543 |  0:00:09s\n",
      "epoch 180| loss: 0.32198 |  0:00:09s\n",
      "epoch 181| loss: 0.31938 |  0:00:09s\n",
      "epoch 182| loss: 0.32164 |  0:00:09s\n",
      "epoch 183| loss: 0.32763 |  0:00:09s\n",
      "epoch 184| loss: 0.32057 |  0:00:09s\n",
      "epoch 185| loss: 0.32019 |  0:00:09s\n",
      "epoch 186| loss: 0.31817 |  0:00:10s\n",
      "epoch 187| loss: 0.31109 |  0:00:10s\n",
      "epoch 188| loss: 0.31437 |  0:00:10s\n",
      "epoch 189| loss: 0.31011 |  0:00:10s\n",
      "epoch 190| loss: 0.31208 |  0:00:10s\n",
      "epoch 191| loss: 0.31083 |  0:00:10s\n",
      "epoch 192| loss: 0.30521 |  0:00:10s\n",
      "epoch 193| loss: 0.30883 |  0:00:10s\n",
      "epoch 194| loss: 0.30188 |  0:00:10s\n",
      "epoch 195| loss: 0.30974 |  0:00:10s\n",
      "epoch 196| loss: 0.31859 |  0:00:10s\n",
      "epoch 197| loss: 0.32243 |  0:00:10s\n",
      "epoch 198| loss: 0.31744 |  0:00:10s\n",
      "epoch 199| loss: 0.30996 |  0:00:10s\n",
      "epoch 200| loss: 0.31697 |  0:00:10s\n",
      "epoch 201| loss: 0.31505 |  0:00:10s\n",
      "epoch 202| loss: 0.31222 |  0:00:10s\n",
      "epoch 203| loss: 0.31437 |  0:00:10s\n",
      "epoch 204| loss: 0.30882 |  0:00:10s\n",
      "epoch 205| loss: 0.30084 |  0:00:11s\n",
      "epoch 206| loss: 0.29611 |  0:00:11s\n",
      "epoch 207| loss: 0.29018 |  0:00:11s\n",
      "epoch 208| loss: 0.29295 |  0:00:11s\n",
      "epoch 209| loss: 0.2903  |  0:00:11s\n",
      "epoch 210| loss: 0.29015 |  0:00:11s\n",
      "epoch 211| loss: 0.28266 |  0:00:11s\n",
      "epoch 212| loss: 0.29073 |  0:00:11s\n",
      "epoch 213| loss: 0.27732 |  0:00:11s\n",
      "epoch 214| loss: 0.29047 |  0:00:11s\n",
      "epoch 215| loss: 0.28414 |  0:00:11s\n",
      "epoch 216| loss: 0.27188 |  0:00:11s\n",
      "epoch 217| loss: 0.27197 |  0:00:11s\n",
      "epoch 218| loss: 0.2722  |  0:00:11s\n",
      "epoch 219| loss: 0.27839 |  0:00:11s\n",
      "epoch 220| loss: 0.26844 |  0:00:11s\n",
      "epoch 221| loss: 0.2695  |  0:00:11s\n",
      "epoch 222| loss: 0.26859 |  0:00:11s\n",
      "epoch 223| loss: 0.27397 |  0:00:11s\n",
      "epoch 224| loss: 0.274   |  0:00:12s\n",
      "epoch 225| loss: 0.272   |  0:00:12s\n",
      "epoch 226| loss: 0.27435 |  0:00:12s\n",
      "epoch 227| loss: 0.26107 |  0:00:12s\n",
      "epoch 228| loss: 0.2637  |  0:00:12s\n",
      "epoch 229| loss: 0.25841 |  0:00:12s\n",
      "epoch 230| loss: 0.26123 |  0:00:12s\n",
      "epoch 231| loss: 0.25732 |  0:00:12s\n",
      "epoch 232| loss: 0.25878 |  0:00:12s\n",
      "epoch 233| loss: 0.25739 |  0:00:12s\n",
      "epoch 234| loss: 0.25941 |  0:00:12s\n",
      "epoch 235| loss: 0.25935 |  0:00:12s\n",
      "epoch 236| loss: 0.25725 |  0:00:13s\n",
      "epoch 237| loss: 0.26592 |  0:00:13s\n",
      "epoch 238| loss: 0.25588 |  0:00:13s\n",
      "epoch 239| loss: 0.25841 |  0:00:13s\n",
      "epoch 240| loss: 0.25239 |  0:00:13s\n",
      "epoch 241| loss: 0.25507 |  0:00:13s\n",
      "epoch 242| loss: 0.25284 |  0:00:13s\n",
      "epoch 243| loss: 0.24976 |  0:00:13s\n",
      "epoch 244| loss: 0.2466  |  0:00:13s\n",
      "epoch 245| loss: 0.24515 |  0:00:13s\n",
      "epoch 246| loss: 0.24911 |  0:00:13s\n",
      "epoch 247| loss: 0.24784 |  0:00:13s\n",
      "epoch 248| loss: 0.25058 |  0:00:13s\n",
      "epoch 249| loss: 0.25062 |  0:00:14s\n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2831\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "2887\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n",
      "Saving predictions in dict!\n",
      "1790\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:21</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">694</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:21\u001b[0m,\u001b[1;36m694\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:21</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">720</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:21\u001b[0m,\u001b[1;36m720\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:21</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">751</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:21\u001b[0m,\u001b[1;36m751\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:29:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">447</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:29:22\u001b[0m,\u001b[1;36m447\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:30:08</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">435</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:30:08\u001b[0m,\u001b[1;36m435\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:30:08</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">463</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:30:08\u001b[0m,\u001b[1;36m463\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:30:08</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">475</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:30:08\u001b[0m,\u001b[1;36m475\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX 6000 Ada Generation') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOM detected during LR Find. Try reducing your batch_size or the model parameters./nOriginal Error: CUDA out of memory. Tried to allocate 4.98 GiB. GPU 0 has a total capacity of 47.49 GiB of which 2.80 GiB is free. Process 4044835 has 564.00 MiB memory in use. Process 4118701 has 21.72 GiB memory in use. Process 422108 has 10.39 GiB memory in use. Including non-PyTorch memory, this process has 10.48 GiB memory in use. Process 428348 has 544.00 MiB memory in use. Of the allocated memory 9.97 GiB is allocated by PyTorch, and 9.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2390\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:30:20</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">788</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:30:20\u001b[0m,\u001b[1;36m788\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:30:20</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">815</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:30:20\u001b[0m,\u001b[1;36m815\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:30:20</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">845</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:30:20\u001b[0m,\u001b[1;36m845\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:30:21</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">445</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:30:21\u001b[0m,\u001b[1;36m445\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:31:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">246</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:31:09\u001b[0m,\u001b[1;36m246\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:31:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">267</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:31:09\u001b[0m,\u001b[1;36m267\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:31:09</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">278</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:31:09\u001b[0m,\u001b[1;36m278\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOM detected during LR Find. Try reducing your batch_size or the model parameters./nOriginal Error: CUDA out of memory. Tried to allocate 4.98 GiB. GPU 0 has a total capacity of 47.49 GiB of which 2.80 GiB is free. Process 4044835 has 564.00 MiB memory in use. Process 4118701 has 21.72 GiB memory in use. Process 422108 has 10.39 GiB memory in use. Including non-PyTorch memory, this process has 10.48 GiB memory in use. Process 428348 has 544.00 MiB memory in use. Of the allocated memory 9.97 GiB is allocated by PyTorch, and 9.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2401\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:31:21</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">473</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:31:21\u001b[0m,\u001b[1;36m473\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:31:21</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">497</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:31:21\u001b[0m,\u001b[1;36m497\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:31:21</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">528</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:31:21\u001b[0m,\u001b[1;36m528\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:31:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">132</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:31:22\u001b[0m,\u001b[1;36m132\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:32:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">027</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:32:10\u001b[0m,\u001b[1;36m027\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:32:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">045</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:32:10\u001b[0m,\u001b[1;36m045\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:32:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">055</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:32:10\u001b[0m,\u001b[1;36m055\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOM detected during LR Find. Try reducing your batch_size or the model parameters./nOriginal Error: CUDA out of memory. Tried to allocate 4.98 GiB. GPU 0 has a total capacity of 47.49 GiB of which 2.80 GiB is free. Process 4044835 has 564.00 MiB memory in use. Process 4118701 has 21.72 GiB memory in use. Process 422108 has 10.39 GiB memory in use. Including non-PyTorch memory, this process has 10.48 GiB memory in use. Process 428348 has 544.00 MiB memory in use. Of the allocated memory 9.97 GiB is allocated by PyTorch, and 9.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2437\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:32:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">253</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:32:22\u001b[0m,\u001b[1;36m253\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:32:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">278</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:32:22\u001b[0m,\u001b[1;36m278\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:32:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">310</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:32:22\u001b[0m,\u001b[1;36m310\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:32:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">909</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:32:22\u001b[0m,\u001b[1;36m909\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:33:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">699</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:33:10\u001b[0m,\u001b[1;36m699\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:33:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">717</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:33:10\u001b[0m,\u001b[1;36m717\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:33:10</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">728</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:33:10\u001b[0m,\u001b[1;36m728\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOM detected during LR Find. Try reducing your batch_size or the model parameters./nOriginal Error: CUDA out of memory. Tried to allocate 4.98 GiB. GPU 0 has a total capacity of 47.49 GiB of which 2.80 GiB is free. Process 4044835 has 564.00 MiB memory in use. Process 4118701 has 21.72 GiB memory in use. Process 422108 has 10.39 GiB memory in use. Including non-PyTorch memory, this process has 10.48 GiB memory in use. Process 428348 has 544.00 MiB memory in use. Of the allocated memory 9.97 GiB is allocated by PyTorch, and 9.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2493\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:33:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">858</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:33:22\u001b[0m,\u001b[1;36m858\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:33:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">884</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:33:22\u001b[0m,\u001b[1;36m884\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:33:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">916</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:33:22\u001b[0m,\u001b[1;36m916\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:33:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">003</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:33:26\u001b[0m,\u001b[1;36m003\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:34:11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">692</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:34:11\u001b[0m,\u001b[1;36m692\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:34:11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">710</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:34:11\u001b[0m,\u001b[1;36m710\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:34:11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">720</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:34:11\u001b[0m,\u001b[1;36m720\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOM detected during LR Find. Try reducing your batch_size or the model parameters./nOriginal Error: CUDA out of memory. Tried to allocate 4.98 GiB. GPU 0 has a total capacity of 47.49 GiB of which 2.79 GiB is free. Process 4044835 has 564.00 MiB memory in use. Process 4118701 has 21.72 GiB memory in use. Process 422108 has 10.39 GiB memory in use. Including non-PyTorch memory, this process has 10.48 GiB memory in use. Process 428348 has 544.00 MiB memory in use. Of the allocated memory 9.97 GiB is allocated by PyTorch, and 9.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2533\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:34:24</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">387</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:34:24\u001b[0m,\u001b[1;36m387\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:34:24</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">411</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:34:24\u001b[0m,\u001b[1;36m411\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:34:24</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">442</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:34:24\u001b[0m,\u001b[1;36m442\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:34:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">043</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:34:25\u001b[0m,\u001b[1;36m043\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:35:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">208</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:35:13\u001b[0m,\u001b[1;36m208\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:35:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">227</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:35:13\u001b[0m,\u001b[1;36m227\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:35:13</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">238</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:35:13\u001b[0m,\u001b[1;36m238\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOM detected during LR Find. Try reducing your batch_size or the model parameters./nOriginal Error: CUDA out of memory. Tried to allocate 4.98 GiB. GPU 0 has a total capacity of 47.49 GiB of which 2.80 GiB is free. Process 4044835 has 564.00 MiB memory in use. Process 4118701 has 21.72 GiB memory in use. Process 422108 has 10.39 GiB memory in use. Including non-PyTorch memory, this process has 10.48 GiB memory in use. Process 428348 has 544.00 MiB memory in use. Of the allocated memory 9.97 GiB is allocated by PyTorch, and 9.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2537\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:35:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">788</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:35:25\u001b[0m,\u001b[1;36m788\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:35:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">813</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:35:25\u001b[0m,\u001b[1;36m813\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:35:25</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">845</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:35:25\u001b[0m,\u001b[1;36m845\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:35:26</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">461</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:35:26\u001b[0m,\u001b[1;36m461\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:36:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">569</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:36:14\u001b[0m,\u001b[1;36m569\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:36:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">589</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:36:14\u001b[0m,\u001b[1;36m589\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:36:14</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">600</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:36:14\u001b[0m,\u001b[1;36m600\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOM detected during LR Find. Try reducing your batch_size or the model parameters./nOriginal Error: CUDA out of memory. Tried to allocate 4.98 GiB. GPU 0 has a total capacity of 47.49 GiB of which 2.80 GiB is free. Process 4044835 has 564.00 MiB memory in use. Process 4118701 has 21.72 GiB memory in use. Process 422108 has 10.39 GiB memory in use. Including non-PyTorch memory, this process has 10.48 GiB memory in use. Process 428348 has 544.00 MiB memory in use. Of the allocated memory 9.97 GiB is allocated by PyTorch, and 9.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2541\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:36:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">391</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:36:27\u001b[0m,\u001b[1;36m391\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:36:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">417</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:36:27\u001b[0m,\u001b[1;36m417\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:36:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">451</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:36:27\u001b[0m,\u001b[1;36m451\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:36:28</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">077</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:36:28\u001b[0m,\u001b[1;36m077\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:37:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">042</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:37:18\u001b[0m,\u001b[1;36m042\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:37:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">061</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:37:18\u001b[0m,\u001b[1;36m061\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:37:18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">072</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:37:18\u001b[0m,\u001b[1;36m072\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOM detected during LR Find. Try reducing your batch_size or the model parameters./nOriginal Error: CUDA out of memory. Tried to allocate 4.98 GiB. GPU 0 has a total capacity of 47.49 GiB of which 2.80 GiB is free. Process 4044835 has 564.00 MiB memory in use. Process 4118701 has 21.72 GiB memory in use. Process 422108 has 10.39 GiB memory in use. Including non-PyTorch memory, this process has 10.48 GiB memory in use. Process 428348 has 544.00 MiB memory in use. Of the allocated memory 9.97 GiB is allocated by PyTorch, and 9.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2591\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:37:30</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">779</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:37:30\u001b[0m,\u001b[1;36m779\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:37:30</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">804</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:37:30\u001b[0m,\u001b[1;36m804\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:37:30</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">836</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:37:30\u001b[0m,\u001b[1;36m836\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:37:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">454</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:37:31\u001b[0m,\u001b[1;36m454\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:38:20</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">763</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:38:20\u001b[0m,\u001b[1;36m763\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:38:20</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">793</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:38:20\u001b[0m,\u001b[1;36m793\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:38:20</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">807</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:38:20\u001b[0m,\u001b[1;36m807\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOM detected during LR Find. Try reducing your batch_size or the model parameters./nOriginal Error: CUDA out of memory. Tried to allocate 4.98 GiB. GPU 0 has a total capacity of 47.49 GiB of which 2.79 GiB is free. Process 4044835 has 564.00 MiB memory in use. Process 4118701 has 21.72 GiB memory in use. Process 422108 has 10.39 GiB memory in use. Including non-PyTorch memory, this process has 10.48 GiB memory in use. Process 428348 has 544.00 MiB memory in use. Of the allocated memory 9.97 GiB is allocated by PyTorch, and 9.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2661\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:38:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">935</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:38:33\u001b[0m,\u001b[1;36m935\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:38:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">960</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:38:33\u001b[0m,\u001b[1;36m960\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:38:33</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">993</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:38:33\u001b[0m,\u001b[1;36m993\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:38:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:38:38\u001b[0m,\u001b[1;36m265\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:40:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">802</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:40:22\u001b[0m,\u001b[1;36m802\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:40:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">908</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:40:22\u001b[0m,\u001b[1;36m908\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:40:22</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">921</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:40:22\u001b[0m,\u001b[1;36m921\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOM detected during LR Find. Try reducing your batch_size or the model parameters./nOriginal Error: CUDA out of memory. Tried to allocate 4.98 GiB. GPU 0 has a total capacity of 47.49 GiB of which 3.33 GiB is free. Process 4044835 has 564.00 MiB memory in use. Process 4118701 has 21.72 GiB memory in use. Process 422108 has 10.39 GiB memory in use. Including non-PyTorch memory, this process has 10.48 GiB memory in use. Of the allocated memory 9.97 GiB is allocated by PyTorch, and 9.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "2670\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:40:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">534</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:40:39\u001b[0m,\u001b[1;36m534\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:40:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">568</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:40:39\u001b[0m,\u001b[1;36m568\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:40:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">608</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:40:39\u001b[0m,\u001b[1;36m608\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:40:40</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">482</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:40:40\u001b[0m,\u001b[1;36m482\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m\n\u001b[1;32m     45\u001b[0m c_test \u001b[38;5;241m=\u001b[39m df_all[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPTGENDER\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPTEDUCAT\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39miloc[idx_test]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Now you can call your `train_model` function with these components\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     fold_dict_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_imputer_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_X_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_y_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_y_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mordinal_imputer_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_ordinal_imputer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontinuous_imputer_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_continuous_imputer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseparate_imputers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Or however you want to specify\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimputation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(fold_dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimputation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \n\u001b[1;32m     60\u001b[0m     dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfitting_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(fold_dict_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfitting_time\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \n",
      "File \u001b[0;32m~/Documents/projects/optimus/notebooks/../src/train.py:211\u001b[0m, in \u001b[0;36mtrain_imputer_model\u001b[0;34m(df_X_train, df_X_test, df_y_train, df_y_test, c_train, c_test, ordinal_model, name_ordinal_imputer, continuous_model, name_continuous_imputer, model, name_model, imputer_model, name_imputer, separate_imputers, ordinal_features)\u001b[0m\n\u001b[1;32m    208\u001b[0m     X_test_adjusted \u001b[38;5;241m=\u001b[39m X_test_adjusted\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m#y_test_adjusted = y_test_adjusted.values\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_adjusted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_adjusted\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    212\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    214\u001b[0m predict_model_time \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Documents/projects/optimus/notebooks/../src/wrapper.py:100\u001b[0m, in \u001b[0;36mTabularModelWrapper.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     92\u001b[0m     df[c] \u001b[38;5;241m=\u001b[39m y[c]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m TabularModel(\n\u001b[1;32m     95\u001b[0m     data_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_config,\n\u001b[1;32m     96\u001b[0m     model_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config,\n\u001b[1;32m     97\u001b[0m     optimizer_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_config,\n\u001b[1;32m     98\u001b[0m     trainer_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer_config\n\u001b[1;32m     99\u001b[0m )\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ideally separate validation\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/optimus-iOcAib6k-py3.12/lib/python3.12/site-packages/pytorch_tabular/tabular_model.py:797\u001b[0m, in \u001b[0;36mTabularModel.fit\u001b[0;34m(self, train, validation, loss, metrics, metrics_prob_inputs, optimizer, optimizer_params, train_sampler, target_transform, max_epochs, min_epochs, seed, callbacks, datamodule, cache_data, handle_oom)\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    792\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain data and datamodule is provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Ignoring the train data and using the datamodule.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Set either one of them to None to avoid this warning.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m         )\n\u001b[0;32m--> 797\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_prob_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(model, datamodule, callbacks, max_epochs, min_epochs, handle_oom)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/optimus-iOcAib6k-py3.12/lib/python3.12/site-packages/pytorch_tabular/tabular_model.py:602\u001b[0m, in \u001b[0;36mTabularModel.prepare_model\u001b[0;34m(self, datamodule, loss, metrics, metrics_prob_inputs, optimizer, optimizer_params)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# Fetching the config as some data specific configs have been added in the datamodule\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minferred_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_parse_config(datamodule\u001b[38;5;241m.\u001b[39mupdate_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig), InferredConfig)\n\u001b[0;32m--> 602\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Unused in SSL tasks\u001b[39;49;00m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Unused in SSL tasks\u001b[39;49;00m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metrics_prob_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_prob_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Unused in SSL tasks\u001b[39;49;00m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_optimizer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43minferred_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minferred_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;66;03m# Data Aware Initialization(for the models that need it)\u001b[39;00m\n\u001b[1;32m    612\u001b[0m model\u001b[38;5;241m.\u001b[39mdata_aware_initialization(datamodule)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/optimus-iOcAib6k-py3.12/lib/python3.12/site-packages/pytorch_tabular/models/gate/gate_model.py:210\u001b[0m, in \u001b[0;36mGatedAdditiveTreeEnsembleModel.__init__\u001b[0;34m(self, config, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: DictConfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/optimus-iOcAib6k-py3.12/lib/python3.12/site-packages/pytorch_tabular/models/base_model.py:169\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, config, custom_loss, custom_metrics, custom_metrics_prob_inputs, custom_optimizer, custom_optimizer_params, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters(config)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# The concatenated output dim of the embedding layer\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_loss()\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_metrics()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/optimus-iOcAib6k-py3.12/lib/python3.12/site-packages/pytorch_tabular/models/gate/gate_model.py:226\u001b[0m, in \u001b[0;36mGatedAdditiveTreeEnsembleModel._build_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_build_network\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Backbone\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backbone \u001b[38;5;241m=\u001b[39m \u001b[43mGatedAdditiveTreesBackbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_continuous_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontinuous_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcat_embedding_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgflu_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgflu_stages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgflu_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgflu_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_trees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_trees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtree_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtree_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbinning_activation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinning_activation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_mask_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_mask_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_norm_continuous_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm_continuous_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchain_trees\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchain_trees\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtree_wise_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_wise_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtree_wise_attention_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_wise_attention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgflu_feature_init_sparsity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgflu_feature_init_sparsity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtree_feature_init_sparsity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_feature_init_sparsity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# Embedding Layer\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backbone\u001b[38;5;241m.\u001b[39m_build_embedding_layer()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/optimus-iOcAib6k-py3.12/lib/python3.12/site-packages/pytorch_tabular/models/gate/gate_model.py:82\u001b[0m, in \u001b[0;36mGatedAdditiveTreesBackbone.__init__\u001b[0;34m(self, cat_embedding_dims, n_continuous_features, gflu_stages, num_trees, tree_depth, chain_trees, tree_wise_attention, tree_wise_attention_dropout, gflu_dropout, tree_dropout, binning_activation, feature_mask_function, gflu_feature_init_sparsity, tree_feature_init_sparsity, learnable_sparsity, batch_norm_continuous_input, virtual_batch_size, embedding_dropout)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearnable_sparsity \u001b[38;5;241m=\u001b[39m learnable_sparsity\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvirtual_batch_size \u001b[38;5;241m=\u001b[39m virtual_batch_size\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/optimus-iOcAib6k-py3.12/lib/python3.12/site-packages/pytorch_tabular/models/gate/gate_model.py:86\u001b[0m, in \u001b[0;36mGatedAdditiveTreesBackbone._build_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_build_network\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgflu_stages \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgflus \u001b[38;5;241m=\u001b[39m \u001b[43mGatedFeatureLearningUnit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_features_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgflu_stages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_mask_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_mask_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgflu_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_sparsity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgflu_feature_init_sparsity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlearnable_sparsity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearnable_sparsity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_trees \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m     96\u001b[0m             [\n\u001b[1;32m     97\u001b[0m                 NeuralDecisionTree(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m             ]\n\u001b[1;32m    108\u001b[0m         )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/optimus-iOcAib6k-py3.12/lib/python3.12/site-packages/pytorch_tabular/models/common/layers/gated_units.py:31\u001b[0m, in \u001b[0;36mGatedFeatureLearningUnit.__init__\u001b[0;34m(self, n_features_in, n_stages, feature_mask_function, feature_sparsity, learnable_sparsity, dropout)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_sparsity \u001b[38;5;241m=\u001b[39m feature_sparsity\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearnable_sparsity \u001b[38;5;241m=\u001b[39m learnable_sparsity\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/optimus-iOcAib6k-py3.12/lib/python3.12/site-packages/pytorch_tabular/models/common/layers/gated_units.py:52\u001b[0m, in \u001b[0;36mGatedFeatureLearningUnit._build_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_build_network\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_in \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m---> 52\u001b[0m         [\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_features_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_features_in\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_stages)]\n\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_out \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m     55\u001b[0m         [nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_stages)]\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_feature_mask()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/optimus-iOcAib6k-py3.12/lib/python3.12/site-packages/torch/nn/modules/linear.py:112\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/optimus-iOcAib6k-py3.12/lib/python3.12/site-packages/torch/nn/modules/linear.py:118\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/optimus-iOcAib6k-py3.12/lib/python3.12/site-packages/torch/nn/init.py:518\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    516\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    params = {\n",
    "            \"ordinal_imputer\": name_ordinal_imputer, \n",
    "            \"continuous_imputer\": name_continuous_imputer, \n",
    "            \"model\": name_model, \"train_shape\" : [df_X.shape[0]-1, df_X.shape[1]],\n",
    "            \"test_shape\": [1, df_X.shape[1]]\n",
    "        }\n",
    "    \n",
    "    # Define the subset of keys you care about\n",
    "    keys_to_check = ['ordinal_imputer', 'continuous_imputer', 'model']  # or whatever subset you want\n",
    "\n",
    "    # Check if a result in all_dict_results has the same values for just those keys\n",
    "    if any(all(result['params'].get(k) == params.get(k) for k in keys_to_check) for result in all_dict_results):\n",
    "        print(f\"Skipping existing combination (subset match): {[params[k] for k in keys_to_check]}\")\n",
    "        continue\n",
    "\n",
    "    dict_results = {\n",
    "            \"params\": params, \n",
    "            \"imputation_time\": [],\n",
    "            \"fitting_time\": [], \n",
    "            \"results_adj\": [], \n",
    "            \"results_org\": []\n",
    "        }\n",
    "\n",
    "    for test_nloc in test_indices: \n",
    "        print(test_nloc)\n",
    "\n",
    "        idx_train = [True for i in range(df_X.shape[0])]\n",
    "        idx_test = [False for i in range(df_X.shape[0])]\n",
    "\n",
    "        idx_test[test_nloc] = True\n",
    "        idx_train[test_nloc] = False\n",
    "\n",
    "        df_X_train = df_X.loc[idx_train]\n",
    "        df_X_test = df_X.loc[idx_test]\n",
    "\n",
    "        df_y_train = df_y.loc[idx_train]\n",
    "        df_y_test = df_y.loc[idx_test]\n",
    "\n",
    "        c_train = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_train]\n",
    "        c_test = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_test]\n",
    "\n",
    "        try: \n",
    "        \n",
    "            # Now you can call your `train_model` function with these components\n",
    "            fold_dict_results = train_imputer_model(\n",
    "                df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "                c_train, c_test,\n",
    "                ordinal_imputer_instance, name_ordinal_imputer,\n",
    "                continuous_imputer_instance, name_continuous_imputer,\n",
    "                model_instance, name_model,\n",
    "                separate_imputers=True  # Or however you want to specify\n",
    "            )\n",
    "            \n",
    "            dict_results[\"imputation_time\"].append(fold_dict_results[\"imputation_time\"]) \n",
    "            dict_results[\"fitting_time\"].append(fold_dict_results[\"fitting_time\"])  \n",
    "            dict_results[\"results_adj\"].append(fold_dict_results[\"results_adj\"])  \n",
    "            dict_results[\"results_org\"].append(fold_dict_results[\"results_org\"])  \n",
    "\n",
    "        except Exception as e:  \n",
    "\n",
    "            print(e)\n",
    "            \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "    # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open(results_file, 'wb') as handle:\n",
    "    pickle.dump(all_dict_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../pickle/training_3_loonona_dict_results.pickle'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_file, \"rb\") as input_file:\n",
    "    dict_results_loo_nona = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_nona = pd.json_normalize(dict_results_loo_nona)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train only on MRI features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = list(df_X.isna().any(axis=1))\n",
    "idx_test = list(~df_X.isna().any(axis=1))\n",
    "\n",
    "set_intersect_rid = set(df_all[idx_train].RID).intersection(set(df_all[idx_test].RID))\n",
    "intersect_rid_idx = df_all.RID.isin(set_intersect_rid)\n",
    "\n",
    "for i, bool_test in enumerate(idx_test): \n",
    "    if intersect_rid_idx.iloc[i] & bool_test:\n",
    "        idx_test[i] = False\n",
    "        idx_train[i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]] = df_X[[\"APOE_epsilon2\", \"APOE_epsilon3\", \"APOE_epsilon4\"]].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train = df_X[dict_select[\"MRIth\"]].loc[idx_train]\n",
    "df_X_test = df_X[dict_select[\"MRIth\"]].loc[idx_test]\n",
    "\n",
    "df_y_train = df_y.loc[idx_train]\n",
    "df_y_test = df_y.loc[idx_test]\n",
    "\n",
    "c_train = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_train]\n",
    "c_test = df_all[[\"AGE\", \"PTGENDER\", \"PTEDUCAT\"]].iloc[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "n_imputation_iter = 10\n",
    "\n",
    "# Define hyperparameters\n",
    "gain_parameters = {\n",
    "    'hint_rate': 0.9,\n",
    "    'alpha': 100,\n",
    "    'iterations': 1000\n",
    "}\n",
    "\n",
    "# Continuous Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "continuous_imputer_list = [\n",
    "    (\"NoImputer\", KNNImputer(n_neighbors=1)),\n",
    "\n",
    "]\n",
    "\n",
    "# Ordinal Imputer List (list of tuples with unique strings and corresponding instances)\n",
    "ordinal_imputer_list = [\n",
    "    (\"NoImputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "]\n",
    "\n",
    "# Predictive Models List (list of tuples with unique strings and corresponding instances)\n",
    "predictive_models_list = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"MultiTaskElasticNet\", MultiTaskElasticNet()),\n",
    "    (\"MultiTaskElasticNet_tuned\", MultiTaskElasticNet(**{'alpha': 0.01, 'l1_ratio': 0.01})),\n",
    "    (\"MultiTaskLasso\", MultiTaskLasso()),\n",
    "    (\"MultiTaskLasso_tuned\", MultiTaskLasso(**{'alpha': 0.001})),\n",
    "    (\"RandomForestRegressor\", RandomForestRegressor()),\n",
    "    (\"XGBoostRegressor\", XGBoostRegressor()),\n",
    "    (\"XGBoostRegressor_tuned\", XGBoostRegressor(**{'colsample_bytree': 0.5079831261101071, 'learning_rate': 0.0769592094304232, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8049983288913105})),\n",
    "    (\"TabNetRegressor_default\", TabNetModelWrapper(n_a=8, n_d=8)),\n",
    "    (\"TabNetRegressor_custom\", TabNetModelWrapper(n_a=32, n_d=32)),\n",
    "    (\"PLSRegression_4_components\", PLSRegression(n_components=4))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: LinearRegression\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskElasticNet\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskElasticNet_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskLasso\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: MultiTaskLasso_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: RandomForestRegressor\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: XGBoostRegressor\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: XGBoostRegressor_tuned\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: TabNetRegressor_default\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: TabNetRegressor_custom\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: PLSRegression_4_components\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: GatedAdditiveTreeEnsembleConfig_tab\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: DANetConfig_tab\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: TabTransformerConfig_tab\n",
      "Continuous Imputer: NoImputer, Ordinal Imputer: NoImputer, Model: TabNetModelConfig_tab\n",
      "Combinations of preprocessing and models to test : 15\n"
     ]
    }
   ],
   "source": [
    "ordinal_features = ['APOE_epsilon2', 'APOE_epsilon3', 'APOE_epsilon4']\n",
    "continuous_features = [col for col in df_X_train.columns if col not in ordinal_features]\n",
    "\n",
    "# Prepare Tabular configurations (shared for all PyTorch models)\n",
    "data_config = DataConfig(\n",
    "    target=df_y_train.columns.tolist(),\n",
    "    continuous_cols=continuous_features,\n",
    "    categorical_cols=[]\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024, max_epochs=1, auto_lr_find=True,\n",
    "    early_stopping=\"valid_loss\", early_stopping_mode=\"min\", early_stopping_patience=5,\n",
    "    checkpoints=\"valid_loss\", load_best=True, progress_bar=\"nones\",\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "head_config = LinearHeadConfig(dropout=0.1).__dict__\n",
    "predictive_models_list += [\n",
    "    (\"GatedAdditiveTreeEnsembleConfig_tab\", \n",
    "    TabularModelWrapper(\n",
    "        GatedAdditiveTreeEnsembleConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        gflu_stages=6,\n",
    "        gflu_dropout=0.0,\n",
    "        tree_depth=5,\n",
    "        num_trees=20,\n",
    "        chain_trees=False,\n",
    "        share_head_weights=True), data_config, trainer_config, optimizer_config \n",
    "    )),\n",
    "    (\"DANetConfig_tab\",\n",
    "    TabularModelWrapper(\n",
    "        DANetConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_layers=8,\n",
    "        k=5,\n",
    "        dropout_rate=0.1), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    (\"TabTransformerConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabTransformerConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        embedding_initialization=\"kaiming_uniform\",\n",
    "        embedding_bias=False), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "    # (\"FTTransformerConfig\",\n",
    "    #     TabularModelWrapper(\n",
    "    #     FTTransformerConfig(\n",
    "    #     task=\"regression\",\n",
    "    #     head=\"LinearHead\",\n",
    "    #     head_config=head_config), data_config, trainer_config, optimizer_config\n",
    "    # )),\n",
    "    (\"TabNetModelConfig_tab\",\n",
    "        TabularModelWrapper(\n",
    "        TabNetModelConfig(\n",
    "        task=\"regression\",\n",
    "        head=\"LinearHead\",\n",
    "        head_config=head_config,\n",
    "        n_d=8,\n",
    "        n_a=8,\n",
    "        n_steps=3,\n",
    "        gamma=1.3,\n",
    "        n_independent=2,\n",
    "        n_shared=2), data_config, trainer_config, optimizer_config\n",
    "    )),\n",
    "]\n",
    "\n",
    "# Generate all combinations\n",
    "\n",
    "# Generate all combinations\n",
    "combinations = list(product(continuous_imputer_list, ordinal_imputer_list, predictive_models_list))\n",
    "\n",
    "# Display all combinations\n",
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    print(f\"Continuous Imputer: {continuous_imputer[0]}, Ordinal Imputer: {ordinal_imputer[0]}, Model: {model[0]}\")\n",
    "\n",
    "print(f\"Combinations of preprocessing and models to test : {len(combinations)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HDF5 file\n",
    "results_file = '../pickle/training_3_loonona_dict_results.pickle'\n",
    "\n",
    "with open('../pickle/training_3_loonona_dict_results.pickle', \"rb\") as input_file:\n",
    "    all_dict_results = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "The 'alpha' parameter of MultiTaskElasticNet must be a float in the range [0.0, inf). Got {'alpha': 0.01, 'l1_ratio': 0.01} instead.\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "The 'alpha' parameter of MultiTaskLasso must be a float in the range [0.0, inf). Got {'alpha': 0.001} instead.\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 3.13311 |  0:00:00s\n",
      "epoch 1  | loss: 1.67434 |  0:00:00s\n",
      "epoch 2  | loss: 1.25121 |  0:00:00s\n",
      "epoch 3  | loss: 1.15286 |  0:00:00s\n",
      "epoch 4  | loss: 1.05784 |  0:00:00s\n",
      "epoch 5  | loss: 0.96523 |  0:00:00s\n",
      "epoch 6  | loss: 0.92765 |  0:00:00s\n",
      "epoch 7  | loss: 0.89131 |  0:00:00s\n",
      "epoch 8  | loss: 0.86607 |  0:00:00s\n",
      "epoch 9  | loss: 0.82915 |  0:00:00s\n",
      "epoch 10 | loss: 0.83506 |  0:00:00s\n",
      "epoch 11 | loss: 0.7886  |  0:00:00s\n",
      "epoch 12 | loss: 0.75816 |  0:00:00s\n",
      "epoch 13 | loss: 0.76323 |  0:00:00s\n",
      "epoch 14 | loss: 0.76212 |  0:00:00s\n",
      "epoch 15 | loss: 0.75083 |  0:00:00s\n",
      "epoch 16 | loss: 0.72542 |  0:00:00s\n",
      "epoch 17 | loss: 0.71856 |  0:00:00s\n",
      "epoch 18 | loss: 0.71925 |  0:00:01s\n",
      "epoch 19 | loss: 0.70533 |  0:00:01s\n",
      "epoch 20 | loss: 0.70061 |  0:00:01s\n",
      "epoch 21 | loss: 0.69113 |  0:00:01s\n",
      "epoch 22 | loss: 0.67685 |  0:00:01s\n",
      "epoch 23 | loss: 0.67698 |  0:00:01s\n",
      "epoch 24 | loss: 0.67197 |  0:00:01s\n",
      "epoch 25 | loss: 0.66981 |  0:00:01s\n",
      "epoch 26 | loss: 0.66185 |  0:00:01s\n",
      "epoch 27 | loss: 0.65994 |  0:00:01s\n",
      "epoch 28 | loss: 0.65575 |  0:00:01s\n",
      "epoch 29 | loss: 0.65478 |  0:00:01s\n",
      "epoch 30 | loss: 0.64765 |  0:00:01s\n",
      "epoch 31 | loss: 0.64554 |  0:00:01s\n",
      "epoch 32 | loss: 0.63432 |  0:00:01s\n",
      "epoch 33 | loss: 0.62939 |  0:00:01s\n",
      "epoch 34 | loss: 0.62534 |  0:00:01s\n",
      "epoch 35 | loss: 0.62469 |  0:00:01s\n",
      "epoch 36 | loss: 0.62174 |  0:00:01s\n",
      "epoch 37 | loss: 0.61922 |  0:00:02s\n",
      "epoch 38 | loss: 0.62018 |  0:00:02s\n",
      "epoch 39 | loss: 0.62086 |  0:00:02s\n",
      "epoch 40 | loss: 0.60617 |  0:00:02s\n",
      "epoch 41 | loss: 0.60947 |  0:00:02s\n",
      "epoch 42 | loss: 0.59172 |  0:00:02s\n",
      "epoch 43 | loss: 0.58443 |  0:00:02s\n",
      "epoch 44 | loss: 0.58781 |  0:00:02s\n",
      "epoch 45 | loss: 0.57683 |  0:00:02s\n",
      "epoch 46 | loss: 0.57306 |  0:00:02s\n",
      "epoch 47 | loss: 0.57694 |  0:00:02s\n",
      "epoch 48 | loss: 0.56855 |  0:00:02s\n",
      "epoch 49 | loss: 0.56316 |  0:00:02s\n",
      "epoch 50 | loss: 0.56719 |  0:00:02s\n",
      "epoch 51 | loss: 0.56633 |  0:00:02s\n",
      "epoch 52 | loss: 0.55997 |  0:00:02s\n",
      "epoch 53 | loss: 0.56786 |  0:00:02s\n",
      "epoch 54 | loss: 0.55522 |  0:00:02s\n",
      "epoch 55 | loss: 0.55718 |  0:00:02s\n",
      "epoch 56 | loss: 0.55059 |  0:00:03s\n",
      "epoch 57 | loss: 0.55296 |  0:00:03s\n",
      "epoch 58 | loss: 0.55163 |  0:00:03s\n",
      "epoch 59 | loss: 0.54834 |  0:00:03s\n",
      "epoch 60 | loss: 0.55297 |  0:00:03s\n",
      "epoch 61 | loss: 0.5449  |  0:00:03s\n",
      "epoch 62 | loss: 0.5401  |  0:00:03s\n",
      "epoch 63 | loss: 0.53168 |  0:00:03s\n",
      "epoch 64 | loss: 0.52247 |  0:00:03s\n",
      "epoch 65 | loss: 0.52582 |  0:00:03s\n",
      "epoch 66 | loss: 0.51292 |  0:00:03s\n",
      "epoch 67 | loss: 0.51423 |  0:00:03s\n",
      "epoch 68 | loss: 0.50964 |  0:00:03s\n",
      "epoch 69 | loss: 0.50876 |  0:00:03s\n",
      "epoch 70 | loss: 0.50005 |  0:00:03s\n",
      "epoch 71 | loss: 0.50396 |  0:00:03s\n",
      "epoch 72 | loss: 0.49637 |  0:00:03s\n",
      "epoch 73 | loss: 0.49309 |  0:00:04s\n",
      "epoch 74 | loss: 0.48867 |  0:00:04s\n",
      "epoch 75 | loss: 0.48842 |  0:00:04s\n",
      "epoch 76 | loss: 0.48183 |  0:00:04s\n",
      "epoch 77 | loss: 0.48301 |  0:00:04s\n",
      "epoch 78 | loss: 0.48205 |  0:00:04s\n",
      "epoch 79 | loss: 0.48052 |  0:00:04s\n",
      "epoch 80 | loss: 0.47403 |  0:00:04s\n",
      "epoch 81 | loss: 0.48338 |  0:00:04s\n",
      "epoch 82 | loss: 0.47462 |  0:00:04s\n",
      "epoch 83 | loss: 0.47119 |  0:00:04s\n",
      "epoch 84 | loss: 0.47134 |  0:00:04s\n",
      "epoch 85 | loss: 0.4746  |  0:00:04s\n",
      "epoch 86 | loss: 0.47936 |  0:00:04s\n",
      "epoch 87 | loss: 0.48048 |  0:00:04s\n",
      "epoch 88 | loss: 0.46815 |  0:00:04s\n",
      "epoch 89 | loss: 0.47599 |  0:00:04s\n",
      "epoch 90 | loss: 0.46779 |  0:00:05s\n",
      "epoch 91 | loss: 0.46429 |  0:00:05s\n",
      "epoch 92 | loss: 0.46651 |  0:00:05s\n",
      "epoch 93 | loss: 0.46411 |  0:00:05s\n",
      "epoch 94 | loss: 0.45887 |  0:00:05s\n",
      "epoch 95 | loss: 0.46711 |  0:00:05s\n",
      "epoch 96 | loss: 0.45915 |  0:00:05s\n",
      "epoch 97 | loss: 0.46154 |  0:00:05s\n",
      "epoch 98 | loss: 0.45802 |  0:00:05s\n",
      "epoch 99 | loss: 0.45159 |  0:00:05s\n",
      "epoch 100| loss: 0.45073 |  0:00:05s\n",
      "epoch 101| loss: 0.45322 |  0:00:05s\n",
      "epoch 102| loss: 0.44542 |  0:00:05s\n",
      "epoch 103| loss: 0.44681 |  0:00:05s\n",
      "epoch 104| loss: 0.44443 |  0:00:05s\n",
      "epoch 105| loss: 0.44417 |  0:00:05s\n",
      "epoch 106| loss: 0.44086 |  0:00:05s\n",
      "epoch 107| loss: 0.44396 |  0:00:05s\n",
      "epoch 108| loss: 0.43298 |  0:00:06s\n",
      "epoch 109| loss: 0.44147 |  0:00:06s\n",
      "epoch 110| loss: 0.42825 |  0:00:06s\n",
      "epoch 111| loss: 0.43181 |  0:00:06s\n",
      "epoch 112| loss: 0.43651 |  0:00:06s\n",
      "epoch 113| loss: 0.42791 |  0:00:06s\n",
      "epoch 114| loss: 0.43285 |  0:00:06s\n",
      "epoch 115| loss: 0.43182 |  0:00:06s\n",
      "epoch 116| loss: 0.4306  |  0:00:06s\n",
      "epoch 117| loss: 0.42956 |  0:00:06s\n",
      "epoch 118| loss: 0.42473 |  0:00:06s\n",
      "epoch 119| loss: 0.42655 |  0:00:06s\n",
      "epoch 120| loss: 0.42597 |  0:00:06s\n",
      "epoch 121| loss: 0.41854 |  0:00:06s\n",
      "epoch 122| loss: 0.41561 |  0:00:06s\n",
      "epoch 123| loss: 0.41897 |  0:00:06s\n",
      "epoch 124| loss: 0.41393 |  0:00:06s\n",
      "epoch 125| loss: 0.41506 |  0:00:06s\n",
      "epoch 126| loss: 0.4127  |  0:00:06s\n",
      "epoch 127| loss: 0.41012 |  0:00:06s\n",
      "epoch 128| loss: 0.42201 |  0:00:07s\n",
      "epoch 129| loss: 0.41418 |  0:00:07s\n",
      "epoch 130| loss: 0.4124  |  0:00:07s\n",
      "epoch 131| loss: 0.40676 |  0:00:07s\n",
      "epoch 132| loss: 0.40992 |  0:00:07s\n",
      "epoch 133| loss: 0.41542 |  0:00:07s\n",
      "epoch 134| loss: 0.411   |  0:00:07s\n",
      "epoch 135| loss: 0.40383 |  0:00:07s\n",
      "epoch 136| loss: 0.40702 |  0:00:07s\n",
      "epoch 137| loss: 0.41123 |  0:00:07s\n",
      "epoch 138| loss: 0.40235 |  0:00:07s\n",
      "epoch 139| loss: 0.41131 |  0:00:07s\n",
      "epoch 140| loss: 0.40156 |  0:00:07s\n",
      "epoch 141| loss: 0.40275 |  0:00:07s\n",
      "epoch 142| loss: 0.40435 |  0:00:07s\n",
      "epoch 143| loss: 0.39838 |  0:00:07s\n",
      "epoch 144| loss: 0.39697 |  0:00:07s\n",
      "epoch 145| loss: 0.40627 |  0:00:07s\n",
      "epoch 146| loss: 0.39956 |  0:00:07s\n",
      "epoch 147| loss: 0.39254 |  0:00:08s\n",
      "epoch 148| loss: 0.39673 |  0:00:08s\n",
      "epoch 149| loss: 0.39021 |  0:00:08s\n",
      "epoch 150| loss: 0.38701 |  0:00:08s\n",
      "epoch 151| loss: 0.39155 |  0:00:08s\n",
      "epoch 152| loss: 0.38715 |  0:00:08s\n",
      "epoch 153| loss: 0.38539 |  0:00:08s\n",
      "epoch 154| loss: 0.3805  |  0:00:08s\n",
      "epoch 155| loss: 0.38658 |  0:00:08s\n",
      "epoch 156| loss: 0.38244 |  0:00:08s\n",
      "epoch 157| loss: 0.38939 |  0:00:08s\n",
      "epoch 158| loss: 0.38604 |  0:00:08s\n",
      "epoch 159| loss: 0.38263 |  0:00:08s\n",
      "epoch 160| loss: 0.383   |  0:00:08s\n",
      "epoch 161| loss: 0.38275 |  0:00:08s\n",
      "epoch 162| loss: 0.3747  |  0:00:08s\n",
      "epoch 163| loss: 0.38457 |  0:00:08s\n",
      "epoch 164| loss: 0.38214 |  0:00:08s\n",
      "epoch 165| loss: 0.38842 |  0:00:08s\n",
      "epoch 166| loss: 0.37563 |  0:00:09s\n",
      "epoch 167| loss: 0.38946 |  0:00:09s\n",
      "epoch 168| loss: 0.39004 |  0:00:09s\n",
      "epoch 169| loss: 0.38555 |  0:00:09s\n",
      "epoch 170| loss: 0.38699 |  0:00:09s\n",
      "epoch 171| loss: 0.38251 |  0:00:09s\n",
      "epoch 172| loss: 0.37322 |  0:00:09s\n",
      "epoch 173| loss: 0.37461 |  0:00:09s\n",
      "epoch 174| loss: 0.37934 |  0:00:09s\n",
      "epoch 175| loss: 0.37561 |  0:00:09s\n",
      "epoch 176| loss: 0.37233 |  0:00:09s\n",
      "epoch 177| loss: 0.36703 |  0:00:09s\n",
      "epoch 178| loss: 0.36985 |  0:00:09s\n",
      "epoch 179| loss: 0.37102 |  0:00:09s\n",
      "epoch 180| loss: 0.37611 |  0:00:09s\n",
      "epoch 181| loss: 0.36982 |  0:00:09s\n",
      "epoch 182| loss: 0.37385 |  0:00:09s\n",
      "epoch 183| loss: 0.36204 |  0:00:09s\n",
      "epoch 184| loss: 0.36449 |  0:00:09s\n",
      "epoch 185| loss: 0.36274 |  0:00:09s\n",
      "epoch 186| loss: 0.3675  |  0:00:09s\n",
      "epoch 187| loss: 0.37158 |  0:00:09s\n",
      "epoch 188| loss: 0.36024 |  0:00:09s\n",
      "epoch 189| loss: 0.36222 |  0:00:10s\n",
      "epoch 190| loss: 0.36547 |  0:00:10s\n",
      "epoch 191| loss: 0.35913 |  0:00:10s\n",
      "epoch 192| loss: 0.37019 |  0:00:10s\n",
      "epoch 193| loss: 0.3661  |  0:00:10s\n",
      "epoch 194| loss: 0.36435 |  0:00:10s\n",
      "epoch 195| loss: 0.3665  |  0:00:10s\n",
      "epoch 196| loss: 0.36435 |  0:00:10s\n",
      "epoch 197| loss: 0.36708 |  0:00:10s\n",
      "epoch 198| loss: 0.37635 |  0:00:10s\n",
      "epoch 199| loss: 0.36185 |  0:00:10s\n",
      "epoch 200| loss: 0.372   |  0:00:10s\n",
      "epoch 201| loss: 0.36781 |  0:00:10s\n",
      "epoch 202| loss: 0.37538 |  0:00:10s\n",
      "epoch 203| loss: 0.39584 |  0:00:10s\n",
      "epoch 204| loss: 0.40129 |  0:00:10s\n",
      "epoch 205| loss: 0.39697 |  0:00:10s\n",
      "epoch 206| loss: 0.39629 |  0:00:10s\n",
      "epoch 207| loss: 0.38705 |  0:00:10s\n",
      "epoch 208| loss: 0.38361 |  0:00:10s\n",
      "epoch 209| loss: 0.37977 |  0:00:10s\n",
      "epoch 210| loss: 0.3787  |  0:00:10s\n",
      "epoch 211| loss: 0.37001 |  0:00:11s\n",
      "epoch 212| loss: 0.3723  |  0:00:11s\n",
      "epoch 213| loss: 0.37208 |  0:00:11s\n",
      "epoch 214| loss: 0.36534 |  0:00:11s\n",
      "epoch 215| loss: 0.37599 |  0:00:11s\n",
      "epoch 216| loss: 0.35701 |  0:00:11s\n",
      "epoch 217| loss: 0.36111 |  0:00:11s\n",
      "epoch 218| loss: 0.35919 |  0:00:11s\n",
      "epoch 219| loss: 0.36595 |  0:00:11s\n",
      "epoch 220| loss: 0.35637 |  0:00:11s\n",
      "epoch 221| loss: 0.36044 |  0:00:11s\n",
      "epoch 222| loss: 0.34806 |  0:00:11s\n",
      "epoch 223| loss: 0.35702 |  0:00:11s\n",
      "epoch 224| loss: 0.35781 |  0:00:11s\n",
      "epoch 225| loss: 0.35377 |  0:00:11s\n",
      "epoch 226| loss: 0.35655 |  0:00:11s\n",
      "epoch 227| loss: 0.35704 |  0:00:11s\n",
      "epoch 228| loss: 0.34493 |  0:00:11s\n",
      "epoch 229| loss: 0.35773 |  0:00:11s\n",
      "epoch 230| loss: 0.34637 |  0:00:11s\n",
      "epoch 231| loss: 0.34904 |  0:00:11s\n",
      "epoch 232| loss: 0.34231 |  0:00:11s\n",
      "epoch 233| loss: 0.34362 |  0:00:12s\n",
      "epoch 234| loss: 0.34797 |  0:00:12s\n",
      "epoch 235| loss: 0.34678 |  0:00:12s\n",
      "epoch 236| loss: 0.34557 |  0:00:12s\n",
      "epoch 237| loss: 0.34948 |  0:00:12s\n",
      "epoch 238| loss: 0.34857 |  0:00:12s\n",
      "epoch 239| loss: 0.35335 |  0:00:12s\n",
      "epoch 240| loss: 0.34921 |  0:00:12s\n",
      "epoch 241| loss: 0.34684 |  0:00:12s\n",
      "epoch 242| loss: 0.34874 |  0:00:12s\n",
      "epoch 243| loss: 0.3433  |  0:00:12s\n",
      "epoch 244| loss: 0.34147 |  0:00:12s\n",
      "epoch 245| loss: 0.3383  |  0:00:12s\n",
      "epoch 246| loss: 0.34105 |  0:00:12s\n",
      "epoch 247| loss: 0.34444 |  0:00:12s\n",
      "epoch 248| loss: 0.34859 |  0:00:12s\n",
      "epoch 249| loss: 0.34221 |  0:00:12s\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "epoch 0  | loss: 4.24287 |  0:00:00s\n",
      "epoch 1  | loss: 2.15244 |  0:00:00s\n",
      "epoch 2  | loss: 1.58292 |  0:00:00s\n",
      "epoch 3  | loss: 1.26691 |  0:00:00s\n",
      "epoch 4  | loss: 1.06626 |  0:00:00s\n",
      "epoch 5  | loss: 0.9529  |  0:00:00s\n",
      "epoch 6  | loss: 0.88656 |  0:00:00s\n",
      "epoch 7  | loss: 0.84019 |  0:00:00s\n",
      "epoch 8  | loss: 0.79465 |  0:00:00s\n",
      "epoch 9  | loss: 0.76328 |  0:00:00s\n",
      "epoch 10 | loss: 0.72436 |  0:00:00s\n",
      "epoch 11 | loss: 0.69744 |  0:00:00s\n",
      "epoch 12 | loss: 0.69848 |  0:00:00s\n",
      "epoch 13 | loss: 0.68787 |  0:00:00s\n",
      "epoch 14 | loss: 0.67379 |  0:00:00s\n",
      "epoch 15 | loss: 0.67166 |  0:00:00s\n",
      "epoch 16 | loss: 0.65451 |  0:00:00s\n",
      "epoch 17 | loss: 0.66407 |  0:00:00s\n",
      "epoch 18 | loss: 0.65599 |  0:00:00s\n",
      "epoch 19 | loss: 0.65139 |  0:00:00s\n",
      "epoch 20 | loss: 0.64927 |  0:00:00s\n",
      "epoch 21 | loss: 0.64578 |  0:00:01s\n",
      "epoch 22 | loss: 0.63364 |  0:00:01s\n",
      "epoch 23 | loss: 0.62991 |  0:00:01s\n",
      "epoch 24 | loss: 0.62301 |  0:00:01s\n",
      "epoch 25 | loss: 0.62647 |  0:00:01s\n",
      "epoch 26 | loss: 0.61679 |  0:00:01s\n",
      "epoch 27 | loss: 0.60603 |  0:00:01s\n",
      "epoch 28 | loss: 0.59713 |  0:00:01s\n",
      "epoch 29 | loss: 0.59047 |  0:00:01s\n",
      "epoch 30 | loss: 0.58662 |  0:00:01s\n",
      "epoch 31 | loss: 0.59503 |  0:00:01s\n",
      "epoch 32 | loss: 0.58066 |  0:00:01s\n",
      "epoch 33 | loss: 0.5762  |  0:00:01s\n",
      "epoch 34 | loss: 0.57702 |  0:00:01s\n",
      "epoch 35 | loss: 0.5748  |  0:00:01s\n",
      "epoch 36 | loss: 0.58111 |  0:00:01s\n",
      "epoch 37 | loss: 0.56751 |  0:00:01s\n",
      "epoch 38 | loss: 0.56548 |  0:00:01s\n",
      "epoch 39 | loss: 0.56671 |  0:00:01s\n",
      "epoch 40 | loss: 0.54826 |  0:00:01s\n",
      "epoch 41 | loss: 0.55204 |  0:00:01s\n",
      "epoch 42 | loss: 0.54543 |  0:00:01s\n",
      "epoch 43 | loss: 0.54748 |  0:00:02s\n",
      "epoch 44 | loss: 0.53076 |  0:00:02s\n",
      "epoch 45 | loss: 0.534   |  0:00:02s\n",
      "epoch 46 | loss: 0.52653 |  0:00:02s\n",
      "epoch 47 | loss: 0.51791 |  0:00:02s\n",
      "epoch 48 | loss: 0.52555 |  0:00:02s\n",
      "epoch 49 | loss: 0.51958 |  0:00:02s\n",
      "epoch 50 | loss: 0.51854 |  0:00:02s\n",
      "epoch 51 | loss: 0.52509 |  0:00:02s\n",
      "epoch 52 | loss: 0.53795 |  0:00:02s\n",
      "epoch 53 | loss: 0.51252 |  0:00:02s\n",
      "epoch 54 | loss: 0.52148 |  0:00:02s\n",
      "epoch 55 | loss: 0.50706 |  0:00:02s\n",
      "epoch 56 | loss: 0.50415 |  0:00:02s\n",
      "epoch 57 | loss: 0.50179 |  0:00:02s\n",
      "epoch 58 | loss: 0.5014  |  0:00:02s\n",
      "epoch 59 | loss: 0.496   |  0:00:02s\n",
      "epoch 60 | loss: 0.49581 |  0:00:02s\n",
      "epoch 61 | loss: 0.49195 |  0:00:02s\n",
      "epoch 62 | loss: 0.49182 |  0:00:02s\n",
      "epoch 63 | loss: 0.48621 |  0:00:02s\n",
      "epoch 64 | loss: 0.48535 |  0:00:02s\n",
      "epoch 65 | loss: 0.48092 |  0:00:03s\n",
      "epoch 66 | loss: 0.48223 |  0:00:03s\n",
      "epoch 67 | loss: 0.47486 |  0:00:03s\n",
      "epoch 68 | loss: 0.48263 |  0:00:03s\n",
      "epoch 69 | loss: 0.46807 |  0:00:03s\n",
      "epoch 70 | loss: 0.47951 |  0:00:03s\n",
      "epoch 71 | loss: 0.47439 |  0:00:03s\n",
      "epoch 72 | loss: 0.46664 |  0:00:03s\n",
      "epoch 73 | loss: 0.46357 |  0:00:03s\n",
      "epoch 74 | loss: 0.45955 |  0:00:03s\n",
      "epoch 75 | loss: 0.44495 |  0:00:03s\n",
      "epoch 76 | loss: 0.45515 |  0:00:03s\n",
      "epoch 77 | loss: 0.44442 |  0:00:03s\n",
      "epoch 78 | loss: 0.44651 |  0:00:03s\n",
      "epoch 79 | loss: 0.44756 |  0:00:03s\n",
      "epoch 80 | loss: 0.43945 |  0:00:03s\n",
      "epoch 81 | loss: 0.4455  |  0:00:03s\n",
      "epoch 82 | loss: 0.42914 |  0:00:03s\n",
      "epoch 83 | loss: 0.42907 |  0:00:04s\n",
      "epoch 84 | loss: 0.42916 |  0:00:04s\n",
      "epoch 85 | loss: 0.43479 |  0:00:04s\n",
      "epoch 86 | loss: 0.4282  |  0:00:04s\n",
      "epoch 87 | loss: 0.43356 |  0:00:04s\n",
      "epoch 88 | loss: 0.4317  |  0:00:04s\n",
      "epoch 89 | loss: 0.42938 |  0:00:04s\n",
      "epoch 90 | loss: 0.42696 |  0:00:04s\n",
      "epoch 91 | loss: 0.43679 |  0:00:04s\n",
      "epoch 92 | loss: 0.43223 |  0:00:04s\n",
      "epoch 93 | loss: 0.43293 |  0:00:04s\n",
      "epoch 94 | loss: 0.43471 |  0:00:04s\n",
      "epoch 95 | loss: 0.43117 |  0:00:04s\n",
      "epoch 96 | loss: 0.43198 |  0:00:04s\n",
      "epoch 97 | loss: 0.42767 |  0:00:04s\n",
      "epoch 98 | loss: 0.44682 |  0:00:04s\n",
      "epoch 99 | loss: 0.42731 |  0:00:04s\n",
      "epoch 100| loss: 0.42579 |  0:00:04s\n",
      "epoch 101| loss: 0.42668 |  0:00:04s\n",
      "epoch 102| loss: 0.41374 |  0:00:04s\n",
      "epoch 103| loss: 0.40874 |  0:00:05s\n",
      "epoch 104| loss: 0.41486 |  0:00:05s\n",
      "epoch 105| loss: 0.4021  |  0:00:05s\n",
      "epoch 106| loss: 0.40922 |  0:00:05s\n",
      "epoch 107| loss: 0.40808 |  0:00:05s\n",
      "epoch 108| loss: 0.4061  |  0:00:05s\n",
      "epoch 109| loss: 0.40864 |  0:00:05s\n",
      "epoch 110| loss: 0.40504 |  0:00:05s\n",
      "epoch 111| loss: 0.39976 |  0:00:05s\n",
      "epoch 112| loss: 0.40696 |  0:00:05s\n",
      "epoch 113| loss: 0.38797 |  0:00:05s\n",
      "epoch 114| loss: 0.40204 |  0:00:05s\n",
      "epoch 115| loss: 0.3983  |  0:00:05s\n",
      "epoch 116| loss: 0.4002  |  0:00:05s\n",
      "epoch 117| loss: 0.40514 |  0:00:05s\n",
      "epoch 118| loss: 0.40652 |  0:00:05s\n",
      "epoch 119| loss: 0.40779 |  0:00:05s\n",
      "epoch 120| loss: 0.41209 |  0:00:05s\n",
      "epoch 121| loss: 0.40213 |  0:00:05s\n",
      "epoch 122| loss: 0.38867 |  0:00:05s\n",
      "epoch 123| loss: 0.38918 |  0:00:05s\n",
      "epoch 124| loss: 0.39164 |  0:00:06s\n",
      "epoch 125| loss: 0.37698 |  0:00:06s\n",
      "epoch 126| loss: 0.37028 |  0:00:06s\n",
      "epoch 127| loss: 0.37927 |  0:00:06s\n",
      "epoch 128| loss: 0.36095 |  0:00:06s\n",
      "epoch 129| loss: 0.36817 |  0:00:06s\n",
      "epoch 130| loss: 0.35612 |  0:00:06s\n",
      "epoch 131| loss: 0.35326 |  0:00:06s\n",
      "epoch 132| loss: 0.35904 |  0:00:06s\n",
      "epoch 133| loss: 0.35966 |  0:00:06s\n",
      "epoch 134| loss: 0.34819 |  0:00:06s\n",
      "epoch 135| loss: 0.35239 |  0:00:06s\n",
      "epoch 136| loss: 0.35978 |  0:00:06s\n",
      "epoch 137| loss: 0.35151 |  0:00:06s\n",
      "epoch 138| loss: 0.36372 |  0:00:06s\n",
      "epoch 139| loss: 0.36289 |  0:00:06s\n",
      "epoch 140| loss: 0.36773 |  0:00:06s\n",
      "epoch 141| loss: 0.36356 |  0:00:06s\n",
      "epoch 142| loss: 0.35889 |  0:00:06s\n",
      "epoch 143| loss: 0.36573 |  0:00:06s\n",
      "epoch 144| loss: 0.36685 |  0:00:06s\n",
      "epoch 145| loss: 0.36418 |  0:00:06s\n",
      "epoch 146| loss: 0.37643 |  0:00:07s\n",
      "epoch 147| loss: 0.36169 |  0:00:07s\n",
      "epoch 148| loss: 0.3599  |  0:00:07s\n",
      "epoch 149| loss: 0.36358 |  0:00:07s\n",
      "epoch 150| loss: 0.38092 |  0:00:07s\n",
      "epoch 151| loss: 0.37406 |  0:00:07s\n",
      "epoch 152| loss: 0.36953 |  0:00:07s\n",
      "epoch 153| loss: 0.37239 |  0:00:07s\n",
      "epoch 154| loss: 0.36479 |  0:00:07s\n",
      "epoch 155| loss: 0.36082 |  0:00:07s\n",
      "epoch 156| loss: 0.35538 |  0:00:07s\n",
      "epoch 157| loss: 0.34878 |  0:00:07s\n",
      "epoch 158| loss: 0.34607 |  0:00:07s\n",
      "epoch 159| loss: 0.34139 |  0:00:07s\n",
      "epoch 160| loss: 0.33437 |  0:00:07s\n",
      "epoch 161| loss: 0.33563 |  0:00:07s\n",
      "epoch 162| loss: 0.34112 |  0:00:07s\n",
      "epoch 163| loss: 0.35652 |  0:00:07s\n",
      "epoch 164| loss: 0.35128 |  0:00:07s\n",
      "epoch 165| loss: 0.34045 |  0:00:07s\n",
      "epoch 166| loss: 0.34459 |  0:00:07s\n",
      "epoch 167| loss: 0.34769 |  0:00:07s\n",
      "epoch 168| loss: 0.33172 |  0:00:08s\n",
      "epoch 169| loss: 0.33832 |  0:00:08s\n",
      "epoch 170| loss: 0.32454 |  0:00:08s\n",
      "epoch 171| loss: 0.33092 |  0:00:08s\n",
      "epoch 172| loss: 0.33065 |  0:00:08s\n",
      "epoch 173| loss: 0.31971 |  0:00:08s\n",
      "epoch 174| loss: 0.31975 |  0:00:08s\n",
      "epoch 175| loss: 0.30903 |  0:00:08s\n",
      "epoch 176| loss: 0.31415 |  0:00:08s\n",
      "epoch 177| loss: 0.32472 |  0:00:08s\n",
      "epoch 178| loss: 0.30966 |  0:00:08s\n",
      "epoch 179| loss: 0.31315 |  0:00:08s\n",
      "epoch 180| loss: 0.31162 |  0:00:08s\n",
      "epoch 181| loss: 0.32061 |  0:00:08s\n",
      "epoch 182| loss: 0.31373 |  0:00:08s\n",
      "epoch 183| loss: 0.31775 |  0:00:08s\n",
      "epoch 184| loss: 0.31597 |  0:00:08s\n",
      "epoch 185| loss: 0.3111  |  0:00:08s\n",
      "epoch 186| loss: 0.31505 |  0:00:08s\n",
      "epoch 187| loss: 0.30957 |  0:00:08s\n",
      "epoch 188| loss: 0.31093 |  0:00:08s\n",
      "epoch 189| loss: 0.31486 |  0:00:08s\n",
      "epoch 190| loss: 0.31301 |  0:00:09s\n",
      "epoch 191| loss: 0.3108  |  0:00:09s\n",
      "epoch 192| loss: 0.30916 |  0:00:09s\n",
      "epoch 193| loss: 0.30084 |  0:00:09s\n",
      "epoch 194| loss: 0.30258 |  0:00:09s\n",
      "epoch 195| loss: 0.30658 |  0:00:09s\n",
      "epoch 196| loss: 0.30231 |  0:00:09s\n",
      "epoch 197| loss: 0.29887 |  0:00:09s\n",
      "epoch 198| loss: 0.3017  |  0:00:09s\n",
      "epoch 199| loss: 0.2996  |  0:00:09s\n",
      "epoch 200| loss: 0.29747 |  0:00:09s\n",
      "epoch 201| loss: 0.302   |  0:00:09s\n",
      "epoch 202| loss: 0.29356 |  0:00:09s\n",
      "epoch 203| loss: 0.29971 |  0:00:09s\n",
      "epoch 204| loss: 0.28496 |  0:00:09s\n",
      "epoch 205| loss: 0.28985 |  0:00:09s\n",
      "epoch 206| loss: 0.28443 |  0:00:09s\n",
      "epoch 207| loss: 0.28011 |  0:00:09s\n",
      "epoch 208| loss: 0.28006 |  0:00:09s\n",
      "epoch 209| loss: 0.27741 |  0:00:09s\n",
      "epoch 210| loss: 0.27276 |  0:00:09s\n",
      "epoch 211| loss: 0.27628 |  0:00:09s\n",
      "epoch 212| loss: 0.2841  |  0:00:10s\n",
      "epoch 213| loss: 0.28351 |  0:00:10s\n",
      "epoch 214| loss: 0.28181 |  0:00:10s\n",
      "epoch 215| loss: 0.29372 |  0:00:10s\n",
      "epoch 216| loss: 0.27954 |  0:00:10s\n",
      "epoch 217| loss: 0.28504 |  0:00:10s\n",
      "epoch 218| loss: 0.281   |  0:00:10s\n",
      "epoch 219| loss: 0.28408 |  0:00:10s\n",
      "epoch 220| loss: 0.28229 |  0:00:10s\n",
      "epoch 221| loss: 0.27346 |  0:00:10s\n",
      "epoch 222| loss: 0.26955 |  0:00:10s\n",
      "epoch 223| loss: 0.27341 |  0:00:10s\n",
      "epoch 224| loss: 0.27228 |  0:00:10s\n",
      "epoch 225| loss: 0.26034 |  0:00:10s\n",
      "epoch 226| loss: 0.26504 |  0:00:10s\n",
      "epoch 227| loss: 0.26373 |  0:00:10s\n",
      "epoch 228| loss: 0.25678 |  0:00:10s\n",
      "epoch 229| loss: 0.2667  |  0:00:10s\n",
      "epoch 230| loss: 0.25775 |  0:00:10s\n",
      "epoch 231| loss: 0.26326 |  0:00:10s\n",
      "epoch 232| loss: 0.25435 |  0:00:11s\n",
      "epoch 233| loss: 0.25562 |  0:00:11s\n",
      "epoch 234| loss: 0.26014 |  0:00:11s\n",
      "epoch 235| loss: 0.2552  |  0:00:11s\n",
      "epoch 236| loss: 0.25414 |  0:00:11s\n",
      "epoch 237| loss: 0.25566 |  0:00:11s\n",
      "epoch 238| loss: 0.25705 |  0:00:11s\n",
      "epoch 239| loss: 0.25846 |  0:00:11s\n",
      "epoch 240| loss: 0.25685 |  0:00:11s\n",
      "epoch 241| loss: 0.26753 |  0:00:11s\n",
      "epoch 242| loss: 0.27538 |  0:00:11s\n",
      "epoch 243| loss: 0.27838 |  0:00:11s\n",
      "epoch 244| loss: 0.27072 |  0:00:11s\n",
      "epoch 245| loss: 0.26688 |  0:00:11s\n",
      "epoch 246| loss: 0.26624 |  0:00:11s\n",
      "epoch 247| loss: 0.26438 |  0:00:11s\n",
      "epoch 248| loss: 0.26376 |  0:00:11s\n",
      "epoch 249| loss: 0.26156 |  0:00:11s\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">696</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:31\u001b[0m,\u001b[1;36m696\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">708</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:31\u001b[0m,\u001b[1;36m708\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">711</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:31\u001b[0m,\u001b[1;36m711\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">728</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:31\u001b[0m,\u001b[1;36m728\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model:                        \n",
       "GatedAdditiveTreeEnsembleModel                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:31</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">923</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:31\u001b[0m,\u001b[1;36m923\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:32</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">143</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:32\u001b[0m,\u001b[1;36m143\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:32</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">152</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:32\u001b[0m,\u001b[1;36m152\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ba87cdbb2c4b30b70b7229c3c3e306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_24fa52a9-8eeb-4568-8ea8-a2a9ff4b8fba.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_24fa52a9-8eeb-4568-8ea8-a2a9ff4b8fba.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:34</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">060</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:34\u001b[0m,\u001b[1;36m060\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:34</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">067</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.models.gate.gate_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">}</span> - INFO - Data Aware Initialization of T0    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:34\u001b[0m,\u001b[1;36m067\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.models.gate.gate_model:\u001b[1;36m255\u001b[0m\u001b[1m}\u001b[0m - INFO - Data Aware Initialization of T0    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:34</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">080</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:34\u001b[0m,\u001b[1;36m080\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                       | Params | Mode \n",
      "------------------------------------------------------------------------\n",
      "0 | _backbone        | GatedAdditiveTreesBackbone | 2.1 M  | train\n",
      "1 | _embedding_layer | Embedding1dLayer           | 400    | train\n",
      "2 | _head            | CustomHead                 | 156    | train\n",
      "3 | loss             | MSELoss                    | 0      | train\n",
      "------------------------------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.417     Total estimated model params size (MB)\n",
      "689       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55437ea020364e8a82758ddcf664f43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44081f304d74bad8f8e2d225c01e0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5eaff2ada54577adfbf8c9193fc065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">954</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:35\u001b[0m,\u001b[1;36m954\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:35</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">955</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:35\u001b[0m,\u001b[1;36m955\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">727</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:36\u001b[0m,\u001b[1;36m727\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">739</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:36\u001b[0m,\u001b[1;36m739\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">742</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:36\u001b[0m,\u001b[1;36m742\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">758</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: DANetModel             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:36\u001b[0m,\u001b[1;36m758\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: DANetModel             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">788</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:36\u001b[0m,\u001b[1;36m788\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">798</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:36\u001b[0m,\u001b[1;36m798\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2653e322aed349bebbec65d3d52ad685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_8900f5fb-714d-4234-bede-1d4dd3847106.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_8900f5fb-714d-4234-bede-1d4dd3847106.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">317</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:37\u001b[0m,\u001b[1;36m317\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">322</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:37\u001b[0m,\u001b[1;36m322\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | _backbone        | DANetBackbone    | 1.4 M  | train\n",
      "1 | _embedding_layer | Embedding1dLayer | 400    | train\n",
      "2 | _head            | LinearHead       | 260    | train\n",
      "3 | loss             | MSELoss          | 0      | train\n",
      "--------------------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.787     Total estimated model params size (MB)\n",
      "156       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bc023bf44e44f7b95069a06da4a6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5fcf5558314f19b8dce0370f7351ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9101d7e4e9bf4a91a341643b4f494b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">668</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:37\u001b[0m,\u001b[1;36m668\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">670</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:37\u001b[0m,\u001b[1;36m670\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">849</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:37\u001b[0m,\u001b[1;36m849\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">861</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:37\u001b[0m,\u001b[1;36m861\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">864</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:37\u001b[0m,\u001b[1;36m864\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">880</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabTransformerModel    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:37\u001b[0m,\u001b[1;36m880\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabTransformerModel    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">901</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:37\u001b[0m,\u001b[1;36m901\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:37</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">909</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:37\u001b[0m,\u001b[1;36m909\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f6ce27f15d47f5bbb7f7a75b0d5052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_e39c0e95-b6c1-4e6a-b1b1-2abf600a0848.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_e39c0e95-b6c1-4e6a-b1b1-2abf600a0848.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">028</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:38\u001b[0m,\u001b[1;36m028\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">030</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:38\u001b[0m,\u001b[1;36m030\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                   | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | _backbone        | TabTransformerBackbone | 271 K  | train\n",
      "1 | _embedding_layer | Embedding2dLayer       | 0      | train\n",
      "2 | _head            | LinearHead             | 804    | train\n",
      "3 | loss             | MSELoss                | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "272 K     Trainable params\n",
      "0         Non-trainable params\n",
      "272 K     Total params\n",
      "1.090     Total estimated model params size (MB)\n",
      "119       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc9d9ff9ab24b72972cacc521521354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3b9715816447d2a1e44f28e0496822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fccd34243804a36bea8b7759d98b1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">167</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:38\u001b[0m,\u001b[1;36m167\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">169</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:38\u001b[0m,\u001b[1;36m169\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Using separate imputers for ordinal and continuous data.\n",
      "No NaN in train data -> Keep as it is. \n",
      "No NaN in test data -> Keep as it is. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">368</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:38\u001b[0m,\u001b[1;36m368\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">379</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:38\u001b[0m,\u001b[1;36m379\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">383</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:38\u001b[0m,\u001b[1;36m383\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">399</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: TabNetModel            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:38\u001b[0m,\u001b[1;36m399\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: TabNetModel            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">420</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:38\u001b[0m,\u001b[1;36m420\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">431</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:38\u001b[0m,\u001b[1;36m431\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2269e4a52f4475ab377967219d8e3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LR finder stopped early after 3 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_7d5431e0-65e2-4f95-8023-78d9b31c5c64.ckpt\n",
      "Restored all states from the checkpoint at /home/cschneuwly/Documents/projects/optimus/notebooks/.lr_find_7d5431e0-65e2-4f95-8023-78d9b31c5c64.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">880</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:38\u001b[0m,\u001b[1;36m880\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[3;35mNone\u001b[0m. For plot and detailed   \n",
       "analysis, use `find_learning_rate` method.                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">881</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:38\u001b[0m,\u001b[1;36m881\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type           | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | _embedding_layer | Identity       | 0      | train\n",
      "1 | _backbone        | TabNetBackbone | 18.9 K | train\n",
      "2 | _head            | Identity       | 0      | train\n",
      "3 | loss             | MSELoss        | 0      | train\n",
      "------------------------------------------------------------\n",
      "18.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "18.9 K    Total params\n",
      "0.075     Total estimated model params size (MB)\n",
      "107       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b259186dc340e0b5c8fbcd285511ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fc413703a744a3a8be8cfdc479436b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5886afd89db6432db2c90b1896513a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">085</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:39\u001b[0m,\u001b[1;36m085\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">11:58:39</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">086</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m12\u001b[0m \u001b[1;92m11:58:39\u001b[0m,\u001b[1;36m086\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL omegaconf.dictconfig.DictConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig])` or the `torch.serialization.safe_globals([omegaconf.dictconfig.DictConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": [
    "for continuous_imputer, ordinal_imputer, model in combinations:\n",
    "    name_continuous_imputer, continuous_imputer_instance = continuous_imputer\n",
    "    name_ordinal_imputer, ordinal_imputer_instance = ordinal_imputer\n",
    "    name_model, model_instance = model\n",
    "\n",
    "    try: \n",
    "    \n",
    "        # Now you can call your `train_model` function with these components\n",
    "        dict_results = train_imputer_model(\n",
    "            df_X_train, df_X_test, df_y_train, df_y_test,\n",
    "            c_train, c_test,\n",
    "            ordinal_imputer_instance, name_ordinal_imputer,\n",
    "            continuous_imputer_instance, name_continuous_imputer,\n",
    "            model_instance, name_model,\n",
    "            separate_imputers=True  # Or however you want to specify\n",
    "        )\n",
    "\n",
    "    except Exception as e:  \n",
    "\n",
    "        print(e)\n",
    "    \n",
    "        params = {\n",
    "        \"ordinal_imputer\": name_ordinal_imputer, \n",
    "        \"continuous_imputer\": name_continuous_imputer, \n",
    "        \"model\": name_model, \"train_shape\" : df_X_train.shape, \n",
    "        \"test_shape\": df_X_test.shape\n",
    "    }\n",
    "        dict_results = {\n",
    "        \"params\": params, \n",
    "        \"imputation_time\": None,\n",
    "        \"fitting_time\": None, \n",
    "        \"results_adj\": None, \n",
    "        \"results_org\": None\n",
    "    }\n",
    "        \n",
    "    # Optionally keep the all_dict_results list updated\n",
    "    all_dict_results.append(dict_results)\n",
    "\n",
    "    # Save the updated results back to the pickle file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(all_dict_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open(results_file, 'wb') as handle:\n",
    "    pickle.dump(all_dict_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_file, \"rb\") as input_file:\n",
    "    all_dict_results = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Table for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickle/training_3_loonona_dict_results.pickle', \"rb\") as input_file:\n",
    "    dict_results_loo_nona = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imputation_time</th>\n",
       "      <th>fitting_time</th>\n",
       "      <th>results_adj</th>\n",
       "      <th>results_org</th>\n",
       "      <th>params.ordinal_imputer</th>\n",
       "      <th>params.continuous_imputer</th>\n",
       "      <th>params.model</th>\n",
       "      <th>params.train_shape</th>\n",
       "      <th>params.test_shape</th>\n",
       "      <th>results_adj.mse_score</th>\n",
       "      <th>results_adj.mae_score</th>\n",
       "      <th>results_adj.r2</th>\n",
       "      <th>results_adj.explained_variance</th>\n",
       "      <th>results_adj.corr</th>\n",
       "      <th>results_org.mse_score</th>\n",
       "      <th>results_org.mae_score</th>\n",
       "      <th>results_org.r2</th>\n",
       "      <th>results_org.explained_variance</th>\n",
       "      <th>results_org.corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3.3089418411254883, 3.2607581615448, 3.286860...</td>\n",
       "      <td>[0.11002111434936523, 0.24651741981506348, 0.1...</td>\n",
       "      <td>[{'y_pred': [[0.56338451 1.36697629 0.61842186...</td>\n",
       "      <td>[{'y_pred': [[0.75794363 1.56522677 0.49123517...</td>\n",
       "      <td>SimpleImputer_most_frequent</td>\n",
       "      <td>KNNImputer</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>[2893, 348]</td>\n",
       "      <td>[1, 348]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3.4814867973327637, 3.5424869060516357, 3.515...</td>\n",
       "      <td>[0.08670425415039062, 0.08908843994140625, 0.0...</td>\n",
       "      <td>[{'y_pred': [[0.10086018 0.10725904 0.05499171...</td>\n",
       "      <td>[{'y_pred': [[ 0.29541929  0.30550953 -0.07219...</td>\n",
       "      <td>SimpleImputer_most_frequent</td>\n",
       "      <td>KNNImputer</td>\n",
       "      <td>MultiTaskElasticNet</td>\n",
       "      <td>[2893, 348]</td>\n",
       "      <td>[1, 348]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3.544888734817505, 3.5088696479797363, 3.4993...</td>\n",
       "      <td>[1.513960361480713, 1.5192315578460693, 1.5161...</td>\n",
       "      <td>[{'y_pred': [[0.43508893 1.25141221 0.45281623...</td>\n",
       "      <td>[{'y_pred': [[0.62964805 1.4496627  0.32562954...</td>\n",
       "      <td>SimpleImputer_most_frequent</td>\n",
       "      <td>KNNImputer</td>\n",
       "      <td>MultiTaskElasticNet_tuned</td>\n",
       "      <td>[2893, 348]</td>\n",
       "      <td>[1, 348]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3.458850145339966, 3.4527065753936768, 3.4638...</td>\n",
       "      <td>[0.0770413875579834, 0.07488107681274414, 0.07...</td>\n",
       "      <td>[{'y_pred': [[-1.14021119e-10 -7.73912862e-10 ...</td>\n",
       "      <td>[{'y_pred': [[ 0.19455912  0.19825049 -0.12718...</td>\n",
       "      <td>SimpleImputer_most_frequent</td>\n",
       "      <td>KNNImputer</td>\n",
       "      <td>MultiTaskLasso</td>\n",
       "      <td>[2893, 348]</td>\n",
       "      <td>[1, 348]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3.5399937629699707, 3.5302608013153076, 3.551...</td>\n",
       "      <td>[1.4914090633392334, 1.5125789642333984, 1.522...</td>\n",
       "      <td>[{'y_pred': [[0.50635131 1.33052966 0.55415854...</td>\n",
       "      <td>[{'y_pred': [[0.70091043 1.52878015 0.42697185...</td>\n",
       "      <td>SimpleImputer_most_frequent</td>\n",
       "      <td>KNNImputer</td>\n",
       "      <td>MultiTaskLasso_tuned</td>\n",
       "      <td>[2893, 348]</td>\n",
       "      <td>[1, 348]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[3.5142085552215576, 3.3101305961608887, 3.446...</td>\n",
       "      <td>[52.41137504577637, 52.029266357421875, 51.895...</td>\n",
       "      <td>[{'y_pred': [[0.29340068 0.37610565 0.22158855...</td>\n",
       "      <td>[{'y_pred': [[0.48795979 0.57435613 0.09440186...</td>\n",
       "      <td>SimpleImputer_most_frequent</td>\n",
       "      <td>KNNImputer</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>[2893, 348]</td>\n",
       "      <td>[1, 348]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>SimpleImputer_most_frequent</td>\n",
       "      <td>KNNImputer</td>\n",
       "      <td>XGBoostRegressor</td>\n",
       "      <td>[2893, 348]</td>\n",
       "      <td>[1, 348]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>SimpleImputer_most_frequent</td>\n",
       "      <td>KNNImputer</td>\n",
       "      <td>XGBoostRegressor_tuned</td>\n",
       "      <td>[2893, 348]</td>\n",
       "      <td>[1, 348]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[3.467628240585327, 3.957735300064087, 3.52839...</td>\n",
       "      <td>[13.44585394859314, 14.944092273712158, 14.055...</td>\n",
       "      <td>[{'y_pred': [[-0.33021685  0.02510688  0.12306...</td>\n",
       "      <td>[{'y_pred': [[-0.13565774  0.22335736 -0.00412...</td>\n",
       "      <td>SimpleImputer_most_frequent</td>\n",
       "      <td>KNNImputer</td>\n",
       "      <td>TabNetRegressor_default</td>\n",
       "      <td>[2893, 348]</td>\n",
       "      <td>[1, 348]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[3.212691068649292, 3.3576810359954834, 3.2116...</td>\n",
       "      <td>[12.390780925750732, 13.0169358253479, 12.1248...</td>\n",
       "      <td>[{'y_pred': [[0.38124186 0.36439073 0.21427807...</td>\n",
       "      <td>[{'y_pred': [[0.57580098 0.56264122 0.08709138...</td>\n",
       "      <td>SimpleImputer_most_frequent</td>\n",
       "      <td>KNNImputer</td>\n",
       "      <td>TabNetRegressor_custom</td>\n",
       "      <td>[2893, 348]</td>\n",
       "      <td>[1, 348]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[3.6080546379089355, 3.3281357288360596, 3.487...</td>\n",
       "      <td>[0.15614581108093262, 0.14843535423278809, 0.1...</td>\n",
       "      <td>[{'y_pred': [[0.57530848 0.50261499 0.2766727 ...</td>\n",
       "      <td>[{'y_pred': [[0.7698676  0.70086548 0.14948601...</td>\n",
       "      <td>SimpleImputer_most_frequent</td>\n",
       "      <td>KNNImputer</td>\n",
       "      <td>PLSRegression_4_components</td>\n",
       "      <td>[2893, 348]</td>\n",
       "      <td>[1, 348]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None</td>\n",
       "      <td>0.068446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>[1.1119234898099377, 0.676686264997117, 0.6199...</td>\n",
       "      <td>[0.7284983137064507, 0.7023597272362397, 0.709...</td>\n",
       "      <td>[-0.12379654970384224, 0.23213669686583083, -0...</td>\n",
       "      <td>[0.02475189284221302, 0.28845770677144666, -0....</td>\n",
       "      <td>[0.3320458491424125, 0.5476551369606042, 0.016...</td>\n",
       "      <td>[1.111923487408025, 0.6766862634588122, 0.6199...</td>\n",
       "      <td>[0.7284983177320566, 0.7023597287538657, 0.709...</td>\n",
       "      <td>[-0.18280005836581759, 0.24192464970535776, -0...</td>\n",
       "      <td>[-0.026452269431272546, 0.29752774244051705, -...</td>\n",
       "      <td>[0.27766053298168986, 0.5501268350934375, 0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>None</td>\n",
       "      <td>0.016177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>MultiTaskElasticNet</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>[1.1801224687295142, 0.9504153428153488, 0.507...</td>\n",
       "      <td>[0.8841416741691975, 0.7868357222322307, 0.653...</td>\n",
       "      <td>[-0.19272375369361128, -0.07847477070736275, -...</td>\n",
       "      <td>[0.11229868297941448, 0.03189858449038585, -0....</td>\n",
       "      <td>[0.42176385593133126, 0.17864276958690972, -0....</td>\n",
       "      <td>[1.1801224777332378, 0.9504153411821713, 0.507...</td>\n",
       "      <td>[0.8841416790017589, 0.78683572655586, 0.65378...</td>\n",
       "      <td>[-0.2553462098327579, -0.06472745435881033, 0....</td>\n",
       "      <td>[0.06569104121430824, 0.0442389812184012, 0.01...</td>\n",
       "      <td>[0.2586375518883661, 0.21083052633240612, 0.14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>MultiTaskElasticNet_tuned</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>None</td>\n",
       "      <td>0.007943</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>MultiTaskLasso</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>[1.3936376654364884, 1.0405633863212842, 0.488...</td>\n",
       "      <td>[0.9912757529866387, 0.7845715674120901, 0.621...</td>\n",
       "      <td>[-0.4085188543166307, -0.18076940566326094, -0...</td>\n",
       "      <td>[-2.220446049250313e-16, 0.0, 0.0, 1.110223024...</td>\n",
       "      <td>[nan, nan, nan, nan]</td>\n",
       "      <td>[1.39363767778841, 1.040563386289657, 0.488669...</td>\n",
       "      <td>[0.9912757578191999, 0.7845715717907388, 0.621...</td>\n",
       "      <td>[-0.4824713618303562, -0.16571814171801091, 0....</td>\n",
       "      <td>[-0.05250374534439928, 0.012747008661847414, 0...</td>\n",
       "      <td>[-0.084866598168827, 0.1145532618513419, 0.272...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>MultiTaskLasso_tuned</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>None</td>\n",
       "      <td>32.180776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>[1.0004929827487936, 0.6583307726546211, 0.476...</td>\n",
       "      <td>[0.7462698406196145, 0.6893144678982224, 0.633...</td>\n",
       "      <td>[-0.011176193613992913, 0.2529654172194542, 0....</td>\n",
       "      <td>[0.29851275592520854, 0.40207199079436096, 0.0...</td>\n",
       "      <td>[0.5463813448253922, 0.6389458445773373, 0.230...</td>\n",
       "      <td>[1.0004929816978152, 0.6583307783476453, 0.476...</td>\n",
       "      <td>[0.7462698467726478, 0.6893144699477052, 0.633...</td>\n",
       "      <td>[-0.0642667148845999, 0.26248785832489097, 0.0...</td>\n",
       "      <td>[0.26168205835166036, 0.40969378155791614, 0.0...</td>\n",
       "      <td>[0.5116701022090266, 0.6455531281709198, 0.318...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>None</td>\n",
       "      <td>0.979116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>XGBoostRegressor</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>[0.9972977744677737, 0.5528588765184321, 0.498...</td>\n",
       "      <td>[0.754455924042236, 0.6345107831666245, 0.6429...</td>\n",
       "      <td>[-0.00794686706886405, 0.37264864819991916, -0...</td>\n",
       "      <td>[0.26511087910522013, 0.47604273516010087, -0....</td>\n",
       "      <td>[0.5217493590966826, 0.698490826142268, 0.2117...</td>\n",
       "      <td>[0.9972977727407778, 0.5528588798070297, 0.498...</td>\n",
       "      <td>[0.7544559347068825, 0.6345107852161074, 0.642...</td>\n",
       "      <td>[-0.060867835929641734, 0.3806454902291253, 0....</td>\n",
       "      <td>[0.22652645919993875, 0.4827216219837368, 0.03...</td>\n",
       "      <td>[0.4896643382156407, 0.7025785550998865, 0.273...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>None</td>\n",
       "      <td>2.150189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>XGBoostRegressor_tuned</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>[1.015361101896433, 0.6385197295978015, 0.4634...</td>\n",
       "      <td>[0.7345770328134464, 0.7145827532706496, 0.630...</td>\n",
       "      <td>[-0.0262030737472283, 0.2754458099021808, 0.03...</td>\n",
       "      <td>[0.22165364381976016, 0.3489773943943325, 0.05...</td>\n",
       "      <td>[0.48104477356404635, 0.590754815507922, 0.306...</td>\n",
       "      <td>[1.0153611005115688, 0.638519728895267, 0.4634...</td>\n",
       "      <td>[0.7345770389664799, 0.7145827553201324, 0.630...</td>\n",
       "      <td>[-0.08008256192789931, 0.2846817006774043, 0.0...</td>\n",
       "      <td>[0.1807875566442022, 0.3572759857952956, 0.122...</td>\n",
       "      <td>[0.4467819127944371, 0.5977575782775101, 0.368...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>None</td>\n",
       "      <td>12.833267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>TabNetRegressor_default</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>[1.0002425052752673, 0.852160951239341, 0.6231...</td>\n",
       "      <td>[0.7806465891501985, 0.8013238013969162, 0.679...</td>\n",
       "      <td>[-0.010923041555324087, 0.03301846562027699, -...</td>\n",
       "      <td>[0.3827598339020245, 0.13551578326538694, -0.2...</td>\n",
       "      <td>[0.6294661381345319, 0.47112118305582457, 0.06...</td>\n",
       "      <td>[1.0002425013137521, 0.8521609524638865, 0.623...</td>\n",
       "      <td>[0.780646592900706, 0.801323803446399, 0.67955...</td>\n",
       "      <td>[-0.06400026830239858, 0.045344574833686035, -...</td>\n",
       "      <td>[0.35035242511877496, 0.14653536835020642, -0....</td>\n",
       "      <td>[0.6085272367008092, 0.4852576960459974, 0.122...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>None</td>\n",
       "      <td>11.839218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>TabNetRegressor_custom</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>[0.9261003635990263, 0.7642873814427985, 0.611...</td>\n",
       "      <td>[0.7133815430335131, 0.7584149374303166, 0.671...</td>\n",
       "      <td>[0.06401078596697707, 0.13273216316732506, -0....</td>\n",
       "      <td>[0.2220996090267332, 0.26101411024552224, -0.2...</td>\n",
       "      <td>[0.5532913119709318, 0.580845291517589, 0.2370...</td>\n",
       "      <td>[0.9261003504544436, 0.7642873867924493, 0.611...</td>\n",
       "      <td>[0.7133815361285443, 0.758414935004866, 0.6711...</td>\n",
       "      <td>[0.01486787447618565, 0.1437872175696555, -0.1...</td>\n",
       "      <td>[0.18125694946016357, 0.2704339649804939, -0.1...</td>\n",
       "      <td>[0.5379584649934801, 0.5901319798405044, 0.273...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>None</td>\n",
       "      <td>0.049735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>PLSRegression_4_components</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>[1.081409082021159, 0.7876511651265149, 0.5694...</td>\n",
       "      <td>[0.7924841444198072, 0.7906801463917367, 0.721...</td>\n",
       "      <td>[-0.09295631069140176, 0.10622033184891166, -0...</td>\n",
       "      <td>[0.1855152043553444, 0.19584409010586212, -0.1...</td>\n",
       "      <td>[0.4328608768006912, 0.4454234137390073, -0.05...</td>\n",
       "      <td>[1.0814090864925796, 0.7876511645655125, 0.569...</td>\n",
       "      <td>[0.7924841505728405, 0.7906801478543428, 0.721...</td>\n",
       "      <td>[-0.15034059906622055, 0.11761333910345084, -0...</td>\n",
       "      <td>[0.14275170798456327, 0.2060946718780181, -0.1...</td>\n",
       "      <td>[0.38124619506043506, 0.4539854875773144, 0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>GatedAdditiveTreeEnsembleConfig_tab</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>DANetConfig_tab</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>TabTransformerConfig_tab</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>NoImputer</td>\n",
       "      <td>TabNetModelConfig_tab</td>\n",
       "      <td>(2881, 200)</td>\n",
       "      <td>(13, 200)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      imputation_time  \\\n",
       "0   [3.3089418411254883, 3.2607581615448, 3.286860...   \n",
       "1   [3.4814867973327637, 3.5424869060516357, 3.515...   \n",
       "2   [3.544888734817505, 3.5088696479797363, 3.4993...   \n",
       "3   [3.458850145339966, 3.4527065753936768, 3.4638...   \n",
       "4   [3.5399937629699707, 3.5302608013153076, 3.551...   \n",
       "5   [3.5142085552215576, 3.3101305961608887, 3.446...   \n",
       "6                                                  []   \n",
       "7                                                  []   \n",
       "8   [3.467628240585327, 3.957735300064087, 3.52839...   \n",
       "9   [3.212691068649292, 3.3576810359954834, 3.2116...   \n",
       "10  [3.6080546379089355, 3.3281357288360596, 3.487...   \n",
       "11                                               None   \n",
       "12                                               None   \n",
       "13                                               None   \n",
       "14                                               None   \n",
       "15                                               None   \n",
       "16                                               None   \n",
       "17                                               None   \n",
       "18                                               None   \n",
       "19                                               None   \n",
       "20                                               None   \n",
       "21                                               None   \n",
       "22                                               None   \n",
       "23                                               None   \n",
       "24                                               None   \n",
       "25                                               None   \n",
       "\n",
       "                                         fitting_time  \\\n",
       "0   [0.11002111434936523, 0.24651741981506348, 0.1...   \n",
       "1   [0.08670425415039062, 0.08908843994140625, 0.0...   \n",
       "2   [1.513960361480713, 1.5192315578460693, 1.5161...   \n",
       "3   [0.0770413875579834, 0.07488107681274414, 0.07...   \n",
       "4   [1.4914090633392334, 1.5125789642333984, 1.522...   \n",
       "5   [52.41137504577637, 52.029266357421875, 51.895...   \n",
       "6                                                  []   \n",
       "7                                                  []   \n",
       "8   [13.44585394859314, 14.944092273712158, 14.055...   \n",
       "9   [12.390780925750732, 13.0169358253479, 12.1248...   \n",
       "10  [0.15614581108093262, 0.14843535423278809, 0.1...   \n",
       "11                                           0.068446   \n",
       "12                                           0.016177   \n",
       "13                                               None   \n",
       "14                                           0.007943   \n",
       "15                                               None   \n",
       "16                                          32.180776   \n",
       "17                                           0.979116   \n",
       "18                                           2.150189   \n",
       "19                                          12.833267   \n",
       "20                                          11.839218   \n",
       "21                                           0.049735   \n",
       "22                                               None   \n",
       "23                                               None   \n",
       "24                                               None   \n",
       "25                                               None   \n",
       "\n",
       "                                          results_adj  \\\n",
       "0   [{'y_pred': [[0.56338451 1.36697629 0.61842186...   \n",
       "1   [{'y_pred': [[0.10086018 0.10725904 0.05499171...   \n",
       "2   [{'y_pred': [[0.43508893 1.25141221 0.45281623...   \n",
       "3   [{'y_pred': [[-1.14021119e-10 -7.73912862e-10 ...   \n",
       "4   [{'y_pred': [[0.50635131 1.33052966 0.55415854...   \n",
       "5   [{'y_pred': [[0.29340068 0.37610565 0.22158855...   \n",
       "6                                                  []   \n",
       "7                                                  []   \n",
       "8   [{'y_pred': [[-0.33021685  0.02510688  0.12306...   \n",
       "9   [{'y_pred': [[0.38124186 0.36439073 0.21427807...   \n",
       "10  [{'y_pred': [[0.57530848 0.50261499 0.2766727 ...   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                               None   \n",
       "14                                                NaN   \n",
       "15                                               None   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18                                                NaN   \n",
       "19                                                NaN   \n",
       "20                                                NaN   \n",
       "21                                                NaN   \n",
       "22                                               None   \n",
       "23                                               None   \n",
       "24                                               None   \n",
       "25                                               None   \n",
       "\n",
       "                                          results_org  \\\n",
       "0   [{'y_pred': [[0.75794363 1.56522677 0.49123517...   \n",
       "1   [{'y_pred': [[ 0.29541929  0.30550953 -0.07219...   \n",
       "2   [{'y_pred': [[0.62964805 1.4496627  0.32562954...   \n",
       "3   [{'y_pred': [[ 0.19455912  0.19825049 -0.12718...   \n",
       "4   [{'y_pred': [[0.70091043 1.52878015 0.42697185...   \n",
       "5   [{'y_pred': [[0.48795979 0.57435613 0.09440186...   \n",
       "6                                                  []   \n",
       "7                                                  []   \n",
       "8   [{'y_pred': [[-0.13565774  0.22335736 -0.00412...   \n",
       "9   [{'y_pred': [[0.57580098 0.56264122 0.08709138...   \n",
       "10  [{'y_pred': [[0.7698676  0.70086548 0.14948601...   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                               None   \n",
       "14                                                NaN   \n",
       "15                                               None   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "18                                                NaN   \n",
       "19                                                NaN   \n",
       "20                                                NaN   \n",
       "21                                                NaN   \n",
       "22                                               None   \n",
       "23                                               None   \n",
       "24                                               None   \n",
       "25                                               None   \n",
       "\n",
       "         params.ordinal_imputer params.continuous_imputer  \\\n",
       "0   SimpleImputer_most_frequent                KNNImputer   \n",
       "1   SimpleImputer_most_frequent                KNNImputer   \n",
       "2   SimpleImputer_most_frequent                KNNImputer   \n",
       "3   SimpleImputer_most_frequent                KNNImputer   \n",
       "4   SimpleImputer_most_frequent                KNNImputer   \n",
       "5   SimpleImputer_most_frequent                KNNImputer   \n",
       "6   SimpleImputer_most_frequent                KNNImputer   \n",
       "7   SimpleImputer_most_frequent                KNNImputer   \n",
       "8   SimpleImputer_most_frequent                KNNImputer   \n",
       "9   SimpleImputer_most_frequent                KNNImputer   \n",
       "10  SimpleImputer_most_frequent                KNNImputer   \n",
       "11                    NoImputer                 NoImputer   \n",
       "12                    NoImputer                 NoImputer   \n",
       "13                    NoImputer                 NoImputer   \n",
       "14                    NoImputer                 NoImputer   \n",
       "15                    NoImputer                 NoImputer   \n",
       "16                    NoImputer                 NoImputer   \n",
       "17                    NoImputer                 NoImputer   \n",
       "18                    NoImputer                 NoImputer   \n",
       "19                    NoImputer                 NoImputer   \n",
       "20                    NoImputer                 NoImputer   \n",
       "21                    NoImputer                 NoImputer   \n",
       "22                    NoImputer                 NoImputer   \n",
       "23                    NoImputer                 NoImputer   \n",
       "24                    NoImputer                 NoImputer   \n",
       "25                    NoImputer                 NoImputer   \n",
       "\n",
       "                           params.model params.train_shape params.test_shape  \\\n",
       "0                      LinearRegression        [2893, 348]          [1, 348]   \n",
       "1                   MultiTaskElasticNet        [2893, 348]          [1, 348]   \n",
       "2             MultiTaskElasticNet_tuned        [2893, 348]          [1, 348]   \n",
       "3                        MultiTaskLasso        [2893, 348]          [1, 348]   \n",
       "4                  MultiTaskLasso_tuned        [2893, 348]          [1, 348]   \n",
       "5                 RandomForestRegressor        [2893, 348]          [1, 348]   \n",
       "6                      XGBoostRegressor        [2893, 348]          [1, 348]   \n",
       "7                XGBoostRegressor_tuned        [2893, 348]          [1, 348]   \n",
       "8               TabNetRegressor_default        [2893, 348]          [1, 348]   \n",
       "9                TabNetRegressor_custom        [2893, 348]          [1, 348]   \n",
       "10           PLSRegression_4_components        [2893, 348]          [1, 348]   \n",
       "11                     LinearRegression        (2881, 200)         (13, 200)   \n",
       "12                  MultiTaskElasticNet        (2881, 200)         (13, 200)   \n",
       "13            MultiTaskElasticNet_tuned        (2881, 200)         (13, 200)   \n",
       "14                       MultiTaskLasso        (2881, 200)         (13, 200)   \n",
       "15                 MultiTaskLasso_tuned        (2881, 200)         (13, 200)   \n",
       "16                RandomForestRegressor        (2881, 200)         (13, 200)   \n",
       "17                     XGBoostRegressor        (2881, 200)         (13, 200)   \n",
       "18               XGBoostRegressor_tuned        (2881, 200)         (13, 200)   \n",
       "19              TabNetRegressor_default        (2881, 200)         (13, 200)   \n",
       "20               TabNetRegressor_custom        (2881, 200)         (13, 200)   \n",
       "21           PLSRegression_4_components        (2881, 200)         (13, 200)   \n",
       "22  GatedAdditiveTreeEnsembleConfig_tab        (2881, 200)         (13, 200)   \n",
       "23                      DANetConfig_tab        (2881, 200)         (13, 200)   \n",
       "24             TabTransformerConfig_tab        (2881, 200)         (13, 200)   \n",
       "25                TabNetModelConfig_tab        (2881, 200)         (13, 200)   \n",
       "\n",
       "                                results_adj.mse_score  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11  [1.1119234898099377, 0.676686264997117, 0.6199...   \n",
       "12  [1.1801224687295142, 0.9504153428153488, 0.507...   \n",
       "13                                                NaN   \n",
       "14  [1.3936376654364884, 1.0405633863212842, 0.488...   \n",
       "15                                                NaN   \n",
       "16  [1.0004929827487936, 0.6583307726546211, 0.476...   \n",
       "17  [0.9972977744677737, 0.5528588765184321, 0.498...   \n",
       "18  [1.015361101896433, 0.6385197295978015, 0.4634...   \n",
       "19  [1.0002425052752673, 0.852160951239341, 0.6231...   \n",
       "20  [0.9261003635990263, 0.7642873814427985, 0.611...   \n",
       "21  [1.081409082021159, 0.7876511651265149, 0.5694...   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24                                                NaN   \n",
       "25                                                NaN   \n",
       "\n",
       "                                results_adj.mae_score  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11  [0.7284983137064507, 0.7023597272362397, 0.709...   \n",
       "12  [0.8841416741691975, 0.7868357222322307, 0.653...   \n",
       "13                                                NaN   \n",
       "14  [0.9912757529866387, 0.7845715674120901, 0.621...   \n",
       "15                                                NaN   \n",
       "16  [0.7462698406196145, 0.6893144678982224, 0.633...   \n",
       "17  [0.754455924042236, 0.6345107831666245, 0.6429...   \n",
       "18  [0.7345770328134464, 0.7145827532706496, 0.630...   \n",
       "19  [0.7806465891501985, 0.8013238013969162, 0.679...   \n",
       "20  [0.7133815430335131, 0.7584149374303166, 0.671...   \n",
       "21  [0.7924841444198072, 0.7906801463917367, 0.721...   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24                                                NaN   \n",
       "25                                                NaN   \n",
       "\n",
       "                                       results_adj.r2  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11  [-0.12379654970384224, 0.23213669686583083, -0...   \n",
       "12  [-0.19272375369361128, -0.07847477070736275, -...   \n",
       "13                                                NaN   \n",
       "14  [-0.4085188543166307, -0.18076940566326094, -0...   \n",
       "15                                                NaN   \n",
       "16  [-0.011176193613992913, 0.2529654172194542, 0....   \n",
       "17  [-0.00794686706886405, 0.37264864819991916, -0...   \n",
       "18  [-0.0262030737472283, 0.2754458099021808, 0.03...   \n",
       "19  [-0.010923041555324087, 0.03301846562027699, -...   \n",
       "20  [0.06401078596697707, 0.13273216316732506, -0....   \n",
       "21  [-0.09295631069140176, 0.10622033184891166, -0...   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24                                                NaN   \n",
       "25                                                NaN   \n",
       "\n",
       "                       results_adj.explained_variance  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11  [0.02475189284221302, 0.28845770677144666, -0....   \n",
       "12  [0.11229868297941448, 0.03189858449038585, -0....   \n",
       "13                                                NaN   \n",
       "14  [-2.220446049250313e-16, 0.0, 0.0, 1.110223024...   \n",
       "15                                                NaN   \n",
       "16  [0.29851275592520854, 0.40207199079436096, 0.0...   \n",
       "17  [0.26511087910522013, 0.47604273516010087, -0....   \n",
       "18  [0.22165364381976016, 0.3489773943943325, 0.05...   \n",
       "19  [0.3827598339020245, 0.13551578326538694, -0.2...   \n",
       "20  [0.2220996090267332, 0.26101411024552224, -0.2...   \n",
       "21  [0.1855152043553444, 0.19584409010586212, -0.1...   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24                                                NaN   \n",
       "25                                                NaN   \n",
       "\n",
       "                                     results_adj.corr  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11  [0.3320458491424125, 0.5476551369606042, 0.016...   \n",
       "12  [0.42176385593133126, 0.17864276958690972, -0....   \n",
       "13                                                NaN   \n",
       "14                               [nan, nan, nan, nan]   \n",
       "15                                                NaN   \n",
       "16  [0.5463813448253922, 0.6389458445773373, 0.230...   \n",
       "17  [0.5217493590966826, 0.698490826142268, 0.2117...   \n",
       "18  [0.48104477356404635, 0.590754815507922, 0.306...   \n",
       "19  [0.6294661381345319, 0.47112118305582457, 0.06...   \n",
       "20  [0.5532913119709318, 0.580845291517589, 0.2370...   \n",
       "21  [0.4328608768006912, 0.4454234137390073, -0.05...   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24                                                NaN   \n",
       "25                                                NaN   \n",
       "\n",
       "                                results_org.mse_score  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11  [1.111923487408025, 0.6766862634588122, 0.6199...   \n",
       "12  [1.1801224777332378, 0.9504153411821713, 0.507...   \n",
       "13                                                NaN   \n",
       "14  [1.39363767778841, 1.040563386289657, 0.488669...   \n",
       "15                                                NaN   \n",
       "16  [1.0004929816978152, 0.6583307783476453, 0.476...   \n",
       "17  [0.9972977727407778, 0.5528588798070297, 0.498...   \n",
       "18  [1.0153611005115688, 0.638519728895267, 0.4634...   \n",
       "19  [1.0002425013137521, 0.8521609524638865, 0.623...   \n",
       "20  [0.9261003504544436, 0.7642873867924493, 0.611...   \n",
       "21  [1.0814090864925796, 0.7876511645655125, 0.569...   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24                                                NaN   \n",
       "25                                                NaN   \n",
       "\n",
       "                                results_org.mae_score  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11  [0.7284983177320566, 0.7023597287538657, 0.709...   \n",
       "12  [0.8841416790017589, 0.78683572655586, 0.65378...   \n",
       "13                                                NaN   \n",
       "14  [0.9912757578191999, 0.7845715717907388, 0.621...   \n",
       "15                                                NaN   \n",
       "16  [0.7462698467726478, 0.6893144699477052, 0.633...   \n",
       "17  [0.7544559347068825, 0.6345107852161074, 0.642...   \n",
       "18  [0.7345770389664799, 0.7145827553201324, 0.630...   \n",
       "19  [0.780646592900706, 0.801323803446399, 0.67955...   \n",
       "20  [0.7133815361285443, 0.758414935004866, 0.6711...   \n",
       "21  [0.7924841505728405, 0.7906801478543428, 0.721...   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24                                                NaN   \n",
       "25                                                NaN   \n",
       "\n",
       "                                       results_org.r2  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11  [-0.18280005836581759, 0.24192464970535776, -0...   \n",
       "12  [-0.2553462098327579, -0.06472745435881033, 0....   \n",
       "13                                                NaN   \n",
       "14  [-0.4824713618303562, -0.16571814171801091, 0....   \n",
       "15                                                NaN   \n",
       "16  [-0.0642667148845999, 0.26248785832489097, 0.0...   \n",
       "17  [-0.060867835929641734, 0.3806454902291253, 0....   \n",
       "18  [-0.08008256192789931, 0.2846817006774043, 0.0...   \n",
       "19  [-0.06400026830239858, 0.045344574833686035, -...   \n",
       "20  [0.01486787447618565, 0.1437872175696555, -0.1...   \n",
       "21  [-0.15034059906622055, 0.11761333910345084, -0...   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24                                                NaN   \n",
       "25                                                NaN   \n",
       "\n",
       "                       results_org.explained_variance  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11  [-0.026452269431272546, 0.29752774244051705, -...   \n",
       "12  [0.06569104121430824, 0.0442389812184012, 0.01...   \n",
       "13                                                NaN   \n",
       "14  [-0.05250374534439928, 0.012747008661847414, 0...   \n",
       "15                                                NaN   \n",
       "16  [0.26168205835166036, 0.40969378155791614, 0.0...   \n",
       "17  [0.22652645919993875, 0.4827216219837368, 0.03...   \n",
       "18  [0.1807875566442022, 0.3572759857952956, 0.122...   \n",
       "19  [0.35035242511877496, 0.14653536835020642, -0....   \n",
       "20  [0.18125694946016357, 0.2704339649804939, -0.1...   \n",
       "21  [0.14275170798456327, 0.2060946718780181, -0.1...   \n",
       "22                                                NaN   \n",
       "23                                                NaN   \n",
       "24                                                NaN   \n",
       "25                                                NaN   \n",
       "\n",
       "                                     results_org.corr  \n",
       "0                                                 NaN  \n",
       "1                                                 NaN  \n",
       "2                                                 NaN  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "5                                                 NaN  \n",
       "6                                                 NaN  \n",
       "7                                                 NaN  \n",
       "8                                                 NaN  \n",
       "9                                                 NaN  \n",
       "10                                                NaN  \n",
       "11  [0.27766053298168986, 0.5501268350934375, 0.07...  \n",
       "12  [0.2586375518883661, 0.21083052633240612, 0.14...  \n",
       "13                                                NaN  \n",
       "14  [-0.084866598168827, 0.1145532618513419, 0.272...  \n",
       "15                                                NaN  \n",
       "16  [0.5116701022090266, 0.6455531281709198, 0.318...  \n",
       "17  [0.4896643382156407, 0.7025785550998865, 0.273...  \n",
       "18  [0.4467819127944371, 0.5977575782775101, 0.368...  \n",
       "19  [0.6085272367008092, 0.4852576960459974, 0.122...  \n",
       "20  [0.5379584649934801, 0.5901319798405044, 0.273...  \n",
       "21  [0.38124619506043506, 0.4539854875773144, 0.03...  \n",
       "22                                                NaN  \n",
       "23                                                NaN  \n",
       "24                                                NaN  \n",
       "25                                                NaN  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_nona = pd.json_normalize(dict_results_loo_nona)\n",
    "df_results_nona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metric_table(\n",
    "    results_list,\n",
    "    targets,\n",
    "    metric_name,\n",
    "    source=\"Adjusted\",\n",
    "    float_format=\"%.3f\",\n",
    "    csv_filename=None,\n",
    "    sort_order=\"ascending\"  # or \"descending\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a LaTeX table for a single metric across targets, models, and imputers.\n",
    "    Optionally export the same table as CSV and sort by mean performance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_list : list of dict\n",
    "        List of experiment results.\n",
    "    targets : list of str\n",
    "        Target names (e.g., ['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN']).\n",
    "    metric_name : str\n",
    "        Metric to extract (e.g., 'mae_score').\n",
    "    source : str\n",
    "        'Adjusted' or 'Original'.\n",
    "    float_format : str\n",
    "        Format for floats (e.g., '%.3f').\n",
    "    csv_filename : str or None\n",
    "        If provided, saves the table to CSV.\n",
    "    sort_order : str\n",
    "        'ascending' or 'descending' for sorting by mean.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        LaTeX-formatted table string.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    version_key = \"results_adj\" if source.lower() == \"adjusted\" else \"results_org\"\n",
    "\n",
    "    for res in results_list:\n",
    "        result_block = res.get(version_key)\n",
    "        if result_block is None:\n",
    "            continue\n",
    "\n",
    "        metric_values = result_block.get(metric_name)\n",
    "        if metric_values is None:\n",
    "            continue\n",
    "\n",
    "        if len(metric_values) != len(targets):\n",
    "            continue\n",
    "\n",
    "        ordinal_imputer = res[\"params\"].get(\"ordinal_imputer\")\n",
    "        model = res[\"params\"].get(\"model\")\n",
    "\n",
    "        values = np.array(metric_values, dtype=np.float64)\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "\n",
    "        row = {\n",
    "            \"Ordinal Imputer\": ordinal_imputer,\n",
    "            \"Model\": model,\n",
    "            \"Mean\": mean_val,  # for sorting\n",
    "            \"Mean ± SD\": f\"{mean_val:.3f} ± {std_val:.3f}\",\n",
    "        }\n",
    "        row.update({target: val for target, val in zip(targets, values)})\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Reorder columns for display\n",
    "    display_cols = [\"Ordinal Imputer\", \"Model\"] + targets + [\"Mean ± SD\"]\n",
    "    df = df.sort_values(by=\"Mean\", ascending=(sort_order == \"ascending\"))\n",
    "    df = df[display_cols]\n",
    "\n",
    "    # Save CSV\n",
    "    if csv_filename:\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # LaTeX output\n",
    "    latex_table = df.to_latex(\n",
    "        index=False,\n",
    "        escape=False,\n",
    "        float_format=float_format,\n",
    "        caption=f\"{metric_name.replace('_', ' ').upper()} across targets\",\n",
    "        label=f\"tab:{metric_name}\",\n",
    "        longtable=False\n",
    "    )\n",
    "\n",
    "    return df, latex_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m latex_df, latex_mae \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_metric_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_dict_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mADNI_MEM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mADNI_EF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mADNI_VS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mADNI_LAN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcorr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdjusted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../tables/2_training_train_test_corr_adjusted_sorted.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescending\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(latex_mae)\n",
      "Cell \u001b[0;32mIn[36], line 44\u001b[0m, in \u001b[0;36mgenerate_metric_table\u001b[0;34m(results_list, targets, metric_name, source, float_format, csv_filename, sort_order)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m metric_values \u001b[38;5;241m=\u001b[39m \u001b[43mresult_block\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(metric_name)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "latex_df, latex_mae = generate_metric_table(\n",
    "    results_list=all_dict_results,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='corr',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/2_training_train_test_corr_adjusted_sorted.csv\",\n",
    "    sort_order=\"descending\"\n",
    ")\n",
    "print(latex_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_df, latex_mae = generate_metric_table(\n",
    "    results_list=all_dict_results,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='r2',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/2_training_train_test_r2_adjusted_sorted.csv\",\n",
    "    sort_order=\"descending\"\n",
    ")\n",
    "print(latex_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_df, latex_mae = generate_metric_table(\n",
    "    results_list=all_dict_results,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='mse_score',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/2_training_train_test_mse_adjusted_sorted.csv\",\n",
    "    sort_order=\"ascending\"\n",
    ")\n",
    "print(latex_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{MAE SCORE across targets}\n",
      "\\label{tab:mae_score}\n",
      "\\begin{tabular}{llrrrrl}\n",
      "\\toprule\n",
      "Ordinal Imputer & Model & ADNI_MEM & ADNI_EF & ADNI_VS & ADNI_LAN & Mean ± SD \\\\\n",
      "\\midrule\n",
      "SimpleImputer_most_frequent & MultiTaskLasso_tuned & 0.622 & 0.551 & 0.578 & 0.595 & 0.586 ± 0.026 \\\\\n",
      "SimpleImputer_most_frequent & LinearRegression & 0.640 & 0.551 & 0.565 & 0.600 & 0.589 ± 0.034 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskElasticNet_tuned & 0.630 & 0.554 & 0.580 & 0.600 & 0.591 ± 0.028 \\\\\n",
      "SimpleImputer_most_frequent & TabNetRegressor_default & 0.691 & 0.608 & 0.536 & 0.595 & 0.607 ± 0.055 \\\\\n",
      "SimpleImputer_most_frequent & RandomForestRegressor & 0.669 & 0.640 & 0.608 & 0.648 & 0.641 ± 0.022 \\\\\n",
      "NoImputer & XGBoostRegressor & 0.754 & 0.635 & 0.643 & 0.644 & 0.669 ± 0.049 \\\\\n",
      "NoImputer & XGBoostRegressor_tuned & 0.735 & 0.715 & 0.630 & 0.613 & 0.673 ± 0.052 \\\\\n",
      "NoImputer & RandomForestRegressor & 0.755 & 0.684 & 0.635 & 0.632 & 0.677 ± 0.050 \\\\\n",
      "NoImputer & MultiTaskLasso_tuned & 0.724 & 0.705 & 0.713 & 0.706 & 0.712 ± 0.008 \\\\\n",
      "NoImputer & MultiTaskElasticNet_tuned & 0.727 & 0.703 & 0.714 & 0.708 & 0.713 ± 0.009 \\\\\n",
      "NoImputer & LinearRegression & 0.728 & 0.702 & 0.710 & 0.712 & 0.713 ± 0.010 \\\\\n",
      "NoImputer & TabNetRegressor_custom & 0.713 & 0.758 & 0.671 & 0.724 & 0.717 ± 0.031 \\\\\n",
      "NoImputer & TabNetRegressor_default & 0.781 & 0.801 & 0.680 & 0.672 & 0.733 ± 0.058 \\\\\n",
      "SimpleImputer_most_frequent & PLSRegression_4_components & 0.805 & 0.738 & 0.689 & 0.764 & 0.749 ± 0.042 \\\\\n",
      "NoImputer & PLSRegression_4_components & 0.792 & 0.791 & 0.722 & 0.735 & 0.760 ± 0.032 \\\\\n",
      "NoImputer & MultiTaskElasticNet & 0.884 & 0.787 & 0.654 & 0.827 & 0.788 ± 0.085 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskElasticNet & 0.884 & 0.787 & 0.654 & 0.827 & 0.788 ± 0.085 \\\\\n",
      "SimpleImputer_most_frequent & TabNetRegressor_custom & 0.765 & 0.969 & 0.765 & 0.763 & 0.815 ± 0.089 \\\\\n",
      "NoImputer & MultiTaskLasso & 0.991 & 0.785 & 0.621 & 0.934 & 0.833 ± 0.144 \\\\\n",
      "SimpleImputer_most_frequent & MultiTaskLasso & 0.991 & 0.785 & 0.621 & 0.934 & 0.833 ± 0.144 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_df, latex_mae = generate_metric_table(\n",
    "    results_list=all_dict_results,\n",
    "    targets=['ADNI_MEM', 'ADNI_EF', 'ADNI_VS', 'ADNI_LAN'],\n",
    "    metric_name='mae_score',\n",
    "    source=\"Adjusted\",\n",
    "    csv_filename=\"../tables/2_training_train_test_mae_adjusted_sorted.csv\",\n",
    "    sort_order=\"ascending\"\n",
    ")\n",
    "print(latex_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model\n",
       "MultiTaskLasso_tuned          2\n",
       "LinearRegression              2\n",
       "MultiTaskElasticNet_tuned     2\n",
       "TabNetRegressor_default       2\n",
       "RandomForestRegressor         2\n",
       "PLSRegression_4_components    2\n",
       "TabNetRegressor_custom        2\n",
       "MultiTaskElasticNet           2\n",
       "MultiTaskLasso                2\n",
       "XGBoostRegressor_tuned        1\n",
       "XGBoostRegressor              1\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "latex_df.Model.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
